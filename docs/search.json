[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression",
    "section": "",
    "text": "Overview\nThese course notes introduce regression models and accompanying assumptions. In addition, they cover the theory of least squares to understand the statistical inference procedures behind linear models. Extensive applications of modeling in R will be demonstrated, with a goal of being able to understand strengths and limitations of models. These will be applied to predict and perform inference, while developing sensitivity to important model assumptions.\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/regression.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "part1-slr.html",
    "href": "part1-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "This section contains notes from Part 1: Simple Linear Regression.",
    "crumbs": [
      "Simple linear regression"
    ]
  },
  {
    "objectID": "notes-slr.html#regression-overview",
    "href": "notes-slr.html#regression-overview",
    "title": "1  Simple linear regression",
    "section": "",
    "text": "Goal is to determine if and how one variable is related to a set of other variables.\n\nVariables\n\nResponse variable, denoted \\(Y\\), represents an outcome whose variation is being studied.\nExplanatory variable, denoted \\(X\\), represents the causes (i.e. potential reasons for variation).\n\n\n\nTwo types of relationships\n\nFunctional (deterministic) → There is an exact relation between two variables (have the form \\(y = ax+ b\\)).\nStatistical (probabilistic) → There is not an exact relation because there are other variables that affect the relationship (have the form \\(y = ax + b + \\epsilon\\)).\n\n\n\n\n\nStatistical models quantify the relationship between a response variable (i.e. a random variable) and explanatory variables, which are usually assumed to be deterministic (i.e. known exactly).\n\nElements of a statistical regression model\n\n\nIn general, observations do not fall directly on the curve of a relationship.\n\n\\(Y \\mid X\\) has a probability distribution.\n\\(E(Y \\mid X)\\) varies deterministically with \\(X\\).\n\n\n\n\nSo the statistical model is:\n\\[\n\\begin{align*}\n  Y &= E(Y \\mid X) + \\epsilon \\\\\n    &= f(X) + \\epsilon, \\hspace{20pt} \\text{where $\\epsilon$ has some distribution}\n\\end{align*}\n\\]\n\n\n\n\nTwo components of a statistical model:\n\n\\(f(X) = E(Y \\mid X)\\) → Defines relationship between \\(Y\\) and \\(X\\); explains the average behavior of the response.\n\\(\\epsilon\\) → An element of randomness (i.e. error). This contains the variation that \\(f(X)\\) cannot explain and/or that is of no interest.\n\n\nThis means \\(f(X) = E(Y \\mid X)\\) will be the same for all samples with the same \\(X\\) values. The only thing that changes is the random error \\(\\epsilon\\) and as a result \\(Y\\). Example \\(Y = 3 + 1X + \\epsilon\\):\n\n\n\n\n\nSelection of predictor variables (how to decide which ones?).\n\nUse of outside information, historical knowledge, and/or experience.\nExploratory data analysis.\nVariable selection techniques → Find a subset of important variables (i.e. practical and easy to find).\n\n\n\nFunctional form of the regression relation (what is form of \\(f(X)\\)?).\n\n&lt; based on same info as (1) &gt;\nIf there is an abundance of data, maybe start with more complex models and then simplify.\n\n\n\nScope of model (when is the model useful?).\n\nWhen the model best predicts or describes the relationship between response and predictor variables.\n\n\n\n\n\nDetermining whether an \\(X\\) “affects” \\(Y\\) or not.\nEstimation of impact of a given \\(X\\) on the \\(Y\\).\nEstimation of the mean of \\(Y\\) for a given \\(X\\) value.\nPrediction of a single value of \\(Y\\) for a given \\(X\\) value.\n\n\n\n\n1.1.1 Simple linear regression model (SLR)\nGoal of SLR\n\nInvestigate the relationship between \\(Y\\) and a single numeric independent variable \\(X\\), assuming that, in the population, the mean of \\(Y\\) is linearly related to the value of \\(X\\).\nPopulation relationship → \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).\nSample relationship → \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nData structure\n\nBoth \\(X\\) and \\(Y\\) on a random sample of \\(n\\) individuals are collected from the population of interest.\nThe resulting data has the form \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nHolds for all \\(n\\) values in the random sample of \\(n\\) pairs of values, \\((X_i, Y_i), \\, i = 1, \\ldots, n\\).\n\\(Y_i\\) → Dependent (or response) variable value. These are independent, but not identically distributed.\n\\(X_i\\) → Independent (or predictor) variable value. These are not random variables, rather known constants.\n\\(\\epsilon_i\\) → Random error term, assumed to have mean zero and variance \\(\\sigma^2\\). \\(\\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = \\mathrm{Corr}(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j : i \\ne j\\). Often, the \\(\\epsilon_i\\) are assumed to be independent and identically distributed \\((iid)\\), i.e. all have the same distribution with the same mean and variance.\n\\(\\beta_0\\) and \\(\\beta_1\\) → Fixed but unknown regression parameters that need to be estimated.\n\\(\\sigma^2\\) → Another parameter that needs estimated, but it is technically not a “regression” parameter since it does not determine the relationship between \\(Y\\) and \\(X\\) (i.e. it only deals with randomness).\nNote that \\(Y_i\\) and \\(\\epsilon_i\\) are random variables and therefore have distributions. Thus, discussing their mean and variances are appropriate.\nAlternate (equivalent) version of regression model\n\n\n\nResults\nDerivation\n\n\n\nModel statement\n\nCan use the deviation \\(X_i - \\bar{X}\\) as the predictor instead of \\(X_i\\). This leads to:\n\n\\[\nY_i = \\beta_0^* + \\beta_1 (X_i - \\bar{X}) + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\beta_0^* = \\beta_0 + \\beta_1 \\bar{X}\n\\]\n\nThis model can be useful in some derivations.\n\nLater results (jumping ahead)\n\n\nEstimated coefficients\n\nLSE for \\(\\beta_1\\) is the same as before.\nNow the LSE for \\(\\beta_0^*\\) can be found using \\(\\hat{\\beta}_0\\) :\n\n\n\n\\[\n\\hat{\\beta}_0^* = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X} = (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 \\bar{X} = \\bar{Y}\n\\]\n\nEstimated regression function\n\n\\[\n\\hat{Y} = \\bar{Y} - \\hat{\\beta}_1 (X - \\bar{X})\n\\]\n\n\nProperty of all regression functions\n\nUsing this alternative model, it is easy to see that all regression functions pass through the point \\((\\bar{X}, \\bar{Y})\\).\n\n\n\n\n\n\n\n\n\nSome implications of above\n\n\nResults\nDerivation\n\n\n\n\nMean of \\(Y_i\\) for given \\(X_i\\) → \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\)\nVariance of \\(Y_i\\) for given \\(X_i\\) → \\(V(Y_i) = \\sigma^2\\)\n\n\n\n\n\n\n\nInterpretation of regression parameters \\((\\beta_0, \\beta_1)\\)\n\n\n\\(\\beta_0\\)\n\n\\(Y\\)-intercept of the regression line and gives \\(Y\\)’s mean when \\(X = 0\\) → \\(E(Y \\mid X = 0) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\)\n\nOnly makes sense to interpret when \\(X=0\\) is within the scope of the model. So, ask two questions when deciding whether or not to interpret:\n\nDoes it make sense to interpret in context?\nDo we have data in the proximity of zero?\n\n\nIf answer “no” to either of these, then no need to interpret the intercept.\n\n\n\n\\(\\beta_1\\)\n\nSlope of the regression line and indicates the change in \\(Y\\)’s mean when \\(X\\) increases by one unit → \\(E(Y \\mid X = x^* + 1) - E(Y \\mid X = x^*) = [\\beta_0 + \\beta_1 (x^* + 1)] - [\\beta_0 + \\beta_1 x^*] = \\beta_1\\)\nDetermines whether a relationship exists between \\(Y\\) and \\(X\\).\nNote that regression does not substantiate or prove a cause-effect relationship. Rather it gives evidence that \\(Y\\) and \\(X\\) are related (but not that \\(X\\) “causes” the value of \\(Y\\)).\n\n\n\nModel scope (and the dangers of extrapolation)\n\nThe structural form of the relationship may not be the same as you move away from the observed predictor space. Therefore, using \\(\\hat{Y}\\) for estimation / prediction at \\(X\\) values outside of model scope would be extrapolation and results are not reliable.\nShould collect data from the entire region of predictor values of research interest.\n\n\nEstimators\n\n\nPurpose\n\nEstimators help us make inferences about population parameters.\nEach estimator corresponds to a population parameter (e.g. \\(\\hat{\\beta}_1 \\rightarrow \\beta_1\\)).\n\n\n\nEstimator / estimate distinction\n\nEstimators are based on formulas (e.g. \\(\\bar{X} = \\frac{1}{n} \\sum X_i\\)).\nEstimates are values computed from estimators.\n\n\n\nGood estimators\n\nBias of an estimator is the difference between the estimator’s expected value and the true value of the parameter being estimated:\n\nIf the bias equals zero, the estimator is said to be unbiased. \\[\\text{Bias} = E(\\hat{\\theta}) - \\theta\\]\n\ne.g. \\(\\bar{X}\\) and \\(\\mu\\) → \\(E(\\bar{X}) \\overset{\\surd} = \\mu\\).\nAnother way to think about it → \\(E(\\bar{X})\\) will be the center of the sampling distribution of \\(\\bar{X}\\) (which will be approximately normal because of the central limit theorem), and we want this to equal \\(\\mu\\).\n\n\n\nThe standard deviation of an estimator is referred to as the standard error of said estimator.\n\nIdeally, estimators will have relatively small standard deviations, in which case they are said to be efficient.\n\n\n\n\n\n1.1.2 Estimation of the regression function\nSetup\n\nFor each point we have an observed value \\(Y_i\\), a fitted value \\(\\hat{Y}_i\\) and a residual \\(\\hat{\\epsilon}_i\\).\nFitted regression function → \\(\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\), where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_0\\) are estimators of \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\nGoal\n\nGoal is to estimate the two “regression” parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nThere are several methods to do this.\n\n1.1.3 Method of least squares\nOverview\n\nThe method of least squares is one way to find “good” estimators of the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nFor each observation \\((X_i, Y_i)\\), this method considers the model error term, which is the deviation of \\(Y_i\\) from its expected value:\n\n\\[\n\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\n\\]\n\nThen we minimize the sum of some function of these errors:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\text{function of } \\epsilon_i \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - E(Y_i)\\big) \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - (\\beta_0 + \\beta_1 X_i)\\big) \\quad\\quad \\text{&lt; for SLR &gt;}\n\\end{align*}\n\\]\n\nFor least squares method specifically, we consider the sum of the \\(n\\) squared errors (deviations). Thus we have:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\epsilon_i^2 \\\\\n    &= \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\end{align*}\n\\]\n\n\nAccording to the method of least squares, the point estimators (functions of the random sample that estimate population quantities) of \\(\\beta_0\\) and \\(\\beta_1\\) are those values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), respectively, that minimize the criterion \\(Q\\) for the sample observations \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\nOnce data is collected, the estimates (realized values) for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are calculated based on the observed sample \\((x_1, y_1), \\ldots, (x_n, y_n)\\). (Note: no good notation for this, maybe \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\); similar idea for means: \\(\\mu\\), \\(\\bar{X}\\) and \\(\\bar{x}\\))\n\nLeast squares estimators\n\n\nThese can be found in two ways.\n\nNumerical methods (optimization) → Search procedures to systematically evaluate the least squares criterion \\(Q\\) for different estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) until the ones that minimize \\(Q\\) are found.\nAnalytical methods → Using calculus. The analytical approach is feasible when the regression model is not mathematically complex.\n\n\n\nThe general process for deriving the least squares estimators of any model is:\n\nWrite \\(Q\\) as a function of the model error term \\(\\epsilon_i\\) as shown above.\nTake derivative with respect to the desired parameter \\(\\beta_i\\).\nSet derivative equal to zero (notation: substituting population term \\(\\beta_i\\) for its estimator \\(\\hat{\\beta}_i\\)) and solve for \\(\\hat{\\beta}_i\\).\nCheck second derivative test to ensure global minimum (take derivative from result of step 2)\n\n\nUsing the analytical approach with the SLR model, we have the following results:\n\n\n\nResults\nDerivation\n\n\n\n\nThe estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are found by solving the simultaneous equations (called the normal equations):\n\n\\[\n\\begin{align*}\n  \\sum Y_i &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum X_i \\\\\n  \\sum X_i Y_i &= \\hat{\\beta}_0 \\sum X_i + \\hat{\\beta}_1 \\sum X_i^2\n\\end{align*}\n\\]\n\nThis leads to\n\n\\[\n\\begin{align*}\n  \\text{Intercept} \\hspace{10pt} \\hat{\\beta}_0 \\hspace{10pt} &= \\hspace{10pt} \\frac{1}{n}\\sum Y_i + \\hat{\\beta}_1 \\frac{1}{n} \\sum X_i \\hspace{10pt} = \\hspace{10pt} \\bar{Y}- \\hat{\\beta}_1 \\bar{X} \\\\\n  \\text{Slope} \\hspace{10pt} \\hat{\\beta_1} \\hspace{10pt} &= \\hspace{10pt} \\frac{\\sum X_i Y_i -\\frac{1}{n} \\sum X_i Y_i}{\\sum X_i^2 - \\frac{1}{n}(\\sum X_i)^2} \\hspace{10pt} = \\hspace{10pt} \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \\hspace{10pt} = \\hspace{10pt} \\frac{S_{XY}}{S_{XX}}\n\\end{align*}\n\\]\n\nNote: We did not have to assume any distribution of the error term. These are the LSE estimators for any SLR model.\n\n\n\n\n\n\n\n\n\nDemo:\n\nFirst we can generate the sample data \\((x_1, y_1), \\ldots, (x_n, y_n)\\).\n\n\n\n\nCode# simulate sample regression dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = B0 + B1X + E\n# -&gt; Y | X ~ Uniform(B0 + B1*X - a, B0 + B1*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nmin &lt;- -2\nmax &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate error terms\n# -&gt; assumption for LSE is E(epsilon) = 0, so symmetric uniform about zero works\nepsilon &lt;- runif(n = n, min = min, max = max)\n\n# calculate y terms as function of x and error terms\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n\n\nCode# display observed sample data\ndata.frame(x, y) %&gt;% \n  head(10) %&gt;% \n  display_nice(col.names = c(\"x\", \"y\"))\n\n\n\n x \n    y \n  \n\n\n 12.129 \n    25.509 \n  \n\n 9.247 \n    20.865 \n  \n\n 10.056 \n    20.220 \n  \n\n 12.868 \n    26.668 \n  \n\n 14.524 \n    29.393 \n  \n\n 10.507 \n    21.917 \n  \n\n 12.553 \n    26.681 \n  \n\n 8.656 \n    16.594 \n  \n\n 8.856 \n    20.582 \n  \n\n 8.134 \n    18.072 \n  \n\n\n\nCode# plot sample data\nplot(x = x, y = y)\n\n\n\n\n\n\n\n\nThen we can fit the model and visualize it on the scatter plot.\n\n\nCode# add regression line to plot\nplot(x = x, y = y)\nabline(lm(formula = y ~ x), col = \"red\")\ntext(x = 14, y = 22, labels = bquote(hat(Y) * \" = \" * hat(beta)[0] + hat(beta)[1] * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nFinally we can get the estimates of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\n\n\nR Functions\nManual\n\n\n\n\n# fit SLR model\n# -&gt; lm() calls lm.fit() behind the scenes --&gt; this performs a QR decomposition to fit the model, which is a more stable way to solve for LSE\nmod_slr &lt;- lm(formula = y ~ x)\n\n# display estimated coefficients\nmod_slr$coefficients\n\n(Intercept)           x \n   1.204946    2.012605 \n\n\n\n# or can do\ncoefficients(mod_slr)\ncoef(mod_slr) # alias for coefficients()\n\nThese are estimates of the population coefficients, so can compare above values to \\(\\beta_0\\) = 1 and \\(\\beta_1\\) = 2.\n\n\n\n# calculate estimates for B0 and B1 using LSE formulas\n# -&gt; b_1 = S_XY / S_XX\n# -&gt; b_0 = Y-bar - b_1 X-bar\ns_xy &lt;- sum((x - mean(x)) * (y - mean(y)))\ns_xx &lt;- sum((x - mean(x))^2)\nb_1 &lt;- s_xy / s_xx\nb_0 &lt;- mean(y) - b_1 * mean(x)\n\n\nCode# compare to results from lm()\ncompare(c(b_0, b_1), mod_slr$coefficients)\n\n$comparison\n[1] TRUE\n\n$`c(b_0, b_1)`\n[1] 1.204946 2.012605\n\n$`mod_slr$coefficients`\n(Intercept)           x \n   1.204946    2.012605 \n\n\n\n\n\nLSE for other models\n\nCan also derive LSE estimators for other models using the same process.\n\nExample 1\n\nRegression through the origin: \\(Y = \\beta X + \\epsilon\\).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X + a, B*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -5, max = 5)\n\n# Y | X ~ Uniform(min = BX + a, max = BX + b)\ny &lt;- runif(n = n, min = beta * x + min, max = beta * x + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 3, y = -5, labels = bquote(hat(Y) * \" = \" * hat(beta) * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model without intercept\nmod_noint &lt;- lm(formula = y ~ -1 + x)\n\n# display estimated coefficient\ncoefficients(mod_noint)\n\n       x \n1.812708 \n\n\n\n\n\n# calculate estimate B using LSE formula\nb &lt;- sum(x * y) / sum(x^2)\n\n\nCode# compare to results from lm()\ncompare(b, coefficients(mod_noint))\n\n$comparison\n[1] TRUE\n\n$b\n[1] 1.812708\n\n$`coefficients(mod_noint)`\n       x \n1.812708 \n\n\n\n\n\n\n\nExample 2\n\nRegression through the origin with squared predictor variable: \\(Y = \\beta_1 X^2 + \\epsilon\\).\nSometimes a transformation can make deriving estimators easier. This way we can get the model in a familiar form and derive like usual (simpler). Then we just have to make the appropriate substitutions at the end in order to get the estimators for the original models.\nThis strategy lessens the number of models we have to know how to derive (note this is just for deriving; no need to do it when fitting models because it’s an extra middle step that doesn’t change anything, leads to equivalent results).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin with squared predictor dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X^2 + a, B*X^2 + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -1, max = 5)\n\n# Y | X ~ Uniform(min = BX^2 + a, max = BX^2 + b)\ny &lt;- runif(n = n, min = beta * x^2 + min, max = beta * x^2 + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 4, y = 15, labels = bquote(hat(Y) * \" = \" * hat(beta) * X^2) , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model with no intercept and squared X\n# -&gt; use asis function I() to treat `^` as an arithmetic operator and not a formula operator\n# -&gt; save design matrix for later demo\nmod_noint_squared &lt;- lm(formula = y ~ -1 + I(x^2), x = TRUE)\n\n# display estimated coefficient\ncoef(mod_noint_squared)\n\n  I(x^2) \n2.007592 \n\n\n\n\n\n# calculate estimates B using LSE formula\nb &lt;- sum(x^2 * y) / sum(x^4)\n\n\nCode# compare to results from lm()\ncompare(b, coef(mod_noint_squared))\n\n$comparison\n[1] TRUE\n\n$b\n[1] 2.007592\n\n$`coef(mod_noint_squared)`\n  I(x^2) \n2.007592 \n\n\n\n# demonstrating that transformed model way is equivalent\n# -&gt; square predictor variable\nx_star &lt;- x^2\nmod_transformed &lt;- lm(formula = y ~ -1 +  x_star, x = TRUE)\n\n# compare design matrices of two models\ncompare(head(mod_noint_squared$x), head(mod_transformed$x))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; Component \\\"dimnames\\\": Component 2: 1 string mismatch &gt;\"\n\n$comparison$`element-wise`\nNULL\n\n\n$`head(mod_noint_squared$x)`\n      I(x^2)\n1  3.7472761\n2 18.2974851\n3  0.2851931\n4 11.3135236\n5 13.5949756\n6  0.1439314\n\n$`head(mod_transformed$x)`\n      x_star\n1  3.7472761\n2 18.2974851\n3  0.2851931\n4 11.3135236\n5 13.5949756\n6  0.1439314\n\n# calculate estimates B using LSE formula\nb_star &lt;- sum(x_star * y) / sum(x_star^2)\n\n# compare betas from usual method and transformed model way\ncompare(b, b_star)\n\n$comparison\n[1] TRUE\n\n$b\n[1] 2.007592\n\n$b_star\n[1] 2.007592\n\n\n\n\n\nProperties of least squares estimators\n\n\nGauss-Markov theorem: If the conditions of the SLR model hold (i.e. \\(Y_i= \\beta_0 + \\beta_1 X_i + \\epsilon_i\\), where \\(\\epsilon_i\\) have mean zero, variance \\(\\sigma^2\\) and are uncorrelated), then the LSE \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the Best Linear Unbiased Estimators (BLUE) (note that \\(\\epsilon_i\\) do not have to be normal).\n\nLinear estimators → Through lots of algebra (shown later), \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta}_1\\) can be written as be written as a linear combination of the \\(Y_i\\): \\(\\sum k_i Y_i\\), where \\(k_i\\) are constant. Thus they are linear estimators.\nUnbiased estimators → Their expected values are \\(\\beta_0\\) and \\(\\beta_1\\), respectively. So neither estimator tends to overestimate or underestimate systematically.\nBest → More precise than all other linear unbiased estimators of \\(\\beta_0\\) and \\(\\beta_1\\) (smaller variance of the sampling distributions).\n\n\n\n1.1.4 Point estimation of the mean response\n\nFor the population relationship \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\), we have the regression function \\(E(Y) = \\beta_0 + \\beta_1 X\\) (because \\(E(\\epsilon) = 0\\)). This is estimated with \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nTerms and what they represent:\n\n\\(Y\\) is a value of the response variable; the observed value.\n\\(E(Y)\\) the mean response. So it is the center of the probability distribution of \\(Y\\) corresponding to the level \\(X\\) of the predictor variable (so technically a conditional expected value \\(E(Y \\mid X)\\).\n\\(\\hat{Y}\\) is a point estimator of the mean response when the level of the predictor variable is \\(X\\); the fitted value.\n\n\n\nApplication of Gauss-Markov theorem\n\n\n\\(\\hat{Y}\\) is the BLUE for \\(E(Y)\\) with \\(V(\\hat{Y}) = V(\\epsilon) = \\sigma^2\\).\n\n\n\n1.1.5 Residuals and model errors\n\nModel error term \\(\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\\) → Measures the difference between an observation and its expected value (unknown true regression line). It is unknown / unobservable.\nResidual \\(\\hat{\\epsilon}_i = e_i = Y_i - \\hat{Y}_i\\) → This is a known, observable estimate of the unobservable model error. Measures the deviation of the observed value from the fitted regression function.\n\n\n\nResiduals are very useful for studying whether the given regression model is appropriate for the data.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#estimation-of-error-terms-variance",
    "href": "notes-slr.html#estimation-of-error-terms-variance",
    "title": "1  Simple linear regression",
    "section": "\n1.2 Estimation of error terms variance",
    "text": "1.2 Estimation of error terms variance\nNeed to estimate the variance \\(\\sigma^2\\) of the error terms \\(\\epsilon_i\\) in a regression model to get an indication of the variability of the probability distributions of \\(Y\\). Also several inferences for models require an estimate of \\(\\sigma^2\\).\nMotivation\n\nFor a single population, the variance \\(\\sigma^2\\) is estimated by the sample variance \\(S^2\\), which is an unbiased estimator. We find \\(S^2\\) by taking the sum of the squared deviations of the observed value and the estimated mean (sum of squares) and then dividing it by the degrees of freedom \\(n - 1\\) (one df is lost when estimating \\(\\mu\\); mean square) .\n\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (Y_i - \\bar{Y})^2}{n - 1}\n\\]\nRegression model\n\nSame logic as above, except we use the residuals as the deviations because each \\(Y_i\\) comes from a different probability distribution with different \\(X\\) (depends on the \\(X_i\\) level).\n\n\\[\n\\text{Error (residual) sum of squares} \\hspace{10pt} SSE = \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 = \\sum_{i = 1}^n e_i^2\n\\]\n\nThen divide by the \\(df = n - 2\\) to the mean square (two dfs are lost when because \\(\\beta_0\\) and \\(\\beta_1\\) need to be estimated when getting the estimated means \\(\\hat{Y_i}\\)).\n\n\\[\n\\text{Error (residual) mean square} \\hspace{10pt} S^2 = MSE = \\frac{SSE}{n - 2} = \\frac{\\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2}{n - 2} = \\frac{\\sum_{i = 1}^n e_i^2}{n - 2}\n\\]\n\nIt will be shown later that \\(MSE\\) is an unbiased estimator for \\(\\sigma^2\\) → \\(E(MSE) = \\sigma^2\\).\nAn estimator of the standard deviation is simply \\(S = \\sqrt{MSE}\\).\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# simulate dataset (using uniform errors)\n# initialize items, generate X values, errors and y values\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; min &lt;- -2; max &lt;- 2\nx &lt;- runif(n = n, min = 5, max = 15)\nepsilon &lt;- runif(n = n, min = min, max = max)\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# display model summary -&gt; looking for residual standard error = S = sqrt(MSE)\n# -&gt; save to object to extract it\n(summ &lt;- summary(mod))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.10557 -0.53713 -0.09689  0.86028  1.75137 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.26209    0.80390   0.326               0.747    \nx            2.08003    0.07968  26.105 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.11 on 28 degrees of freedom\nMultiple R-squared:  0.9605,    Adjusted R-squared:  0.9591 \nF-statistic: 681.4 on 1 and 28 DF,  p-value: &lt; 0.00000000000000022\n\n# display S and MSE = S^2\nsumm$sigma\n\n[1] 1.110427\n\nsumm$sigma^2\n\n[1] 1.233049\n\n\nThese are estimates of population quantities for the error terms, and when simulating the data we assumed \\(\\epsilon_i \\sim \\text{Uniform}\\,(a, b) \\Longrightarrow \\sigma^2 = V(\\epsilon_i) = \\frac{(b - a)^2}{12}\\). For \\(a\\) = -2 and \\(b\\) = 2, \\(V(\\epsilon_i)\\) = 1.333.\n\n\n\n# SIDENOTE -&gt; showing multiple equivalent ways to do the same thing (just keep the last way)\n\n# get fitted values using functions\ny_hat &lt;- mod$fitted.values\ny_hat &lt;- fitted(mod)\ny_hat &lt;- fitted.values(mod) # alias for fitted()\ny_hat &lt;- predict(mod) # discussed more later\n\n# extract residuals using functions (or some manual calculations)\ne &lt;- residuals(mod)\ne &lt;- resid(mod) # alias for residuals()\ne &lt;- mod$residuals\ne &lt;- y - y_hat\n\n# calculate SSE and df\nsse &lt;- sum(e^2)\ndf &lt;- mod$df.residual # extract from model object\ndf &lt;- df.residual(mod)\ndf &lt;- summ$df[2] # extract from model summary -&gt; error df is the second item (even if have MLR model)\ndf &lt;- n - 2 # for SLR (with intercept)\n\n# calculate MSE\nmse &lt;- sse / df\n\n# calculate residual standard deviation\ns &lt;- sqrt(mse)\n\n# compare to results from summary(lm())\n# -&gt; s\ncompare(summ$sigma, s)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma`\n[1] 1.110427\n\n$s\n[1] 1.110427\n\n# -&gt; MSE\ncompare(summ$sigma^2, mse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2`\n[1] 1.233049\n\n$mse\n[1] 1.233049\n\n# -&gt; SSE = MSE * df\ncompare(summ$sigma^2 * summ$df[2], sse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2 * summ$df[2]`\n[1] 34.52536\n\n$sse\n[1] 34.52536",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#normal-error-regression-model",
    "href": "notes-slr.html#normal-error-regression-model",
    "title": "1  Simple linear regression",
    "section": "\n1.3 Normal error regression model",
    "text": "1.3 Normal error regression model\nLeast squares results\n\nNo matter what may be the form of the distribution of the error terms \\(\\epsilon_i\\) (and thus of the \\(Y_i\\)), the LSE provides unbiased point estimators of \\(\\beta_0\\) and \\(\\beta_1\\), that have minimum variance among all unbiased linear estimators (BLUEs).\n\nAssumptions on error term distribution\n\nThese assumptions on \\(\\epsilon_i\\) are needed to set up interval estimates and make tests.\nThe standard assumption is that the error terms are normally distributed. This greatly simplifies the theory of regression analysis and is justifiable in many real-world situations where regression analysis is applied.\n\nNew regression model\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\n\\]\n\nThis is the same regression model as before, just with specified error distribution now (\\(iid\\) = independent and identically distributed).\nBecause this model assumes normal errors, the assumption of uncorrelatedness of the original (unspecified error) model now becomes an independence assumption (\\(\\perp \\!\\!\\! \\perp\\Longrightarrow \\mathrm{Corr}(\\epsilon_i,\\epsilon_j) = 0\\). So the outcome in anyone trial has no effect on the error term for any other trial (in terms of positive or negative, small or large).\nThis model means \\(Y_i \\overset{\\perp \\!\\!\\! \\perp}\\sim \\text{Normal}\\,\\) with \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\) and \\(V(Y_i) = \\sigma^2\\).\n\n\nJustification of the normality assumption.\n\nError terms frequently represent the effects of factors omitted from the model that affect the response to some extent and that vary at random without reference to the variable \\(X\\).\nThese random effects have a degree of mutual independence, the composite error term representing all these factors tends to normal as the number of factors becomes large (by the CLT).\nAlso, the estimation and testing procedures shown later are based on the \\(t\\) distribution and are usually only sensitive to large departures from normality. So, unless the departures from normality are serious, particularly with respect to skewness, the actual confidence coefficients and risks of errors will be close to the levels for exact normality.\n\n\n1.3.1 Estimation of parameters by method of maximum likelihood\nOverview\n\n\nThe method of maximum likelihood chooses as estimates values of the parameters that are most consistent with the sample data. The measure of consistency is the product of densities and is called the likelihood value \\(L(\\mu)\\).\n\nIf the value of \\(\\mu\\) is consistent with the sample data \\(\\Longrightarrow\\) densities relatively large \\(\\Longrightarrow\\) large likelihood value. If not, both will be small.\n\n\n\n\n\nJust like the LSE, there are two ways to find MLEs: a systematic numerical search and by use of an analytical solution.\n\nThe product of the densities viewed as a function of the unknown parameters is called the likelihood function.\n\nIf the likelihood function is relatively peaked in the neighborhood of the maximum, then the MLE estimate is precise because values of \\(\\mu\\) not near the MLE are much less consistent with the data.\nWhen it is relatively flat in a fairly wide region around the MLE, many values of the parameter are almost as consistent with the sample data as the MLE and therefore the MLE is relatively imprecise.\n\n\n\nSteps to MLEs\n\n\nFor multivariate parameter vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_{k})\\), write the likelihood function (i.e. joint density function) and the log-likelihood function\n\n\n\n\\[\nL(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\prod_{i = 1}^n f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\hspace{20pt} \\rightarrow \\hspace{20pt} \\ell(\\boldsymbol{\\theta}) = \\ln[L(\\boldsymbol{\\theta} \\mid \\mathbf{y})]\n\\]\n\nOptimize the log-likelihood function by taking the partial derivatives with respect to each parameters of interest \\(\\theta_1, \\ldots, \\theta_{k}\\).\n\n\nSet to zero and solve for each parameter of interest.\n\n\\[\n\\text{For } j = 1, \\ldots, k \\hspace{20pt} \\ell'(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\theta_j} \\ell(\\boldsymbol{\\theta}) = 0 \\hspace{20pt} \\rightarrow \\hspace{20pt} \\hat{\\boldsymbol{\\theta}} = (\\hat{\\theta}_1, \\ldots, \\hat{\\theta}_{k}) = \\text{potential MLE}\n\\]\n\nVerify that the global maximum of the log-likelihood function occurs at \\(\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}\\).\nFind the second derivative of the log-likelihood function, then plug in \\(\\hat{\\boldsymbol{\\theta}}\\) and see if less than zero.\n\nMLEs for normal error regression model\n\nGenerally\n\n\n\nResults\nDerivation\n\n\n\n\n\n\n\n\n\nParameter\nMLE\n\n\n\n\\(\\beta_0\\)\n\n\\(\\hat{\\beta}_0 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\beta_1\\)\n\n\\(\\hat{\\beta}_1 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\sigma^2\\)\n\\(\\displaystyle \\hat{\\sigma}^2 = \\frac{\\sum (Y_i - \\hat{Y_i})^2}{n}\\)\n\n\n\nNotes\n\n\nProperties of estimators\n\nSince MLEs for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the same as the LSE estimators, they have the same properties: BLUE (unbiased and minimum variance in class of unbiased linear estimators).\nAnd because they are MLEs, we can also say they are: consistent (\\(\\hat{\\beta}_i \\overset{p}\\rightarrow \\beta_i\\), converge in probability to their respective parameters); sufficient (captures all of the information about \\(\\beta_i\\) contained in the sample); minimum variance in class of unbiased estimators (linear or otherwise).\n\n\n\n\n\nMLE \\(\\hat{\\sigma}^2\\) and MSE\n\nRelationship \\(S^2 = MSE = \\frac{n}{n - 2} \\hat{\\sigma}^2 \\hspace{20pt} \\Longleftrightarrow \\hspace{20pt} \\hat{\\sigma}^2 = \\frac{n - 2}{n} MSE\\). However, for large \\(n\\), the difference is small.\nMLE \\(\\hat{\\sigma}^2\\) is biased → \\(E(\\hat{\\sigma}^2) = \\frac{n - 2}{n} \\sigma^2\\), which is an underestimation of \\(\\sigma^2\\). Because of this, MSE is generally used because it is unbiased. However, \\(\\hat{\\sigma}^2\\) is asymptotically unbiased (as \\(n \\rightarrow \\infty\\)) and has a smaller variance than MSE, which may be preferred in some scenarios (tradeoff between bias and precision).\n\n\nGenerally MLE gives the same results as LSE.\n\n\n\n\n\n\n\nCan also derive MLEs for transformed models using the same strategy as with LSE.\n\nExample: Suppose we have \\(\\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\). We can transform \\(Y_i^* = \\beta_0^* + \\beta_1^* X_i + \\epsilon_i\\), derive like usual, then substitute at end to get untransformed estimators.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#inferences-concerning-beta_1",
    "href": "notes-inference.html#inferences-concerning-beta_1",
    "title": "2  Inference",
    "section": "",
    "text": "Test on slope \\(\\beta_1\\) and the implications\n\nWe often want to make inferences about \\(\\beta_1\\). A common test on \\(\\beta_1\\) has the form below.\nIf \\(\\beta_1 = 0\\) \\(\\Longrightarrow\\) Regression line in horizontal, which means there is no linear association between \\(Y\\) and \\(X\\), and even more no relation of any type because all probability distributions of \\(Y\\) are identical at all levels of \\(X\\): normal with \\(E(Y) = \\beta_0 + (0) X = \\beta_0\\) and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n2.1.1 Sampling distribution of \\(\\hat{\\beta}_1\\)\n\n\n\nResults\nDerivation\n\n\n\nSampling distribution of \\(\\hat{\\beta}_1\\)\n\nRefers to distribution of \\(\\hat{\\beta}_1\\) from repeated sampling when the levels of the predictor variable \\(X\\) are held constant from sample to sample.\nRecall \\(\\displaystyle \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\); this is the point estimator.\nDistribution of \\(\\hat{\\beta}_1\\) is Normal with mean and variance:\n\n\\[\n\\begin{align*}\n   E(\\hat{\\beta}_1) &= \\beta_1 \\\\\n   V(\\hat{\\beta}_1) &= \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2}\n\\end{align*}\n\\]\n\nThen we can estimate the variance by replacing the parameter \\(\\sigma^2\\) with \\(MSE\\), the unbiased estimator of \\(\\sigma^2\\). This gives us \\(S^2_{\\hat{\\beta}_1}\\), which is an unbiased estimator for the variance of the sampling distribution of \\(\\hat{\\beta}_1\\). And we can take the positive square root to give us \\(S_{\\hat{\\beta}_1}\\), which is the point estimator of \\(\\sigma_{\\hat{\\beta}_1}\\).\n\n\\[\n  S^2_{\\hat{\\beta}_1} = \\frac{MSE}{\\sum(X_i - \\bar{X})^2} = \\frac{MSE}{S_{XX}} \\hspace{20pt} \\longrightarrow \\hspace{20pt} s_{\\hat{\\beta}_1} = \\sqrt{\\frac{MSE}{S_{XX}}} = \\frac{S}{\\sqrt{S_{XX}}}\n\\]\n\n\nThus, \\(S^2_{\\hat{\\beta}_1}\\) is an unbiased estimator for the variance of the sampling distribution of \\(\\hat{\\beta}_1\\).\n\nThis is also called the standard error (another way to think about it is the standard deviation of the sampling distribution) → \\(S_{\\hat{\\beta}_1} = SE(\\hat{\\beta}_1)\\)\n\n\n\n\n\n\nMain result for normal, mean and variance\n\n\n\\(\\hat{\\beta}_1\\) can be written as a linear combination of the observations \\(Y_i\\):\n\n\\[\n\\hat{\\beta_1}=\\sum k_i Y_i, \\hspace{20pt} \\text{where} \\hspace{10pt} k_i = \\frac{X_i - \\bar{X}}{\\sum(X_i - \\bar{X})^2}\n\\]\n\n\\(\\sum \\text{Normal} \\sim \\text{Normal}\\,\\), so we know \\(\\hat{\\beta}_1\\) must be Normal. Then properties of the coefficients \\(k_i\\) can be used to show the mean and variance.\nNote that \\(k_i\\)’s are fixed quantities because they are a function of only \\(X_i\\)’s (which are fixed quantities). Therefore, \\(\\hat{\\beta}_1\\) is a linear combination of \\(Y_i\\), where the coefficients are solely a function of the fixed \\(X_i\\) (this is why \\(\\hat{\\beta}_1\\) is a linear estimator).\n\nProperties of coefficients \\(k_i\\):\n\nProof of linear combination:\n\nProofs of properties of \\(k_i\\):\n\nProof of mean:\n\nProof of variance:\n\nCan also prove that \\(\\hat{\\beta}_1\\) has minimum variance among all unbiased linear estimators.\n\n\n\n\n\n2.1.2 Sampling distribution of \\((\\hat{\\beta}_1 - \\beta_1) / S_{\\hat{\\beta}_1}\\)\n\n\n\nResults\nDerivation\n\n\n\n\\[\n\\frac{\\hat{\\beta}_1 - \\beta_1}{S_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sqrt{MSE / S_{XX}}} \\sim \\text{t}\\,_{n - 2}\n\\]\n\n\nDeriving the distribution of the standardized slope → \\(\\displaystyle \\frac{\\hat{\\beta}_1 - E(\\hat{\\beta}_1)}{SE(\\hat{\\beta}_1)}\\)\n\nTechnically when standardizing with an estimated standard deviation it is referred to as studentized statistic.\n\n\n\n\n\n\n\n2.1.3 Confidence interval for \\(\\beta_1\\)\n\nForming interval\n\\[\n\\begin{align*}\n  & P(-t_{\\alpha/2, n-2} \\le \\frac{\\hat{\\beta}_1 - \\beta_1}{S_{\\hat{\\beta}_1}} \\le t_{\\alpha/2, n-2}) = 1 - \\alpha \\\\\n  & \\Longleftrightarrow \\hspace{20pt} P(\\hat{\\beta}_1 - t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1} \\le \\frac{ - \\beta_1}{S_{\\hat{\\beta}_1}} \\le \\hat{\\beta}_1 + t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1}) = 1 - \\alpha \\\\\n  & \\Longleftrightarrow \\hspace{20pt}  100(1 - \\alpha)\\% \\text{ CI } = \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1} \\hspace{10pt} = \\hspace{10pt} \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{MSE / S_{XX}}\n\\end{align*}\n\\]\nInterpretation\n\nWith &lt; \\(100(1 - \\alpha)\\) &gt;% confidence, we estimate that the average &lt; \\(Y\\) context &gt; increases by between &lt; lower bound &gt; and &lt; upper bound &gt; for each additional unit increase in &lt; \\(X\\) context &gt;.\nRemember the scope of the regression model is restricted to some range of values of the predictor variable → May not be reasonable to use these slope estimates outside this range as the regression relation may not be linear then.\n\nDemo:\n\n\nR functions\nManual\n\n\n\n\n# simulate sample dataset for normal error regression model\n# -&gt; E ~ Normal(0, sigma^2)\n# -&gt; Y = B0 + B1*X + E\n# -&gt; Y | X ~ Normal(B0 + B1*X, sigma^2)\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 50\nbeta_0 &lt;- 2\nbeta_1 &lt;- 5\nsigma2 &lt;- 10\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 10)\n\n# generate error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sqrt(sigma2))\n      \n# calculate observations Y | X ~ Normal(B0 + B1*X, sigma^2)\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# plot sample data with regression line\nplot(x = x, y = y)\nabline(lm(formula = y ~ x ), col = \"red\")\n\n\n\n\n\n\n# display error terms\nx_plot &lt;- seq(from = -3 * sqrt(sigma2), to = 3 * sqrt(sigma2), by = 0.01)\nhist(epsilon, freq = FALSE, xlab = bquote(epsilon * \"~ Normal (\" * mu * \",\" * sigma^2 * \")\"), main = \"\")\nlines(x = x_plot,\n      y = dnorm(x = x_plot, mean = 0, sd = sqrt(sigma2)))\n\n\n\n\n\n\n\n\n# fit model\nmod_normal &lt;- lm(y ~ x)\n\n# display only coefficient summaries of slope\nsummary(mod_normal)$coefficients[2,] # Estimate is the middle of the interval\n\n                        Estimate                       Std. Error \n 5.08808868675245662416273262352  0.28185386038708926115248232236 \n                         t value                         Pr(&gt;|t|) \n18.05222280711229387861749273725  0.00000000000000000000004917718 \n\n# calculate confidence interval for slope\nalpha &lt;- 0.05\nconfint(mod_normal, parm = \"x\", level = 1 - alpha)\n\n     2.5 %   97.5 %\nx 4.521384 5.654794\n\n\n\n\n\n# using R functions to get the needed values (can reference how to calculate these manually above)\n\n# extract / calculate needed items\n# -&gt; point estimate = beta1-hat\n# -&gt; critical value = t_alpha/2, n-2\n# -&gt; SE(beta1-hat) = MSE / S_XX = S / sqrt(S_XX)\npe &lt;- as.numeric(coef(mod_normal)[\"x\"]) # SIDENOTE -&gt; as.numeric() just to remove the named number data type\nt_crit &lt;- abs(qt(p = alpha / 2, df = mod_normal$df.residual))\ns &lt;- summary(mod_normal)$sigma\ns_xx &lt;- sum((x - mean(x))^2)\nse_beta1_hat &lt;- s / sqrt(s_xx)\n\n# compare to Std. Error from summary of model coefficients\ncompare(summary(mod_normal)$coefficients[\"x\",\"Std. Error\"], se_beta1_hat)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"Std. Error\"]`\n[1] 0.2818539\n\n$se_beta1_hat\n[1] 0.2818539\n\n# calculate confidence interval for beta1\nci_limits &lt;- c(lower = pe - t_crit * se_beta1_hat, upper = pe + t_crit * se_beta1_hat)\n\n# compare to results from confint(lm())\ncompare(confint(mod_normal, parm = \"x\", level = 1 - alpha), ci_limits)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for target but not for current &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is matrix, current is numeric\"                             \n\n$comparison$`element-wise`\nNULL\n\n\n$`confint(mod_normal, parm = \"x\", level = 1 - alpha)`\n     2.5 %   97.5 %\nx 4.521384 5.654794\n\n$ci_limits\n   lower    upper \n4.521384 5.654794 \n\n\n\n\n\n\n2.1.4 Tests concerning \\(\\beta_1\\)\n\nOverview\n\nThe test shown below is called a test of utility of the model.\nIf reject → We conclude that \\(X\\) does contribute information for the prediction of \\(Y\\) when using the straight-line model.\n\nIf fail to reject → Then we conclude there is no linear relationship between \\(Y\\) and \\(X\\) (horizontal model). But keep in mind:\n\nAdditional data might indicate that \\(\\beta_1\\) differs from zero.\nA more complex relationship between \\(Y\\) and \\(X\\) may exist, which would require fitting a model other than the straight-line model.\n\n\nAll assumptions about the error terms (\\(\\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\\)) should be satisfied.\n\nTwo-tailed test (most common)\n\nHypotheses\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\nTest statistic\n\n\\[\nTS = t^* = \\frac{\\hat{\\beta}_1 - 0}{S_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1}{\\sqrt{MSE / S_{XX}}}\n\\]\n\nRejection region and p-value\n\n\\[\n\\begin{align*}\n  RR &= \\{\\lvert t^* \\rvert &gt; t_{\\alpha/2, n - 2}\\} \\\\\n  p\\text{-value} &= 2 \\cdot P(t_{n-2} \\ge \\lvert t^* \\rvert)\n\\end{align*}\n\\]\n\n\nDecision\n\nReject \\(H_0\\) and conclude \\(H_A\\) if \\(\\hspace{10pt}\\) \\(TS \\in RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} \\le \\alpha\\)\nFail to reject \\(H_0\\) if \\(\\hspace{10pt}\\) \\(TS \\notin RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} &gt; \\alpha\\)\nCan also look at the \\(100(1 - \\alpha)\\%\\) CI for \\(\\beta_1\\) to see if contains 0.\n\n\n\nConclusion / Interpretation\n\nAt the \\(\\alpha\\) significance level, we &lt; have / do not have &gt; sufficient evidence of a significant linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;. &lt; if yes… &gt; This is a &lt; positive / negative &gt; linear relationship, indicating that as &lt; \\(X\\) context &gt; increases, &lt; \\(Y\\) context &gt; &lt; increases / decreases &gt;, on average.\n\n\n\nOther tests\n\n\nOne-tailed tests\n\n\\(H_A: \\beta_1 &lt; 0 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} RR = \\{t^*&lt; t_{\\alpha, n - 2}\\} \\hspace{10pt} \\text{and} \\hspace{10pt} p\\text{-value} = P(t_{n-2} \\le t^*)\\)\n\\(H_A: \\beta_1 &gt; 0 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} RR = \\{t^* &gt; t_{\\alpha, n - 2}\\} \\hspace{10pt} \\text{and} \\hspace{10pt} p\\text{-value} = P(t_{n-2} \\ge t^*)\\)\n\n\n\nTests against specified nonzero value of \\(\\beta_{1,0}\\)\n\n\\(TS = t^* = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{S_{\\hat{\\beta}_1}}\\)\nRR, \\(p\\)-value and decisions are the same, just based on new \\(t^*\\).\n\n\n\nDemo:\n\n\nR functions\nManual\nSimulation\n\n\n\n\n# using same model from the confidence interval for beta1 demo\n\n# display model summary, focusing on coefficient summaries for slope\n# -&gt; looking for t value (if doing traditional method test with RR) and Pr(&gt;|t|) = p-value\nsummary(mod_normal) # compare p-value to alpha\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.683 -2.429  0.619  2.243  5.302 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   0.6807     2.1004   0.324               0.747    \nx             5.0881     0.2819  18.052 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.854 on 48 degrees of freedom\nMultiple R-squared:  0.8716,    Adjusted R-squared:  0.8689 \nF-statistic: 325.9 on 1 and 48 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n# calculate test statistic t* = beta1-hat / SE(beta1-hat)\nbeta1_hat &lt;- as.numeric(coef(mod_normal)[2])\nse_beta1_hat &lt;- summary(mod_normal)$coefficients[\"x\",\"Std. Error\"]\nt_star &lt;- beta1_hat / se_beta1_hat\n\n# compare to result from summary(lm())\ncompare(summary(mod_normal)$coefficients[\"x\",\"t value\"], t_star)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"t value\"]`\n[1] 18.05222\n\n$t_star\n[1] 18.05222\n\n# calculate p-value = 2 * P(t_(n-2) &gt;= |t*|)\np_value &lt;- 2 * pt(q = t_star, df = df.residual(mod_normal), lower.tail = FALSE)\n\n# compare to results from summary(lm())\ncompare(summary(mod_normal)$coefficients[\"x\",\"Pr(&gt;|t|)\"], p_value)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"Pr(&gt;|t|)\"]`\n[1] 0.00000000000000000000004917718\n\n$p_value\n[1] 0.00000000000000000000004917718\n\n\n\n\nSimulation to determine how the magnitude of \\(\\beta_1\\) and \\(\\sigma\\) affect the resulting significance of the \\(X\\) variable in SLR when simulating data. For example, suppose \\(\\beta_1 = 1a\\) and \\(\\sigma = 3a\\), where \\(a = 1, 2, 3\\). Is there the same resulting significance for all values of \\(a\\)?\n\n# initialize simulation settings\n# -&gt; first the parameters of interest (that are variable)\n# -&gt; create all combos\n# -&gt; add in the constant settings\n# -&gt; arrange in a good order\n# -&gt; add simulation id column\n# -&gt; set row names equal to id (helps keep track of results later)\nparams &lt;- expand.grid(beta_1 = c(1,5,10),\n                      ratio = c(1:3,5)) %&gt;% \n  mutate(sigma = beta_1 * ratio) %&gt;% \n  mutate(n = 30,\n         beta_0 = 2) %&gt;%\n  arrange(beta_1, ratio) %&gt;% \n  bind_cols(data.frame(sim_id = 1:nrow(.)))\nrownames(params) &lt;- paste0(\"sim\", params$sim_id)\n\n# define function to run on each unique simulation setting\nrun_trial &lt;- function(vec, vec_names, m = 1000) {\n  \n  # redefine arguments for easier reference\n  # -&gt; numeric values get coerced to character because of `model` data type\n  names(vec) = vec_names\n  n = as.numeric(vec[\"n\"])\n  beta_0 = as.numeric(vec[\"beta_0\"])\n  beta_1 = as.numeric(vec[\"beta_1\"])\n  beta_2 = as.numeric(vec[\"beta_2\"])\n  sigma = as.numeric(vec[\"sigma\"])\n  sim_id = as.numeric(vec[\"sim_id\"])\n  \n  # initialize results vector\n  t = rep(NA, m)\n  \n  # loop to simulate m models and extract summaries\n  for (i in 1:m) {\n    \n    # generate data\n    x = runif(n = n, min = 5, max = 10)\n    y = rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n    \n    # fit model\n    mod = lm(y ~ x)\n    \n    # get summaries\n    # -&gt; just want t-stat\n    t[i] = summary(mod)$coefficients[\"x\", \"t value\"]\n    \n  }\n  \n  return(cbind(t, sim_id = sim_id))\n  \n}\n\n\n# define function to run simulation and format results\nsimulation &lt;- function(params, sim_function, iterations = 100, format_results = FALSE) {\n  \n  # run simulation for each parameter settings\n  # -&gt; have to transpose so that now each column is a parameter settings and convert to dataframe so can pass to map(), which needs a list\n  results_raw = params %&gt;% \n    t %&gt;% \n    data.frame %&gt;% \n    map(\\(vec) sim_function(vec, vec_names = names(params), m = iterations), .progress = T)\n  \n  if (!format_results){\n    \n    return(results_raw)\n    \n  }else{\n    \n    # reformat results\n    # -&gt; convert each results matrix to a dataframe, then combine\n    # -&gt; attach the simulation settings to the results\n    # -&gt; pivot longer to have one column for the name of the summary measure and one for its value\n    # -&gt; convert parameters to factors for plot\n    results = results_raw %&gt;% \n      map(\\(x) data.frame(x)) %&gt;% \n      reduce(bind_rows) %&gt;% \n      left_join(params, by = \"sim_id\") %&gt;% \n      pivot_longer(cols = 1:(ncol(results_raw[[1]])-1),\n                   names_to = \"statistic\",\n                   values_to = \"value\") %&gt;% \n      mutate(across(1:ncol(params), as.factor))\n    return(results)\n    \n  }\n  \n}\n\n# run simulation\nresults &lt;- simulation(params = params, sim_function = run_trial, iterations = 1000, format_results = TRUE)\nhead(results)\n\n# A tibble: 6 × 8\n  sim_id beta_1 ratio sigma n     beta_0 statistic value\n  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 1      1      1     1     30    2      t          6.69\n2 1      1      1     1     30    2      t          9.24\n3 1      1      1     1     30    2      t         10.6 \n4 1      1      1     1     30    2      t          6.08\n5 1      1      1     1     30    2      t         11.1 \n6 1      1      1     1     30    2      t          7.40\n\n# create a plot facetted by beta value, with density curves for the sampling distributions of the t-stats for each value of the sigma / beta1 ratio\n# -&gt; add reference line for significance cutoff\n\n# set more informative labels for the facets\nlabels_beta &lt;- c(\"1\" = \"beta1 = 1\",\n                 \"5\" = \"beta1 = 5\",\n                 \"10\" = \"beta1 = 10\")\nggplot() + \n  geom_density(aes(x = value,\n                   group = ratio,\n                   color = ratio,\n                   fill = ratio),\n               data = results,\n               alpha = 0.2) + \n  geom_vline(aes(xintercept = t),\n             data = data.frame(t = abs(qt(0.025, df = params[1,\"n\"]-1))),\n             color = \"darkgrey\") + \n  facet_grid(beta_1 ~ .,\n             labeller = as_labeller(labels_beta)) + \n  guides(\n    fill = guide_legend(bquote(sigma *  \"/\" * beta[1] * \" ratio\")),\n    color = \"none\")\n\n\n\n\n\n\n\nBased on the sampling distributions, it appears that the relative magnitude is what is important. And if it is the same even for different values, then will get similar results.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#some-considerations-when-making-inferences",
    "href": "notes-inference.html#some-considerations-when-making-inferences",
    "title": "2  Inference",
    "section": "\n2.2 Some considerations when making inferences",
    "text": "2.2 Some considerations when making inferences\nEffects of departures from normality:\n\nIf the probability distributions of \\(Y\\) are not exactly normal but do not depart seriously \\(\\Longrightarrow\\) Sampling distribution of \\(\\hat{\\beta_1}\\) \\(\\approx\\) normal \\(\\Longrightarrow\\) Using the \\(t\\) distribution will provide \\(\\approx\\) \\((100 - \\alpha)\\%\\) CIs and \\(\\alpha\\)-level tests.\nThe estimator \\(\\hat{\\beta_1}\\) generally has the property of asymptotic normality (i.e. it’s distribution approaches normality under very general conditions as the sample size increases). So with a sufficiently large sample size, inference procedures are still valid even if distributions of \\(Y\\) have large departures from normality. Can switch from \\(t\\)-based procedures to \\(Z\\)-based with large \\(n\\).\n\nInterpretation of confidence coefficient and risks of errors\n\nOur regression model assumes that the \\(X_i\\) are known constants. So the confidence coefficient and risks of errors are interpreted with respect to taking repeated samples where the \\(X\\) observations are kept at the same levels as in the observed sample.\nFor example, confidence interval for \\(\\beta_1\\) with confidence coefficient 95% → If many independent samples are taken where the levels of \\(X\\) are the same as in the dataset, approximately 95% of the constructed confidence intervals would capture the true value of \\(\\beta_1\\).\n\nSpacing of the \\(X\\) levels\n\nFor a given \\(n\\) and \\(\\sigma^2\\), the variance of \\(\\hat{\\beta_1}\\) is affected by the spacing of the \\(X\\) levels in the observed data. As the spread in the \\(X\\) levels increases, \\(S_{xx}\\) increases and therefore \\(V(\\hat{\\beta_1})\\) decreases.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#interval-estimation-of-ey_h",
    "href": "notes-inference.html#interval-estimation-of-ey_h",
    "title": "2  Inference",
    "section": "\n2.3 Interval estimation of \\(E(Y_h)\\)\n",
    "text": "2.3 Interval estimation of \\(E(Y_h)\\)\n\nOverview\n\nA common objective is to estimate the mean of one or more probability distributions of \\(Y\\).\n\nSetup\n\nLet \\(X_h\\) = level of \\(X\\) that we wish to estimate the mean response for (may be a value which occurred in the sample, or some other value within the scope of the model).\nMean response when \\(X = X_h\\) is \\(E(Y_h)\\); this is what we are estimating.\n\n\n\n\n2.3.1 Sampling distribution of \\(\\hat{Y_h}\\)\n\n\n\nResults\nDerivation\n\n\n\nSampling distribution of \\(\\hat{Y_h}\\)\n\nAgain, this refers to the different values of \\(\\hat{Y_h}\\) from repeated sampling when the levels of the predictor variable \\(X\\) are held constant from sample to sample.\nPoint estimator for \\(E(Y_h)\\) → \\(\\hat{Y_h} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_h\\)\nDistribution of \\(\\hat{Y_h}\\) is Normal with mean and variance:\n\n\\[\n  \\begin{align*}\n    E(\\hat{Y_h}) &= E(Y_h) \\\\\n    V(\\hat{Y_h}) &= \\sigma^2 \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]\n  \\end{align*}\n\\]\n\nEstimate variance (and standard deviation) by substituting \\(MSE\\). This gives us:\n\n\\[\n  S^2_{\\hat{Y_h}} = MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg] \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{\\hat{Y_h}} = \\sqrt{S^2_{\\hat{Y_h}}} = S \\sqrt{\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{S_{XX}}}\n\\]\nNotes\n\nThe variability of the sampling distribution of \\(Y_h\\) is affected by how far \\(X_h\\) is from \\(\\bar{X}\\) (from the numerator of second term in \\(V(\\hat{Y_h})\\)) \\(\\Longrightarrow\\) \\(V(\\hat{Y_h})\\) increases the further \\(X_h\\) is from \\(\\bar{X}\\).\n\nIntuitive / visual explanation of this affect\n\nIn the picture below, two regression lines are assumed to go through the same \\((\\bar{X}, \\bar{Y})\\) point to isolate the effect of variation in the estimated slope \\(\\hat{\\beta}_1\\) from sample to sample.\n\n\n\nWe see the difference between estimated responses is much smaller when \\(X\\) is near the mean \\(\\bar{X}\\). So the variation in slope from sample to sample has a much more pronounced effect for \\(X\\) levels far from the mean.\n\n\nWhen \\(X_h = 0\\) \\(\\Longrightarrow\\) \\(V(\\hat{Y_h})\\) reduces to \\(V(\\hat{\\beta}_0)\\) (likewise for the estimated variances). This is because when \\(X_h = 0\\), \\(\\hat{Y_h} = \\hat{\\beta}_0 + \\hat{\\beta}_1 (0) = \\hat{\\beta}_0\\).\n\n\n\n\n\n\n\n\n2.3.2 Sampling distribution of \\((\\hat{Y_h} - E(Y_h)) / S_{\\hat{Y_h}}\\)\n\n\\[\n\\frac{\\hat{Y_h} - E(Y_h)}{S_{\\hat{Y_h}}} = \\frac{\\hat{Y_h} - E(Y_h)}{\\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}} \\sim \\text{t}\\,_{n - 2}\n\\]\n\n2.3.3 Confidence interval for \\(E(Y_h)\\)\n\n\nGoal → Estimate the mean value of \\(Y\\) for a given value of \\(X\\).\nResults → Using the same general format / derivation of a \\(t\\) interval, we have\n\n\\[\n100(1 - \\alpha)\\% \\text{ CI } = \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{Y_h}} \\hspace{10pt} = \\hspace{10pt} \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}\n\\]\n\nInterpretation → With &lt; \\(1 - \\alpha\\) &gt;% confidence, we estimate that the true mean value of &lt; \\(Y\\) context &gt; for all individuals with an &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\n\nNotes\n\nInterpretations → Same interpretation rules about repeated sampling for constant \\(X\\) levels (because \\(X_i\\) are known constants in the regression model).\n\nConfidence interval width is the smallest when \\(X_h = \\bar{X}\\) (assuming everything else remains equal).\n\nDesign of experiments → Thus in an experiment to estimate the mean response at a particular level \\(X_h\\), the precision of the estimate will be best if (everything else remaining equal) the observations on \\(X\\) are spaced so that \\(\\bar{X} = X_h\\).\n\n\nTests → Can use the CI to perform a two-sided test as well.\n\nRobust → Confidence limits shown here for \\(\\hat{Y_h}\\) are not sensitive to moderate departures from the assumption that the error terms are normally distributed. If there is substantial departures from normality, still not sensitive if large \\(n\\); this robustness comes from robustness of CIs for \\(\\beta_0\\) and \\(\\beta_1\\).\n\nCan also think of these CIs as robust because they are only concerned with the center (location) of the distribution of \\(Y_h\\).\n\n\nMultiple intervals → ** CIs apply when a estimating a single mean response from the study. Will show later how to adjust when estimating several mean responses. **\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# simulate sample dataset for normal error regression model\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30\nbeta_0 &lt;- 2\nbeta_1 &lt;- 5\nsigma &lt;- 10 \n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate response Y | X ~ Normal(B0 + B1*X, sigma^2)\n# -&gt; specify the conditional means and then incorporate the random error\n# -&gt; rnorm() can take a vector of means and iterates through them\n# --&gt; if there n &gt; length(means), then the means get recycled from the start\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# specify new X level (X range is 5 - 15)\n# -&gt; newdata frame should have columns with same name as data for the model\nx_h &lt;- data.frame(x = 12)\n\n# calculate just the point estimate of estimated E(Y_h)\n# -&gt; technically using the predict() function for class lm(), so just calling predict() is masking predict.lm() --&gt; other classes like predict.glm() have slightly different argument options\n# -&gt; by default predict() returns the fits (regression line) for all of the obs (X values) used in the model fit, can specify new data points with newdata = &lt; data_frame &gt;\npredict(mod, newdata = x_h)\n\n       1 \n64.22818 \n\n# calculate lower and upper bounds of interval estimate for E(Y_h)\n# -&gt; by default does 95% CI and returns the point estimate (fit) as well\npredict(mod, newdata = x_h, interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 64.22818 60.28883 68.16752\n\n# show items related to standard error of estimation se(Y_h-hat)\n# -&gt; residual.scale = S = sqrt(MSE)\npredict(mod, newdata = x_h, se.fit = TRUE)\n\n$fit\n       1 \n64.22818 \n\n$se.fit\n[1] 1.923126\n\n$df\n[1] 28\n\n$residual.scale\n[1] 9.243313\n\n# alternate way from ALSM package\n# -&gt; this is the package that goes with the textbook; provides some functions that are nowhere else, and some alternatives to common functions\n# --&gt; the implementation of this function is essentially what is done in the \"manual\" section\n# --&gt; type = \"m\" gives CI for mean observation; by default does CL of 95%, but specifies alpha\n# -&gt; SIDENOTE -&gt; formatting: original result is a dataframe which displays poorly, so convert to matrix\nALSM::ci.reg(mod, newdata = x_h, type = \"m\", alpha = 0.05) %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 64.22818   60.28883   68.16752\n\n\n\n\n\n# calculate point estimate using estimated coefficients\n# -&gt; hat(Y_h) = hat(beta_0) + hat(beta_1) X_h\nb &lt;- as.numeric(coef(mod))\npe &lt;- b[1] + b[2] * as.numeric(x_h)\n\n# calculate critical value\n# -&gt; use lower.tail = FALSE to get the positive version of t_alpha/2\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate se(Y_h-hat) = MSE (1/n (X_h - X-bar)^2 / S_XX) %&gt;% sqrt OR S * sqrt(1/n + S_XX)\nn &lt;- length(x)\nx_bar &lt;- mean(x)\ns_xx &lt;- sum((x - x_bar)^2)\nmse &lt;- (summary(mod)$sigma)^2\nmse &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$residual.scale\nse_yh_hat &lt;- sqrt(mse * (1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\nse_yh_hat &lt;- summary(mod)$sigma * sqrt((1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\n\n# calculate CI for E(Y_h)\nci_limits &lt;- c(lower = pe - t_crit * se_yh_hat, upper = pe + t_crit * se_yh_hat)\n\n# compare results to predict(lm(), type = \"confidence\")\n# -&gt; Y_h-hat and interval bounds\ncompare(predict(mod, newdata = x_h, interval = \"confidence\"), c(\"point estimate\" = pe, ci_limits))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for target but not for current &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is matrix, current is numeric\"                             \n\n$comparison$`element-wise`\nNULL\n\n\n$`predict(mod, newdata = x_h, interval = \"confidence\")`\n       fit      lwr      upr\n1 64.22818 60.28883 68.16752\n\n$`c(`point estimate` = pe, ci_limits)`\npoint estimate          lower          upper \n      64.22818       60.28883       68.16752 \n\n# -&gt; se of estimation\n# --&gt; by default predict uses s = sqrt(mse) = residual standard error to get the se.fit (and the df as well = df.residual(mod)) ---&gt; assumes future obs have same error variance as originals used for fitting\n# --&gt; can specify a different variance to use for future obs with pred.var if desired\ncompare(predict(mod, newdata = x_h, se.fit = TRUE)$se.fit, se_yh_hat)\n\n$comparison\n[1] TRUE\n\n$`predict(mod, newdata = x_h, se.fit = TRUE)$se.fit`\n[1] 1.923126\n\n$se_yh_hat\n[1] 1.923126\n\n# can return fit, bounds, and standard error info from predict using type = \"terms\"\npredict(mod, newdata = x_h, type = \"terms\", interval = \"conf\")\n\n$fit\n         x\n1 8.081096\nattr(,\"constant\")\n[1] 56.14708\n\n$se.fit\n          x\n1 0.9222007\n\n$lwr\n         x\n1 6.192054\nattr(,\"constant\")\n[1] 56.14708\n\n$upr\n         x\n1 9.970139\nattr(,\"constant\")\n[1] 56.14708\n\n$df\n[1] 28\n\n$residual.scale\n[1] 9.243313\n\n# -&gt; for the fit and bounds, for some reason this actually uses the alternate regression model -&gt; Y_h-hat = bar(y) + beta_1-hat (X_h - bar(x))\n# -&gt; results: constant = beta_1-hat (X_h - X-bar) and attr = Y-bar\n# -&gt; same process to get upper and lower too, add to Y-bar\n(x_star &lt;- predict(mod, newdata = x_h, type = \"terms\"))\n\n         x\n1 8.081096\nattr(,\"constant\")\n[1] 56.14708\n\ncompare(pe, mean(y) + b[2] * (as.numeric(x_h) - mean(x)))\n\n$comparison\n[1] TRUE\n\n$pe\n[1] 64.22818\n\n$`mean(y) + b[2] * (as.numeric(x_h) - mean(x))`\n[1] 64.22818\n\n\n\n\n\n# compare width of confidence intervals at two X levels\n\n# specify new X levels (X range is 5 - 15)\n# -&gt; optimal width will be at X = X-bar\nx_h_mean &lt;- data.frame(x = mean(x))\nx_h2 &lt;- data.frame(x = 8)\n\n# calculate width of intervals\npredict(mod, newdata = x_h_mean, interval = \"confidence\")[2:3] %&gt;% diff\n\n[1] 6.913744\n\npredict(mod, newdata = x_h2, interval = \"confidence\")[2:3] %&gt;% diff\n\n[1] 8.717547",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#prediction-of-new-observation",
    "href": "notes-inference.html#prediction-of-new-observation",
    "title": "2  Inference",
    "section": "\n2.4 Prediction of new observation",
    "text": "2.4 Prediction of new observation\nOverview\n\nGoal → Predict a new observation \\(Y\\) for a given \\(X\\) value. This new observation is viewed as the result of a new trial, independent of the trials the model is based on.\n\nSetup\n\nAgain, \\(X_h\\) is the \\(X\\) level for the new trial. Still assuming the underlying regression model is appropriate for the new observation.\nThe new observation on \\(Y\\) is \\(Y_{h(new)}\\); this is what we are estimating.\n\n\n\nDistinction between (1) estimation of the mean response \\(E(Y_h)\\) and (2) prediction of a new response \\(Y_{h(new)}\\)\n\nWe estimate the mean of the distribution of \\(Y\\).\nWe predict an individual outcome drawn from the distribution of \\(Y\\). Obviously, most outcomes deviate from the mean response; so this must be taken into account when predicting \\(Y_{h(new)}\\).\n\n\nPrediction has extra variability \\(\\Longrightarrow\\) Less precision.\n\n\n2.4.1 Prediction interval for \\(Y_{h(new)}\\) when parameters are known\nDemonstration of prediction intervals\n\nFor a simple example, assume the relevant parameters of the regression model are known:\n\n\\[\n\\beta_0 = 0.10, \\, \\beta_1 = 0.95, \\, \\sigma = 0.12 \\hspace{10pt} \\longrightarrow \\hspace{10pt} E(Y) = 0.10 + 0.95 X\n\\]\n\nIf we have a new observation with \\(X_h = 3.425\\) → \\(E(Y_h) = 0.10 + 0.95 (3.5) = 3.425\\) (so we know the center of the normal distribution of \\(Y_h\\)). Thus, using the empirical rule we have the following prediction interval:\n\n\\[\n99.7\\% \\text{ CI } = E(Y_h) \\pm 3 \\sigma \\hspace{10pt} = \\hspace{10pt} 3.425 \\pm 3 (0.12) \\hspace{10pt} \\Longrightarrow \\hspace{10pt} 3.065 \\le Y_{h(new)} \\le 3.785\n\\]\n\nBasic idea of prediction used here\n\nChoose a range in the distribution of \\(Y\\) where most of the observations will fall and then declare that the next observation will fall in this range.\nThe usefulness of the prediction interval depends on the width of the interval and the needs for precision by the user.\n\nGeneralizing for this simple scenario\n\nWhen the regression parameters of normal error regression model are known:\n\n\\[\n100(1 - \\alpha)\\% \\text{ PI for } Y_{h(new)} = E(Y_h) \\pm z_{\\alpha / 2} \\cdot \\sigma\n\\]\n\nCentering the limits around \\(E(Y_h)\\) results in the narrowest interval consistent with the specified probability of a correct prediction.\n\n2.4.2 Prediction interval for \\(Y_{h(new)}\\) when parameters are unknown\nOverview and demo\n\n\nWhen the regression parameters are unknown, they must be estimated.\n\nThe mean of the distribution of \\(Y\\) is estimated by \\(\\hat{Y_h}\\) as usual, and the variance of the distribution of \\(Y\\) is estimated with \\(MSE\\).\nHowever, we cannot simply use the previous PI with the parameters replaced by the corresponding point estimators. Here’s a demo of why:\nThe picture below shows two possible probability distributions of \\(Y\\), corresponding to the lower and upper limits of a CI for \\(E(Y_h)\\). In other words, the distribution of \\(Y\\) could be located as far left as the one shown, as far right as the other one shown, or anywhere in between.\n\n\n\n\n\n\nResults\n\nSince we do not know the mean \\(E(Y_h)\\) and only estimate it by a confidence interval, we cannot be certain of the location of the distribution of \\(Y\\).\nBecause of this uncertainty, PIs for \\(Y_{h(new)}\\) clearly must take into account two elements:\n\n\nVariation in possible location of the distribution of \\(Y\\).\nVariation within the probability distribution of \\(Y\\).\n\n\n\nPrediction interval for \\(Y_{h(new)}\\)\n\n\nSampling distribution\n\nNote that this studentized statistic uses the point estimator \\(\\hat{Y_h}\\) in the numerator rather than the true mean \\(E(Y_h)\\) because the true mean is unknown and cannot be used in making a prediction.\nFor \\(E(Y_h)\\) CIs shown earlier, we used \\([\\hat{Y_h} - E(Y_h)] / S_{\\hat{Y_h}}\\), which was okay because the only unknown was \\(E(Y_h)\\) and it is what we are estimating. But now there are two layers of uncertainty (variability). So the reference value in the numerator is also an estimate, not the true value.\n\n\n\n\\[\n\\frac{Y_{h(new)} - \\hat{Y_h}}{S_{pred}} \\sim \\text{t}\\,_{n-2}\n\\]\n\n\nPrediction interval\n\nInterpretation → With &lt; \\(1-\\alpha\\) &gt;% confidence, we predict that the true value of &lt; \\(Y\\) context &gt; for a single (or the next) &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\n\n\n\n\\[\n100(1 - \\alpha)\\% \\text{ PI for } Y_{h(new)} = \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{pred}\n\\]\nStandard deviation of prediction \\(\\sigma^2_{pred}\\)\n\nThe numerator of the studentized statistic represents how far the new observation will deviate from the estimated mean (based on the original \\(n\\) cases in the study). This difference can be viewed as the prediction error, with \\(\\hat{Y_h}\\) serving as the best point estimate of the value of the new observation \\(Y_{h(new)}\\).\nWe can easily find the variance of this difference (because of independence of the new \\(Y_{h(new)}\\) and original \\(n\\) cases on which \\(\\hat{Y_h}\\) is based).\n\n\\[\n\\sigma^2_{pred} = V(Y_{h(new)} - \\hat{Y_h}) = V(Y_{h(new)}) + V(\\hat{Y_h}) = \\sigma^2 + \\sigma^2_{\\hat{Y_h}}.\n\\]\n\n\nThis has two components:\n\nVariance of the distribution of \\(Y\\) at \\(X = X_h\\) → \\(\\sigma^2\\)\nVariance of the sampling distribution of \\(\\hat{Y_h}\\) → \\(\\sigma^2_{\\hat{Y_h}}\\)\n\n\nThe unbiased estimator of \\(\\sigma^2_{pred}\\) is\n\n\\[\nS^2_{pred} = MSE + S^2_{\\hat{Y_h}} = MSE + MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg] = MSE \\bigg[1 + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum (X_i - \\bar{X})^2}\\bigg] \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{pred} = \\sqrt{S^2_{pred}}\n\\]\nNotes about prediction intervals\n\n\nAnalogous results / interpretations to estimation\n\nInterpretation → Again, the confidence coefficient refers to taking repeated samples based on the same set of \\(X\\) values, and calculating prediction limits for \\(Y_{h(new)}\\) for each sample.\nPrecision → PI width is the smallest when \\(X_h = \\bar{X}\\) (assuming everything else remains equal).\nOne interval → PIs apply for a single prediction based on the sample data.\n\n\nNot robust → PIs (unlike CIs for the mean response) are sensitive to departures from normality of the error terms distribution. Can think of this non-robustness a result of having to take into account the center of the distribution of \\(Y_h\\) (just like with CIs) AND also the tails (spread) of the distribution.\n\nPrecision → Even if PIs are too wide for useful predictions, they can still be informative for control / modelling purposes, specifically the estimated variance of prediction.\n\n\\(S^2_{pred} = MSE + S^2_{\\hat{Y_h}} \\hspace{10pt}\\) has two pieces: (1) \\(MSE\\) measures \\(X\\)-to-\\(X\\) variation within the probability distribution for \\(Y\\) (different response values for observations with same \\(X\\) level) and (2) \\(S^2_{\\hat{Y_h}}\\) measures sample-to-sample variation (mean response of samples with overall same \\(X\\) levels).\nSo if \\(MSE\\) is very large compared to \\(S^2_{\\hat{Y_h}}\\), e.g. \\(\\frac{MSE}{S^2_{pred}} \\ge 0.8 \\text{ or } 0.9\\), then the majority of the variation is from sample-to-sample. This could reflect other factors that aren’t being taken into account by the model. So perhaps a multiple linear regression model should be used, which could result in more useful predictions. NOT SURE HOW THIS WORKS\n\n\n\nEstimation vs Prediction\n\nLocation → For a particular \\(X_h\\), CIs and PIs have the same point estimate \\(\\hat{Y_h}\\), which is the estimate of the mean \\(E(Y_h)\\).\n\nPrecision\n\nThe difference between CIs and PIs then lies in the relative accuracy of the interval.\nCIs are narrower than PIs at the same \\(X_h\\).\nThe only way to obtain more accurate prediction for a new value of \\(Y\\) is to reduce the standard deviation of the regression model. This can be accomplished by using a curvilinear model, adding new independent variables, etc. or by collecting more data (width of both intervals decrease when the sample size increases).\n\n\nConceptual difference → CIs resemble PIs, except: A CI represents an inference on a parameter and is an interval that is intended to cover the value of the parameter; and a PI is a statement about the value to be taken by a random variable, the new observation \\(Y_{h(new)}\\).\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate just the point estimate of predicted Y_h(new)\n# -&gt; this is the same as the PE for a CI of E(Y_h) \npredict(mod, newdata = x_h)\n\n       1 \n64.22818 \n\n# calculate lower and upper bounds of prediction interval Y_h(new)\n# -&gt; by default does 95% CI and returns the point estimate (fit) as well\npredict(mod, newdata = x_h, interval = \"prediction\")\n\n       fit      lwr      upr\n1 64.22818 44.88865 83.56771\n\npredict(mod, newdata = x_h, interval = \"confidence\")\n\n       fit      lwr      upr\n1 64.22818 60.28883 68.16752\n\n# show items related to standard error of ESTIMATION se(Y_h-hat)\n# -&gt; ALWAYS returns this, even if specify interval = \"pred\"...\n# --&gt; suppose it implicitly adds the extra MSE term (from residual.scale) to get the prediction lwr and upr\npredict(mod, newdata = x_h, se.fit = TRUE)\n\n$fit\n       1 \n64.22818 \n\n$se.fit\n[1] 1.923126\n\n$df\n[1] 28\n\n$residual.scale\n[1] 9.243313\n\n# alternative way using ALSM\n# -&gt; type = \"n\" gives PI for single new observation\nALSM::ci.reg(mod, newdata = x_h, type = \"n\") %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 64.22818   44.88865   83.56771\n\n\n\n\n\n# calculate point estimate and critical value\n# -&gt; same as for confidence interval\nb &lt;- as.numeric(coef(mod))\npe &lt;- b[1] + b[2] * as.numeric(x_h)\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate standard error in prediction se(pred) = sqrt(MSE + var(Y-h-hat))\n# -&gt; save se(estimation = fit), shown earlier, then have to add in the extra MSE term\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nse_pred &lt;- sqrt(summary(mod)$sigma^2 + se_fit^2)\nse_pred &lt;- sqrt(summary(mod)$sigma^2 + se_fit^2)\n\n# OR calculate se(pred) using expanded formula se(pred) = MSE * (1 + 1/n (X_h - X-bar)^2 / S_XX) %&gt;% sqrt OR s * sqrt(...)\nn &lt;- length(x)\nx_bar &lt;- mean(x)\ns_xx &lt;- sum((x - x_bar)^2)\ns &lt;- (summary(mod)$sigma)\nse_pred &lt;- s * sqrt((1 + 1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\n\n# calculate PI for Y_h(new)\npi_limits &lt;- c(lower = pe - t_crit * se_pred, upper = pe + t_crit * se_pred)\n\n# compare results to predict(lm(), type = \"prediction)\n# -&gt; Y_h-hat and interval bounds\ncompare(predict(mod, newdata = x_h, interval = \"prediction\"), c(\"point estimate\" = pe, pi_limits))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for target but not for current &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is matrix, current is numeric\"                             \n\n$comparison$`element-wise`\nNULL\n\n\n$`predict(mod, newdata = x_h, interval = \"prediction\")`\n       fit      lwr      upr\n1 64.22818 44.88865 83.56771\n\n$`c(`point estimate` = pe, pi_limits)`\npoint estimate          lower          upper \n      64.22818       44.88865       83.56771 \n\n\n\n\n\n# compare width of prediction intervals at two X levels\n\n# specify new X levels (X range is 5 - 15)\n# -&gt; optimal width will again be at X = X-bar\nx_h_mean &lt;- data.frame(x = mean(x))\nx_h2 &lt;- data.frame(x = 8)\n\n# calculate width of intervals\npredict(mod, newdata = x_h_mean, interval = \"prediction\")[2:3] %&gt;% diff\n\n[1] 38.4941\n\npredict(mod, newdata = x_h2, interval = \"prediction\")[2:3] %&gt;% diff\n\n[1] 38.85861\n\n# compare widths of CI vs PI at the same X level\n\n# calculate width of intervals\npredict(mod, newdata = x_h, interval = \"conf\")[2:3] %&gt;% diff\n\n[1] 7.878692\n\npredict(mod, newdata = x_h, interval = \"pred\")[2:3] %&gt;% diff\n\n[1] 38.67906\n\n\n\n\n\n\n\n2.4.3 Prediction of mean of \\(m\\) observations for given \\(X_h\\)\n\nOverview\n\nGoal → Predict the mean of \\(m\\) new observations on \\(Y\\) for a given level of the predictor variable.\nSetup → \\(\\bar{Y}_{h(new)}\\) represents the mean of the new \\(Y\\) observations to be predicted.\n\nResults\n\n\n\\(100(1 - \\alpha)\\%\\) Prediction interval for \\(\\bar{Y}_{h(new)}\\) (assuming the new observations are independent):\n\n\\[\n\\hat{Y_h} \\pm t_{(1 - alpha /2, n - 2)} \\cdot S_{predmean}\n\\]\n\nStandard deviation in prediction of a mean \\(S_{predmean}\\)\n\n\n\\[\nS^2_{predmean} = \\frac{MSE}{m} + S^2_{\\hat{Y_h}}  = MSE \\bigg[\\frac{1}{m} + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{S_{XX}}\\bigg]  \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{predmean} = \\sqrt{S^2_{predmean}}\n\\]\n\n\nThis has two components:\n\nVariance of the distribution mean of \\(m\\) observations from the probability distrubtion of \\(Y\\) at \\(X = X_h\\) → \\(\\sigma^2 / m\\)\nVariance of the sampling distribution of \\(\\hat{Y_h}\\) → \\(\\sigma^2_{\\hat{Y_h}}\\)\n\n\n\nNotes\n\nInterpretation → With &lt; \\(1 - \\alpha\\) &gt;% confidence, we predict that the true value of &lt; \\(Y\\) context &gt; for &lt; \\(m\\) &gt; &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\nInterval still has the same center as when estimating \\(E(Y_h)\\) and predicting a single \\(Y_{h(new)}\\).\nThis prediction interval is narrower than when predicting for a single observation (because it involves the prediction of the mean for a group), but still wider than the confidence interval.\n\nWe can then obtain the prediction interval for the total of the \\(m\\) observations by multiplying each limit by \\(m\\).\n\ne.g) For \\(m = 4\\), if \\(5 \\le \\bar{Y}_{h(new)} \\le 15 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} 5(4) = 20 \\le \\sum Y_{h(new)} \\le 15(4)= 60\\)\n\n\n\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate the predicted mean for m observations of Y_h(new)\n# -&gt; type = \"nm\" is gives PR for mean of m new observations at X_h\nALSM::ci.reg(mod, newdata = x_h, type = \"nm\") %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 64.22818   44.88865   83.56771\n\n\n\n\n\n# set number of observations to predict at X_h\nm &lt;- 4\n\n# calculate just the point estimate of predicted Y-bar_h(new) and critical value\n# -&gt; PE is the same as previous CI and PI\npe &lt;- as.numeric(predict(mod, newdata = x_h))\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate standard error in prediction se(pred) = sqrt(MSE / m + var(Y-h-hat))\n# -&gt; save se(fit), shown earlier, then have to add in the extra MSE / m term\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nse_pred &lt;- sqrt(summary(mod)$sigma^2 / m + se_fit^2)\n\n# calculate PI for Y-bar_h(new)\n(pi_limits &lt;- c(lower = pe - t_crit * se_pred, upper = pe + t_crit * se_pred))\n\n   lower    upper \n53.97424 74.48211 \n\n# compare to (widths) of previous types of intervals\n# -&gt; order from most to least precise: CI for E(Y_h), PI for Y-bar_h(new), PI for single Y_h-hat\npredict(mod, newdata = x_h, interval = \"conf\")\n\n       fit      lwr      upr\n1 64.22818 60.28883 68.16752\n\npredict(mod, newdata = x_h, interval = \"pred\")\n\n       fit      lwr      upr\n1 64.22818 44.88865 83.56771\n\n# interval for sum (total) of m predictions at X_h\n# -&gt; depends on context if this is meaningful\npi_limits * m\n\n   lower    upper \n215.8970 297.9284",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#confidence-band-for-regression-line",
    "href": "notes-inference.html#confidence-band-for-regression-line",
    "title": "2  Inference",
    "section": "\n2.5 Confidence band for regression line",
    "text": "2.5 Confidence band for regression line\nOverview\n\nThe goal is to obtain a confidence band for the entire regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\).\nThis band enables us to see the region in which the entire regression line lies and is particularly useful for determining the appropriateness of a fitted regression function.\n\nResults\n\n\nThis confidence band formula below the same form as the CI formula for \\(E(Y_h)\\), the mean response at \\(X_h\\) (\\(\\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{Y_h}}\\)), except it uses a different multiplier to adjust for multiple comparisons \\(\\Longrightarrow\\) Same point estimate and standard error.\n\nSome alternative procedures for developing confidence bands have been developed.\nThe one shown below is the Working-Hotelling confidence band (more will be said about this method later).\nThe simplicity of this method is that it is a direct extension of the confidence limits for a single mean response \\(E(Y_h)\\) shown earlier.\n\n\nThe \\(100(1 - \\alpha)\\%\\) confidence band for the regression line has boundaries at any level \\(X_h\\):\n\n\\[\n\\hat{Y_h} \\pm W \\cdot S_{\\hat{Y_h}} = \\hat{Y_h} \\pm W \\cdot \\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}\n\\]\n\n(Initial steps of deriving CI) → We are finding the two values such that \\(P(\\text{lower} \\le \\beta_0 + \\beta_1 X_h \\le \\text{upper}) = 1- \\alpha\\); And \\(\\hat{Y_h}\\) is the point estimator for \\(E(Y_h) = \\beta_0 + \\beta_1 X_h\\).\n\\(W\\) multiplier\n\n\\[\nW^2 = 2 \\cdot F_{(1-\\alpha; \\, 2, n-2)}\n\\]\n\nWe multiply by two for SLR becuase there are two estimated coefficients.\nThe \\(W\\)-multiplier is larger than the \\(t\\) multiplier because the confidence band must encompass the entire regression line, whereas the confidence limits for \\(E(Y_h)\\) at \\(X_h\\) apply only at the single level \\(X_h\\).\n\nNotes\n\n\nConfidence band confidence level\n\nInterpretation → Indicates the proportion of time that the estimating procedure will yield a band that covers the entire line, in a long series of samples in which the \\(X\\) observations are kept at the same level as in the actual study.\nTechnical application → The confidence band applies to the entire regression line over all real-numbered values of \\(X\\) from \\(-\\infty\\) to \\(\\infty\\).\nPractical application → In practice, the confidence band is ignored for that part of the regression line which is not of interest, so the confidence coefficient for this limited segment is somewhat higher than \\(1-\\alpha\\). Thus, \\(1-\\alpha\\) serves as a lower bound to the confidence coefficient.\n\n\nThings to look for → Can see if the slope is clearly positive or negative, can look at the levels of the regression line at different levels of \\(X\\) to gauge relative precision, etc.\n\nPrecision\n\nWith the somewhat wider limits for the entire regression line, we are able to draw conclusions about any and all mean responses for the entire regression line and not just about the mean response at a given \\(X\\) level.\nAlthough, generally confidence band lines at any value \\(X_h\\) often are not substantially wider than the confidence limits for the mean response at that single \\(X_h\\) level.\nJust like with the CI for \\(E(Y_h)\\), the boundary points of the confidence band for the regression line are wider apart the further \\(X_h\\) is from the mean \\(\\bar{X}\\) of the \\(X\\) observations.\n\n\nFormula → The lower and upper bounds together actually define a hyperbola.\n\nDemo\n\n\nR functions\nManual\nOther functions\nIn practice\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate confidence band limits across scope of model\n# -&gt; initialize many x_h values covering min to max of original sample\nx_h &lt;- data.frame(x = seq(from = min(x), to = max(x), length = 20))\n\n# calculate confidence bands\n# -&gt; type = \"w\" uses the Working-Hotelling method with the W multiplier\nconf_band &lt;- ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05)\n\n# display results\nkable(conf_band[1:5,], format = \"html\", digits = 3) %&gt;% \n  kable_styling(full_width = FALSE,\n                position = \"left\")\n\n\n\nx\nFit\nLower.Band\nUpper.Band\n\n\n\n5.249\n31.419\n22.920\n39.917\n\n\n5.759\n33.898\n26.018\n41.778\n\n\n6.269\n36.377\n29.094\n43.659\n\n\n6.779\n38.856\n32.145\n45.567\n\n\n7.289\n41.335\n35.161\n47.509\n\n\n\n\n# compare width when estimating a single E(X_h) to the confidence bands\nc(x_h[1,], predict(mod, newdata = data.frame(x = x_h[1,]), interval = \"conf\")) %&gt;% \n  set_names(c(\"x\", \"fit\", \"lwr\", \"upr\"))\n\n        x       fit       lwr       upr \n 5.248661 31.418696 24.683385 38.154008 \n\n# plot fitted line and confidence bands on scatterplot\n# -&gt; also add reference line for X-bar showing where the most precision is\nconf_band %$% \n  matplot(x = x, y = cbind(Fit, Lower.Band, Upper.Band), type =  \"l\", lty = c(1,2,2), col = c(\"red\", \"blue\", \"blue\"), xlab = expression(X[h]), ylab = expression(hat(Y)[h]), main = bquote(.(100 * (1 - alpha)) * \"% Confidence band\"))\npoints(x, y)\nabline(v = mean(x), lty = 2, col = \"lightgrey\")\ntext(x = mean(x) + 0.5, y = min(y) + 1, labels = expression(bar(X)) , col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\n\n# calculate W multiplier = 2 F_crit(regression = of coefficients, residual)\n# degrees of freedom\n# -&gt; numerator -&gt; = df regression (# of coefficients) = 2 for SLR\n# -&gt; denominator -&gt; df residual (n - # of coefficients) = n - 2 for SLR\n# probability -&gt; F is right-tailed, so need to use 1 - alpha now\n# -&gt; no dividing by two or taking absolute value\nalpha &lt;- 0.05\n(W &lt;- sqrt(2 * qf(1 - alpha, df1 = length(coef(mod)), df2 = df.residual(mod))))\n\n[1] 2.584719\n\n# compare W to t at a specific X_h\nabs(qt(alpha / 2, df = df.residual(mod)))\n\n[1] 2.048407\n\n# calculate confidence bands\n# -&gt; use seq of x values from before\n# -&gt; get the fits and se(estimation) for each new x\n# -&gt; calculate lower and upper confidence band limits = Y_h-hat +- W * se(estimation)\nfit &lt;- as.numeric(predict(mod, newdata = x_h))\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nlower &lt;- fit - W * se_fit\nupper &lt;- fit + W * se_fit\n\n# combine above info\ndata_conf_band &lt;- data.frame(x_h = x_h$x, fit, se_fit, lower, upper, width = upper - lower) \n\n# illustrate process of confidence bands over X range\ndata_conf_band %&gt;% \n  display_nice(col.names = c(\"X_h\", \"fit = Y_h-hat\", \"SE(fit) = S_{Y_h-hat}\", \"LB = Y_h-hat - W x S_{Y_h-hat}\", \"UB = Y_h-hat + W x S_{Y_h-hat}\", \"Width\"))\n\n\n\n X_h \n    fit = Y_h-hat \n    SE(fit) = S_{Y_h-hat} \n    LB = Y_h-hat - W x S_{Y_h-hat} \n    UB = Y_h-hat + W x S_{Y_h-hat} \n    Width \n  \n\n\n 5.249 \n    31.419 \n    3.288 \n    22.920 \n    39.917 \n    16.997 \n  \n\n 5.759 \n    33.898 \n    3.049 \n    26.018 \n    41.778 \n    15.760 \n  \n\n 6.269 \n    36.377 \n    2.817 \n    29.094 \n    43.659 \n    14.565 \n  \n\n 6.779 \n    38.856 \n    2.596 \n    32.145 \n    45.567 \n    13.422 \n  \n\n 7.289 \n    41.335 \n    2.389 \n    35.161 \n    47.509 \n    12.347 \n  \n\n 7.799 \n    43.814 \n    2.197 \n    38.134 \n    49.494 \n    11.360 \n  \n\n 8.309 \n    46.293 \n    2.028 \n    41.052 \n    51.535 \n    10.483 \n  \n\n 8.820 \n    48.772 \n    1.886 \n    43.898 \n    53.647 \n    9.749 \n  \n\n 9.330 \n    51.251 \n    1.778 \n    46.657 \n    55.846 \n    9.190 \n  \n\n 9.840 \n    53.730 \n    1.710 \n    49.311 \n    58.150 \n    8.840 \n  \n\n\n\n# compare results to ci.reg(type = \"w)\ncompare(head(ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05)), head(data_conf_band[,c(1,2,4,5)]))\n\n$comparison\n[1] TRUE\n\n$`head(ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05))`\n         x      Fit Lower.Band Upper.Band\n1 5.248661 31.41870   22.91995   39.91744\n2 5.758791 33.89777   26.01767   41.77788\n3 6.268920 36.37685   29.09448   43.65923\n4 6.779050 38.85593   32.14480   45.56706\n5 7.289180 41.33501   35.16128   47.50874\n6 7.799310 43.81409   38.13429   49.49388\n\n$`head(data_conf_band[, c(1, 2, 4, 5)])`\n       x_h      fit    lower    upper\n1 5.248661 31.41870 22.91995 39.91744\n2 5.758791 33.89777 26.01767 41.77788\n3 6.268920 36.37685 29.09448 43.65923\n4 6.779050 38.85593 32.14480 45.56706\n5 7.289180 41.33501 35.16128 47.50874\n6 7.799310 43.81409 38.13429 49.49388\n\nhead(data_conf_band)\n\n       x_h      fit   se_fit    lower    upper    width\n1 5.248661 31.41870 3.288073 22.91995 39.91744 16.99749\n2 5.758791 33.89777 3.048729 26.01767 41.77788 15.76021\n3 6.268920 36.37685 2.817472 29.09448 43.65923 14.56475\n4 6.779050 38.85593 2.596463 32.14480 45.56706 13.42226\n5 7.289180 41.33501 2.388550 35.16128 47.50874 12.34746\n6 7.799310 43.81409 2.197451 38.13429 49.49388 11.35959\n\n\n\n\n\n# demo to verify what geom_smooth() gives\n\n# results\n# -&gt; lm regression lines line up as expected, BUT...\n# -&gt; the confidence bands using se = TRUE are actually the lower and upper **pointwise** confidence interval around the mean\n# -&gt; ** this means it is plotting ALL of the INDIVIDUAL CIs for E(Y_h), whereas Working-Hotelling confidence bands represent a confidence \"interval\" for the ENTIRE regression line\n\n# create dataframe of original observations the model was built on\ndata_original &lt;- data.frame(x, y)\n\n# set t multiplier for making single interval estimates of E(Y_h)\nt_crit &lt;- abs(qt(alpha / 2, df = df.residual(mod)))\n\n# create confidence band demo plotting dataset\n# -&gt; rename current lower and upper to have an indication of W multiplier\n# -&gt; calculate new lower and upper bounds based on t multiplier\n# -&gt; remove unneeded columns for plot\n# -&gt; reshaoe to long with to long with and columns for type of bound and value\n# -&gt; create indicator variable for type of multiplier used in calculating the lower and upper bounds (check what suffix is of bound) and then remove multiplier indication from bound column (just take off suffix)\ndata_conf_band_plot &lt;- data_conf_band %&gt;% \n  rename(lower_w = lower,\n         upper_w = upper) %&gt;% \n  mutate(lower_t = fit - t_crit * se_fit,\n         upper_t = fit + t_crit * se_fit) %&gt;% \n  select(-c(width, se_fit)) %&gt;% \n  pivot_longer(cols = starts_with(c(\"lower\", \"upper\")),\n               names_to = \"bound\",\n               values_to = \"value\") %&gt;% \n  mutate(multiplier = \n           case_when(\n             str_sub(bound, start = -1) == \"t\" ~ \"t\",\n             TRUE ~ \"W\"),\n         bound = str_sub(bound, end = -3))\n\n# create demo plot\n# layer:  geom_smooth()\n# -&gt; this adds regression line\n# -&gt; se + TRUE adds the shaded ribbon representing \"confidence bands\"\n# layer: geom_line() first one\n# -&gt; add manually calculated regression line, should line up exactly with above\n# layer: geom_line() second and third ones\n# -&gt; confidence bands for both multipliers\nggplot() + \n  geom_smooth(aes(x = x,\n                  y = y),\n              data = data_original,\n              method = \"lm\",\n              formula = y ~ x,\n              level = 1 - alpha,\n              se = TRUE,\n              color = \"yellow\",\n              linewidth = 1,\n              alpha = 0.5) + \n  geom_line(aes(x = x_h,\n                y = fit),\n            data = data_conf_band_plot,\n            color = \"red\",\n            linewidth = 0.5) + \n  geom_line(aes(x = x_h,\n                y = value,\n                color = multiplier),\n            data = filter(data_conf_band_plot, bound == \"lower\"),\n            linetype = 2) + \n  geom_line(aes(x = x_h,\n                y = value,\n                color = multiplier),\n            data = filter(data_conf_band_plot, bound == \"upper\"),\n            linetype = 2) + \n  scale_color_manual(name = \"Multiplier\", values = c(t = \"purple\", W = \"blue\")) + \n  labs(title = \"Demo of confidence bands\",\n       x = expression(X[h]),\n       y = expression(hat(Y)[h]))\n\n\n\n\n\n\n\nWhere the multipliers are \\(t_{\\alpha / 2, n - p}\\) and \\(W = 2 \\cdot F_{1 - \\alpha; p, n - p}\\).\n\n\nDemo to verify confidence bands and prediction bands from predict().\n\n# create two datasets of should be equivalent confidence limits (from different functions)\ndata_bands_plot1 &lt;-predict(mod, newdata = x_h, interval = \"conf\") %&gt;% \n  data.frame() %&gt;% \n  mutate(fun = \"pred\") %&gt;% \n  bind_rows(\n    ALSM::ci.reg(mod, newdata = x_h, type = \"m\") %&gt;% \n      data.frame %&gt;% \n      mutate(fun = \"ci.reg\") %&gt;% \n      select(fit = Fit, lwr = Lower.Band, upr = Upper.Band, fun)\n  ) %&gt;% \n  bind_cols(bind_rows(x_h, x_h), .)\n\nggplot(data = data_bands_plot1) + \n  geom_line(aes(x = x,\n                y = fit)) + \n  geom_line(aes(x = x,\n                y = lwr, \n                color = fun,\n                linetype = fun)) + \n  geom_line(aes(x = x,\n                y = upr,\n                color = fun,\n                linetype = fun))\n\n\n\n\n\n\n# SAME\n\n# try again for predictions\n# create two datasets of should be equivalent prediction limits (from different functions)\ndata_bands_plot2 &lt;-predict(mod, newdata = x_h, interval = \"pred\") %&gt;% \n  data.frame() %&gt;% \n  mutate(fun = \"pred\") %&gt;% \n  bind_rows(\n    ALSM::ci.reg(mod, newdata = x_h, type = \"n\") %&gt;% \n      data.frame %&gt;% \n      mutate(fun = \"ci.reg\") %&gt;% \n      select(fit = Fit, lwr = Lower.Band, upr = Upper.Band, fun)\n  ) %&gt;% \n  bind_cols(bind_rows(x_h, x_h), .)\n\nggplot(data = data_bands_plot2) + \n  geom_line(aes(x = x,\n                y = fit)) + \n  geom_line(aes(x = x,\n                y = lwr, \n                color = fun,\n                linetype = fun)) + \n  geom_line(aes(x = x,\n                y = upr,\n                color = fun,\n                linetype = fun))\n\n\n\n\n\n\n# SAME\n\nSo can just use predict() with dataset spread over range of predictor to form pointwise confidence / prediction bands. Assuming this is still valid because limits of the confidence band for the regression line at any value \\(X_h\\) are not substantially wider than the confidence limits for the mean response at that single \\(X_h\\) level.\n\n# create dataset of both pointwise confidence and prediction limits\nconfs &lt;- predict(mod, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod, newdata = x_h, interval = \"pred\")\nplot(x = data_original$x, y = data_original$y, type = \"p\")\nlines(x = x_h$x, y = confs[,\"fit\"],  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = confs[,\"lwr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = confs[,\"upr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = preds[,\"lwr\"],  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = preds[,\"upr\"],  type = \"l\", col = \"blue\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#analysis-of-variance-approach-to-regression",
    "href": "notes-inference.html#analysis-of-variance-approach-to-regression",
    "title": "2  Inference",
    "section": "\n2.6 Analysis of variance approach to regression",
    "text": "2.6 Analysis of variance approach to regression\n\nThis approach is very useful for multiple linear regression and other types of linear statistical models.\n\n\n2.6.1 Partitioning of total sum of squares\nOverview\n\nIn a regression setting, analysis of variance (ANOVA) allows us to capture the different sources of variability in the model.\nWe do this by partitioning the sums of squares and degrees of freedom associated with the the response variable \\(Y\\).\n\n\nTypes of sum of squares\n\nTotal sum of squares → Measured in terms of the deviations of the \\(Y_i\\) around their mean \\(\\bar{Y}\\).\n\n\\[\nSSTO = \\sum (Y_i - \\bar{Y})^2 = S_{YY}\n\\]\n\nMeasures the total variation of \\(Y\\), which tells us the uncertainty related to \\(Y\\) when the predictor variable \\(X\\) is not taken into account.\nIf all observations are the same, then \\(SSTO = 0\\). More variation in \\(Y_i\\), the larger \\(SSTO\\) is.\nSum of squares error → Measured in terms of the deviations of the \\(Y_i\\) around the fitted regression line \\(\\hat{Y_i}\\) (i.e. the residuals \\(e_i\\)).\n\n\\[\nSSE = \\sum (Y_i - \\hat{Y_i})^2\n\\]\n\nMeasures the remaining variation / uncertainty in the \\(Y_i\\)’s after we utilize the predictor variable \\(X\\) (i.e. “unexplained” variation).\nIf all observations fall on the fitted line, \\(SSE = 0\\). More variation in \\(Y\\) around the fitted line, the larger \\(SSE\\) is.\nSum of squares regression → Measured in terms of the deviations of the fitted \\(\\hat{Y_i}\\) around their mean \\(\\bar{Y}\\).\n\n\\[\nSSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\n\\]\n\nMeasures the variation in the \\(Y_i\\)’s that is associated with the regression line.\nIn other words, it measures the variation in the \\(Y_i\\)’s that is accounted for by the relationship between \\(Y\\) and \\(X\\) (i.e. “explained” variation). We are essentially upgrading our original prediction for \\(Y\\) from \\(\\bar{Y}\\) to now \\(\\hat{Y}\\); so this \\(\\approx\\) (conceptually) measures how much better the predictions become.\nSo, the larger \\(SSR\\) is in relation to \\(SSTO\\), the greater is the effect of the regression relation in accounting for the total variation in the \\(Y_i\\) observations.\n\n\n\nResults\nDerivation\n\n\n\n\nPartitioning individual deviations\n\n\n\n\nThe two components are:\n\nThe deviation of the fitted value \\(\\hat{Y_i}\\) around the mean \\(\\bar{Y}\\).\nThe deviation of the observation \\(Y_i\\) around the fitted regression line.\n\n\nThis relationship holds for the sum of the squared deviations as well:\n\n\\[\n\\begin{align*}\n  \\sum (Y_i - \\bar{Y})^2 &= \\sum (\\hat{Y_i} - \\bar{Y})^2 + \\sum (Y_i - \\hat{Y_i})^2\\\\\n  SSTO &= SSR + SSE\n\\end{align*}\n\\]\n\n\n\nPartitioning total sum of squares\n\n\n\nThe formulas for \\(SSTO\\), \\(SSE\\) and \\(SSR\\) above are best for computations. But an alternate form of \\(SSR\\) is useful for deriving analytical results is shown below:\n\n\\[\n\\begin{align*}\n  SSR &= \\sum [\\hat{Y_i} - \\bar{Y}]^2 \\\\\n      &= \\sum [(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i) - \\bar{Y}]^2 \\\\\n      &= \\sum [(\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 X_i - \\bar{Y}]^2 \\\\\n      &= \\hat{\\beta}_1^2 \\sum [X_i - \\bar{X}]^2\n\\end{align*}\n\\]\n\n\n\n\n2.6.2 Breakdown of degrees of freedom\nDegrees of freedom for each sum of square\n\n\n\\(SSTO\\) → Has \\(n-1\\) degrees of freedom associated with it.\n\nOne degree of freedom is lost because the deviations \\(Y_i - \\bar{Y}\\) are subject to one constraint: they must sum to zero (\\(\\sum (Y_i - \\bar{Y}) = 0\\)).\nEquivalently, one degree of freedom is lost because the sample mean \\(\\bar{Y}\\) is used to estimate the population mean.\n\n\n\n\\(SSE\\) → Has \\(n-2\\) degrees of freedom associated with it.\n\nTwo degrees of freedom are lost because the two parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated in obtaining the fitted values \\(\\hat{Y_i}\\).\n\n\n\n\\(SSR\\) → Has 1 degrees of freedom associated with it.\n\nAlthough there are \\(n\\) deviations \\(\\hat{Y_i} - \\bar{Y}\\), all fitted values are calculated from the same estimated regression line. So the regression line just has two degrees of freedom (corresponding to the slope and intercept, for SLR \\(p = 2\\))…\nBUT then, one degree of freedom is lost because again we have to estimate the mean in order to calculate \\(\\sum (\\hat{Y_i} - \\bar{Y})^2\\), thus \\(p - 1\\) \\(\\Longleftrightarrow\\) the deviations \\(\\hat{Y_i} - \\bar{Y}\\) are subject to one constraint: they must sum to zero.\n\n\n(Note: The constraints on the deviations come from the properties of the fitted LSE line.)\n\nProperty of degrees of freedom\n\nDfs are additive\n\n\\[\n\\begin{align*}\n  n - 1 &= 1 + (n-2) \\\\\n  df_{TO} &= df_{R} +df_{E}\n\\end{align*}\n\\]\n\n2.6.3 Mean squares\n\nGeneral definition → A sum of squares divided by its associated degrees of freedom is called a mean square (\\(MS\\)).\n\nTwo mean squares\n\nMean square regression:\n\n\\[MSR = \\frac{SSR}{df_R} = \\frac{SSR}{1}\\]\n\nMean square error:\n\n\\[MSE = \\frac{SSR}{df_E} = \\frac{SSE}{n-2}\\]\n\nNote → Mean squares are not additive \\(\\Longrightarrow\\) \\(MSR + MSE \\ne MSTO\\)\n\n2.6.4 ANOVA table\n\nThe breakdowns of the total sum of squares and associated degrees of freedom are displayed in the form of an analysis of variance table.\nBelow shows an extra column for expected mean squares, which will be needed for inference (usual ANOVA tables have everything except this column).\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\n\\(df\\)\n\\(SS\\)\n\\(MS\\)\n\\(E(MS)\\)\n\\(F\\)\n\n\\(p\\)-value\n\n\n\nRegression\n\\(df_R = 1\\)\n\n\\(SSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\\) 1\n\\(MSR = \\frac{SSR}{1}\\)\n\\(\\sigma^2 + \\beta_1 \\sum (X_i - \\bar{X})^2\\)\n\\(\\frac{MSR}{MSE}\\)\n\\(P(F_{(df_R, df_E)} &gt; F)\\)\n\n\nError\n\\(df_E = n - 2\\)\n\\(SSE = \\sum (Y_i - \\hat{Y_i})^2\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\\(\\sigma^2\\)\n\n\n\n\nTotal\n\\(df_{TO} = n -1\\)\n\\(SSTO = \\sum (Y_i - \\bar{Y})^2\\)\n\n\n\n\n\n\n\n\nCould convert \\(SSTO = S_{YY}\\) to a mean square as well by dividing by \\(df_{TO} = n - 1\\) → This gives us \\(\\frac{1}{n - 1}{\\sum (Y_i - \\bar{Y})^2}\\) = Sample variance of \\(Y_i\\) (totally unrelated to the regression line; just the variance of a set of numbers, which is technically a mean square).\nThis is different than the regression \\(S^2 = MSE\\) which is an estimate of the error variance which is found by taking the deviations relative to the fitted line (not \\(bar{Y}\\)).\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 3; sigma &lt;- 15 \n\n# generate data\nx &lt;- runif(n = n, min = 5, max = 15)\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# now introducing another function to fit models and the different results / outputs from each\n\n# fit equivalent models\n# -&gt; lm() fits linear models\n# --&gt; also one-way anova (ancova) models, but aov() works better with anova analyses\nmod_lm &lt;- lm(y ~ x)\nmod_aov &lt;- aov(y ~ x)\n\n# the main difference between lm() and aov() is mainly in the form of the output\n\n# calling (printing) the model object\n# -&gt; for lm object gives estimated coefficients\nmod_lm\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      8.039        2.292  \n\n# -&gt; for aov object it gives breakdown of SS and df for each variable and sigma estimate\nmod_aov\n\nCall:\n   aov(formula = y ~ x)\n\nTerms:\n                       x Residuals\nSum of Squares   835.060  5926.886\nDeg. of Freedom        1        28\n\nResidual standard error: 14.54904\nEstimated effects may be unbalanced\n\n# summary() function\n# -&gt; for lm it gives regression-style output, i.e. regression coefficients with standard errors and t-tests\nsummary(mod_lm)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.3369  -9.5934   0.0609   7.8225  28.7457 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    8.039     10.952   0.734   0.4691  \nx              2.292      1.154   1.986   0.0569 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.55 on 28 degrees of freedom\nMultiple R-squared:  0.1235,    Adjusted R-squared:  0.09219 \nF-statistic: 3.945 on 1 and 28 DF,  p-value: 0.05688\n\n# -&gt; for aov it gives anova table,which is the same information but represented as sums of squares estimates with F ratios\nsummary(mod_aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nx            1    835   835.1   3.945 0.0569 .\nResiduals   28   5927   211.7                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# can switch between the two summaries by calling a summary method\n# -&gt; get anova table from lm object\nsummary.aov(mod_lm)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nx            1    835   835.1   3.945 0.0569 .\nResiduals   28   5927   211.7                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# -&gt; get regression-style output from aov object\nsummary.lm(mod_aov)\n\n\nCall:\naov(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.3369  -9.5934   0.0609   7.8225  28.7457 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    8.039     10.952   0.734   0.4691  \nx              2.292      1.154   1.986   0.0569 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.55 on 28 degrees of freedom\nMultiple R-squared:  0.1235,    Adjusted R-squared:  0.09219 \nF-statistic: 3.945 on 1 and 28 DF,  p-value: 0.05688\n\n# alternative (more straight-forward) way to get anova table \n# -&gt; use anova() on lm object &lt;==&gt; anova.lm() -&gt; this is one of the two main uses of this function\n# -&gt; looking for Df, Sum Sq, and Mean Sq\n# --&gt; gives sequential SS (will look into more later)\nanova(mod_lm) %&gt;% as.matrix # could do anova(mod_aov), but makes for sense just to do summary() if already have aov object\n\n          Df    Sum Sq  Mean Sq  F value     Pr(&gt;F)\nx          1  835.0602 835.0602 3.945021 0.05687836\nResiduals 28 5926.8857 211.6745       NA         NA\n\n# save usual lm model as another object to simplify notation\nmod &lt;- mod_lm\n\n\n\n\n# recreating each value in the anova table output\n\n# degrees of freedom \n# -&gt; regression: each individual coefficient gets 1 df, then lose 1; so for SLR df = 2 - 1 = 1\n# -&gt; error: df = n - # of predictors (p) as usual\ndf_e &lt;- mod$df.residual\ndf_r &lt;- length(coef(mod)) - 1\n\n# sums of squares\n# -&gt; SSR = explained error (improved prediction) -&gt; Y-hat - Y-bar\n# -&gt; SSE = still unexplained error (residuals) -&gt; Y - Y-hat\nssr &lt;- sum((fitted(mod) - mean(y))^2)\nsse &lt;- sum((y - fitted(mod))^2)\n\n# mean squares = respective SS / df\nmsr &lt;- ssr / df_r\nmse &lt;- sse / df_e\n\n# combine (organize) into anova table layout for comparison\n# -&gt; fill matrix by column\nat &lt;- matrix(data = c(c(df_r, df_e), c(ssr, sse), c(msr, mse)), nrow = 2, byrow = FALSE)\ncolnames(at) &lt;- c(\"df\", \"SS\", \"MS\")\nrownames(at) &lt;- c(\"x\", \"error\")\n\n# compare results to relevant pieces of anova(lm())\ncompare(anova(mod_lm)[,1:3], at)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Modes: list, numeric\"                                                \n[2] \"Lengths: 3, 6\"                                                       \n[3] \"Attributes: &lt; Names: 2 string mismatches &gt;\"                          \n[4] \"Attributes: &lt; Component 1: Modes: character, numeric &gt;\"              \n[5] \"Attributes: &lt; Component 1: target is character, current is numeric &gt;\"\n[6] \"Attributes: &lt; Component 2: Modes: character, list &gt;\"                 \n[7] \"Attributes: &lt; Component 2: target is character, current is list &gt;\"   \n[8] \"current is not list-like\"                                            \n\n$comparison$`element-wise`\n       Df Sum Sq Mean Sq\n[1,] TRUE   TRUE    TRUE\n[2,] TRUE   TRUE    TRUE\n\n\n$`anova(mod_lm)[, 1:3]`\n          Df Sum Sq Mean Sq\nx          1  835.1  835.06\nResiduals 28 5926.9  211.67\n\n$at\n      df        SS       MS\nx      1  835.0602 835.0602\nerror 28 5926.8857 211.6745\n\n\n\n\n\n# demonstrate additive df and SS\n\n# df total = n - 1 (have to estimate pop mean, so lose 1)\ndf_to &lt;- length(y) - 1\n\n# SSTO = total deviation (S_YY) -&gt; Y - Y-bar\nssto &lt;- sum((y - mean(y))^2)\n\n# compare pieces from anova to calculated totals\ncompare(c(sum(anova(mod)[, \"Df\"]), sum(anova(mod)[, \"Sum Sq\"])), c(df_to, ssto))\n\n$comparison\n[1] TRUE\n\n$`c(sum(anova(mod)[, \"Df\"]), sum(anova(mod)[, \"Sum Sq\"]))`\n[1]   29.000 6761.946\n\n$`c(df_to, ssto)`\n[1]   29.000 6761.946\n\n\n\n\n\n\n2.6.5 Expected mean squares\nGoal → In order to make inferences based on the analysis of variance approach, we need to know the expected value of each of the mean squares.\n\n\nResults\nDerivation\n\n\n\n\nThe expected value of a mean square is the mean of its sampling distribution and tells us what is being estimated by the mean square.\nIt can be shown that:\n\n\\[\n\\begin{align*}\n  E(MSE) &= \\sigma^2 \\\\\n  E(MSR) &= \\sigma^2 + \\beta_1^2 \\sum (X_i - \\bar{X})^2\n\\end{align*}\n\\]\n\nNote that the first result goes with earlier statement that \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nImportant implications\n\nThe mean of the sampling distribution of \\(MSE\\) is \\(\\sigma^2\\) whether or not \\(Y\\) and \\(X\\) are linearly related (i.e. whether or not \\(\\beta_1 = 0\\)).\nThe mean of the sampling distribution of \\(MSR\\) is also \\(\\sigma^2\\) when \\(\\beta_1 = 0\\).\n\n\nThus, when \\(\\beta_1 = 0\\), the sampling distributions of \\(MSE\\) and \\(MSR\\) are located identically and \\(MSE\\) and \\(MSR\\) will tend to be relatively close to each other.\nBut, when \\(\\beta_1 \\ne 0\\), the mean of the sampling distribution of \\(MSR\\) will be greater than \\(\\sigma^2\\) (because \\(\\beta_1^2 \\sum (X_i - \\bar{X})^2\\) then must be positive) and therefore located to the right of that of \\(MSE\\). So, \\(MSR\\) will tend to be larger than \\(MSE\\).\nThese results suggest that a comparison of \\(MSR\\) and \\(MSE\\) is useful for testing whether or not \\(\\beta_1 = 0\\).",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#f-test-of-beta_1-0-vs-beta_1-ne-0",
    "href": "notes-inference.html#f-test-of-beta_1-0-vs-beta_1-ne-0",
    "title": "2  Inference",
    "section": "\n2.7 \\(F\\) test of \\(\\beta_1 = 0\\) vs \\(\\beta_1 \\ne 0\\)\n",
    "text": "2.7 \\(F\\) test of \\(\\beta_1 = 0\\) vs \\(\\beta_1 \\ne 0\\)\n\nOverview\n\nThe analysis of variance approach lets us perform very useful test for regression models (and other linear statistical models).\n\n\n\nResults\nDerivation\n\n\n\nTest on slope\n\nHypotheses → For the SLR, ANOVA gives us a test for:\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\nTest statistic → For ANOVA, the test statistic \\(F^*\\) compares \\(MSR\\) and \\(MSE\\)\n\n\n\\[\nTS = F^* = \\frac{MSR}{MSE}\n\\]\n\n(see derivation) Under \\(H_0: \\beta_1 = 0\\) → \\(F^* \\sim \\text{F}\\,_{(1, n-2)}\\)\n\nRejection region and p-value\n\n\n\\(F^*\\) values near 1 support \\(H_0\\) and large \\(F^*\\) values support \\(H_A\\) (if the model is useful, we expect \\(MSR\\) to be large compared to \\(MSE\\)) \\(\\Longrightarrow\\) Upper-tailed test.\n\n\n\n\\[\n\\begin{align*}\n  RR &= \\{F^* &gt; F_{(1 - \\alpha;\\, 1, n - 2)}\\} \\\\\n  p\\text{-value} &= P(F_{(1, n-2)} \\ge F^*)\n\\end{align*}\n\\]\n\nNote → \\(F_{(1-\\alpha;\\,1,n-2)}\\) is the \\(100 (1-\\alpha)\\) percentile of the appropriate \\(F\\) distribution (different notation meaning than \\(t_{\\alpha / 2}\\) because not a symmetric distribution now).\n\nDecision → Same rules as usual, now just with \\(F\\)-distribution\n\nReject \\(H_0\\) and conclude \\(H_A\\) if \\(\\hspace{10pt}\\) \\(TS \\in RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} \\le \\alpha\\); Fail to reject \\(H_0\\) if previous not true.\n\n\n\nConclusion / Interpretation\n\nAt the \\(\\alpha\\) significance level, we &lt; have / do not have &gt; sufficient evidence of a significant linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;.\n\n\n\nEquivalence of \\(F\\) test and \\(t\\) test\n\n\nThe \\(F\\)-test is algebraically equivalent to the two-tailed \\(t\\) test → \\(F^* = (t^*)^2\\)\n\nSame relationship for the critical values when defining the rejection region / p-value → \\(F_{(1-\\alpha; \\, 1, n-2)} = t_{\\alpha/2, n-2}^2\\).\n\n\nWill get same conclusion either way, but the \\(t\\) test is more flexible because it can be used for one-sided alternatives involving \\(\\beta_1\\), while the \\(F\\) test cannot.\n\n\n\nSampling distribution of \\(F^*\\)\n\nGoal → In order to be able to construct a statistical decision rule and examine its properties, we need to know the sampling distribution of \\(F^*\\).\n\nDerivation → Start by considering the sampling distribution of \\(F^*\\) under \\(H_0: \\beta_1 = 0\\). We will use the following theorem:\n\n\nCochran’s theorem → Let \\(Y_1, \\ldots, Y_{n}\\) represent a random sample from the same normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Suppose \\(SSTO = \\sum (Y_i - \\bar{Y})^2\\) is partitioned into \\(k\\) sums of squares \\(SS_r\\), each with degrees of freedom \\(df_r\\). If \\(\\displaystyle \\sum_{r=1}^k df_{r} = n - 1\\), then each of the \\(\\frac{SS_r}{\\sigma^2}\\) terms are independent \\(\\chi^2\\) random variables with \\(df_r\\) degrees of freedom.\n\n\n\n\n\n\nUnder \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\\(F^* \\sim \\text{Non-central F}\\,_{(1, n - 2)}\\) with non-centrality parameter \\(\\lambda\\).\nStill \\(SSR \\perp \\!\\!\\! \\perp SSE\\) and \\(SSE / \\sigma^2 \\sim \\chi^2_{n-2}\\). But the condition that both \\(SSR / \\sigma^2\\) and \\(SSE / \\sigma^2\\) are \\(\\chi^2\\) random variables requires \\(\\beta_1 = 0\\).\n\n\n\nEquivalence of \\(F\\) test and \\(t\\) test\n\n\n\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# continuing previous anova table demo\n\n# show anova table\n# -&gt; looking for F value` and Pr(&gt;F)\nanova(mod) %&gt;% as.matrix\n\n          Df    Sum Sq  Mean Sq  F value     Pr(&gt;F)\nx          1  835.0602 835.0602 3.945021 0.05687836\nResiduals 28 5926.8857 211.6745       NA         NA\n\n# equivalence of F test and two sided t-test\ncompare(anova(mod)[\"x\",\"F value\"], summary(mod)$coefficients[\"x\",\"t value\"]^2)\n\n$comparison\n[1] TRUE\n\n$`anova(mod)[\"x\", \"F value\"]`\n[1] 3.945021\n\n$`summary(mod)$coefficients[\"x\", \"t value\"]^2`\n[1] 3.945021\n\n\n\n\n\n# continuing to recreate each value in the anova table output\n\n# calculate F and p-value\n# -&gt; TS F* = MSR / MSE\n# -&gt; p-value = P(F(df reg, df error) &gt; F*)\nF_star &lt;- msr / mse\np_value &lt;- pf(q = F_star, df1 = df_r, df2 = df_e, lower.tail = FALSE)\n\n# compare results to relevant pieces of anova(lm())\ncompare(anova(mod_lm)[1,4:5], c(F_star, p_value))\n\nError in `[.data.frame`(tmp_item_2, , i): undefined columns selected",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#general-linear-test-approach",
    "href": "notes-inference.html#general-linear-test-approach",
    "title": "2  Inference",
    "section": "\n2.8 General linear test approach",
    "text": "2.8 General linear test approach\nOverview\n\n\nThe ANOVA \\(F\\) test above is an example of a General Linear Test (GLT) (also called a global or an omnibus test) for a statistical model, which is an approach that can be used for highly complex tests of linear statistical models, as well as for simple tests.\n\nFor SLR, the global test (the significance of a model test), the ANVOA \\(F\\) test, and the \\(t\\) test for the linear impact are all equivalent.\n\n\n\nIt has three basic steps, which are described in more detail below:\n\nFit the full model and obtain the error sum of squares \\(SSE(F)\\).\nFit the reduced model under Ho and obtain the error sum of squares \\(SSE(R)\\).\nUse test statistic / p-value to make decision.\n\n\n\nFull model\n\n\nStart with the model considered to be appropriate for the data (or the model with all available predictors); this is called the full / unrestricted model.\n\nFor SLR, the full model is just the normal error regression model:\n\n\n\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{Full model}\n\\]\n\nWe need to fit the full model and get the error sum of squares, denoted \\(SSE(F)\\) (deviations of \\(Y_i\\) and its estimated expected value \\(\\hat{Y_i}\\), which is the fitted regression line). For the full model, we have:\n\n\\[\nSSE(F) = \\sum (Y_i - \\hat{Y_i})^2 = \\sum [Y_i - (\\hat{\\beta}_0  + \\hat{\\beta}_1 X_i)]^2 = SSE\n\\]\n\n\nWith a SLR full model, the error sum of squares is the usual \\(SSE\\).\n\nReduced model\n\nNext we consider \\(H_0\\). For SLR, we have:\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\hspace{20pt} \\text{Reduced model is appropriate} \\\\\n  H_A &: \\beta_1 \\ne 0 \\hspace{20pt} \\text{Full model is appropriate}\n\\end{align*}\n\\]\n\nThe model under \\(H_0\\) is called the reduced / restricted model. When \\(\\beta_1 = 0\\), the full model reduces to:\n\n\\[\nY_i = \\beta_0 + 0 \\cdot X_i + \\epsilon_i = \\beta_0 + \\epsilon_i \\hspace{20pt} \\text{Reduced model}\n\\]\n\n\nThen we fit the reduced model and again get the error sum of squares, now denoted \\(SSE(R)\\).\n\n\nFor this particular (a SLR) reduced model, it can easily be shown that the LSE and MLE estimator of \\(\\beta_0\\) is \\(\\bar{Y}\\). Thus the estimated expected value of each observation is \\(\\hat{\\beta}_0 = \\bar{Y}\\) and we can get the error sum of squares with:\n\n\n\n\\[\nSSE(R) = \\sum (Y_i - \\hat{Y_i})^2 = \\sum (Y_i - \\hat{\\beta}_0)^2 = \\sum (Y_i - \\bar{Y})^2 = SSTO\n\\]\n\nThus, for any intercept-only reducted model \\(\\Longrightarrow\\) \\(SSE(R) = SSTO\\).\n\nTest statistic and decision\n\n\nLogic → Now we compare the two sum of squares from the full and reduced model using the fact that the reduced \\(SSE\\) is always greater than or equal to the full \\(SSE\\).\n\n\n\\(SSE(R) \\ge SSE(F)\\) → More parameters in the model ALWAYS leads to a better fit (i.e. less unexplained variability = more explained variability) \\(\\Longrightarrow\\) Smaller are the deviations around the fitted regression function.\n\n\n\nComparison scenarios\n\n\n\\(SSE(F)\\) are close \\(SSE(R)\\)\n\n\n\nUsing the full model does not account for much more unexplained variability than does the reduced model \\(\\Longrightarrow\\) Added parameters in the full model do not really help to reduce the unexplained variation \\(\\Longrightarrow\\) Reduced model is adequate and \\(H_0\\) holds.\n\n\n\\(SSE(R) &lt;&lt; SSE(F)\\)\n\n\nThe additional parameters in the full model do help to substantially reduce the unexplained variability in \\(Y_i\\), which means \\(H_A\\) holds.\n\n\nTest statistic is a function of the difference in two \\(SSE\\)s, relative to the full \\(SSE\\):\n\n\\[\nTS = F^* = \\frac{SSE(R) - SSE(F)}{df_R - df_F} \\Big/ \\frac{SSE(F)}{df_F} = \\frac{SSE(R) - SSE(F)}{df_R - df_F}  \\Big/ MSE(F)\n\\tag{2.1}\\]\n\nUnder \\(H_0: \\beta_1 = 0 \\hspace{10pt} \\text{Reduced model}\\) → \\(F^* \\sim \\text{F}\\,_{(df_R - df_F, df_F)}\\)\n\nRejection region and p-value\n\nAgain, we reject for large values of \\(F^*\\) (large difference \\(SSE(R) - SSE(F)\\) supports \\(H_A\\)).\n\n\n\n\\[\n\\begin{align*}\n  RR &= \\{F^* &gt; F_{(1 - \\alpha;\\, df_R - df_F, \\, df_F)}\\} \\\\\n  p\\text{-value} &= P(F_{(df_R - df_F, \\, df_F} \\ge F^*)\n\\end{align*}\n\\]\n\nFor SLR (testing whether or not \\(\\beta_1 = 0\\)), we have:\n\n\\[\n\\begin{align*}\n  SSE(R) &= SSTO \\hspace{20pt} SSE(F) = SSE \\\\\n  f_R &= n - 1 \\hspace{45pt} df_F = n - 2\n\\end{align*}\n\\]\n\nSo the test statistic becomes\n\n\\[\nF^* = \\frac{SSTO - SSE}{(n - 1) - (n - 2)} \\Big/ \\frac{SSE}{n - 2} = \\frac{SSR}{1} \\Big/ \\frac{SSE}{n - 2} = \\frac{MSR}{MSE}\n\\]\n\nThis is equivalent to the ANOVA \\(F\\) test shown earlier.\n\n\n\nR functions\nManual\n\n\n\n\n# using the same data as anova table demos\n\n# fit full model\n# -&gt; for SLR, full model is E(Y) = B0 + B1 X\nmod_full &lt;- lm(y ~ x)\n\n# fit reduced model\n# -&gt; for SLR, full model is E(Y) = B0\n# -&gt; to fit an intercept-only model, specify 1 on the RHS\nmod_reduced &lt;- lm(y ~ 1)\n\n# perform general linear test of reduced vs full model\n# -&gt; testing H0: beta_1 = 0 (reduced model) vs HA: beta_1 != 0 (full model)\n# function call\n# -&gt; anova() on multiple lm objects &lt;==&gt; anova.lmlist() -&gt; this is the second main uses of this function\n# -&gt; typically will supply models smallest to largest (this makes for a natural interpretation of the results); but it works regardless\n# -&gt; models MUST BE nested AND fit on the same dataset in order for results to make statistical sense\n# results\n# -&gt; sequentially gives the change in dfs and SS from mod 1 to mod 2 (then from mod 2 to mod 3, and so on...)\n# -&gt; so it is df_1 - df_2 and SS_1 - SS_2 ==&gt; if nested from smallest to largest model, results will all be positive and interpreted as \"additional reductions\"\n# -&gt; by default, performs F test comparing models in the order specified (again sequentially)\nanova(mod_reduced, mod_full, test = \"F\") %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq        F     Pr(&gt;F)\n1     29 6761.946 NA        NA       NA         NA\n2     28 5926.886  1  835.0602 3.945021 0.05687836\n\n# demo with more than two models\nmod_squared &lt;- lm(y ~ x + I(x^2))\nanova(mod_reduced, mod_full, mod_squared) %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq         F     Pr(&gt;F)\n1     29 6761.946 NA        NA        NA         NA\n2     28 5926.886  1  835.0602 3.8699197 0.05951207\n3     27 5826.123  1  100.7630 0.4669662 0.50020874\n\n# demo showing how the sequential comparison works\n# -&gt; above compared mod 1 to mod 2 and then mod 2 to mod 3\n# -&gt; now comparing mod 1 directly to mod 3\nanova(mod_reduced, mod_squared) %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq        F    Pr(&gt;F)\n1     29 6761.946 NA        NA       NA        NA\n2     27 5826.123  2  935.8233 2.168443 0.1338619\n\n\n\n\n\n# general linear test of reduced vs full model\n\n# using the same data as anova table demos\n\n# for full model -&gt;  get the corresponding SSE and df \n# -&gt; for SLR, full model is E(Y) = B0 + B1 X\nsse_f &lt;- resid(mod_full)^2 %&gt;% sum\ndf_f &lt;- df.residual(mod_full)\n\n# for reduced model -&gt; get the corresponding SSE and df\nsse_r &lt;- resid(mod_reduced)^2 %&gt;% sum\ndf_r &lt;- df.residual(mod_reduced)\n\n# calculate F stat and p-value\n# -&gt; using the shortcut for second term: F* = [(SSE(R) - SSE(F)) / (df_R - df_F)] / MSE(F)\nmse_f &lt;- summary(mod_full)$sigma^2\nF_star &lt;- ((sse_r - sse_f) / (df_r - df_f)) / mse_f\np_value &lt;- pf(q = F_star, df1 = df_r - df_f, df2 = df_f, lower.tail = FALSE)\n\n# combine (organize) into anova table layout for comparison\nat &lt;- matrix(data = c(c(df_r, df_f), c(sse_r, sse_f), c(NA, df_r - df_f), c(NA, sse_r - sse_f), c(NA, F_star), c(NA, p_value)), nrow = 2, byrow = FALSE)\ncolnames(at) &lt;- c(\"df_E Original\", \"SSE Original\", \"Change df\", \"Change SSE\", \"F*\", \"p-value\")\nrownames(at) &lt;- c(\"Mod 1 - Reduced\", \"Mod 2 - Full\")\n\n# compare to results from anova(reduced mod, full model)\ncompare(anova(mod_reduced, mod_full), at)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Modes: list, numeric\"                                                \n[2] \"Lengths: 6, 12\"                                                      \n[3] \"Attributes: &lt; Names: 2 string mismatches &gt;\"                          \n[4] \"Attributes: &lt; Length mismatch: comparison on first 2 components &gt;\"   \n[5] \"Attributes: &lt; Component 1: Modes: character, numeric &gt;\"              \n[6] \"Attributes: &lt; Component 1: target is character, current is numeric &gt;\"\n[7] \"Attributes: &lt; Component 2: Modes: character, list &gt;\"                 \n[8] \"Attributes: &lt; Component 2: target is character, current is list &gt;\"   \n[9] \"current is not list-like\"                                            \n\n$comparison$`element-wise`\n     Res.Df  RSS   Df Sum of Sq    F Pr(&gt;F)\n[1,]   TRUE TRUE   NA        NA   NA     NA\n[2,]   TRUE TRUE TRUE      TRUE TRUE   TRUE\n\n\n$`anova(mod_reduced, mod_full)`\nAnalysis of Variance Table\n\nModel 1: y ~ 1\nModel 2: y ~ x\n  Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)  \n1     29 6761.9                             \n2     28 5926.9  1    835.06 3.945 0.05688 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$at\n                df_E Original SSE Original Change df Change SSE       F*\nMod 1 - Reduced            29     6761.946        NA         NA       NA\nMod 2 - Full               28     5926.886         1   835.0602 3.945021\n                   p-value\nMod 1 - Reduced         NA\nMod 2 - Full    0.05687836\n\n# for SLR ==&gt; equivalent to F test on beta 1\ncompare(anova(mod_reduced, mod_full)[2, \"F\"], anova(mod_full)[\"x\", \"F value\"])\n\n$comparison\n[1] TRUE\n\n$`anova(mod_reduced, mod_full)[2, \"F\"]`\n[1] 3.945021\n\n$`anova(mod_full)[\"x\", \"F value\"]`\n[1] 3.945021",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#descriptive-measures-of-linear-association-between-x-and-y",
    "href": "notes-inference.html#descriptive-measures-of-linear-association-between-x-and-y",
    "title": "2  Inference",
    "section": "\n2.9 Descriptive measures of linear association between \\(X\\) and \\(Y\\)\n",
    "text": "2.9 Descriptive measures of linear association between \\(X\\) and \\(Y\\)\n\nOverview\n\nThere is no one single measure to completely describe the usefulness of a regression model for a particular application.\nIf the goal is estimation of parameters and means and predicting new observations, usefulness of estimates or predictions depends upon the width of the interval and the user’s needs for precision. This can vary from one application to another.\nRather than making inferences, goals could be to describe the degree of linear association between \\(Y\\) and \\(X\\). Again the usefulness of which measure and its value depend on the application.\n\n\n2.9.1 Coefficient of determination\nOverview\n\nA very common measure because of its simplicity is the coefficient of determination \\(R^2\\), which is a measure of the effect of \\(X\\) in reducing the uncertainty in predicting \\(Y\\). This reduction in sum of squares (\\(SSTO - SSE = SSR\\)) gets expressed as a proportion:\n\n\\[\nR^2 = \\frac{SSR}{SSTO} = 1-\\frac{SSE}{SSTO}, \\hspace{20pt} \\text{range:} \\hspace{10pt} 0 \\le R^2 \\le 1\n\\]\nInterpretation\n\n\n&lt; \\(R^2 *100\\) &gt;% of the variation in &lt; \\(Y\\) context &gt; can be explained by the linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;.\n\nSo, the larger \\(R^2\\) is, the more the total variation of \\(Y\\) is reduced by introducing the predictor variable \\(X\\) \\(\\Longleftrightarrow\\) greater degree of linear association between \\(Y\\) and \\(X\\).\n\n\nPractically, this indicates the quality of the fit by measuring the proportion of variability explained by the fitted model.\n\nLimiting values\n\nWhen the fitted regression line is horizontal (\\(\\hat{\\beta}_1 = 0\\) and \\(\\hat{Y_i} = \\bar{Y}\\)) \\(\\Longrightarrow\\) \\(SSE = SSTO\\) and \\(R^2 = 0\\).\nWhen there is a perfect fit (all of the points lie on the fitted regression line) \\(\\Longrightarrow\\) \\(SSE = 0\\) and \\(R^2 = 1\\).\nIn practice, unlikely to be exactly equal to either of these. Also note that context of data (scientific field of the application) has a big impact on general values of \\(R^2\\) and consequently what is interpreted as “strong”.\nIf \\(R^2\\) is small, we can consider adding other independent variables that can explain a significant portion of the remaining unexplained variability in the model.\n\n\n\nLimitations of \\(R^2\\)\n\n\nUsefulness in prediction → A high coefficient of determination does not necessarily indicate that useful (precise) predictions can be made.\n\nThis is because \\(R^2\\) measures only a relative reduction from \\(SSTO\\) and provides no information about absolute precision for estimating a mean response or predicting a new observation.\n\n\n\nQuality of fit → A high coefficient of determination does not necessarily indicate that the estimated regression line is a good fit and similarly an \\(R^2\\) near zero does not necessarily indicate that \\(Y\\) and \\(X\\) are not related.\n\nThis is because \\(R^2\\) measures the degree of linear association between \\(Y\\) and \\(X\\), whereas the actual regression relation may be curvilinear. So make sure to look at the scatterplot.\n\n\n\n\nInflating \\(R^2\\)\n\n\n\\(X\\) level spacing → The value taken by \\(R^2\\) in a given sample tends to be affected by the spacing of the \\(X\\) observations: Wider spacing in \\(X_i\\) \\(\\Longrightarrow\\) higher \\(R^2\\). Here’s why:\n\nWider spacing (larger spread) in \\(X_i\\) in the sample when \\(\\hat{\\beta}_1 \\ne 0\\) \\(\\Longrightarrow\\) larger spread of the observed \\(Y_i\\) around \\(\\bar{Y}\\) \\(\\Longrightarrow\\) Larger \\(SSTO\\).\nAnd since \\(SSE\\) is unaffected (\\(V(Y_i) = \\sigma^2\\) for all \\(X_i\\)), \\(SSR\\) has to increase. Then we can see from either representation \\(R_2 = SSR/ SSTO = 1 - SSE / SSTO\\), that \\(R^2\\) will increase.\n\nOverfitting → \\(R^2\\) can be artificially inflated by including additional model terms (adding extra predictors).\n\nThis is because \\(SSR\\) always increases with more predictors, even if they are completely unrelated to the response variable.\nLater \\(R^2_{adj}\\) will be discussed which corrects for the inclusion of extra predictors.\n\n\n\n\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 5; sigma &lt;- 10\n\n# generate data\nx &lt;- runif(n = n, min = 5, max = 15)\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# display summary\n# -&gt; looking for Multiple R-squared\nsummary(mod)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.789  -7.255   1.494   6.289  19.406 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)  -1.5916     6.4776  -0.246         0.808    \nx             5.2781     0.5968   8.844 0.00000000135 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.74 on 28 degrees of freedom\nMultiple R-squared:  0.7364,    Adjusted R-squared:  0.727 \nF-statistic: 78.21 on 1 and 28 DF,  p-value: 0.000000001348\n\n\n\n# plot x and y with regression line\n# -&gt; add annotation for R^2 value\nplot(x = x, y = y)\nabline(mod, col = \"red\")\ntext(x = 14, y = 30, labels = bquote(R^2 * \" = \" * .(round(summary(mod)$r.squared,3))), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n# get anova table for model\nat &lt;- anova(mod)\nas.matrix(at)\n\n          Df   Sum Sq    Mean Sq  F value           Pr(&gt;F)\nx          1 7420.348 7420.34776 78.21394 0.00000000134759\nResiduals 28 2656.429   94.87245       NA               NA\n\n# calculate R^2 from sums of squares from anova table\n# R^2 = SSR / SSTO = SSR / (SSR + SSE) = 1 - SSE / SSTO\nssr &lt;- at[\"x\", \"Sum Sq\"]\nsse &lt;- at[\"Residuals\", \"Sum Sq\"]\nr_squared &lt;- ssr / (ssr + sse)\nr_squared &lt;- 1 - sse / (ssr + sse)\n\n# compare results from summary(lm())\ncompare(summary(mod)$r.squared, r_squared)\n\n$comparison\n[1] TRUE\n\n$`summary(mod)$r.squared`\n[1] 0.7363811\n\n$r_squared\n[1] 0.7363811\n\n\n\n\n\n# demo to show how more spacing of X levels increases R^2 value\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 3; sigma &lt;- 5 \n\n# generate two datasets\n# -&gt; generate common error terms, so the only difference is the spacing of the X levels\n# -&gt; generate two X vectors, one with larger spread of X values\n# -&gt; calculate Y\n# -&gt; add indicator for spread and then combine into one dataframe\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\ndata_samples &lt;- list(runif(n = n, min = 8, max = 12),\n                     runif(n = n, min = 2, max = 16)) %&gt;% \n  map(\\(x) data.frame(x = x,\n                      y = beta_0 + beta_1 * x + epsilon)) %&gt;% \n  map2(c(\"smaller\", \"larger\"), \\(df, spread) mutate(df, spread = spread)) %&gt;% \n  bind_rows\n\n# confirm same error terms\n# -&gt; calculate epsilon as Y - E(Y) and then add observation number column to sort by\ndata_samples %&gt;% \n  mutate(epsilon = y - (beta_0 + beta_1 * x)) %&gt;% \n  mutate(observation = 1:n, .by = spread) %&gt;% \n  arrange(observation) %&gt;% \n  head(n = 6) %&gt;% \n  display_nice\n\n\n\n x \n    y \n    spread \n    epsilon \n    observation \n  \n\n\n 11.965 \n    47.535 \n    smaller \n    9.640 \n    1 \n  \n\n 2.603 \n    19.449 \n    larger \n    9.640 \n    1 \n  \n\n 9.329 \n    25.828 \n    smaller \n    -4.160 \n    2 \n  \n\n 12.753 \n    36.099 \n    larger \n    -4.160 \n    2 \n  \n\n 11.905 \n    38.171 \n    smaller \n    0.456 \n    3 \n  \n\n 2.650 \n    10.406 \n    larger \n    0.456 \n    3 \n  \n\n\n\n# calculate R^2 for each spread dataset\ndata_samples %&gt;% \n  split(.$spread) %&gt;% \n  map_dbl(\\(df) summary(lm(y ~ x, data = df))$r.squared) %&gt;% \n  round(3)\n\n larger smaller \n  0.831   0.380 \n\n# create plot of both sets of points and regression line\n# -&gt; add reference line for Y-bar for each dataset (to show how SSTO is calculated)\nggplot(aes(x = x,\n           y = y,\n           color = spread),\n       data = data_samples) + \n  geom_point() + \n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE,\n              fullrange = TRUE) + \n  geom_hline(aes(yintercept = y_bar,\n                 color = spread),\n             data = summarize(data_samples, y_bar = mean(y), .by = spread),\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n2.9.2 Coefficient of correlation\n\nA measure of linear association between \\(Y\\) and \\(X\\) when both \\(Y\\) and \\(X\\) are random is the coefficient of correlation. This is the signed square root of \\(R^2\\):\n\n\\[\nr = \\pm \\sqrt{R^2}\n\\]\n\nA plus or minus sign is attached to this measure according to whether the slope of the fitted regression line is positive or negative. Thus, \\(-1 \\le r \\le 1\\).\nONLY in SLR can the coefficient of determination \\(R^2\\) be computed as the square of the correlation coefficient \\(r^2\\).\n\n\n# continuing previous example\n\n# calculate correlation\nr &lt;- cor(x, y)\n\n# compare to squared-correlation to R^2\ncompare(summary(mod)$r.squared, r^2)\n\n$comparison\n[1] TRUE\n\n$`summary(mod)$r.squared`\n[1] 0.7363811\n\n$`r^2`\n[1] 0.7363811\n\n\n\nNote → Regression models do not contain a parameter to be estimated by \\(R^2\\) or \\(r\\). These are simply descriptive measures of the degree of linear association between \\(Y\\) and \\(X\\) in the sample observations that may, or may not, be useful in any instance.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#considerations-in-applying-regression-analysis",
    "href": "notes-inference.html#considerations-in-applying-regression-analysis",
    "title": "2  Inference",
    "section": "\n2.10 Considerations in applying regression analysis",
    "text": "2.10 Considerations in applying regression analysis\nReminders when implementing regression models\n\nFrequently, regression analysis is used to make inferences for the future (e.g. a school board wants to predict future enrollments by using a regression model containing several demographic variables as predictor variables). In these situations, the validity of the regression application depends on whether basic conditions in the future will be similar to those at the time the regression analysis is based on.\nIn predicting new observations on \\(Y\\), the predictor variable \\(X\\) itself often has to be predicted. Therefore, predictions are dependent upon the correctness of the population projection (i.e. they are conditional predictions).\nBe careful of extrapolation. We cannot be sure that the regression function that fits the past data is appropriate over a wider range of the predictor variable.\nA statistical test that concludes \\(\\beta_1 \\ne 0\\) does not establish a cause-and-effect relation between the predictor and response variables. With nonexperimental data, both the \\(X\\) and \\(Y\\) variables may be simultaneously influenced by other variables not in the regression model. On the other hand, the existence of a regression relation in controlled experiments is often good evidence of a cause-and-effect relation.\nWe frequently wish to estimate several mean responses or predict several new observations for different levels of the predictor variable; this causes some special problems to arise. The confidence coefficients for the limits for estimating a mean response and for the prediction limits for a new observation only for a single level of \\(X\\) for a given sample.\nWhen observations on the predictor variable \\(X\\) are subject to measurement errors, the resulting parameter estimates are generally no longer unbiased.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-predictor-variable",
    "href": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-predictor-variable",
    "title": "3  Diagnostics and remedial measures",
    "section": "",
    "text": "We need diagnostic information about the predictor variable in order to:\n\nSee if there are any outlying \\(X\\) values that could influence the appropriateness of the fitted regression function or if the \\(X\\) distribution is skewed, which can impact some aspects of the model behavior / performance.\nGet info about the range and concentration of the \\(X\\) levels in the study. This is useful for figuring out the range of validity of the regression analysis.\nCan use simple graphical tools to help.\n\n\n\n\n\n\nDot plot → Good when the sample size is not large.\n\nCan look at min and max; are observations spread throughout this interval? any far outlying observations? replication at different \\(X\\) levels?.\n\n\n\nSequence plot → Should be utilized whenever data are obtained in a sequence, such as over time or for adjacent geographic areas.\n\nShould look like a random walk (i.e. no obvious pattern).\nFor example, if one level of \\(X\\) occurs way more frequently in the beginning and then another occurs way more at end, this info can be very helpful when looking at diagnostics later for how appropriate the model is.\n\n\n\nBox plots → Good when have a larger sample size.\n\nCan look at min and max, location of IQR, outliers, shape (skewness).\n\n\n\nHistograms → Good with really big sample sizes.\n\nCan look for same characteristics as boxplots, as well as modality.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#residuals",
    "href": "notes-diagnostics-and-remedial-measures.html#residuals",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.2 Residuals",
    "text": "3.2 Residuals\nOverview\n\nGoal of residual analysis → Assess the aptness of a statistical model.\n\nWhy use residuals\n\nDirect diagnostic plots for the response variable \\(Y\\) are ordinarily not too useful in regression analysis because the response variable observations are a function of the level of predictor variable.\nSo, instead we look at diagnostics for \\(Y\\) indirectly by examining the residuals.\n\n\n\nResiduals and model error\n\nRecall residual \\(e_i = Y_i - \\hat{Y_i}\\) is an estimate of the unobservable model error \\(\\epsilon = Y_i - E(Y_i)\\).\n** For our regression model, we assume \\(\\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,\\,(0, \\sigma^2\\)). So if the model is appropriate for the data at hand, the residuals should reflect these properties.\nThis is the basic idea of a residual analysis, which is a highly useful way to check if a model is appropriate.\n\n\n\n\n3.2.1 Properites of residuals\n\n\nMean\n\n\nThe mean of the \\(n\\) residuals for the SLR is shown below.\n\nSince this is always 0, it doesn’t give any info as to whether the true errors \\(\\epsilon_i\\) have expected value \\(E(\\epsilon_i) = 0\\).\n\n\n\n\n\n\\[\n\\bar{e} = \\sum e_i = 0\n\\]\n\n\nVariance\n\n\nThe variance of the \\(n\\) residuals for the SLR is shown below.\n\nAgain, if the model is appropriate \\(MSE\\) is an unbiased estimator of the variance of the error terms \\(\\sigma^2\\)\n\n\n\n\n\n\n\\[\nS^2 = \\frac{\\sum (e_i - \\bar{e})^2}{n - 2} = \\frac{\\sum e_i^2}{n - 2} = \\frac{SSE}{n - 2} = MSE\n\\]\n\n\nNonindependence\n\nThe residuals \\(e_i\\) are not independent random variables because they involve the fitted values \\(\\hat{Y_i}\\), which are computed on the same \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nAs a result, the residuals are subject to two constraints: \\(\\sum e_i = 0\\) and \\(\\sum X_i e_i = 0\\).\nHowever, when the sample size \\(n\\) is much larger than the number of parameters \\(p\\), we can ignore the minor dependence.\n\n\n\n3.2.2 Semistudentized residuals\n\nAt times, it is helpful to standardize the residuals for residual analysis. Since the standard deviation of the error terms \\(\\epsilon_i\\) is \\(\\sigma\\), which is estimated by \\(\\sqrt{MSE}\\), we can naturally standardize with\n\n\\[\ne_i^* = \\frac{e_i - \\bar{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}}\n\\]\n\nStudentized → If \\(\\sqrt{MSE}\\) were an estimate of the standard deviation of the residual \\(e_i\\), we would call \\(e_i^*\\) a studentized residual. But it is not…\nSemistudentized → \\(\\sigma_{e_i}\\) is complex and varies for the different residuals \\(e_i\\), and \\(\\sqrt{MSE}\\) is only an approximation of the standard deviation of \\(\\sigma_{e_i}\\). So, instead it is a semistudentized residual.\n\n3.2.3 Departures from model to be studied by residuals\n\n\nWe are going to use residuals to study the following departures from the SLR model with normal errors:\n\nThe regression function is not linear → Is the functional form of the model appropriate?\nThe error terms do not have constant variance.\nThe error terms are not independent.\nThe model fits all but one or a few outlier observations → Do any of the data points have a disproportionate influence on the parameter estimates?\nThe error terms are not normally distributed.\nOne or several important predictor variables have been omitted from the model.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-residuals",
    "href": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-residuals",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.3 Diagnostics for residuals",
    "text": "3.3 Diagnostics for residuals\nTypes of plots\n\nHere are the informal diagnostic plots of residuals we will use to see if any of the above 6 departures from the SLR model are present,\n\n\nResiduals against predictor variable.\nAbsolute or squared residuals against predictor variable.\nResiduals against fitted values.\nResiduals against time or other sequence.\nResiduals against omitted predictor variables.\nBoxplot of residuals.\nNormal probability plot of residuals.\n\nDemo\n\nHow to make plots and what they look like when assumptions are met.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# view diagnostic plots (base R)\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# NOT RUN -&gt; note this works inline\n# view ggplot versions of diagnostic plots\nggplot2::autoplot(mod)\n\n\nNext section\n\nNow we will go through how to visually assess each assumption to see if it is met and diagnosis (identify) each violation if present.\nAt then end, we will discuss the impacts of each departure on things such as \\(MSE\\), slope standard errors, prediction accuracy, etc.\n\n\n3.3.1 Nonlinearity of regression function\n\n\nContent\nIdeal\n\n\n\nOverview\n\nWe can look at the following plots to check if a linear regression function is appropriate:\n\n\n\nResidual plot against the fitted values → This is the preferred plot for this assumption check.\n\nWhen a linear regression model is appropriate, the residuals then fall within a horizontal band centered around 0, displaying no systematic tendencies to be positive and negative (randomly scattered around 0).\nWhen the linearity assumption is violated, there are systematic deviations.\n\n\n\n\n\n\nResidual plot against the predictor variable\n\nFor SLR only, this shows the same info as the residuals vs fitted values because the \\(\\hat{Y_i}\\) are a linear function of the \\(X_i\\).\nSo the basic pattern of the plotted points is not affected whether the residual plot is against the \\(X_i\\) or the \\(\\hat{Y_i}\\) (only the \\(X\\) scale values are affected).\nFor curvilinear regression and multiple regression, separate plots of the residuals against the fitted values and against the predictor variable(s) are usually helpful.\n\n\nScatterplot of \\(Y\\) vs \\(X\\) → If the actual trend in the data in linear, then it makes sense to fit a straight-line model to the data.\n\n\nBut, not always as effective as the residual plots. Residual plots are preferred over scatteplot, because it has two advantages:\n\n\n\nCan also easily be used for checking other assumptions.\nThere are occasions when the scaling of the scatter plot places the \\(Y_i\\) observations close to the fitted values \\(\\hat{Y_i}\\), for instance, when there is a steep slope. It then becomes more difficult to study the appropriateness of a linear regression function from the scatterplot, while the residual plot can clearly show any systematic pattern in the deviations around the fitted regression line under these conditions.\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_ideal &lt;- lm(y ~ x)\n\n\n\nResiduals vs fitted plot\n\nR also adds a trend line, which we want to be approximately horizontal.\n\n\n\n\n# residual vs fitted plot\nplot(mod_ideal, which = 1)\n\n# verify smoothing line that plot.lm() performs (see discussion in nonconstant variance ideal section)\nplot(mod_ideal, which = 1)\nlines(x = lowess(fitted(mod_ideal), y = resid(mod_ideal)), col = \"blue\")\n\n\n\n\n\n\n\n\n\nResiduals vs \\(X\\) plot\n\nFor SLR → Shows same pattern as above, different scale on \\(X\\) axis.\n\n\n\n\n# create residuals vs X plot\n# -&gt; add smoothing line\nplot(x = x, y = residuals(mod_ideal), ylab = \"residuals\", main = \"Residuals vs X\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\nlines(lowess(x = x, y = resid(mod_ideal)), col = \"red\")\n\n\n\n\n\n\n\n\n\nDemo for difference when in MLR\n\nResults → Can see different pattern (even if only slightly based on values chosen) pattern in each.\n\n\n\n\n# initialize new items\nbeta_2 &lt;- 3\n\n# generate new X sample\nx_2 &lt;- runif(n = n, min = 0, max = 5)\n\n# calculate new Y observations using same errors, but with both xs\ny_2 &lt;- beta_0 + beta_1 * x + beta_2 * x_2  + epsilon\n\n# fit new MLR model\nmod_mlr &lt;- lm(y_2 ~ x + x_2)\n\n\n# plot residuals vs fitted, X and X2\n# -&gt; create dataset so can display all three plots\n# -&gt; also add smoothing line to make differences in patterns more noticeable\ndata.frame(X1 = x,\n           X2 = x_2,\n           fitted = fitted(mod_mlr),\n           residuals = residuals(mod_mlr)) %&gt;%\n  pivot_longer(1:3,\n               names_to = \"X_axis\",\n               values_to = \"value\") %&gt;% \n  ggplot(aes(x = value,\n             y = residuals),\n         data = .) + \n  geom_point() + \n  geom_smooth(se = FALSE,\n              method = \"loess\",\n              formula = y ~ x) + \n  facet_grid(. ~ X_axis,\n             scales = \"free\") + \n  geom_hline(yintercept = 0,\n             col = \"grey\",\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\nScatterplot\n\n\n# scatterplot of y vs x with regression line added\nplot(x = x, y = y, main = \"Scatterplot of Y vs X\")\nabline(mod_ideal, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nPolynomial\nNonlinear regression model\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\text{Normal}(0, \\sigma^2)\\)\n\nNote that this is still a linear model (even though there is a higher order term), becuase the model in linear in the parameters.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; sigma &lt;- 5; beta_0 &lt;- 1; beta_1 &lt;- 2;\nbeta_2 &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x  + beta_2 * x^2 + epsilon\n\n# fit model (only on SLR x with linear term)\nmod_squared_x &lt;- lm(y ~ x)\n\n\nResiduals vs fitted plot\n\n\n# residual vs fitted plot\nplot(mod_squared_x, which = 1)\n\n\n\n\n\n\n\n\nScatterplot\n\n\n# scatterplot of y vs x with regression line added\nplot(x = x, y = y, main = \"Squared predictor\")\nabline(mod_squared_x, col = \"red\")\n\n\n\n\n\n\n\n\n\nDemo to show how steep slope can hide lack of fit in scatterplot, but residual plot still picks it up.\n\nResults → For a true quadratic model, the scatterplot of \\(Y\\) vs \\(X\\) looks just linear (not noticeable curvature) when there is a large \\(\\beta_1\\) term (coefficient of linear term). But this is misleading, especially after seeing the residual plot which shows the missing evidence of curvature.\n\n\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 50; beta_0 &lt;- 1; sigma &lt;- 5\n\n# SMALL slope of linear term\nbeta_1 &lt;- 3\nbeta_2 &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x  + beta_2 * x^2 + epsilon\n\n# fit model (only on SLR x with linear term)\nmod_squared &lt;- lm(y ~ x)\n\n# scatterplot of y vs x with regression line added\n# -&gt; can see curvature\nplot(x = x, y = y, main = \"Samll slope\")\nabline(mod_squared, col = \"red\")\n\n\n\n\n\n\n# residuals vs fitted plot\n# -&gt; curvature very obvious here\nplot(mod_squared, which = 1)\n\n\n\n\n\n\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 50; beta_0 &lt;- 1; sigma &lt;- 5\n\n# LARGE slope of linear term\nbeta_1 &lt;- 10\nbeta_2 &lt;- 1\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x  + beta_2 * x^2 + epsilon\n\n# fit model (only on SLR x with linear term)\nmod_squared &lt;- lm(y ~ x)\n\n# scatterplot of y vs x with regression line added\n# -&gt; curvature is hidden more, regression line looks okay\nplot(x = x, y = y, main = \"Large slope\")\nabline(mod_squared, col = \"red\")\n\n\n\n\n\n\n# residuals vs fitted plot\n# -&gt; curvature still visible here\nplot(mod_squared, which = 1)\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 \\, \\mathrm{e}^{\\beta_1 \\, X_i} + \\text{Normal}(0, \\sigma^2)\\)\n\nNonlinear because \\(Y_i\\) cannot be expressed as a linear combination of the \\(\\beta_i\\)s.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 0.1; beta_1 &lt;- 0.5; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 * exp(beta_1 * x) + epsilon\n\n# fit model\nmod_nonlinear &lt;- lm(y ~ x)\n\n\n\nScattterplot of \\(Y\\) vs \\(X\\) and residual plot\n\nCan see obvious nonlinear pattern in scatterplot and systematic deviations in residual plot.\n\n\n\n\n# scatterplot of y vs x with regression line\n# -&gt; obviously not a linear pattern\nplot(x = x, y = y, main = \"Scatterplot of Y vs X\")\nabline(mod_nonlinear, col = \"red\")\n\n\n\n\n\n\n# residual vs fitted plot\n# -&gt; definite pattern to residuals\nplot(mod_nonlinear, 1)\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Nonconstant variance\n\n\nContent\nIdeal\n\n\n\nOverview\n\nWe can look at the following plots to check if the error terms have constant variance:\n\n\n\nResidual plot against the fitted values\n\nAgain for SLR, this shows the same info as residuals vs \\(X\\) plot, but in MLR we want to use the fitted values.\nWhen there is a constant error variance, points again should fall within a horizontal band. So there is a constant spread of the residuals as move across the scope of fitted (or \\(X\\)) values.\n“Tipped over tornado” effect of the points indicates a non-constant variance (i.e. as the fitted values increase, the residuals vary more, or vice versa). Reverse megaphone (decreasing variance with increasing levels of \\(X\\) or \\(\\hat{Y}\\)) is possible as well or varying in some other complex fashion. A nonconstant variance in called heteroscedasticity (the assumption is a homoscedastic error variance).\n\n\n\n\n\n\nScale-location plot → Refined version of above, preferred plot for this assumption.\n\nPlots standardized residuals against fitted values.\nThere are a few ways to transform the residuals for this type of plot, all of which get at the same purpose (options include: absolute value, square root of absolute value, squared, standardized (studentized, semistudentized), square root of standardized).\nBy doing this, it places all of the information on changing magnitudes of the residuals above the horizontal zero line, which makes it easier to see whether the magnitude of the residuals is changing with the level \\(\\hat{Y}\\) (\\(\\pm\\) is not important for this condition).\nThe scale-location plot specifically uses the standardized residuals (which have equal variance by assumption), and are given by \\(e_i^* = \\frac{e_i}{\\sqrt{MSE (1 - h_{ii})}}\\), where the leverages \\(h_{ii}\\) are the diagonal entries of the hat matrix. It then takes the absolute value and square root \\(\\sqrt{\\lvert e_i^* \\rvert}\\) to diminish the skewness because under \\(\\text{Normal}(0, \\sigma^2)\\), \\(\\sqrt{\\lvert e_i^* \\rvert}\\) is much less skewed than \\(\\lvert e_i^* \\rvert\\).\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nResiduals vs fitted plot\n\nLooking residuals to fall within horizontal bands on either side of 0.\n\n\n\n\n# using ideal model from linearity assumption demo\n\n# again, same plot as to check linearity\nplot(mod_ideal, which = 1)\n\n\n\n\n\n\n\n\n\nScale-location plot\n\nNow we want a flat trend line.\n\n\n\n\n# scale-location plot\nplot(mod_ideal, which = 3)\n\n\n\n\n\n\n\n\nDemo of different ways to standardize residuals (including the scale-location plot)\n\n\n# scale-location plot\nplot(mod_ideal, which = 3)\n\n# demo of the standardized residuals used above\n# -&gt; check help page for plot.lm() for more details on methods behind diagnostic plots\n\n# standardize the residuals\n# -&gt; manual calculation using the formula for e* or r functions\nmse &lt;- summary(mod_ideal)$sigma^2\ne_star &lt;- resid(mod_ideal) %&gt;% divide_by(sqrt(mse * (1 - hatvalues(mod_ideal)))) \ne_star &lt;- rstandard(mod_ideal)\n\n# view final transformation of residuals\ndata.frame(e = resid(mod), e_star, f_e_star = sqrt(abs(e_star))) %&gt;% \n  head(n = 5) %&gt;% \n  display_nice(col.names = c(\"Residuals e\", \"Standardized residuals e-star\", \"f(e-star ) = sqrt(abs(e-star))\"))\n\n\n\n Residuals e \n    Standardized residuals e-star \n    f(e-star ) = sqrt(abs(e-star)) \n  \n\n\n 5.258 \n    -0.080 \n    0.283 \n  \n\n 5.258 \n    -0.276 \n    0.525 \n  \n\n -0.521 \n    1.442 \n    1.201 \n  \n\n 2.209 \n    -0.402 \n    0.634 \n  \n\n -7.529 \n    1.130 \n    1.063 \n  \n\n\n\n# overlay function of standardized residuals to scale-location plot to confirm they line up \npoints(x = fitted(mod_ideal), y = sqrt(abs(e_star)), col = \"blue\", pch = 3)\n\n\n\n\n\n\n\n\n# demo different residual plots AND different smoothers\n\n# create dataset of different standardized residuals so can display all\n# 0) residuals\n# 1) f(e*) from above (sqrt of abs of standardized residuals from scale-location)\n# 2) semistudentized residuals = e / sqrt(MSE)\n# 3) abs(residuals)\n# 4) sqrt(abs(residuals))\n# 5) residuals^2\ne &lt;- resid(mod_ideal)\ndata_plot &lt;- data.frame(\"fitted\" = fitted(mod_ideal),\n                        e,\n                        f_e_star = sqrt(abs(e_star)),\n                        e_semi = e / sqrt(mse),\n                        e_abs = abs(e),\n                        e_abs_sqrt = sqrt(abs(e)),\n                        e_squared = e^2) %&gt;%\n  pivot_longer(c(starts_with(\"e\"), \"f_e_star\"),\n               names_to = \"type\",\n               values_to = \"residuals\")\n\n# create dataset for lowess smoothers of each residual type\ndata_lowess &lt;- data_plot %&gt;% \n  split(.$type) %&gt;% map2(names(.), \\(df, type) data.frame(lowess(x = df$fitted, y = df$residuals), type = type)) %&gt;% \n  reduce(bind_rows)\n\n# set more informative labels for the facets\nlabels_residuals &lt;- c(e = \"residuals\",\n                      f_e_star = \"f(standardized residuals\",\n                      e_semi = \"residuals / sqrt(MSE)\",\n                      e_abs = \"abs(residuals)\",\n                      e_abs_sqrt = \"sqrt(abs(residuals))\",\n                      e_squared = \"residuals^2\")\n\n# then plot with different smoothing lines\n# smoothing line\n# -&gt; geom_smooth() default method is loess smoother, which is a local polynomial regression fit\n# -&gt; plot() uses lowess smoother, which uses locally-weighted polynomial regression \n# -&gt; summary -&gt; similar methods, slight difference\n# --&gt; lowess is for adding a smooth curve to a scatterplot, i.e., for univariate smoothing, while loess is for fitting a smooth surface to multivariate data\ndata_plot %&gt;% \n  ggplot(aes(x = fitted,\n           y = residuals),\n       data = .) + \n  geom_point() + \n  geom_smooth(se = FALSE,\n              method = \"loess\",\n              formula = y ~ x) + \n    geom_line(aes(x = x,\n                y = y),\n            data = data_lowess,\n            col = \"red\") + \n  facet_wrap(. ~ type,\n             scales = \"free\",\n             labeller = as_labeller(labels_residuals)) + \n  geom_hline(yintercept = 0,\n             col = \"grey\",\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nIncreasing variance\nComplex variance\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2_i = 5 X_i)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# specify a vector of variances, which is a function of X\nsigma &lt;- 5 * x\n\n# generate response Y\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n# fit model\nmod_increasing_error_variance &lt;- lm(y ~ x)\n\n\n\nScatterplot\n\nCan see more variation around ftted line as \\(X\\) increases.\n\n\n\n\n# view scatterplot with regression line\nplot(x = x, y = y)\nabline(mod_increasing_error_variance, col = \"red\")\n\n\n\n\n\n\n\n\n\nResiduals vs fitted plot\n\nLooking for a pattern that doesn’t follow horizontal bands as move left to right, trend line may still be horizontal though.\n\n\n\n\n# residual plot\nplot(mod_increasing_error_variance, which = 1)\n\n\n\n\n\n\n\n\n\nScale-location plot\n\nNow trend line will not be horizontal after transforming residuals, should increase with an increasing variance.\n\n\n\n\n# scale-location plot\nplot(mod_increasing_error_variance, which = 3)\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\text{ decreasing then increasing } \\sigma^2_i)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# -&gt; specify a vector of variances instead so it sequentially increases\nsigma &lt;- c(seq(from = 20, to = 1, length.out = n / 2), seq(from = 1, to = 20, length.out = n / 2))\n\n# generate X values\n# -&gt; need pattern of variances to follow as X increases, so sort Xs after\nx &lt;- runif(n = n, min = 5, max = 15) %&gt;% sort\n\n# generate response Y\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n# fit model\nmod_complex_error_variance &lt;- lm(y ~ x)\n\n\n# view scatterplot with regression line\n# -&gt; now see values are closer to line in middle of plot\nplot(x = x, y = y)\nabline(mod_complex_error_variance, col = \"red\")\n\n\n\n\n\n\n# residual plot\n# -&gt; see ribbon-like pattern\nplot(mod_complex_error_variance, which = 1)\n\n\n\n\n\n\n# scale-location plot\n# -&gt; see U kinda pattern\nplot(mod_complex_error_variance, which = 3)\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Presence of unusual observations\n\n\nContent\nIdeal\n\n\n\nOverview\n\n\nUnusual observations can create much difficulty when fitting models. When present, they can lead to a misleading fit because the line was estimated by minimizing the squared deviations.\n\nThus, a fitted line may be pulled disproportionately toward an unusual observation. Unusual observations far from \\(\\bar{X}\\) have a larger impact on the model and estimates than those near \\(\\bar{X}\\) (this is the idea of leverage / influence).\n\n\n\n\n\n\nThere are two types unusual observations and how we define them, check for them, and their impact is different.\n\n\nOutliers → Extreme values of the response (outliers in \\(Y\\)).\n\n\nA rough rule of thumb → When the sample size is large, semistudentized (or standardized) residuals with absolute value more than 3 or 4 can be considered outliers (i.e. outliers if \\(\\lvert \\frac{e_i}{\\sqrt{MSE}} \\text{ or } e_i^* \\rvert \\ge 3 \\text{ or } 4\\)).\nWe can look at the following plots to check if residual outliers are present:\n\n\n\nResidual plot against the fitted values (or \\(X\\))\n\n\nJust looking for points far away from the pattern of the rest. In SLR, can also look at the scatterplot of \\(Y\\) vs \\(X\\) for points far from overall pattern.\nPlotting of semistudentized residuals is helpful for distinguishing outiers because it’s easy to identify residuals that lie many standard deviations from zero.\n\n\n\n\n\n\n\nBoxplots, histograms, Normal QQ of the residuals (or standardized residuals)\n\nCan also look for outliers in the usual way based on these plots. Note that different visuals / rules will give more or less evidence for outliers.\n\n\n\n\n\n\nHigh leverage points → Observations whose predictor values are far from the center of the predictor space (extreme values (outliers) of \\(X\\) or unusual combination of \\(X\\)s).\n\nPlots → Residuals vs Leverage plot (or Cook’s distance plot, Cook’s dist vs Leverage (1 - Leverage) plot) → These use measures not covered yet.\n\n\n\n(3.) Influential points → High leverage points that actually influence the slope of the regression line (outlier in \\(X\\) AND \\(Y\\)).\n\nPlots → In order to determine if a point is influential, visualize the regression line with and without the point. Does the slope of the line change considerably? If so, then the point is influential. If not, then it’s not an influential point.\n\n\nRemoving unusual observations\n\nBecause unusual observations can create lots of difficulty, when we find one, we initially think that the observation resulted from a mistake or some extraneous effect. So it should be removed.\nOn the other hand, unusual observations may convey significant information (such as if it occurs because of an interaction with another predictor variable omitted from the model).\nA safe rule → Discard an unusual observation only if there is direct evidence that it represents an error in recording, a miscalculation, a malfunctioning of equipment, or a similar type of circumstance.\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nResiduals vs fitted plot\n\nLooking for observations with residuals far below or above 0.\n\n\n\n\n# using ideal model from linearity assumption demo\n# -&gt; randomness may result in an outlier or two\n\n# again, same plot as to check linearity and nonconstant variance\nplot(mod_ideal, which = 1)\n\n\n\n\n\n\n# by default, on all the diagnostic plots R labels the 3 observations with the most extreme residuals \n# -&gt; this says nothing about being potential outliers, just the top three\n# -&gt; can change how many get labeled and what the label is (by default it is the observation number)\nplot(mod_ideal, which = 1, id.n = 10, labels.id = paste(\"x = \", round(x, 3), \"\\n y = \", round(y, 3)))\n\n\n\n\n\n\n# could also add lines for 3 * sqrt(MSE) to get the same comparison as below, just in units of the original residuals\n# -&gt; NOTE: bounds are off plot cause not any outliers\nplot(mod_ideal, which = 1)\nmse &lt;- summary(mod_ideal)$sigma^2\nabline(h = c(-3 * sqrt(mse), 3 * sqrt(mse)), col = \"orange\")\n\n\n\n\n\n\n\n\n\nStandardized residuals vs fitted plot\n\nLooking for observations beyond with residuals 3 or 4 away from zero after standardizing.\nBoth ways of standardizing get essentially the same outcomes \\(\\Longrightarrow\\) An outlier in one will be an outlier in the other, so can just use the default R\n\n\n\n\n# show how to check the standardized residual plots and compare different versions\n\n# create dataset of two different standardized residuals for variation of residual plot\n# 1) semistudentized residuals\n# 2) e* from above (standardized residuals from scale-location)\ndata_plot &lt;- data.frame(\"fitted\" = fitted(mod_ideal),\n                        e_semi = resid(mod_ideal) / sqrt(mse),\n                        e_star = rstandard(mod_ideal)) %&gt;%\n  pivot_longer(starts_with(\"e\"),\n               names_to = \"type\",\n               values_to = \"residuals\")\n               \n# create residual plot with smoothing line for both types of residuals\n# -&gt; add bands at +/- 3 \"standardized units\" from zero\ndata_plot %&gt;% \n  ggplot(aes(x = fitted,\n             y = residuals,\n             color = type),\n         data = .,\n         alpha = 0.01) + \n  geom_point() + \n  geom_smooth(se = FALSE,\n              method = \"loess\",\n              formula = y ~ x) + \n  geom_hline(yintercept = c(0,-3,3),\n             col = c(\"grey\", \"orange\", \"orange\"),\n             linetype = c(\"dashed\", \"solid\", \"solid\")) + \n  scale_color_manual(name = \"Type of residual\",\n                     values = c(e_semi = \"#F8766D\", e_star = \"#00BFC4\"), # use hexcode of default colors when two values\n                     labels = c(e_semi = \"semistudentized\", e_star =  \"standardized\"))\n\n\n\n\n\n\n\n\n\nBoxplot, histogram and Normal QQ plot of residuals\n\nShould not see any observations far away from the general patterns.\n\n\n\n\n# boxplot of residuals\nboxplot(resid(mod_ideal), horizontal = TRUE,  main = \"Boxplot of residuals\")\n\n\n\n\n\n\n# histogram of residuals\nhist(resid(mod_ideal), main = \"Histogram  of residuals\")\n\n\n\n\n\n\n# normal qq plot\nplot(mod_ideal, which = 2)\n\n\n\n\n\n\n\n\nResiduals vs Leverage plot, Cook’s distance plot and Cook’s dist vs Leverage / (1 - Leverage) plot\n\n\n# diagnostic plots of: residuals vs lev, cooks dist, and cooks dist vs lev\n# NOT RUN -&gt; note works inline\nautoplot(mod_ideal, which = c(5,4,6))\n\nError in `autoplot()`:\n! Objects of class &lt;lm&gt; are not supported by autoplot.\nℹ Have you loaded the required package?\n\n# residuals vs lev plot uses standardized residuals (and not transforming them with sqrt(abs(.)) like the scale-location plot)\n# -&gt; so could add the +/- 3 lines to this plot (and for now ignore the leverage lines)\n# note -&gt; lines are off plot cause no outliers\nplot(mod_ideal, which = 5)\nabline(h = c(-3, 3), col = \"orange\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nOutlier\nHigh-leverage point\nInfluential point\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2), \\hspace{20pt} \\text{except} \\hspace{10pt} Y_{n+1} = \\beta_0^* + \\beta_1^* X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 4; sigma &lt;- 5\n\n# generate X values and y values in a dataframe\n# (SIDENOTE -&gt; needs to be a separate mutate() statement so y uses the x's just generated and not ones from a previous vector)\ndata_without &lt;- data.frame(x = runif(n = n, min = 5, max = 15)) %&gt;% \n  mutate(y = rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma))\n\n\n# introduce outlier\n# -&gt; new point follows different population regression model, but with typical X value\n# --&gt; X ranges from 5 to 15 from above\ndata_new &lt;- data.frame(x = 10) %&gt;% \n  mutate(y = rnorm(n = 1, mean = (beta_0 + 4) + (beta_1 + 4) * x, sd = sigma))\ndata_outlier &lt;- bind_rows(data_without, data_new)\n\n\n\nDiagnostic plots with outlier (typical \\(X\\) but extreme \\(Y\\) \\(\\Longrightarrow\\) Outlier in \\(Y\\))\n\nCan see a point with unusually large residuals (outlier in boxplot, way above / below in residual plot).\n\n\n\n\n# scatterplot of data with outlier\ndata_outlier %$% plot(x = x, y = y)\n\n\n\n\n\n\n# fit model with outlier\nmod_outlier &lt;- data_outlier %$% lm(y ~ x)\n\n# boxplot of residuals\nboxplot(resid(mod_outlier), horizontal = TRUE, main  = \"Boxplot of residuals\")\n\n\n\n\n\n\n# if there is an outlier in the boxplot, can extract it\n# -&gt; then find the corresponding (x,y) pair\n(outlier &lt;- boxplot(resid(mod_outlier), horizontal = TRUE)$out)\n\n\n\n\n\n\n\n      31 \n38.20448 \n\ndata_outlier[which(mod_outlier$residuals == outlier),] %&gt;% display_nice\n\n\n\n   \n    x \n    y \n  \n\n 31 \n    10 \n    79.319 \n  \n\n\n# residual vs fitted plot with reference lines to confirm outlier\nplot(mod_outlier, which = 1)\nmse &lt;- summary(mod_outlier)$sigma^2\nabline(h = c(-3 * sqrt(mse), 3 * sqrt(mse)), col = \"orange\")\n\n\n\n\n\n\n# get standardized residual value of the identified outlier\nrstandard(mod_outlier)[31]\n\n      31 \n4.177826 \n\n\n\n# view regression lines with and without outlier\ndata_outlier %$% plot(x = x, y = y, main = \"Outlier\")\nabline(mod_outlier, col = \"red\")\ndata_outlier[-31,] %$% abline(lm(y ~ x), col = \"purple\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2), \\hspace{20pt} \\text{except} \\hspace{10pt} Y_{n+1} = \\beta_0 + \\beta_1 X_i^* + \\text{Normal}(0, \\sigma^2)\\)\n\n# introduce high-leverage point\n# -&gt; new point follows same population regression model, just with extreme X value\ndata_new &lt;- data.frame(x = 20) %&gt;% \n  mutate(y = rnorm(n = 1, mean = beta_0 + beta_1 * x, sd = sigma))\ndata_high_leverage &lt;- bind_rows(data_without, data_new)\n\n\n\nDiagnostic plots with high-leverage point (extreme \\(X\\) but follow pattern of \\(Y\\) \\(\\Longrightarrow\\) Outlier in \\(X\\))\n\nResidual of high-leverage point will be typical, but far away horizontally from the rest of the points.\n\n\n\n\n# scatterplot of data with high-leverage point\ndata_high_leverage %$% plot(x = x, y = y)\n\n\n\n\n\n\n# fit model with high-leverage point\nmod_high_leverage &lt;- data_high_leverage %$% lm(y ~ x)\n\n# residual vs leverage plot with reference lines to confirm high-leverage (outlier in X) and also outlier (in Y)\nplot(mod_high_leverage, which = 5)\nabline(h = c(-3,3), col = \"orange\")\n\n\n\n\n\n\n\n\n# view regression lines with and without outlier\ndata_high_leverage %$% plot(x = x, y = y, main = \"High-leverage point\")\nabline(mod_high_leverage, col = \"red\")\ndata_high_leverage[-31,] %$% abline(lm(y ~ x), col = \"purple\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2), \\hspace{20pt} \\text{except} \\hspace{10pt} Y_{n+1} = \\beta_0^* + \\beta_1^* X_i^* + \\text{Normal}(0, \\sigma^2)\\)\n\n# introduce influential point\n# -&gt; new point follows different population regression model AND extreme X value\ndata_new &lt;- data.frame(x = 20) %&gt;% \n  mutate(y = rnorm(n = 1, mean = (beta_0 + 4) + (beta_1 + 4) * x, sd = sigma))\ndata_influential &lt;- bind_rows(data_without, data_new)\n\n\n\nDiagnostic plots with influential point (extreme \\(X\\) and does NOT follow patter of \\(Y\\) \\(\\Longrightarrow\\) Outlier in \\(X\\) and \\(Y\\))\n\nResidual of influential point will be extreme AND far away horizontally from the rest of the points.\n\n\n\n\n# scatterplot of data with influential point\ndata_influential %$% plot(x = x, y = y, main = \"Influential point\")\n\n\n\n\n\n\n# fit model with influential point\nmod_influential &lt;- data_influential %$% lm(y ~ x)\n\n# residual vs leverage plot with reference lines to confirm high-leverage and not outlier\nplot(mod_influential, which = 5)\nabline(h = c(-3,3), col = \"orange\")\n\n\n\n\n\n\n# view regression lines with and without outlier\ndata_influential %$% plot(x = x, y = y, main = \"Influential point\")\nabline(mod_influential, col = \"red\")\ndata_influential[-31,] %$% abline(lm(y ~ x), col = \"purple\")\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 Nonindependence of error terms\n\n\nContent\nIdeal\n\n\n\nOverview\n\nIdeally, any potential source of dependence is handled at the experimental design stage (or the sampling scheme), so that it is either eliminated by randomization or explicitly included in the data and we have one observation per subject.\n\nWhenever data are obtained in a time sequence or some other type of sequence, such as for adjacent geographic areas, we can examine the potential dependence of error terms using a sequence plot of residuals.\n\nThis is used to see if there is any correlation between error terms that are near each other in the sequence.\n\n\n\nSequence plot of residuals\n\nWe want to plot the residuals against time, collection order, spatial coordinates, or some other indicator of the sequence that we think might affect the data.\nIf errors are independent, residuals hould look like a random walk around the base line zero (i.e. no obvious pattern).\nDependence (lack of randomness) can appear as a trend or cyclical pattern\n\n\n\n\nModelling with nonindependence or apparent nonindependence\n\nIt can be useful to view the problem of nonindependence of the error terms as one in which and important variable has been omitted from the model (whether it be time, observation number, etc.).\nMore subtle dependencies can be difficult to detect, especially if the information needed to detect them has not been included with the dataset.\nWhen the residuals are plotted against \\(X\\), the plot may not appear to be random. But the basic problem could be from a poorly fitting regression function rather than a lack of independence of the error terms. Example, where a quadratic term should be included:\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nSequence plot of residuals vs observation number\n\nLooking for random walk.\n\n\n\n\n# using ideal model from linearity assumption demo\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_ideal)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Sequence plot of residuals vs collection order\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nDependent errors\nLinear trend effect\nCyclical nonindependence\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\sim \\text{Normal}\\,(\\mu = \\epsilon_{i - 1}, \\sigma^2)\\)\n\nError terms are dependent now because the mean for \\(\\epsilon_i\\) mean is based (dependent) on the previous error term \\(\\epsilon_{i - 1}\\).\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate dependent normal error terms\n# -&gt; setup to first generate the deviations from rnorm()\n# -&gt; the first deviation is from mean = 0, the second will now be from (centered on) the first deviation\n# --&gt; so just add the first deviation and the new second deviations together, which will give the effect of a new mean from which rnorm() generated the next deviation\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma) %&gt;% cumsum\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_dependent_error &lt;- lm(y ~ x)\n\n\n# scatterplot of y vs x with regression line\n# -&gt; nothing looks amiss\nplot(x = x, y = y, main = \"Scatterplot - Dependent errors\")\nabline(mod_dependent_error, col = \"red\")\n\n\n\n\n\n\n# residual vs fitted plot\n# -&gt; again, seems fine\nplot(mod_dependent_error, 1, \"Residuals vs fitted - Dependent errors\")\n\n\n\n\n\n\n\n\n\nSequence plot with dependent errors (\\(\\epsilon_i\\) are no longer iid).\n\nNow there is not a random walk around zero.\n\n\n\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_dependent_error)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Dependent errors\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\sim \\text{Normal}\\,(\\text{increasing } \\mu, \\sigma^2)\\)\n\nDependent now because successive error terms are more likely to be greater.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate dependent normal error terms\n# -&gt; increasing means\nepsilon &lt;- rnorm(n = n, mean = seq(from = -20, to = 20, length.out = n), sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_linear_trend_error &lt;- lm(y ~ x)\n\n\n\nSequence plot with linear trend effect (still dependent).\n\nClear increase from in residuals with collection order → Early points more have negative residuals and later ones are positive.\n\n\n\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_linear_trend_error)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Linear trend effect\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\sim \\text{Normal}\\,(\\text{cyclical } \\mu, \\sigma^2)\\)\n\nDependent now because adjacent errors are more likely to be closer to each other.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate dependent normal error terms\n# -&gt; cyclical (decreasing, then increasing, then decreasing, repeat) means\nepsilon &lt;- rnorm(n = n, mean = c(seq(from = -20, to = 20, length.out = n / 3),\n                                 seq(from = 20, to = -20, length.out = n / 3),\n                                 seq(from = -20, to = 20, length.out = n / 3)),\n                 sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_cyclical_error &lt;- lm(y ~ x)\n\n\n\nSequence plot with cyclical pattern in errors (still dependent).\n\nCyclical pattern in residuals against collection order.\n\n\n\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_cyclical_error)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Cyclical nonindependence\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n3.3.5 Nonnormality of errors\n\n\nContent\nIdeal\n\n\n\nOverview\n\nSmall departures from normality do not create any serious problems, but major departures should be of concern.\nWe can check the normality of error terms in a variety of ways:\n\n\n\nDistribution plots of residuals → This is one of the two preferred plots.\n\nBoxplots are helpful for seeing if residuals are symmetric and if there are any possible outliers.\nHistograms can also be used for the same purpose, looking for roughly normal. But the sample size needs to be reasonably large for this plot to convey reliable information about the shape of the distribution of error terms (there can be lots of fluctuation in the shape with small samples) \\(\\Longrightarrow\\) Moderate departures from normality do not imply a serious violation of this assumption.\nFor both of these plots, as long as there is not severe departures, it is okay.\n\n\n\nComparison of relative frequencies → Use the empirical rule to compare observed relative frequencies to expected under normality.\n\nUsing \\(\\sqrt{MSE}\\) as an estimate for \\(\\sigma\\), for large sample size \\(n\\) we expect approximately:\n\n\n68% of residuals to fall within \\(\\pm \\sqrt{MSE}\\).\n95% of residuals to fall within \\(\\pm 2 \\sqrt{MSE}\\).\n99.7% of residuals to fall within \\(\\pm 3 \\sqrt{MSE}\\).\n\n\nIf the sample size is moderately large, then we can the corresponding \\(t_{n-1}\\) critical values (multipliers).\nJust looking for relative consistency with these rules.\n\n\n\n\n\n\nNormal probability (QQ) plot of residuals → This is the other preffered plot.\n\nEach standardized residual is plotted against its theoretical percentile (aka quantile, which gives us the expected value) under normality. Note that there are a few ways to make the QQ plots (different ways of standardizing residuals and different algorithms for theoretical quantiles), but none of these variations affect the nature of the plot.\nA plot that is nearly linear suggests agreement with normality, whereas a plot that departs substantially from linearity suggests that the error distribution is not normal.\nAgain a moderate departure from normality is of little concern, only extreme departures are of note.\n\n\n\n\nDifficulties in assessing normality\n\n\nThe analysis for model departures regarding normality is often more difficult than departures of other types because…\n\n\nRandom variation can be particularly mischievous when studying the nature of a probability distribution unless the sample size is quite large.\n\nEven worse, other types of departures can and do affect the distribution of the residuals.\ne.g. Residuals may appear to be not normally distributed because an inappropriate regression function is used or because the error variance is not constant.\n\n\nSo, it is usually a good strategy to investigate these other types of departures first, before assessing the normality of the error terms.\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nDistribution plots\n\nLooking roughly symmetric boxplot and a roughly symmetric histogram, both with no extreme outliers.\n\n\n\n\n# using ideal model from linearity assumption demo\n\n# again, same distribution plots used to check residuals for outliers\n# boxplot of residuals\nboxplot(resid(mod_ideal), horizontal = TRUE,  main = \"Boxplot of residuals\")\n\n\n\n\n\n\n# histogram of residuals\nhist(resid(mod_ideal), main = \"Histogram  of residuals\")\n\n\n\n\n\n\n\n\n\nComparison of relative frequencies of residuals\n\nPercentages should match the empirical rule.\n\n\n\n\n# create relative frequency table for interval probabilities of residuals\n# -&gt; assuming here 30 is large enough to use Z multipliers\n# -&gt; pass z multipliers to map() statement calculating proportion of abs value residuals less than multiplier * sqrt(MSE) -&gt; (gets interval rel freq because centered around zero)\nmse &lt;- summary(mod_ideal)$sigma^2\ne &lt;- resid(mod_ideal)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n        0.733         0.933         0.967 \n\n\n\n\nNormal QQ plot of residuals\n\nWant standard residuals to follow the trend line.\n\n\n\n\n# normal qq plot\nplot(mod_ideal, which = 2)\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nSkewed errors\nLight-tailed errors\nHeavy-tailed errors\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Exponential}(\\lambda = 1/5)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# generate right-skewed error terms from exponential distribution\nepsilon &lt;- rexp(n = n, rate = 1/5)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_skewed_error &lt;- lm(y ~ x)\n\n\n\n\nComparison of frequencies, distribution plots and QQ plot of residuals\n\nCan see relative frequencies don’t match the empirical rule; right skew in boxplot (with high outliers) and histogram; concave up pattern in qq plot.\n\n\n\n\n# comparison of frequencies\nmse &lt;- summary(mod_skewed_error)$sigma^2\ne &lt;- resid(mod_skewed_error)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n         0.82          0.96          0.98 \n\n# boxplot of residuals\nboxplot(resid(mod_skewed_error), horizontal = TRUE,  main = \"Right-skewed error\")\n\n\n\n\n\n\n# histogram of residuals\nhist(resid(mod_skewed_error), freq = FALSE, main = \"Right-skewed error\")\nx_plot &lt;- seq(from = -3, to = 3, by = 0.01)\nlines(x = x_plot, dnorm(x = x_plot))\n\n\n\n\n\n\n# normal qq plot\nplot(mod_skewed_error, which = 2)\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Uniform}(a = -2, b = 2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# generate light-tailed error terms from uniform distribution\nepsilon &lt;- runif(n = n, min = -2, max = 2)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_light_tailed_error &lt;- lm(y ~ x)\n\n\nSame methods → Relative frequencies don’t line up; likely no outliers on boxplot; the histogram may show more data closer to the center; S-shape in QQ plot.\n\n\n# comparison of frequencies\nmse &lt;- summary(mod_light_tailed_error)$sigma^2\ne &lt;- resid(mod_light_tailed_error)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n         0.56          1.00          1.00 \n\n# boxplot of residuals\nboxplot(resid(mod_light_tailed_error), horizontal = TRUE,  main = \"Light-tailed error\")\n\n\n\n\n\n\n# histogram of standardized residuals (so can overlay Z density curve)\nhist(rstandard(mod_light_tailed_error), freq = FALSE, breaks = 20, main = \"Light-tailed error\")\nx_plot &lt;- seq(from = -3, to = 3, by = 0.01)\nlines(x = x_plot, dnorm(x = x_plot))\n\n\n\n\n\n\n# normal qq plot\nplot(mod_light_tailed_error, which = 2)\n\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{t}_4\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# generate heavy-tailed error terms from t-distribution\n# -&gt; df = 4 so not the most extreme\nepsilon &lt;- rt(n = n, df = 4)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_heavy_tailed_error &lt;- lm(y ~ x)\n\n\nSame methods → Again, relative frequencies don’t line up; likely outliers on both sides of boxplot; the histogram will may show more data away from the center; flipped S-shape in QQ plot.\n\n\n# comparison of frequencies\nmse &lt;- summary(mod_heavy_tailed_error)$sigma^2\ne &lt;- resid(mod_heavy_tailed_error)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n         0.76          0.96          0.98 \n\n# boxplot of residuals\nboxplot(resid(mod_heavy_tailed_error), horizontal = TRUE,  main = \"Heavy-tailed error\")\n\n\n\n\n\n\n# histogram of standarized residuals (so can overlay Z density curve)\nhist(rstandard(mod_heavy_tailed_error), freq = FALSE, breaks = 20, main = \"Heavy-tailed error\")\nx_plot &lt;- seq(from = -3, to = 3, by = 0.01)\nlines(x = x_plot, dnorm(x = x_plot))\n\n\n\n\n\n\n# normal qq plot\nplot(mod_heavy_tailed_error, which = 2)\n\n\n\n\n\n\n\n\n\n\n\n3.3.6 Omission of important predictor variables\n\n\nContent\nIdeal\n\n\n\nOverview\n\n\nResiduals should also be plotted against variables omitted from the model that might have important effects on the response.\n\nAllows us to see whether or not the residuals tend to vary systematically with the level of the additional predictor variable (can be qualitative or quantitative).\n\n\n\n\nOnly a few of the factors operating on any response variable \\(Y\\) in real-world situations can be included explicitly in a regression model (depends on what data was collected).\n\nSo when using residual analysis to identify other important predictors, we are simply testing the adequacy of the model and seeing if it could be improved materially by adding one or more predictor variables.\nIn doing so, we can provide important additional descriptive and predictive power to the model.\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_{1,i} + 0 \\, X_{2,i} + \\text{Normal}(0, \\sigma^2)\\)\n\nSimplifies to: \\(Y_i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2 X_{2,i} + \\epsilon_i \\hspace{20pt} \\text{with unrelated } X_{2,i}\\)\n\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 3; sigma &lt;- 5\n\n# unimportant predictor\nbeta_2 &lt;- 0\n\n# generate X samples\nx_1 &lt;- runif(n = n, min = 5, max = 15)\nx_2 &lt;- runif(n = n, min = 0, max = 5)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate Y observations using with both xs\n# -&gt; beta_2 = 0 cancels out x_2 terms\ny &lt;- beta_0 + beta_1 * x_1 + beta_2 * x_2 + epsilon\n\n# fit simplified (correct) model\nmod_correct &lt;- lm(y ~ x_1)\n\n\n\nResiduals vs fitted plot\n\nLooks like random noise, no systematic deviations when residuals for correct model are plotted against unimportant (unrelated) omitted predictor.\n\n\n\n\n# residual plot against unimportant predictor not included in model\n# -&gt; no systematic variation of residuals by additional predictor\nplot(x = x_2, y = resid(mod_correct), main = \"Residuals vs unimportant predictor\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\nlines(lowess(x = x_2, y = resid(mod_correct)), col = \"red\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\nMissing important predictor\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2 X_{2,i} + \\text{Normal}(0, \\sigma^2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 3; sigma &lt;- 5\n\n# important predictor\nbeta_2 &lt;- 5\n\n# generate X samples\nx_1 &lt;- runif(n = n, min = 5, max = 15)\nx_2 &lt;- runif(n = n, min = 0, max = 5)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate Y observations using with both xs\ny &lt;- beta_0 + beta_1 * x_1 + beta_2 * x_2 + epsilon\n\n# fit incorrect model (missing X2)\nmod_missing_important_x &lt;- lm(y ~ x_1)\n\n\n\nResiduals vs fitted plot\n\nNow there is a pattern in the residuals for the omitted important predictor.\n\n\n\n\n# residual plot against important predictor not included in model\n# -&gt; now there is a clear pattern\nplot(x = x_2, y = resid(mod_missing_important_x), main = \"Missing predictor\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\nlines(lowess(x = x_2, y = resid(mod_missing_important_x)), col = \"red\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#effects-of-departures",
    "href": "notes-diagnostics-and-remedial-measures.html#effects-of-departures",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.4 Effects of departures",
    "text": "3.4 Effects of departures\n\n3.4.1 Setup and functions\nSimulation to determine the effect of various model departures on model performance.\n\nCode# initialize simulation settings\n# -&gt; first the parameter(s) of interest (that are variable), excluding nonlinear cause scales are totally different\n# -&gt; create all combos\n# -&gt; add in the constant settings\n# -&gt; arrange in good order if necessary\n# -&gt; add simulation id column\n# -&gt; set row names equal to id (helps keep track of results later)\nparams &lt;- expand.grid(model = c(\"ideal\", \"skewed x\", \"squared x\", \"increasing error variance\", \"complex error variance\", \"outlier\", \"high leverage point\", \"influential point\", \"dependent error\", \"linear trend error\", \"cyclical error\", \"skewed error\", \"light tailed error\", \"heavy tailed error\", \"missing important x\")) %&gt;% \n  mutate(model = as.character(model),\n         n = 50,\n         beta_0 = 2,\n         beta_1 = 2,\n         beta_2 = 0,\n         sigma = 5) %&gt;%\n  bind_cols(data.frame(sim_id = 1:nrow(.)))\nrownames(params) &lt;- paste0(\"sim\", params$sim_id)\n\n# set beta2 for models that need it\nparams[which(params$model == \"squared x\"), \"beta_2\"] &lt;- 2\nparams[which(params$model == \"missing important x\"), \"beta_2\"] &lt;- 2\n\nhead(params)\n\n                         model  n beta_0 beta_1 beta_2 sigma sim_id\nsim1                     ideal 50      2      2      0     5      1\nsim2                  skewed x 50      2      2      0     5      2\nsim3                 squared x 50      2      2      2     5      3\nsim4 increasing error variance 50      2      2      0     5      4\nsim5    complex error variance 50      2      2      0     5      5\nsim6                   outlier 50      2      2      0     5      6\n\n\n\nCode# define function to generate the model of interest\n# -&gt; conditionally generates X values, error terms and Y values, then fits model\ngenerate_mod &lt;- function(model = c(\"ideal\", \"skewed x\", \"squared x\", \"increasing error variance\", \"complex error variance\", \"outlier\", \"high leverage point\", \"influential point\", \"dependent error\", \"linear trend error\", \"cyclical error\", \"skewed error\", \"light tailed error\", \"heavy tailed error\", \"missing important x\"), n = 30, beta_0 = 1, beta_1 = 2, beta_2 = 0, sigma = 5) {\n  \n  # set model type by matching argument\n  # -&gt; if no model is specified, defaults to the first one\n  # -&gt; partial string matches work\n  model = match.arg(model)\n  \n  # generate X samples\n  # -&gt; only model with specific x dist is skewed predictor\n  # -&gt; need to sort x for complex error variance\n  x = \n    if (identical(model, \"skewed x\")) {truncdist::rtrunc(n = 50, spec = \"exp\", a = 5, b = 15)}\n  else {runif(n = n, min = 5, max = 15)}\n  if (identical(model, \"complex error variance\")) {x = sort(x)}\n  x_2 = runif(n = n, min = 0, max = 5)\n  \n  # generate error terms\n  epsilon = \n    if (identical(model, \"increasing error variance\")) {rnorm(n = n, mean = 0, sd = 5 * x)}\n  else if (identical(model, \"complex error variance\")) {rnorm(n = n, mean = 0 , sd = c(seq(from = 20, to = 1, length.out = n / 2), seq(from = 1, to = 20, length.out = n / 2)))}\n  else if (identical(model, \"linear trend error\")) {rnorm(n = n, mean = seq(from = -20, to = 20, length.out = n), sd = sigma)}\n  else if (identical(model, \"cyclical error\")) {rnorm(n = n, mean = c(seq(from = -20, to = 20, length.out = n / 3), seq(from = 20, to = -20, length.out = n / 3), seq(from = -20, to = 20, length.out = n / 3)), sd = sigma)}\n  else if (identical(model, \"skewed error\")) {rexp(n = n, rate = 1/sigma)}\n  else if (identical(model, \"light tailed error\")) {runif(n = n, min = -2, max = 2)}\n  else if (identical(model, \"heavy tailed error\")) {rt(n = n, df = 4)}\n  else {rnorm(n = n, mean = 0, sd = sigma)}\n  if (identical(model, \"dependent error\")) {epsilon = cumsum(epsilon)}\n  \n  # calculate Y observations using with both xs\n  # -&gt; if X2 is not needed beta_2 = 0 cancels it out\n  y = \n    if (identical(model, \"squared x\")) {beta_0 + beta_1 * x + beta_2 * x^2 + epsilon}\n  else {beta_0 + beta_1 * x + beta_2 * x_2 + epsilon}\n  \n  # change last observation for unusual observations\n  if (identical(model, \"outlier\")) {x[n] = runif(1, min = 8, max = 12); y[n] = rnorm(n = 1, mean = (beta_0 + 4) + (beta_1 + 4) * x[n], sd = sigma)}\n  else if (identical(model, \"high leverage point\")) {x[n] = runif(1, min = 18, max = 22); y[n] = beta_0 + beta_1 * x[n] + epsilon[n]} # same regression function (use same error), extreme x value\n  else if (identical(model, \"influential point\")) {x[n] = runif(1, min = 18, max = 22); y[n] = rnorm(n = 1, mean = (beta_0 + 4) + (beta_1 + 4) * x[n], sd = sigma)}\n  \n  # fit model\n  mod = lm(y ~ x)\n  \n}\n\n# define function to run on each unique simulation setting\nrun_trial &lt;- function(vec, vec_names, m = 1000) {\n  \n  # redefine arguments for easier reference\n  # -&gt; numeric values get coerced to character because of `model` data type\n  names(vec) = vec_names\n  model = vec[\"model\"]\n  n = as.numeric(vec[\"n\"])\n  beta_0 = as.numeric(vec[\"beta_0\"])\n  beta_1 = as.numeric(vec[\"beta_1\"])\n  beta_2 = as.numeric(vec[\"beta_2\"])\n  sigma = as.numeric(vec[\"sigma\"])\n  sim_id = as.numeric(vec[\"sim_id\"])\n  \n  # initialize vectors of quantities of interest\n  beta0_hat = rep(NA, m)\n  beta1_hat = rep(NA, m)\n  se_beta1_hat = rep(NA, m)\n  t_star = rep(NA, m)\n  mse = rep(NA, m)\n  r_squared = rep(NA, m)\n  \n  # loop to simulate m models and extract summaries\n  for (i in 1:m) {\n    \n    # fit model\n    mod &lt;- generate_mod(model = model, n = n, beta_0 = beta_0, beta_1 = beta_1, beta_2 = beta_2, sigma = sigma)\n    \n    # get summaries\n    # -&gt; just want t-stat\n    beta0_hat[i] = summary(mod)$coefficients[\"(Intercept)\", \"Estimate\"]\n    beta1_hat[i] = summary(mod)$coefficients[\"x\", \"Estimate\"]\n    se_beta1_hat[i] = summary(mod)$coefficients[\"x\", \"Std. Error\"]\n    t_star[i] = summary(mod)$coefficients[\"x\", \"t value\"]\n    mse[i] = summary(mod)$sigma^2\n    r_squared[i] = summary(mod)$r.squared\n    \n  }\n  \n  return(cbind(beta0_hat, beta1_hat, se_beta1_hat, t_star, mse, r_squared, sim_id = sim_id))\n  \n}\n\n# define function to run simulation and format results\nsimulation &lt;- function(params, sim_function, iterations = 100, format_results = FALSE) {\n  \n  # run simulation for each parameter settings\n  # -&gt; have to transpose so that now each column is a parameter settings and convert to dataframe so can pass to map(), which needs a list\n  results_raw = params %&gt;% \n    t %&gt;% \n    data.frame %&gt;% \n    map(\\(vec) sim_function(vec, vec_names = names(params), m = iterations), .progress = T)\n  \n  if (!format_results){\n    \n    return(results_raw)\n    \n  }else{\n    \n    # reformat results\n    # -&gt; convert each results matrix to a dataframe, then combine\n    # -&gt; attach the simulation settings to the results\n    # -&gt; pivot longer to have one column for the name of the summary measure and one for its value\n    # -&gt; convert parameters to factors for plot\n    results = results_raw %&gt;% \n      map(\\(x) data.frame(x)) %&gt;% \n      reduce(bind_rows) %&gt;% \n      left_join(params, by = \"sim_id\") %&gt;% \n      pivot_longer(cols = 1:(ncol(results_raw[[1]])-1),\n                   names_to = \"statistic\",\n                   values_to = \"value\") %&gt;% \n      mutate(across(1:ncol(params), as.factor))\n    return(results)\n  }\n  \n}\n\n# define function to plot standard results\nplot_results &lt;- function(res, mods) {\n  \n  res %&gt;% \n  filter(model %in% mods) %&gt;% \n    ggplot(aes(x = value,\n               group = model,\n               color = model,\n               fill = model),\n           data = .) + \n    geom_density(alpha = 0.2) + \n    facet_wrap(statistic ~ .,\n               scales = \"free\") \n}\n\n\n\n3.4.2 Results\n\n# run simulation\nresults &lt;- simulation(params = params, sim_function = run_trial, iterations = 1000, format_results = TRUE)\n\nhead(results)\n\n# A tibble: 6 × 9\n  sim_id model n     beta_0 beta_1 beta_2 sigma statistic     value\n  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1      ideal 50    2      2      0      5     beta0_hat    -0.668\n2 1      ideal 50    2      2      0      5     beta1_hat     2.20 \n3 1      ideal 50    2      2      0      5     se_beta1_hat  0.237\n4 1      ideal 50    2      2      0      5     t_star        9.32 \n5 1      ideal 50    2      2      0      5     mse          21.0  \n6 1      ideal 50    2      2      0      5     r_squared     0.644\n\n\n\n# unusual observations results\nplot_results(res = results, mods = c(\"ideal\", \"outlier\", \"high leverage point\", \"influential point\"))\n\n\n\n\n\n\n\nTakeaways\n\n\nEstimated coefficients:\n\nHigh leverage points really have no impact on anything (slightly inflate \\(R^2\\)).\nOnly influential points make estimated coefficients biased.\nBoth influential points and outliers inflate \\(MSE\\), with influential points having more of an impact.\nOutliers have the largest decrease in \\(R^2\\); influential points decrease it as well.\nStandard errors of estimated coefficients are much larger for influential points, then followed by outliers.\nLeast likely to reject test on coefficient when there is an outlier, then followed by influential points (which makes sense cause outliers lead to same estimated coefficients, but larger standard errors).\n\n\n\n\n# nonconstant error variance\nplot_results(res = results, mods = c(\"ideal\", \"increasing error variance\", \"complex error variance\"))\n\n\n\n\n\n\n\nTakeaways\n\nNon constant error variance messes everything up, this definitely needs to be remedied first.\n\n\n# dependent error variance\nplot_results(res = results, mods = c(\"ideal\", \"dependent error\", \"linear trend error\", \"cyclical error\"))\n\n\n\n\n\n\n\nTakeaways\n\nNot independent errors are just the wrong type of model for SLR.\nAll drastically lower \\(R^2\\) and higher standard error of coefficients (less likely to reject), but similar estimates.\n\n\n# non normal error distribution\nplot_results(res = results, mods = c(\"ideal\", \"skewed error\", \"light tailed error\", \"heavy tailed error\"))\n\n\n\n\n\n\n\nTakeaways\n\nSkewed errors seem to have little impact.\nNot sure what to make of the others, they seem extreme.\n\n\n# other\nplot_results(res = results, mods = c(\"ideal\", \"skewed x\", \"missing important x\"))\n\n\n\n\n\n\n\nTakeaways\n(not looking at when missing \\(x^2\\), everything way off)\n\nSame estimated coefficients, lots more variability when skewed \\(X\\). This makes sense in the formula below.\n\n\n\nSkewed \\(X\\) results in a drastically lower \\(R^2\\). Interesting.\nMissing important predictors leads to inflated \\(MSE\\), which makes sense.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#tests",
    "href": "notes-diagnostics-and-remedial-measures.html#tests",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.5 Tests",
    "text": "3.5 Tests\n\n\n\n\n\n3.5.1 \\(F\\) test for lack of fit\nThis is a formal test for determining whether a specific type of regression function adequately fits the data.\nAssumptions\n\nThe lack of fit test assumes that the observations Y for given X are (1) independent and (2) normally distributed, and that (3) the distributions of the same variance \\(\\sigma^2\\).\n\nThe lack of fit test requires repeat observations at one or more \\(X\\) levels (just this is sufficient). In observational studies, these just happen by chance.\n\nRepeat trials for the same level of the predictor variable are called replications. The resulting observations are called replicates.\n\n\n\nNotation\n\n\nWe need to modify our notation to recognize the existence of replications at some levels of \\(X\\).\n\nLet the different \\(X\\) levels in the study, whether or not replicated observations are present, as \\(X_1, \\ldots , X_c\\) (so there are \\(c\\) unique values of \\(X\\)).\nLet the number of replicates for the \\(j\\)th level of \\(X\\) as \\(n_j\\). Thus the total number of observations is given by \\(\\displaystyle n = \\sum_{j = 1}^c n_j\\).\nLet the observed value of the response variable for the \\(i\\)th replicate for the \\(j\\)th level of \\(X\\) by \\(Y_{ij}\\), where \\(i = 1, \\ldots, n_j, \\hspace{10pt} j = 1, \\ldots, c\\).\nLet the mean of the \\(Y\\) observations at the level \\(X = X_j\\) by \\(\\bar{Y}_j\\).\n\n\n\nFull model\n\nThe general linear test approach begins with the specification of the full model. The full model used for the lack of fit test makes the same assumptions as the simple linear regression model, except for assuming a linear regression relation (which is what we are testing against in this case).\nSo the full model is (where \\(\\mu_{ij}\\) are parameters \\(j = 1, \\ldots, c\\) and \\(\\epsilon_{ij} \\overset{iid}\\sim \\text{Normal}\\,(0, \\sigma^2)\\):\n\n\\[Y_{ij} = \\mu_j + \\epsilon_{ij} \\hspace{20pt} \\text{full model}\\] - Thus the parameter \\(\\mu_j\\) is the mean response when \\(X = X_j\\):\n\\[E(Y_{ij}) = \\mu_j\\]\n\nThis full model is like the SLR model in that it specifies the each response is made of two components: the mean response when \\(X = X_j\\) and a random error term. The difference is that in the full model there is no restrictions on the means \\(\\mu_j\\), whereas the SLR model the mean responses are linearly related to \\(X\\) (i.e. \\(E(Y) = \\beta_0 + \\beta_1 X\\)).\nTo fit the full model to the data, we require the least squares or maximum likelihood estimators for the parameters \\(\\mu_j\\). It can be shown the estimators are the sample means:\n\n\\[\\hat{\\mu}_j = \\bar{Y}_j\\]\n\nThus, the estimated expected value for observation \\(Y_{ij}\\) is \\(\\bar{Y}_j\\), and the error sum of squares for the full model therefore is:\n\n\\[SSE(F) = \\sum_j^c \\sum_i^{n_j} (Y_{ij} - \\hat{Y}_{ij})^2 = \\sum_j^c \\sum_i^{n_j} (Y_{ij} - \\bar{Y}_j)^2 = SSPE\\]\n\nIn the context of the test for lack of fit, the full model error sum of squares above is called the pure error sum of squares (SSPE).\nNote that \\(SSPE\\) is made up of the sums of squared deviations at each \\(X\\) level. At level \\(X = X_j\\), this sum of squared deviations is shown below, which then get added over all of the \\(X\\) levels (\\(j = 1, \\ldots , c\\)):\n\n\\[\\sum_i (Y_{ij} - \\bar{Y}_j)^2\\] - Also note that if there is no replication at an \\(X\\) level, then \\(Y_{1j} = \\bar{Y}_j\\) and the sum of square is simply 0 (this is why we need replication).\n\nThe degrees of freedom associated with \\(SSPE\\) is found like usual. At each \\(X_j\\), there are \\(n_j - 1\\) df. Then sum over \\(j\\):\n\n\\[df_F = \\sum_j^c (n_j - 1) = \\sum_j^c n_j - c = n - c\\]\n\nNote that any \\(X\\) level with no replications makes no contribution to \\(df_F\\) because \\(n_j - 1 = 1 -  1 = 0\\).\n\nReduced model\n\nThe general linear test approach next requires consideration of the reduced model under \\(H_0\\). For testing the appropriateness of a linear regression relation, the hypotheses are:\n\n\\[\nH_0: E(Y) = \\beta_0 + \\beta_1 X \\hspace{20pt} \\text{Reduced model}\\\\\nH_A: E(Y) \\ne \\beta_0 + \\beta_1 X \\hspace{20pt} \\text{Full model}\n\\]\n\nThus, \\(H_0\\) postulates that \\(\\mu_j\\) in the full model is linearly related to \\(X_j\\):\n\n\\[\\mu_j = \\beta_0 + \\beta_1 X_j\\]\n\nThe reduced model under \\(H_0\\) therefore is:\n\n\\[Y_{ij} = \\beta_0 + \\beta_1 X_j + \\epsilon \\hspace{20pt} \\text{Reduced model}\\]\n\nNote that the reduced model is the ordinary simple linear regression model as usual, with the subscripts modified to recognize the existence of replications. We know that the estimated expected value for observation \\(Y_{ij}\\) with usual regression model is the fitted value \\(Y_{ij}\\)\n\n\n\\[Y_{ij} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_j\\]\n\nHence, the error sum of squares for the reduced model is the usual error sum of squares \\(SSE\\)\n\n\n\\[SSE(R) = \\sum_j^c \\sum_i^{n_j} (Y_{ij} - \\hat{Y}_{ij})^2 = \\sum_j^c \\sum_i^{n_j} (Y_{ij} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_j))^2 = SSE\\]\n\nWe also that the degrees of freedom associated with \\(SSE(R)\\) are:\n\n\\[df_R = n - 2\\]\nTest statistic\n\nThe general linear test statistic Equation 2.1 (testing significant reduction is \\(SSE\\)) becomes:\n\n\\[F^* = \\frac{SSE - SSPE}{(n - 2) - (n - c)} \\big/ \\frac{SSPE}{n - c}\\]\n\nThe difference between the two error sums of squares is called the lack of fit sum of squares \\(SSLF\\).\n\n\\[SSLE = SSE - SSPE\\]\n\nWe can then express the test statistic as follows (and reject \\(H_0\\) in favor of \\(H_A\\) for large values of \\(F^*\\)):\n\n\\[F^* = \\frac{SSLF}{c - 2} \\big/ \\frac{SSPE}{n - c} = \\frac{MSLF}{MSPE}\\]\nANOVA table\n\nThe definition of the lack of fit sum of squares \\(SSLF\\) above indicates that we have decomposed the error sum of squares SSE into two components:\n\n\\[SSE = SSPE + SSLF\\]\n\nThe decomposition comes from the identity (which shows that the error deviations in \\(SSE\\) are made up of a pure error component and a lack of fit component):\n\n\n\n\n\n\nLast formula above indicates clearly why \\(SSLF\\) measures lack of fit.\n\nIf the linear regression function is appropriate, then the means \\(\\bar{Y}_j\\) will be near the fitted values \\(\\hat{Y}_j\\) calculated from the estimated linear regression function and \\(SSLF\\) will be small.\nOn the other hand, if the linear regression function is not appropriate, the means \\(\\bar{Y}_j\\) will not be near the fitted values calculated from the estimated linear regression function and \\(SSLF\\) will be large.\n\n\n\nThis formulas also indicates why \\(c - 2\\) degrees of freedom are associated with \\(SSLF\\).\n\nThere are \\(c\\) means \\(\\bar{Y}_j\\) in the sum of squares (\\(\\approx\\) \\(c\\) observations), and two dfs are lost in estimating the parameters \\(\\beta_0\\) and \\(\\beta_1\\) of the linear regression function to obtain the fitted values \\(\\hat{Y}_j\\).\n\n\nAn ANOVA table can be constructed for the decomposition of SSE.\n\n\nNotes\n\nThe reason for the term “pure error” is that \\(MSPE\\) is always an unbiased estimator of the error term variance \\(\\sigma^2\\), no matter what is the true regression function. But we still always use \\(MSE\\) becuase it has more df.\nThe general linear test approach just explained can be used to test the appropriateness of other regression functions. Only the degrees of freedom for \\(SSLF\\) will need be modified. In general, \\(c - p\\) degrees of freedom are associated with \\(SSLF\\), where \\(p\\) is the number of parameters in the regression function.\nThe alternative \\(H_A\\) includes all regression functions other than a linear one. For instance, it includes a quadratic regression function or a logarithmic one. If \\(H_A\\) is concluded, a study of residuals can be helpful in identifying an appropriate function.\n\nDemo\n\n\nR functions - Testing linear model\nManual\nTesting complex model\nMultiple regression higher order\n\n\n\n\n# generate data from ideal model (no lack of fit -&gt; expecting to not reject)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 50; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values (asuming there will be some replicates)\nx &lt;- extraDistr::rdunif(n = n, min = 0, max = 10)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + rnorm(n = n, mean = 0, sd = sigma)\n\n# plot and add lines (don't have a straight line anymore, so can't use abline() for unrestricted model)\n# -&gt; have to plot x vs y-hat, but sorted according to x\nplot(x, y)\nlm(y ~ as.factor(x)) %&gt;% \n  {data.frame(y_hat = fitted(.), x)} %&gt;% \n  arrange(x) %$% \n  lines(x, y_hat, col = \"blue\")\nabline(lm(y ~ x), col = \"red\")\n\n\n\n\n\n\n\nReduced model trend line follows group means line fairly well.\nTesting\n\\[\nH_0: E(Y) = \\beta_0 + \\beta_1 X \\hspace{20pt} \\text{Reduced model}\\\\\nH_A: E(Y) \\ne \\beta_0 + \\beta_1 X \\hspace{20pt} \\text{Full model}\n\\]\n\n# fit full model\n# -&gt; E(Y) = mu_j = anything other than B0 + B1X\nmod_full &lt;- lm(y ~ as.factor(x))\nsummary(mod_full)\n\n\nCall:\nlm(formula = y ~ as.factor(x))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.7481 -2.5202 -0.3219  2.6018  9.0833 \n\nCoefficients:\n               Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)       1.857      2.161   0.859    0.395503    \nas.factor(x)1     2.522      3.529   0.715    0.479081    \nas.factor(x)2     3.060      2.695   1.135    0.263191    \nas.factor(x)3     4.537      3.241   1.400    0.169552    \nas.factor(x)4     5.190      3.056   1.698    0.097418 .  \nas.factor(x)5     8.514      3.529   2.413    0.020631 *  \nas.factor(x)6    12.332      3.056   4.035    0.000247 ***\nas.factor(x)7    15.318      3.529   4.341 0.000097597 ***\nas.factor(x)8    16.856      3.056   5.516 0.000002436 ***\nas.factor(x)9    16.156      3.529   4.578 0.000046906 ***\nas.factor(x)10   18.337      3.056   6.000 0.000000518 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.832 on 39 degrees of freedom\nMultiple R-squared:  0.6946,    Adjusted R-squared:  0.6163 \nF-statistic: 8.871 on 10 and 39 DF,  p-value: 0.000000222\n\n# fit reduced model\n# -&gt; SLR model E(Y) + B0 + B1X\nmod_reduced &lt;- lm(y ~ x)\nsummary(mod_reduced)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.696 -2.422 -0.864  3.540  9.405 \n\nCoefficients:\n            Estimate Std. Error t value          Pr(&gt;|t|)    \n(Intercept)   1.2109     1.1448   1.058             0.295    \nx             1.9876     0.2011   9.883 0.000000000000372 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.525 on 48 degrees of freedom\nMultiple R-squared:  0.6705,    Adjusted R-squared:  0.6636 \nF-statistic: 97.66 on 1 and 48 DF,  p-value: 0.0000000000003724\n\n\n\n# run lack of fit test\n# -&gt; looking at F and PR(&gt;F)\n(at &lt;- anova(mod_full, mod_reduced))\n\nAnalysis of Variance Table\n\nModel 1: y ~ as.factor(x)\nModel 2: y ~ x\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     39 910.61                           \n2     48 982.62 -9   -72.014 0.3427 0.9546\n\n# other terms\n# -&gt; Sum of Sq = SSLF --&gt; MSLF = SSLF / change in Df\n# -&gt; RSS(1) = SSPE --&gt; MSPE = SSPE / Res. Df(1)\n# -&gt; F = MSLF / MSPE\n\n\n\nSetup dataset to manually calculate the required mean squares.\n\n# order data by X variable\n# -&gt; use this dataset for the remainder of the calculations\ndata_sorted &lt;- data.frame(y, x) %&gt;% \n  arrange(x)\n\n# refit model on sorted data (so fitted values and residuals pull in correctly later)\nmod_reduced &lt;- lm(y ~ x, data_sorted)\n\n# calculate group means by X variable\ndata_group_means &lt;- data_sorted %&gt;% \n  group_by(x) %&gt;% \n  summarize(group_mean = mean(y))\n\n# complete table to manually perform LOF test\n# -&gt; append predicted values from reduced model\n# -&gt; add group means by merging with group means dataset\n# -&gt; calculate:\n# --&gt; residual = Y - hat(Y) = obs - pred value -&gt; measure total deviation\n# --&gt; pure error = Y - bar(Y)_j = obs - group mean -&gt; measures the pure error, which is the randomness result from the data, not from the choice of model\n# --&gt; lack of fit error = bar(Y)_j - hat(Y) = group mean - pred value -&gt; measures the lack of fit deviation, which is the error result from the choice of model and could be improved with a better mode\ndata_lof_process &lt;- data_sorted %&gt;% \n  mutate(predicted_value = fitted(mod_reduced),\n         residual = residuals(mod_reduced)) %&gt;% \n  left_join(data_group_means, by = \"x\") %&gt;% \n  mutate(pure_error = y - group_mean,\n         lof_error = group_mean - predicted_value)\nhead(data_lof_process, n = 10)\n\n           y x predicted_value    residual group_mean pure_error  lof_error\n1  -6.198976 0        1.210904 -7.40988023   1.856616 -8.0555926  0.6457124\n2   2.462339 0        1.210904  1.25143455   1.856616  0.6057222  0.6457124\n3   6.148262 0        1.210904  4.93735791   1.856616  4.2916456  0.6457124\n4  10.615555 0        1.210904  9.40465124   1.856616  8.7589389  0.6457124\n5  -3.744098 0        1.210904 -4.95500170   1.856616 -5.6007141  0.6457124\n6   3.125743 1        3.198511 -0.07276864   4.378507 -1.2527646  1.1799959\n7   4.049028 1        3.198511  0.85051606   4.378507 -0.3294799  1.1799959\n8   5.960752 1        3.198511  2.76224033   4.378507  1.5822444  1.1799959\n9   4.194942 2        5.186119 -0.99117676   4.916399 -0.7214570 -0.2697198\n10  3.502219 2        5.186119 -1.68389966   4.916399 -1.4141799 -0.2697198\n\n\nFind sum of squares.\n\n# compute final sums of squares\n# -&gt; lack of fit dataset is setup so that each quantity is calculated for every row\n# --&gt; just need to square each value and sum\n(data_ss &lt;- data_lof_process %&gt;% \n  summarize(error = sum(residual^2),\n            lf = sum(lof_error^2),\n            pe = sum(pure_error^2)))\n\n     error       lf       pe\n1 982.6224 72.01433 910.6081\n\ncompare(abs(at[2, \"Sum of Sq\"]), data_ss$lf)\n\n$comparison\n[1] TRUE\n\n$`abs(at[2, \"Sum of Sq\"])`\n[1] 72.01433\n\n$`data_ss$lf`\n[1] 72.01433\n\ncompare(abs(at[1, \"RSS\"]), data_ss$pe)\n\n$comparison\n[1] TRUE\n\n$`abs(at[1, \"RSS\"])`\n[1] 910.6081\n\n$`data_ss$pe`\n[1] 910.6081\n\n# sums of squares are additive and SSPE and SSLF are partitioning SSE\n# -&gt; so check that SSE = SSPE + SSLF\ncompare(sum(data_ss[,2:3]), data_ss[,1])\n\n$comparison\n[1] TRUE\n\n$`sum(data_ss[, 2:3])`\n[1] 982.6224\n\n$`data_ss[, 1]`\n[1] 982.6224\n\n\nFind df.\n\n# set vector of df\n# -&gt; df sse = n - 2 (for slr, in general p)\n# -&gt; df sslf = c - 2 = # of x levels - 2 \n# -&gt; df sspe = n - c\nn &lt;- nrow(data_sorted)\nc &lt;- nrow(data_group_means)\ndf &lt;- c(\"error\" = n - length(coef(mod_reduced)),\n        \"lf\" = c - length(coef(mod_reduced)),\n        \"pe\" = n - c)\n\n# reshape SS data to add df\n(data_anova_table &lt;- data_ss %&gt;% \n  pivot_longer(cols = 1:3,\n               names_to = \"type\",\n               values_to = \"ss\") %&gt;% \n  bind_cols(data.frame(df)))\n\n# A tibble: 3 × 3\n  type     ss    df\n* &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 error 983.     48\n2 lf     72.0     9\n3 pe    911.     39\n\n\nCalculate mean squares.\n\n# add MS to anova table\n# -&gt; MS = SS / df\n(data_anova_table %&lt;&gt;% \n  mutate(ms = ss / df))\n\n# A tibble: 3 × 4\n  type     ss    df    ms\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 error 983.     48 20.5 \n2 lf     72.0     9  8.00\n3 pe    911.     39 23.3 \n\n# compute final LOF test statistic\n# -&gt; F* = MSLF / MSPE\n# -&gt; convert to numeric so end result isn't a dataframe\n(F_star &lt;- as.numeric(data_anova_table[data_anova_table$type == \"lf\", \"ms\"]) / as.numeric(data_anova_table[data_anova_table$type == \"pe\", \"ms\"]))\n\n[1] 0.3426964\n\ncompare(at[2, \"F\"], F_star)\n\n$comparison\n[1] TRUE\n\n$`at[2, \"F\"]`\n[1] 0.3426964\n\n$F_star\n[1] 0.3426964\n\n# compute critical value\n# -&gt; LOF critical value (and test statistic) ~ F(c-2, n-c)\n# -&gt; right tailed test\nalpha &lt;- 0.05\n(F_crit &lt;- qf(p = alpha, df1 = c - 2, df2 = n - c, lower.tail = FALSE))\n\n[1] 2.130597\n\n\nConclusion\n\n# compare test statistic and critical value\n# -&gt; critical value way: if F* &gt;&gt; F_crit, then reject\n# -&gt; p-value way: if P(F &gt; F*) &lt; alpha = then reject\nF_star &gt; F_crit\n\n[1] FALSE\n\n(p_value &lt;- pf(q = F_star, df1 = c - 2, df2 = n - c, lower.tail = FALSE))\n\n[1] 0.954596\n\np_value &lt; alpha\n\n[1] FALSE\n\ncompare(at[2, \"Pr(&gt;F)\"], p_value)\n\n$comparison\n[1] TRUE\n\n$`at[2, \"Pr(&gt;F)\"]`\n[1] 0.954596\n\n$p_value\n[1] 0.954596\n\n\nFAIL TO REJECT. There is not a lack of fit issue with the data.\n\n\n\n# generate data from complex model (lack of fit -&gt; expecting to reject)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 50; beta_0 &lt;- 1; beta_1 &lt;- 2; beta_2 &lt;- 2; beta_3 &lt;- 2; sigma &lt;- 10\n\n# generate X values (asuming there will be some replicates)\nx &lt;- extraDistr::rdunif(n = n, min = 1, max = 10)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + beta_2 * x^2 + beta_3 * log(x) + rnorm(n = n, mean = 0, sd = sigma)\n\n# plot and add lines\nplot(x, y)\nlm(y ~ as.factor(x)) %&gt;% \n  {data.frame(y_hat = fitted(.), x)} %&gt;% \n  arrange(x) %$% \n  lines(x, y_hat, col = \"blue\")\nlm(y ~ x + I(log(x))) %&gt;% \n  {data.frame(y_hat = fitted(.), x)} %&gt;% \n  arrange(x) %$% \n  lines(x, y_hat, col = \"red\")\n\n\n\n\n\n\n\nLines are close, but the reduced model is relatively far (keep in mind \\(Y\\) scale) the unrestricted model at certain \\(X\\) levels.\nTesting:\n\\[\nH_0: E(Y) = \\beta_0 + \\beta_1 X + \\beta_2 \\ln(X) \\hspace{20pt} \\text{Reduced model}\\\\\nH_A: E(Y) \\ne \\beta_0 + \\beta_1 X + \\beta_2 \\ln(X)  \\hspace{20pt} \\text{Full model}\n\\]\n\n# fit full model\n# -&gt; E(Y) = mu_j = anything other than B0 + B1 X + B2 ln(X)\nmod_full &lt;- lm(y ~ as.factor(x))\nsummary(mod_full)\n\n\nCall:\nlm(formula = y ~ as.factor(x))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.366  -5.048  -1.182   4.810  19.324 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       4.986      3.241   1.538                0.132    \nas.factor(x)2     5.301      4.584   1.156                0.254    \nas.factor(x)3    25.173      4.771   5.276    0.000004897349306 ***\nas.factor(x)4    44.119      4.771   9.247    0.000000000017580 ***\nas.factor(x)5    71.487      6.876  10.396    0.000000000000622 ***\nas.factor(x)6    82.453      5.918  13.932 &lt; 0.0000000000000002 ***\nas.factor(x)7   105.557      6.876  15.351 &lt; 0.0000000000000002 ***\nas.factor(x)8   139.367      4.771  29.210 &lt; 0.0000000000000002 ***\nas.factor(x)9   173.071      4.584  37.755 &lt; 0.0000000000000002 ***\nas.factor(x)10  212.277      5.375  39.491 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.576 on 40 degrees of freedom\nMultiple R-squared:  0.9889,    Adjusted R-squared:  0.9864 \nF-statistic:   397 on 9 and 40 DF,  p-value: &lt; 0.00000000000000022\n\n# fit reduced model\n# -&gt; polynomial model E(Y) + B0 + B1 X + B2 ln(X)\nmod_reduced &lt;- lm(y ~ x + I(log(x)))\nsummary(mod_reduced)\n\n\nCall:\nlm(formula = y ~ x + I(log(x)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.0174  -7.0840  -0.1601   5.4293  20.3323 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  -26.641      2.818  -9.453      0.0000000000019 ***\nx             33.352      1.501  22.216 &lt; 0.0000000000000002 ***\nI(log(x))    -43.364      6.077  -7.136      0.0000000050826 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.557 on 47 degrees of freedom\nMultiple R-squared:  0.9838,    Adjusted R-squared:  0.9832 \nF-statistic:  1431 on 2 and 47 DF,  p-value: &lt; 0.00000000000000022\n\n# perform lack of fit test\nanova(mod_full, mod_reduced)\n\nAnalysis of Variance Table\n\nModel 1: y ~ as.factor(x)\nModel 2: y ~ x + I(log(x))\n  Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)  \n1     40 2941.9                             \n2     47 4292.4 -7   -1350.4 2.623 0.02503 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# this matched when ran same scenario through manual process!\n\nThis shows significant lack of fit. So add in missing polynomial and retry.\n\n# plot and add line (don't have a straight line anymore, so can't use abline())\n# -&gt; have to plot x vs y-hat, but sorted according to x\n# -&gt; NOTE both TRUE and FALSE for raw lead to SAME predictions, but DRASTISCALLY different estimates, perhaps because multicollinearity gives biased estimates\nplot(x, y)\nlm(y ~ as.factor(x)) %&gt;% \n  {data.frame(y_hat = fitted(.), x)} %&gt;% \n  arrange(x) %$% \n  lines(x, y_hat, col = \"blue\")\nlm(y ~ poly(x, 2, raw = TRUE)) %&gt;% \n  {data.frame(y_hat = fitted(.), x)} %&gt;% \n  arrange(x) %$% \n  lines(x, y_hat, col = \"red\")\n\n\n\n\n\n\n\nNow the unrestriced and the reduced model are almost equivalent at all points.\n\n# fit full model\n# -&gt; E(Y) = mu_j = anything other than B0 + B1 X + B2 ln(X)\nmod_full &lt;- lm(y ~ as.factor(x))\nsummary(mod_full)\n\n\nCall:\nlm(formula = y ~ as.factor(x))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.366  -5.048  -1.182   4.810  19.324 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       4.986      3.241   1.538                0.132    \nas.factor(x)2     5.301      4.584   1.156                0.254    \nas.factor(x)3    25.173      4.771   5.276    0.000004897349306 ***\nas.factor(x)4    44.119      4.771   9.247    0.000000000017580 ***\nas.factor(x)5    71.487      6.876  10.396    0.000000000000622 ***\nas.factor(x)6    82.453      5.918  13.932 &lt; 0.0000000000000002 ***\nas.factor(x)7   105.557      6.876  15.351 &lt; 0.0000000000000002 ***\nas.factor(x)8   139.367      4.771  29.210 &lt; 0.0000000000000002 ***\nas.factor(x)9   173.071      4.584  37.755 &lt; 0.0000000000000002 ***\nas.factor(x)10  212.277      5.375  39.491 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.576 on 40 degrees of freedom\nMultiple R-squared:  0.9889,    Adjusted R-squared:  0.9864 \nF-statistic:   397 on 9 and 40 DF,  p-value: &lt; 0.00000000000000022\n\n# fit reduced model\n# -&gt; polynomial model E(Y) + B0 + B1 X + B2 ln(X)\nmod_reduced &lt;- lm(y ~ poly(x, 2, raw = TRUE) + I(log(x)))\nsummary(mod_reduced)\n\n\nCall:\nlm(formula = y ~ poly(x, 2, raw = TRUE) + I(log(x)))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.986  -5.665  -1.486   5.851  18.704 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)              -4.4481     7.3011  -0.609  0.54536   \npoly(x, 2, raw = TRUE)1   6.4247     8.4044   0.764  0.44851   \npoly(x, 2, raw = TRUE)2   1.5391     0.4740   3.247  0.00218 **\nI(log(x))                 0.2874    14.5391   0.020  0.98431   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.713 on 46 degrees of freedom\nMultiple R-squared:  0.9869,    Adjusted R-squared:  0.986 \nF-statistic:  1152 on 3 and 46 DF,  p-value: &lt; 0.00000000000000022\n\n# perform lack of fit test\nanova(mod_full, mod_reduced)\n\nAnalysis of Variance Table\n\nModel 1: y ~ as.factor(x)\nModel 2: y ~ poly(x, 2, raw = TRUE) + I(log(x))\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     40 2941.9                           \n2     46 3491.9 -6   -549.96 1.2462 0.3039\n\n\nThis shows no lack of fit now!\n\n\nTesting:\n\\[\nH_0: E(Y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2\\hspace{20pt} \\text{Reduced model}\\\\\nH_A: E(Y) \\ne \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2\\  \\hspace{20pt} \\text{Full model}\n\\]\nNote that each \\(X\\) variable is centered and scaled (to avoid multicollinearity of higher order terms): \\(x_{i1} = X_{i1} - \\bar{X_1}\\) and \\(x_{i2} = X_{i2} - \\bar{X_2}\\).\n\n# read in data (from table 8.1 on page 300 of textbook)\nx1 &lt;- c(-1,0,1,-1,0,0,0,1,-1,0,1)\nx2 &lt;- c(-1,-1,-1,0,0,0,0,0,1,1,1)\ny &lt;- c(150,86,49,288,157,131,184,109,279,235,224)\n\n# fit full model\n# -&gt; E(Y) = mu_j = anything other than the reduced model\n# -&gt; note that x1*x2 denotes factor crossing = x1 + x2 + x1:x2\nmod_full &lt;- lm(y ~ as.factor(x1) * as.factor(x2))\nsummary(mod_full)\n\n\nCall:\nlm(formula = y ~ as.factor(x1) * as.factor(x2))\n\nResiduals:\n                     1                      2                      3 \n  0.000000000000003116   0.000000000000001792  -0.000000000000002450 \n                     4                      5                      6 \n -0.000000000000005551  -0.333333333333327431 -26.333333333333339255 \n                     7                      8                      9 \n 26.666666666666664298   0.000000000000005247   0.000000000000002271 \n                    10                     11 \n -0.000000000000000173  -0.000000000000003009 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)                     150.00      26.50   5.660   0.0298 *\nas.factor(x1)0                  -64.00      37.48  -1.708   0.2298  \nas.factor(x1)1                 -101.00      37.48  -2.695   0.1145  \nas.factor(x2)0                  138.00      37.48   3.682   0.0665 .\nas.factor(x2)1                  129.00      37.48   3.442   0.0750 .\nas.factor(x1)0:as.factor(x2)0   -66.67      48.39  -1.378   0.3022  \nas.factor(x1)1:as.factor(x2)0   -78.00      53.00  -1.472   0.2790  \nas.factor(x1)0:as.factor(x2)1    20.00      53.00   0.377   0.7422  \nas.factor(x1)1:as.factor(x2)1    46.00      53.00   0.868   0.4770  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.5 on 2 degrees of freedom\nMultiple R-squared:  0.9768,    Adjusted R-squared:  0.8841 \nF-statistic: 10.54 on 8 and 2 DF,  p-value: 0.08953\n\n# fit reduced model\n# -&gt; second order model\n# -&gt; can use I() because data is already transformed for multicollinearity\nmod_reduced &lt;- lm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2))\nsummary(mod_reduced)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1 * x2))\n\nResiduals:\n      1       2       3       4       5       6       7       8       9      10 \n-21.465   9.263  12.202  41.930  -5.842 -31.842  21.158 -25.404 -20.465   7.263 \n     11 \n 13.202 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   162.84      16.61   9.805 0.000188 ***\nx1            -55.83      13.22  -4.224 0.008292 ** \nx2             75.50      13.22   5.712 0.002297 ** \nI(x1^2)        27.39      20.34   1.347 0.235856    \nI(x2^2)       -10.61      20.34  -0.521 0.624352    \nI(x1 * x2)     11.50      16.19   0.710 0.509184    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.37 on 5 degrees of freedom\nMultiple R-squared:  0.9135,    Adjusted R-squared:  0.8271 \nF-statistic: 10.57 on 5 and 5 DF,  p-value: 0.01086\n\n# perform lack of fit test\nanova(mod_full, mod_reduced)\n\nAnalysis of Variance Table\n\nModel 1: y ~ as.factor(x1) * as.factor(x2)\nModel 2: y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1 * x2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      2 1404.7                           \n2      5 5240.4 -3   -3835.8 1.8205 0.3738\n\n# this matched when ran same scenario through manual process!\n# -&gt; just had to group_by(x1, x2) for data_group_means and join_by(\"x1\", \"x2\")\n\nThis shows that there is no lack of fit, so a higher order model isn’t needed.\nNow we can do a partial \\(F\\) test to determine if the second order terms are needed or if a first order model is sufficient.\nTesting:\n\\[\nH_0: \\beta_{11} = \\beta_{22} x_2^2 = \\beta_{12} = 0\\\\\nH_A: \\text{Not all } \\beta \\text{s in } H_0\\text{ equal } 0\n\\]\n\n# fit new full model (second order)\n# -&gt; second order model\nmod_full &lt;- lm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2))\nsummary(mod_full)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1 * x2))\n\nResiduals:\n      1       2       3       4       5       6       7       8       9      10 \n-21.465   9.263  12.202  41.930  -5.842 -31.842  21.158 -25.404 -20.465   7.263 \n     11 \n 13.202 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   162.84      16.61   9.805 0.000188 ***\nx1            -55.83      13.22  -4.224 0.008292 ** \nx2             75.50      13.22   5.712 0.002297 ** \nI(x1^2)        27.39      20.34   1.347 0.235856    \nI(x2^2)       -10.61      20.34  -0.521 0.624352    \nI(x1 * x2)     11.50      16.19   0.710 0.509184    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.37 on 5 degrees of freedom\nMultiple R-squared:  0.9135,    Adjusted R-squared:  0.8271 \nF-statistic: 10.57 on 5 and 5 DF,  p-value: 0.01086\n\n# fit new reduced model (first order)\n# -&gt; needs to be nested\nmod_reduced &lt;- lm(y ~ x1 + x2)\nsummary(mod_reduced)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.000 -13.750  -7.167  10.167  60.167 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  172.000      9.354  18.387 0.0000000788 ***\nx1           -55.833     12.666  -4.408     0.002262 ** \nx2            75.500     12.666   5.961     0.000338 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.02 on 8 degrees of freedom\nMultiple R-squared:  0.8729,    Adjusted R-squared:  0.8412 \nF-statistic: 27.48 on 2 and 8 DF,  p-value: 0.0002606\n\n# perform partial F test\nanova(mod_full, mod_reduced)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1 * x2)\nModel 2: y ~ x1 + x2\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      5 5240.4                           \n2      8 7700.3 -3   -2459.9 0.7823 0.5527\n\n\nThis says that we can drop the higher order terms and a first order model is sufficient.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#overview-of-remedial-measures",
    "href": "notes-diagnostics-and-remedial-measures.html#overview-of-remedial-measures",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.6 Overview of remedial measures",
    "text": "3.6 Overview of remedial measures\n\n\nOverview\nMore details on transformations\n\n\n\n\n\nIf the normal error SLR model is not appropriate for a dataset, there are two basic choices, both of which have their advantages and disadvantages:\n\nAbandon this model and develop and use a more appropriate model.\n\n\nPro → May lead to a more complex model that could yield better insights.\nCon → May also lead to more complex procedures for estimating the parameters.\n\n\nUse some transformation on the data so that the normal error SLR model is appropriate for the transformed data.\n\n\nPro → Leads to relatively simple methods of estimation with smaller models (less parameters), which is desirable when the sample size is small.\nCon → Transformations may obscure the fundamental interconnections between the variables, though at other times they may illuminate them.\nNow we consider only transformations (will cover more complex models in later chapters).\n\n\n\n\n\n\nHere is a more in depth discussion of when to use which transformation:\nNonlinearity of the regression function\n\n\nWhen the regression function is not linear, a direct approach is to modify the SLR regression model by altering the nature of the regression function (these methods will be discussed much later). For example,\n\nQuadratic regression function (this is an example of a polynomial regression function) → \\(E(Y) = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\)\nExponential regression function (another example of a nonlinear regression function) → \\(E(Y) = \\beta_0 \\cdot \\beta_1^X\\)\n\n\nThe transformation approach uses a transformation to linearize (at least approximately linearize) a nonlinear regression function.\nWhen the nature of the regression function is unknown, exploratory analysis that does not require specifying a particular type of function is often useful (such as LOWESS regression).\n\nNonconstancy of the error variance\n\nWhen the error variance is not constant but varies in a systematic fashion, a direct approach is to modify the model to allow for this and use the method of weighted least squares to obtain the estimators of the parameters.\nTransformations can also be effective in stabilizing the variance, some will be shown shortly.\n\nNonindependence of error terms\n\nWhen the error terms are correlated, the easiest method is to switch to a time series model.\n\nNonnormality of error terms\n\nLack of normality and nonconstant error variances occur together. Fortunately, it is often the case that the same transformation that helps stabilize the variance is also helpful in approximately normalizing the error terms.\nThis is why we should address the nonconstant variance (apply the appropriate stabilizing transformation), then do residual analysis to see if there are still serious departures from normality present.\n\nOmission of important predictor variables\n\nWhen residual analysis indicates that an important predictor variable has been omitted from the model, the solution is to modify the model with multiple regression (two or more predictor variables).\n\nUnusual observations\n\nWhen unusual observations are present (could be outliers, high-leverage points, or influential points) using LSE or MLE may lead to serious distortions in the estimated regression function.\nWhen these unusual obesrvations are not data errors (and thus shouldn’t be thrown out), it may be desirable to use an estimation procedure that places less emphasis on them. One type of these models is discuss much later.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#transformations",
    "href": "notes-diagnostics-and-remedial-measures.html#transformations",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.7 Transformations",
    "text": "3.7 Transformations\n\nNow we will talk about the use of transformations on one or both of the original variables before carrying out the regression analysis.\nSimple transformations of either the response variable \\(Y\\) or the predictor variable \\(X\\), or of both, are often sufficient to make the SLR model appropriate for the transformed data.\n\n\n3.7.1 Transformations for nonlinear relation only\nOverview\n\nGoal → Linearize a nonlinear regression relation.\nUse when → Distribution of error terms is reasonably close to a normal distribution AND have approximately constant variance.\n\nStrategy → Transformations on \\(X\\) should be attempted.\n\nTransformations on \\(Y\\), such as \\(Y' = \\sqrt{Y}\\), may not be desirable here because they can materially change the shape of the distribution of the error terms from the normal distribution and may also lead to substantially differing error term variances.\ne.g.) \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\), where \\(\\epsilon \\sim \\text{Normal}\\,\\) \\(\\rightarrow\\) \\(\\sqrt{Y} = \\beta_0 + \\beta_1 X + \\text{new }\\epsilon\\)\n\n\n\nSpecific transformations\n\n\nBelow are some prototype nonlinear regression relations with some simple transformations on \\(X\\) that may be helpful for linearizing without affecting the distribution of \\(Y\\).\n\n\nSeveral transformations can be tried.\n\nScatter plots and residual plots based on each should be made and analyzed to decide which transformation is most effective.\n\n\n\n\n\n\n\n\nAt times, it may be helpful to introduce a constant into the transformation.\n\ne.g. If some of the \\(X\\) data are near zero and the reciprocal transformation is desired, we can shift the origin by using the transformation \\(X' = \\frac{1}{X + k}\\), where \\(k\\) is an appropriately chosen constant.\n\n\n\nDemo\n\n\nIncreasing at decreasing rate\nIncreasing at increasing rate\nDecreasing at decreasing rate\nEffect of constants and scalars\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nMain issue → See curvilinear pattern (in this case \\(Y\\) grows slower as \\(X\\) increases). So standard SLR model not appropriate (note that in reality, we have no idea what type model this came from).\nRoughly same variance in \\(Y\\) as \\(X\\) increases (imagining spread of observations relative to the smooth curve; the residual plot looks bad because of the lack of fit) and QQ plot looks decent enough → So only issue is the linearity, which means we want to transform \\(X\\) based on the specific pattern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt two transformations chosen based on the observed pattern: 1a) \\(X' = \\log_{10}(X)\\) 1b) \\(X' = \\ln(X)\\) and 2) \\(X' = \\sqrt{X}\\). Then view transformed scatterplots.\n\nAll greatly improve the linear aspect of the regression. Note that \\(\\log_{10}(X)\\) and \\(\\ln(X)\\) have the same linearizing effect, obviously the scales are different for the results.\nThe variability of the scatter at the different \\(X\\) levels is the same as before, since we did not make a transformation on \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at diagnostics for fitted models based on transformed \\(X\\)s.\n\nResidual plots are just fine now and normal QQ plots still look good.\n\n\n\n\n# fit models on transformed X variables\nmod_log10_x &lt;- lm(y ~ log10(x))\nmod_ln_x &lt;- lm(y ~ log(x))\nmod_sqrt_x &lt;- lm(y ~ sqrt(x))\n\n# view some diagnostic plots for models with transformations\nplot(mod_log10_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_ln_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_sqrt_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# view model summaries (just the R^2)\n# -&gt; ln and log10 models give different coefficients, but SAME MSE and R^2, see last tab 'effect of scalars and constants' for why\nsummary(mod_log10_x)\n\n\nCall:\nlm(formula = y ~ log10(x))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48449 -0.17303  0.00477  0.18981  0.57549 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   2.0812     0.1319   15.77 0.00000000000000184 ***\nlog10(x)      2.0364     0.1294   15.73 0.00000000000000197 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2807 on 28 degrees of freedom\nMultiple R-squared:  0.8984,    Adjusted R-squared:  0.8947 \nF-statistic: 247.5 on 1 and 28 DF,  p-value: 0.00000000000000197\n\nsummary(mod_ln_x)\n\n\nCall:\nlm(formula = y ~ log(x))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48449 -0.17303  0.00477  0.18981  0.57549 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  2.08115    0.13194   15.77 0.00000000000000184 ***\nlog(x)       0.88440    0.05622   15.73 0.00000000000000197 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2807 on 28 degrees of freedom\nMultiple R-squared:  0.8984,    Adjusted R-squared:  0.8947 \nF-statistic: 247.5 on 1 and 28 DF,  p-value: 0.00000000000000197\n\nsummary(mod_sqrt_x)$r.squared\n\n[1] 0.9444231\n\n\n\nCompare these the models \\(R^2\\) to the original untransformed: 0.9\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nSee curvilinear pattern (in this case \\(Y\\) grows faster as \\(X\\) increases) \\(\\Longrightarrow\\) SLR model not appropriate.\nConstant variance through smooth curve and okay QQ plot \\(\\Longrightarrow\\) Try \\(X\\) transformation to straighten out \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt two transformations chosen based on the observed pattern: 1) \\(X' = X^2\\) and 2) \\(X' = \\mathrm{e}^X\\).\n\nBoth seem to improve the linearity, although not perfectly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at diagnostics for fitted models based on transformed \\(X\\)s.\n\nStill a bit of curvature in the residual plot, but normal QQ plots still look good.\n\n\n\n\n# fit models on transformed X variables\nmod_squared_x &lt;- lm(y ~ I(x^2))\nmod_exp_x &lt;- lm(y ~ exp(x))\n\n# view some diagnostic plots for models with transformations\nplot(mod_squared_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_exp_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# view model summaries (just the R^2)\nsummary(mod_squared_x)$r.squared\n\n[1] 0.9054508\n\nsummary(mod_exp_x)$r.squared\n\n[1] 0.9091815\n\n\n\nCompare these the models \\(R^2\\) to the original untransformed: 0.814\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nSee curvilinear pattern (in this case \\(Y\\) decreases slower as \\(X\\) increases) \\(\\Longrightarrow\\) SLR again model not appropriate.\nStill constant variance and approximately normal residuals, \\(\\Longrightarrow\\) Just transform \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt two transformations chosen based on the observed pattern: 1a) \\(X' = 1 / X\\) 1b) \\(X' = 1 / (X + k)\\) and 2) \\(X' = \\mathrm{e}^{-X}\\).\n\n\\(1 / X\\) does not result in a linear relationship, so no need to continue with that. However all \\(X\\) values are in between 0 and 1, so we can try to introduce a constant to move data away from origin, say \\(X' = 1 / (1 + X)\\). This works better. \\(\\mathrm{e}^{-X}\\) seems to helps as well.\nNote that both transformations are decreasing, which means the association between \\(Y\\) and \\(X'\\) gets flipped and is now increasing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at diagnostics for fitted models based on transformed \\(X\\)s.\n\nStill a bit of curvature in the residual plot, but normal QQ plots still look good.\n\n\n\n\n# fit models on transformed X variables\nmod_reciprocal_x_k &lt;- lm(y ~ I(1/(x+1)))\nmod_exp_neg_x &lt;- lm(y ~ exp(-x))\n\n# view some diagnostic plots for models with transformations\nplot(mod_reciprocal_x_k, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_exp_neg_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# view model summaries (just the R^2)\nsummary(mod_reciprocal_x_k)$r.squared\n\n[1] 0.8441997\n\nsummary(mod_exp_neg_x)$r.squared\n\n[1] 0.8128284\n\n\n\nCompare these the models \\(R^2\\) to the original untransformed: 0.728\n\n\n\nQuick demo of introducing constants to one or both sides of regression equation (so to the sample essentially) to see how impact results\nResults\n\n\nCoefficients\n\nAdding constants (all combos \\(X + k\\), \\(Y + k\\) and both) → Changes intercept, doesn’t affect slope.\nMultiplying by scalar → \\(k \\cdot X\\) doesn’t affect intercept, but does affect slope; \\(k \\cdot Y\\) does affect intercept and slope; Both \\(k \\cdot X\\) and \\(k \\cdot Y\\) have same intercept as only \\(k \\cdot Y\\) (because \\(k \\cdot X\\) doesn’t affect it), but the same slope as the original \\(X\\) and \\(Y\\) (in essence cancelling each others’ effect).\n\n\n\n\\(R^2\\) and \\(F\\)-stat\n\nAll models have the same.\n\n\n\n\\(MSE\\) and likelihood-related statistics (\\(loglik\\), \\(AIC\\), \\(BIC\\))\n\nOnly \\(k \\cdot Y\\) models have different \\(MSE\\) (larger because \\(Y\\) values are stretched with \\(\\lvert k \\rvert &gt; 1\\)) and likelihood-related statistics (optimizing a function of \\(Y\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n call \n    b0 \n    se(b0) \n    b1 \n    se(b1) \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n  \n\n\n lm = y ~ x \n    1.211 \n    1.028 \n    2.000 \n    0.143 \n    0.875 \n    0.871 \n    3.187 \n    196.706 \n    0 \n    1 \n    -76.303 \n    158.605 \n    162.809 \n  \n\n lm = y ~ I(x + k1) \n    -4.790 \n    1.402 \n    2.000 \n    0.143 \n    0.875 \n    0.871 \n    3.187 \n    196.706 \n    0 \n    1 \n    -76.303 \n    158.605 \n    162.809 \n  \n\n lm = I(y + k1) ~ x \n    4.211 \n    1.028 \n    2.000 \n    0.143 \n    0.875 \n    0.871 \n    3.187 \n    196.706 \n    0 \n    1 \n    -76.303 \n    158.605 \n    162.809 \n  \n\n lm = I(y + k1) ~ I(x + k1) \n    -1.790 \n    1.402 \n    2.000 \n    0.143 \n    0.875 \n    0.871 \n    3.187 \n    196.706 \n    0 \n    1 \n    -76.303 \n    158.605 \n    162.809 \n  \n\n lm = y ~ I(k2 * x) \n    1.211 \n    1.028 \n    1.000 \n    0.071 \n    0.875 \n    0.871 \n    3.187 \n    196.706 \n    0 \n    1 \n    -76.303 \n    158.605 \n    162.809 \n  \n\n lm = I(k2 * y) ~ x \n    2.422 \n    2.057 \n    4.001 \n    0.285 \n    0.875 \n    0.871 \n    6.373 \n    196.706 \n    0 \n    1 \n    -97.097 \n    200.194 \n    204.397 \n  \n\n lm = I(k2 * y) ~ I(k2 * x) \n    2.422 \n    2.057 \n    2.000 \n    0.143 \n    0.875 \n    0.871 \n    6.373 \n    196.706 \n    0 \n    1 \n    -97.097 \n    200.194 \n    204.397 \n  \n\n lm = y ~ log(x) \n    3.043 \n    1.660 \n    7.046 \n    0.950 \n    0.663 \n    0.651 \n    5.243 \n    55.020 \n    0 \n    1 \n    -91.238 \n    188.476 \n    192.680 \n  \n\n lm = y ~ log10(x) \n    3.043 \n    1.660 \n    16.223 \n    2.187 \n    0.663 \n    0.651 \n    5.243 \n    55.020 \n    0 \n    1 \n    -91.238 \n    188.476 \n    192.680 \n  \n\n\n\n\n\\(log_{10}(X)\\) and \\(\\ln(X)\\)\n\nThese equate to the models \\(y \\sim \\ln(X)\\) and \\(y \\sim k \\cdot log_{10}(X)\\), so we can switch between the two and these results show why these two transformations are essentially equivalent. Only the slopes of the resulting models differ, every other model summary is the same.\n\n\n\n\n\n\n3.7.2 Transformations for Nonnormality and Unequal Error Variances\nOverview\n\nUnequal error variances and nonnormality of the error terms frequently appear together.\n\nOften, these two departures show up as increasing skewness and increasing variability of the distributions of the error terms as the mean response \\(E(Y)\\) increases.\n\ne.g. Regression on yearly household expenditures on vacation versus household income. There will tend to be more variation and greater positive skewness (i.e. some very high yearly vacation expenditures) for high-income households than for low-income households, who tend to consistently spend much less for vacations.\n\n\nGoal → Fix nonnormality and nonconstant variance of regression relation.\n\nStrategy → We need a transformation on \\(Y\\) because the shapes and spreads of the distributions of \\(Y\\) need to be changed.\n\nA transformation like this may also help to linearize a curvilinear regression relation.\nOther times, a simultaneous transformation on \\(X\\) may be needed to obtain or maintain a linear regression relation.\n\n\n\nSpecific transformations\n\n\nBelow are some prototype regression relations with some simple transformations on \\(Y\\) that may be helpful for fixing non normality and unequal variances of the error distributions.\n\nSeveral alternative transformations on \\(Y\\) may be tried, as well as some simultaneous transformations on \\(X\\).\nAgain, scatter plots and residual plots should be made to determine the most effective transformation(s).\n\n\n\n\n\nSame as with the reciprocal transformation \\(X' = \\frac{1}{X + k}\\), we can consider use of constant values to validate the transformation function. e.g. If \\(Y\\) is negative, we can shift the origin for the log transformation \\(Y' = \\ln(Y + k)\\) so all \\(Y + k &gt; 0\\) for all \\(Y\\).\n\nDemo\n\nIncreasing linear trend, with increasing variance\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nMain issue → Because of the nonconstant variance, this leads us to think that there is issues with the error variance, perhaps also the structure of the model (but impossible to verify this totally). No serious departures from normality. Slight curvilinear pattern, but could be just because of the random non-constant variance\nBefore attempting transformations to linearize (working on \\(X\\)) or nonlinear regression, we should transform \\(Y\\) based on this pattern. This transformation may help to linearize and improve the normality as well (even though not of concern). If still not good, can try transformation on \\(X\\) in conjunction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt transformations chosen based on the observed pattern: \\(Y' = \\frac{1}{Y}\\). Then view transformed scatterplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the non-constant variance appears to be fixed, but there is a curvi-linear relationship now. So we can try to transform \\(X\\) at the same time according to the prototype pattern.\n\n\n# view scatterplot, regression line and diagnostic plots for potential transformations\nplot(x = 1/x, y = 1/y, main = \"Transformation = 1/Y and 1/X\")\nabline(lm(1/y ~ I(1/x)), col = \"red\")\n\n\n\n\n\n\nplot(lm(1/y ~ I(1/x)), which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis introduces some other issues, so perhaps another method such as weighted least squares would be more appropriate.\n\n\n\n\n\n3.7.3 Box-Cox transformations\nMotivation:\n\nIt is often difficult to determine from diagnostic plots which transformation of \\(Y\\) is most appropriate for correcting skewness of the distributions of error terms, unequal error variances, and nonlinearity of the regression function.\nThe Box-Cox procedure automatically identifies a transformation from the family of power transformations on \\(Y\\).\n\nPower transformations have the form:\n\\[\nY' = Y^\\lambda,\n\\]\nwhere \\(\\lambda\\) is a parameter to be determined from the data.\nThis family encompasses the following simple transformations:\n\nThe normal error regression model with the response variable a member of the family of transformations becomes:\n\\[\nY^\\lambda = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\n\nThis includes an additional parameter \\(\\lambda\\) that needs to be estimated.\nThe Box-Cox procedure uses the method of maximum likelihood to estimate \\(\\lambda\\), as well as the other parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\).\nIn this way, the Box-Cox procedure identifies \\(\\hat{\\lambda}\\), the maximum likelihood estimate of \\(\\lambda\\) to use in the power transformation.\n\nDemo\n\n\nR functions\nManual\nResults\n\n\n\n\n# use same setup as linearly increasing trend with increasing variance\n\n## initialize items\n# -&gt; sample size, population parameters\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 0, max = 20)\n\n# generate error terms\n# -&gt; uniform errors that can increase with x\nepsilon &lt;- runif(n = n, min = 0, max = 2*x)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# scatter plot with poorly fit regression line and smoothed curve added\nplot(x = x, y = y, main = \"Nonnormal errors with increasing variance \\n ~ Increasing to with linear trend\")\nabline(lm(y ~ x), col = \"red\")\n\n\n\n\n\n\n\nBy observation of the prototype pattern and the results, we tried the model \\(1/Y \\sim 1/X\\) to some success. Let’s see what the Box-Cox procedure recommends.\n\n# run boxcox procedure\n# -&gt; plot MLE of lambda\nMASS::boxcox(lm(y ~ x))\n\n# extract lambda\nbc &lt;- MASS::boxcox(lm(y ~ x))\n\n\n\n\n\n\nbc$x[which.max(bc$y)]\n\n[1] 0.5050505\n\n# run boxcox procedure\n# -&gt; plot of SSE\nALSM::boxcox.sse(x, y, l = seq(from = -1, to = 1, by = 0.1)) # to shorten output\n\n\n\n\n\n\n\n   lambda       SSE\n1    -1.0 5233.8247\n2    -0.9 4372.5719\n3    -0.8 3664.2073\n4    -0.7 3081.1363\n5    -0.6 2601.0649\n6    -0.5 2205.9735\n7    -0.4 1881.2990\n8    -0.3 1615.2812\n9    -0.2 1398.4423\n10   -0.1 1223.1711\n21    0.0 1083.3934\n11    0.1  974.3100\n12    0.2  892.1916\n13    0.3  834.2179\n14    0.4  798.3553\n15    0.5  783.2663\n16    0.6  788.2449\n17    0.7  813.1761\n18    0.8  858.5166\n19    0.9  925.2949\n20    1.0 1015.1301\n\n\n\n\nCan use standard regression procedures to obtain a \\(\\hat{lambda}\\) when software doesn’t automatically provide it. This procedure involves a numerical search in a range of potential \\(\\lambda\\) values such as \\(\\lambda = -2, -1.75, ..., 1.75, 2\\). For each \\(\\lambda\\) value:\n\n\n\\(Y_i^\\lambda\\) observations are first standardized so that the magnitude of the error sum of squares does not depend on the value of \\(\\lambda\\):\n\n\\[\nW_i = \\left\\{\n  \\begin{array}{ll}\n     K_1 (Y_i^\\lambda - 1) & \\lambda \\ne 0\\\\\n     K_2 \\ln(Y_i) & \\lambda = 0\\\\\n  \\end{array}\n\\right.\n\\]\n\nwhere \\(K_2 = \\big(\\prod_{i = 1}^{n}Y_i \\big)^{1/n}\\) and \\(K_1 = \\frac{1}{\\lambda K_2^{\\lambda -1}}\\) (note that \\(K_2\\) is the geometric mean of the \\(Y_i\\) observations).\n\n\nOnce the standardized observations \\(W_i\\) have been obtained for a given \\(\\lambda\\) value, they are regressed on the predictor variable \\(X\\) and the error sum of squares \\(SSE\\) is obtained.\n\n\nIt can be shown that the maximum likelihood estimate \\(\\hat{\\lambda}\\) is that value of \\(\\lambda\\) for which \\(SSE\\) is a minimum.\nIf desired, a finer search can be conducted in the neighborhood of the \\(\\lambda\\) value that minimizes \\(SSE\\). However, the Box-Cox procedure ordinarily is used only to provide a guidefor selecting a transformation, so overly precise results are not needed.\n\n\n# define set of lambdas\nlambdas &lt;- seq(from = -1, to = 1, by = 0.01)\n\n# standardize repsonses\nbox_cox &lt;- function(y, x, lambda = 1) {\n  \n  # define constants\n  K_2 = prod(y)^(1 / length(y))\n  K_1 = 1 / (lambda * K_2^(lambda - 1))\n  \n  # standardize y observations\n  W = if(lambda == 0){\n    K_2 * log(y)\n  }else{\n    K_1 * y^lambda - 1\n  }\n  \n  # regress W on X and get sse\n  sum(resid(lm(W ~ x))^2)\n}\n\n# calculate sse for each lambda value\nsse &lt;- lambdas %&gt;% map_dbl(\\(l) box_cox(y, x, lambda = l))\n\n# plt results and get minimum lambda\nplot(x = lambdas, y = sse, type = \"l\")\nabline(h = min(sse), col = \"grey\")\nabline(v = lambdas[which.min(sse)], col = \"grey\")\n\n\n\n\n\n\nlambdas[which.min(sse)]\n\n[1] 0.52\n\n\n\n\nBoth the R functions and the manual search suggest \\(\\hat{\\lambda} = 0.5\\) \\(\\longrightarrow\\) \\(Y' = \\sqrt{Y}\\). Lets fit the new model and view the diagnostic plots.\n\n# view scatterplot, regression line and diagnostic plots of transformation\nplot(x = x, y = sqrt(y), main = \"Transformation = sqrt(Y)\")\nabline(lm(sqrt(y) ~ x), col = \"red\")\n\n\n\n\n\n\nplot(lm(sqrt(y) ~ x), which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCould now try other remedial measures. But there seems to be issues beyond what a simple transformation can fix. Other methods should be used for this particular example.\n\n\n\nIn practice:\n\nWhen the Box-Cox procedure leads to a \\(\\lambda\\) value near 1, no transformation of \\(Y\\) may be needed.\n\nIt is reasonable to choose a nearby \\(\\lambda\\) value (e.g. can use \\(\\hat{\\lambda} = -0.5\\) instead of \\(\\hat{\\lambda} = -0.79\\)).\n\nThis can make it easier to understand without sacrificing much in terms of the effectiveness of the transformation.\nWhen choosing a more intuitive \\(\\hat{\\lambda}\\), we can look flatness of the likelihood function near the maximum or at the confidence interval for \\(\\lambda\\).\nThis works because the \\(SSE\\) is often fairly stable in the neighborhood of the estimate.\n\n\nAfter a transformation has been tentatively selected, residual plots and other analyses described earlier need to be employed to ascertain that the simple linear regression model is appropriate for the transformed data.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#using-the-transformed-model",
    "href": "notes-diagnostics-and-remedial-measures.html#using-the-transformed-model",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.8 Using the transformed model",
    "text": "3.8 Using the transformed model\n\n3.8.1 Overview\nWhen implementing a transformation, interpretations of the results have now changed.\nRegression coefficients:\n\n\nThe estimators \\(\\beta_0\\) and \\(\\beta_1\\) obtained by least squares have the least squares properties with respect to the transformed observations, not the original ones.\n\ni.e. Estimated coefficients need to be interpreted with respect to the transformed scales.\n\n\nThere is no straightforward way to ‘untransform’ the coefficients to values that can be interpreted on the original scales.\nWe cannot directly compare regression coefficients between models where the response variable transformation is different.\n\nEstimation and prediction:\n\n\nEven if we transform the response, we still want to express model estimates and predictions back in the original scale.\n\nThis is simply a matter of ‘back transforming’ by using the inverse function of the transformation.\nThis is very helpful for communicating results to the public.\n\n\n\n3.8.2 Interpreting confidence limits\nIn general, let \\(Y' = f(Y)\\) and let \\(f'\\) be the back-transformation (inverse) function.\n\nFor example, if \\(Y' = f(Y) = Y^2)\\) \\(\\longrightarrow\\) \\(f'(Y') = Y\\), so \\(f'(Y') = \\sqrt{Y^2} = Y\\).\n\nThen, back transform the mean and single response confidence interval \\((a, b)\\) as following: \\((f'(a), f'(b))\\). For example: \\((\\sqrt{a}, \\sqrt{b})\\).\nBack transforming the coefficients or the standard error is not as straightforward.\n\nDO NOT DO: \\(\\hat{Y} = f'(b_0) + f'(b_1)X = \\sqrt{b_0} + \\sqrt{b_1}X\\).\nHave to do: \\(\\hat{Y} = f'(\\hat{Y'}) = \\sqrt{b_0 + b_1X}\\).\n\nIf only \\(X\\) is transformed to \\(X'\\), then ne need to back transform the \\(Y's\\) estimation because \\(Y\\) hasn’t been transformed. For example: \\(\\hat{Y} = b_0 + b_1 X'\\).\nDemo\n\nOriginal data and model (with assumption violations)\n\n\n# plot data with model\nplasma &lt;- ALSM::Plasma \nmod_plasma &lt;- lm(y ~ x, data = plasma)\nplasma %$% plot(x, y)\nabline(mod_plasma)\n\n\n\n\n\n\n# diagnostic plots\nplot(mod_plasma, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference with bad model\n\n\n# plot data with model\nplasma &lt;- ALSM::Plasma \nmod_plasma &lt;- lm(y ~ x, data = plasma)\nplasma %$% plot(x, y)\nabline(mod_plasma)\n\n\n\n\n\n\n# diagnostic plots\nplot(mod_plasma, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# create dataset of both pointwise confidence and prediction limits\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconfs &lt;- predict(mod_plasma, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod_plasma, newdata = x_h, interval = \"pred\")\nplot(x = plasma$x, y = plasma$y, type = \"p\", main = \"Inference on bad model\")\nlines(x = x_h$x, y = confs[,\"fit\"],  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = confs[,\"lwr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = confs[,\"upr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = preds[,\"lwr\"],  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = preds[,\"upr\"],  type = \"l\", col = \"blue\")\n\n\n\n\n\n\n\n\nFix model\n\n\n# run boxcox and extract lambda\nMASS::boxcox(mod_plasma)\nbc &lt;- MASS::boxcox(mod_plasma)\n\n\n\n\n\n\nbc$x[which.max(bc$y)]\n\n[1] -0.5050505\n\n# fit transformed model -&gt; Y' = 1 / sqrt(Y)\nmod_plasma_prime &lt;- lm(1 / sqrt(y) ~ x, data = plasma)\n\n# plot model and diagnostic plots\nplasma %$% plot(x, 1 / sqrt(y), main = \"Transformed model\")\nabline(mod_plasma_prime)\n\n\n\n\n\n\nplot(mod_plasma_prime, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the working model is $ = $ 0.27 \\(+\\) 0.04 \\(X\\) (remember coefficients are in terms of new \\(Y'\\) scale).\nBands before back transformation\n\n\n# create dataset of both pointwise confidence and prediction limits of transformed model\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconfs &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"pred\")\nplot(x = plasma$x, y = 1 / sqrt(plasma$y), type = \"p\", main = \"Inference on transformed model (before back transformation)\")\nlines(x = x_h$x, y = confs[,\"fit\"],  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = confs[,\"lwr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = confs[,\"upr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = preds[,\"lwr\"],  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = preds[,\"upr\"],  type = \"l\", col = \"blue\")\n\n\n\n\n\n\n\n\nNow we can back transform \\(Y' = \\frac{1}{\\sqrt{Y}}\\) \\(\\Longrightarrow\\) \\(f'(Y') = (Y^{-1/2})^{-2} = (Y')^{-2} = Y\\).\n\nBands after back transformation\n\nThe predicted values should be \\(\\hat{Y} = (\\hat{Y'})^{-2}\\)\nThe confidence interval for the prediction, with for the mean or single response, should also be back transformed with \\((value)^{-2}\\).\n\n\n\n\n# create dataset of both pointwise confidence and prediction limits of back transformed model\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconfs &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"pred\")\nplot(x = plasma$x, y = plasma$y, type = \"p\", main = \"Inference on transformed model AFTER back transformation\")\nlines(x = x_h$x, y = (confs[,\"fit\"])^-2,  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = (confs[,\"lwr\"])^-2,  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = (confs[,\"upr\"])^-2,  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = (preds[,\"lwr\"])^-2,  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = (preds[,\"upr\"])^-2,  type = \"l\", col = \"blue\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#summary-of-remedial-measures",
    "href": "notes-diagnostics-and-remedial-measures.html#summary-of-remedial-measures",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.9 Summary of remedial measures",
    "text": "3.9 Summary of remedial measures\n\n\nFor nonlinear functional relationships with well behaved residuals:\n\nTry transforming \\(X\\)\nMay require a polynomial or piecewise fit\n\n\n\nFor non-constant or non-normal error variance, possible with a nonlinear functional form\n\nTry transforming \\(Y\\)\nThe Box-Cox procedure may be helpful\nIf the transformation on \\(Y\\) doesn’t fix the non-constant variance problem, weighted least squares can be used\n\n\nTransformations of \\(X\\) and \\(Y\\) can be used together.\n\nAny time you consider a transformation\n– Remember to recheck all the diagnostics.\n– Consider whether you gain enough to justify losing interpretability.\n– Reciprocal transformations make interpretation especially hard.\n– Consider back-transforming the results of the final model for presentation.\n\nFor very non-normal errors, especially those arising from discrete responses, generalized linear models are often a better option, but linear regression may be “good enough”.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#exploration-of-shape-regression-function",
    "href": "notes-diagnostics-and-remedial-measures.html#exploration-of-shape-regression-function",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.10 Exploration of shape regression function",
    "text": "3.10 Exploration of shape regression function\nMotivation\n\nScatter plots often indicate readily the nature of the regression function. But at other times however, the scatter plot is complex and it becomes difficult to see the nature of the regression relationship, if any, from the plot. In these cases, it is helpful to explore the nature of the regression relationship by fitting a smoothed curve without any constraints on the regression function.\nThese smoothed curves are also called nonparametric regression curves. They are useful not only for exploring regression relationships but also for confirming the nature of the regression function.\nThere are lots of methods for time series, such as moving averages (or the same thing but with medians to reduce the influence of outlying observations). But these methods are for when the \\(X\\) values (time points) are equally spaced out.\nThere are also many smoothing methods for regression data, with a main one being lowess method.\n\n\n3.10.1 Lowess method\nOverview\n\nThe lowess method (locally weighted regression scatter plot smoothing) obtains a smoothed curve by fitting successive linear regression functions in local neighborhoods and taking the fitted value at that particular \\(X\\) as the smoothed value.\n\nProcedure\n\nThe lowess method uses a number of refinements in obtaining the final smoothed values to improve the smoothing and to make the procedure robust to outlying observations.\n\n\nThe linear regression is weighted to give cases further from the middle \\(X\\) level in each neighborhood smaller weights.\nTo make the procedure robust to outlying observations, the linear regression fitting is repeated, with the weights revised so that cases that had large residuals in the first fitting receive smaller weights in the second fitting.\nTo improve the robustness of the procedure further, step 2 is repeated one or more times by revising the weights according to the size of the residuals in the latest fitting.\n\n\n\nTo implement the lowess procedure, one must choose (→ in practice):\n\nThe size of the successive neighborhoods to be used when fitting each linear regression → This is the primary choice.\nThe weight function that gives less weight to neighborhood cases with \\(X\\) values far from each center \\(X\\) level → Default weight functios suggested by the creator are adequate for many circumstances.\nAnother weight function that gives less weight to cases with large residuals → Default.\nThe number of iterations to make the procedure robust must be chosen → Two iterations appears to be sufficient.\n\n\n\nChoosing the neighborhood size\n\nThe larger the size, the smoother the function but the greater the danger that the smoothing will lose essential features of the regression relationship.\nIt may require some experimentation with different neighborhood sizes in order to find the size that best brings out the regression relationship.\n\n\n\nDemo\n\n# get dataset\nplasma &lt;- ALSM::Plasma\n\n# plot dataset with lowess smoother\n# arguments (with defaults)\n# -&gt; f = the smoother span (proportion of points in the plot which influence the smooth at each value) ~ neighborood size\nplot(plasma$x, plasma$y)\nlines(lowess(x = plasma$x, y = plasma$y, f = 2/3, iter = 3))\nlines(lowess(x = plasma$x, y = plasma$y, f = 1/4, iter = 3), col = \"blue\")\n\n\n\n\n\n\n\nNotes\n\nSmoothed curves, such as the lowess curve, do not provide an analytical expression for the functional form of the regression relationship. They only suggest the shape of the regression curve.\nThe lowess procedure is not restricted to fitting linear regression functions in each neighborhood. Higher-degree polynomials can also be utilized with this method.\nShowed previously that lowess is used in residual plots to show their pattern\n\n3.10.2 Use of smoothed curves to confirm the fitted regression function\nOverview\n\nSmoothed curves are useful not only in the exploratory a regression model is selected but they are also helpful in confirming the regression function chosen.\nThe procedure for confirmation is simple: The curve is plotted together with the confidence band for the fitted regression function. If the smoothed curve falls within the confidence band, we have supporting evidence of the appropriateness of the fitted regression function.\n\nDemo\n\n# fit untransformed model\nmod_plasma &lt;- lm(y ~ x, data = plasma)\n\n# calculate confidence band using R functions\n# -&gt; using textbook function (to confirm)\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconf_band1 &lt;- ALSM::ci.reg(mod_plasma, newdata = x_h, type = \"w\")\n\n# -&gt; using predict() for in practice\nalpha &lt;- 0.05\nW &lt;- sqrt(2 * qf(1 - alpha, df1 = length(coef(mod_plasma)), df2 = df.residual(mod_plasma)))\nconf_band2 &lt;- predict(mod_plasma, newdata = x_h) %&gt;% \n  data.frame(fit = .) %&gt;% \n  mutate(x = x_h$x,\n         se = predict(mod_plasma, newdata = x_h, interval = \"pred\", se.fit = TRUE)$se.fit,\n         lwr = fit - W * se,\n         upr = fit + W * se)\n\n# plot smooth curve and actual confidence bands\nplot(x = plasma$x, y = plasma$y, main = \"Untransformed model\")\nlines(lowess(x = plasma$x, y = plasma$y, f = 2/3, iter = 3))\nconf_band2 %$% lines(x = x, y = lwr, col = \"green\")\nconf_band2 %$% lines(x = x, y = upr, col = \"green\")\nconf_band1 %$% lines(x = x, y = Lower.Band, col = \"blue\", lty = 2)\nconf_band1 %$% lines(x = x, y = Upper.Band, col = \"blue\", lty = 2)\n\n\n\n\n\n\n\n\n# fit transformed model\nmod_plasma_prime &lt;- lm(1 / sqrt(y) ~ x, data = plasma)\n\n# calculate confidence bands\nconf_bands &lt;- ALSM::ci.reg(mod_plasma_prime, newdata = x_h, type = \"w\")\n\n# plot smooth curve and actual confidence bands after back transforming\nplot(x = plasma$x, y = plasma$y, main = \"Transformed model\")\nlines(lowess(x = plasma$x, y = plasma$y, f = 2/3, iter = 3))\nconf_bands %$% lines(x = x, y = Lower.Band^(-2), col = \"blue\")\nconf_bands %$% lines(x = x, y = Upper.Band^(-2), col = \"blue\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#matrix-algebra",
    "href": "notes-matrix-slr.html#matrix-algebra",
    "title": "4  Matrix approach to SLR",
    "section": "4.1 Matrix algebra",
    "text": "4.1 Matrix algebra",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#regression-matrices",
    "href": "notes-matrix-slr.html#regression-matrices",
    "title": "4  Matrix approach to SLR",
    "section": "4.2 Regression matrices",
    "text": "4.2 Regression matrices",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#sec-slr-matrix",
    "href": "notes-matrix-slr.html#sec-slr-matrix",
    "title": "4  Matrix approach to SLR",
    "section": "4.3 SLR in matrix form",
    "text": "4.3 SLR in matrix form\n(Assuming matrix knowledge (or at least can be found elsewhere))\n** Nice site showing r examples of most of the following: important-matrices-in-regression\n\n4.3.1 Model statement\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{10pt} \\text{where} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\\)\n\\[\\Longrightarrow \\hspace{20pt} \\underset{n \\times 1}{\\boldsymbol{Y}} = \\underset{n \\times 2}{\\boldsymbol{X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\beta}} + \\underset{n \\times 1}{\\boldsymbol{\\epsilon}} \\hspace{10pt} \\text{and} \\hspace{10pt} \\underset{n \\times 1}{\\boldsymbol{E(Y)}} = \\underset{n \\times 1}{\\boldsymbol{X\\beta}} \\tag{4.1}\\]\nbecause / where \\(\\hspace{10pt} \\underset{n \\times 1}{\\boldsymbol{E(\\epsilon)}} = \\underset{n \\times 1}{\\boldsymbol{0}} \\hspace{10pt} \\text{and} \\hspace{10pt} \\underset{n \\times n}{\\boldsymbol{\\sigma^2(\\epsilon)}} = \\underset{n \\times n}{\\boldsymbol{\\sigma^2 I}}\\)\n\n\n4.3.2 Least squares estimation of regression parameters\nThe normal equations \\[\n\\begin{align*}\n  \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum X_i &= \\sum Y_i \\\\\n  \\hat{\\beta}_0 \\sum X_i + \\hat{\\beta}_1 \\sum X_i^2 &= \\sum X_i Y_i\n\\end{align*}\n\\]\nin matrix terms are\n\\(\\underset{2 \\times 2}{\\boldsymbol{X'X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} = \\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\)\nwhere \\(\\hspace{10pt} \\underset{1 \\times 2}{\\boldsymbol{\\hat{\\beta'}}} = [\\hat{\\beta}_1, \\hat{\\beta}_2] \\hspace{10pt}\\) is the vector of least squares regression coefficients.\n\n\n4.3.3 Estimated regression coefficients\nSimply use some matrix algebra:\n\\((\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1} \\underset{2 \\times 2}{\\boldsymbol{X'X}} \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} = (\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1}\\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\)\n\\(\\Longrightarrow\\) \\(\\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} = (\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1}\\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\)\n\n\n4.3.4 Fitted values\nThe vector of the fitted values \\(\\hat{Y}_i\\) is \\(\\underset{1 \\times n}{\\boldsymbol{Y'}} = [\\hat{Y}_1, \\ldots, \\hat{Y}_n]\\).\nIn matrix notation, we have the following which can also be expressed using the hat matrix:\n\\[\n\\begin{align*}\n  \\underset{n \\times 1}{\\boldsymbol{Y}} &= \\underset{n \\times 2}{\\boldsymbol{X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} \\\\\n   &= \\underset{n \\times 2}{\\boldsymbol{X}} \\, (\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1}\\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\\\\n   &= \\underset{n \\times n}{\\boldsymbol{H}} \\, \\underset{n \\times 1}{\\boldsymbol{Y}}\n\\end{align*}\n\\]\nwhere \\(\\hspace{10pt} \\underset{n \\times n}{\\boldsymbol{H}} = \\boldsymbol{X(X'X)^{-1}X'}\\).\nWe see from above that the fitted values \\(\\hat{Y}_i\\) can be expressed as linear combinations of the response variable observations \\(Y_i\\) , with the coefficients being elements of the matrix \\(\\boldsymbol{H}\\). And the hat matrix involves only the observations of the predictor variable \\(X\\).\nNotes about hat matrix:\n\nSymmetric (and thus square: \\(n \\times n\\) dimensions)\nIdempotent \\(\\Longrightarrow\\) \\(\\boldsymbol{H}\\boldsymbol{H} = \\boldsymbol{H}\\)\nPlays an important role in diagnostics for regression analysis when we determine the influence of outliers.\n\n\n\n4.3.5 Residuals\nLet the vector of the residuals \\(e_i = Y_i - \\hat{Y}_i\\) be\n\\(\\underset{1 \\times n}{\\boldsymbol{e'}} = [e_1, \\ldots, e_n]\\)\nIn matrix notation, we have:\n\\(\\underset{n \\times 1}{\\boldsymbol{e}} = \\underset{n \\times 1}{\\boldsymbol{Y}} - \\underset{n \\times 1}{\\boldsymbol{\\hat{Y}}} = \\underset{n \\times 1}{\\boldsymbol{Y}} - \\underset{n \\times 2}{\\boldsymbol{X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}}\\)\nVariance covariance matrix of residuals\n\nThe residuals \\(e_i\\), like the fitted values \\(\\hat{Y}_i\\) , can be expressed as linear combinations of the response variable observations \\(Y_i\\)\n\n\\(\\boldsymbol{e} = \\boldsymbol{Y} - \\boldsymbol{\\hat{Y}} = \\boldsymbol{Y} - \\boldsymbol{HY} = (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{Y}\\)\nThus we have the important result:\n\\(\\underset{1 \\times n}{\\boldsymbol{e'}} = (\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}}) \\, \\underset{n \\times 1}{\\boldsymbol{Y}}\\)\nThe matrix \\(\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}}\\) is symmetric and idempotent like \\(\\boldsymbol{H}\\).\nThe variance-covariance of the vector of residuals involves this matrix:\n\\[\n\\begin{align*}\n  \\underset{n \\times n}{\\boldsymbol{\\sigma^2\\{e\\}}} &= \\boldsymbol{\\sigma^2\\{(\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{Y}\\}} \\\\\n   &= (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{\\sigma^2\\{Y\\}}(\\boldsymbol{I} - \\boldsymbol{H})' \\\\\n   &= \\sigma^2 (\\boldsymbol{I} - \\boldsymbol{H}) (\\boldsymbol{I} - \\boldsymbol{H}) \\hspace{20pt} \\text{because symmetry and }\\boldsymbol{\\sigma^2\\{Y\\}} = \\boldsymbol{\\sigma^2\\{\\epsilon\\}}= \\sigma^2 \\boldsymbol{I} \\\\\n   &= \\sigma^2 (\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}})\n\\end{align*}\n\\]\nand is estimated by\n\\(\\underset{n \\times n}{\\boldsymbol{s^2\\{e\\}}} = MSE (\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}})\\)",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#sec-slr-anova-matrix",
    "href": "notes-matrix-slr.html#sec-slr-anova-matrix",
    "title": "4  Matrix approach to SLR",
    "section": "4.4 ANOVA results",
    "text": "4.4 ANOVA results\nTotal sum of squares: \\(SSTO = \\sum (Y_i - \\bar{Y})^2 = \\sum Y_i^2 - \\frac{(\\sum Y_i)^2}{n}\\).\n\nIn matrix form: \\(SSTO = \\boldsymbol{Y'}\\boldsymbol{Y} - (\\frac{1}{n})\\boldsymbol{Y'}\\boldsymbol{J}\\boldsymbol{Y}\\)\n\nSum of squared errors can be expressed similarly: \\(SSE = \\sum e_i^2 = \\sum (Y_i - \\hat{Y_i})^2\\)\n\nIn matrix form: \\(SSE = \\boldsymbol{e'}\\boldsymbol{e} = (\\boldsymbol{Y} - \\boldsymbol{X\\hat{\\beta}})'(\\boldsymbol{Y} - \\boldsymbol{X\\hat{\\beta}})\\).\nThis can be shown equivalent to (not worrying about derivation of this or next one):\n\n\\(SSE = \\boldsymbol{Y'Y} - \\boldsymbol{\\hat{\\beta}'X'Y}\\)\n\n\nSum of squares regression: \\(SSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\\)\n\nIn matrix form: \\(SSR = \\boldsymbol{\\hat{\\beta}'X'Y} - (\\frac{1}{n})\\boldsymbol{Y'}\\boldsymbol{J}\\boldsymbol{Y}\\)\n\n\n4.4.1 Sums of squares as quadradic forms\nQuadratic form:\n\\[\\underset{1 \\times 1}{\\boldsymbol{Y'AY}} = \\sum_{i = 1}^n \\sum_{j = 1}^na_{ij} Y_i Y_j\\]\nwhere \\(a_{ij} = a_{ji}\\) and \\(\\boldsymbol{A}\\) is a symmetric \\(n \\times n\\) called the matrix of the quadratic form.\nThe ANOVA sums of squares \\(SSTO\\), \\(SSE\\), and \\(SSR\\) are all quadratic forms, as can be seen by reexpressing \\(\\boldsymbol{\\hat{\\beta}'X'}\\):\n\\[\n\\begin{align*}\n\\boldsymbol{\\hat{\\beta}'X'} &= (\\boldsymbol{X\\hat{\\beta}})' \\\\\n  &= \\boldsymbol{\\hat{Y}}' \\\\\n  &= (\\boldsymbol{HY})' \\\\\n  &= \\boldsymbol{Y'H} \\hspace{20pt} \\text{because symmetry}\n\\end{align*}\n\\]\nThis result enables us to express the ANOVA sums of squares as follows (just pre and post multiply and simplify to get to the original forms):\n\\[\n\\begin{align*}\nSSTO &= \\boldsymbol{Y'} \\big[\\boldsymbol{I} - \\big(\\frac{1}{n}\\big) \\boldsymbol{J}\\big] \\boldsymbol{Y} \\\\\nSSE &= \\boldsymbol{Y'}(\\boldsymbol{I - H})\\boldsymbol{Y} \\\\\nSSR &= \\boldsymbol{Y'} \\big[\\boldsymbol{H} - \\big(\\frac{1}{n}\\big) \\boldsymbol{J}\\big] \\boldsymbol{Y}  \\\\\n\\end{align*}\n\\]",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#inferences-in-regression-analysis",
    "href": "notes-matrix-slr.html#inferences-in-regression-analysis",
    "title": "4  Matrix approach to SLR",
    "section": "4.5 Inferences in regression analysis",
    "text": "4.5 Inferences in regression analysis\n\n4.5.1 Regression coefficients\n\n(Can find proof in textbook)\n\n\n4.5.2 Mean response\n\n\n\n4.5.3 Prediction of new observation",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "part2-mlr.html",
    "href": "part2-mlr.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "This section contains notes from Part 2: Multiple Linear Regression.",
    "crumbs": [
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#overview",
    "href": "notes-multiple-regression-1.html#overview",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.1 Overview",
    "text": "5.1 Overview\nMultiple regression analysis is one of the most widely used of all statistical methods. In this section, we first discuss a variety of multiple regression models.\nSince the matrix expressions for multiple regression are the same as for simple linear regression, can just refer to Chapter 4.\nThe need for multiple predictors\n\nOften times, a single predictor variable in the model provides an inadequate description since a number of key variables affect the response variable in important and distinctive ways.\nFurthermore, in situations like this, we frequently find that predictions of the response variable based on a model containing only a single predictor variable are too imprecise to be useful.\nMultiple predictors is useful in observational studies when there is no control over the variables as well as in experimental situations where the experimenter can control the \\(X\\) levels and investigate different combination simultaneously.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#multiple-regression-models",
    "href": "notes-multiple-regression-1.html#multiple-regression-models",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.2 Multiple regression models",
    "text": "5.2 Multiple regression models\n\n5.2.1 First order model with two predictors\nWhen there are two predictor variables \\(X_l\\) and \\(X_2\\), the regression model:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\epsilon_i\\]\nThis model is still linear in its predictors. It follows that if \\(E(\\epsilon_i) = 0\\)\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_1 + \\beta_1 X_2\\]\nNote that now predictions are a plane and any point on the response plane corresponds to the mean response \\(E(Y)\\) at the given combination of levels of \\(X_1\\) and \\(X_2\\).\n\nHere the vertical distance from \\(Y_i\\) to the response plane represents the error term \\(\\epsilon_i = Y_i - E(Y)\\).\nFrequently the regression function in multiple regression is called a regression surface or a response surface.\nMeaning of regression coefficients\n\n\nThe parameter \\(\\beta_0\\) is the intercept of the regression plane.\n\nIf the scope of the model includes \\(X_1 = 0\\) and \\(X_2 = 0\\), then \\(\\beta_0 = 10\\) for example represents the mean response \\(E(Y)\\) at \\(X_1 = 0\\) and \\(X_2 = 0\\).\n\n\n\nThe parameter \\(\\beta_1\\) indicates the change in the mean response \\(E(Y)\\) per unit increase in \\(X_1\\) when \\(X_2\\) is held constant. Same but reverse for \\(\\beta_2\\).\n\nFor example: If \\(E(Y) = 10 + 2X_1 + 5X_2\\) and the level of \\(X_2\\) is held at 2, then \\(E(Y) = 10 + 2X_1 + 5(2) = 20 + 2X_1\\).\nThis response function is a straight line with slope \\(\\beta_1 = 2\\). This is true for all other values of \\(X_2\\), only the value of the intercept changes.\n\n\n\nAdditive models vs interaction models\n\nWhen the effect of \\(X_1\\) on the mean response does not depend on the level of \\(X_2\\), and correspondingly the effect of \\(X_2\\) does not depend on the level of \\(X_1\\), the two predictor variables are said to have additive effects or not to interact.\nThus, the first-order regression model is designed for predictor variables whose effects on the mean response are additive or do not interact.\nThe parameters \\(\\beta_1\\) and \\(\\beta_2\\) are sometimes called partial regression coefficients because they reflect the partial effect of one predictor variable when the other predictor variable is included in the model and is held constant.\n\n5.2.2 First order model with more than two predictor variables\nWe consider now the case where there are \\(p - 1\\) predictor variables \\(X_1, \\ldots , X_{p-1}\\). The regression model:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1} + \\epsilon_i \\tag{5.1}\\]\nis called a first-order model with \\(p - 1\\) predictor variables. It can also be written as:\n\\[Y_i = \\beta_0 + \\sum_{k = 1}^{p - 1} \\beta_k X_{ik} + \\epsilon_i\\]\nIt follows that if \\(E(\\epsilon_i) = 0\\)\n\\[E(Y_i) =  \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1}\\]\nNotes about model:\n\nThis response function is a hyperplane, which is a plane in more than two dimensions. It is no longer possible to picture this plane.\nThe meaning of the parameters is the same as when there were two predictors, just extended to all other \\(X\\) levels held constant.\nThis first-order regression model is still designed for predictor variables whose effects on the mean response are additive and therefore do interact.\nWhen \\(p - 1 = 1\\), this model reduces to the SLR model we are used to.\n\n5.2.3 General linear regression model\nIn general, the variables \\(X_1, \\ldots, X_{p-1}\\) in a regression model do not need to represent different predictor variables, as we will see soon. Therefore, we define the general linear regression model, with normal error terms, simply in terms of \\(X\\) variables (same as before, just now more general of what the \\(X\\)s can represent):\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1} + \\epsilon_i \\tag{5.2}\\]\nwhere\n\n\\(\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}\\) are parameters (note there are \\(p\\) parameters because of the intercept)\n\\(X_{i1}, \\ldots, X_{i,p-1}\\) are known constants\n\\(\\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0, \\sigma^2)\\)\n\\(i = 1, \\ldots, n\\)\n\nAgain since \\(E(\\epsilon_i) = 0\\),\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1}\\]\nThus, the general linear regression model with normal error terms implies that the observations \\(Y_i\\) are independent normal variables, with mean \\(E(Y)\\) as given by above and with constant variance \\(\\sigma^2\\).\nThis general linear model encompasses a vast variety of situations. We consider a few of these now.\n\\(p - 1\\) predictor variables\n\nWhen \\(X_1, \\ldots, X_{p-1}\\) represent \\(p - 1\\) different predictor variables, the general linear regression model above is simply Equation 5.1 seen earlier (first-order model in which there are no interaction effects between the predictor variables).\n\nQualitative predictors\n\nModel in Equation 5.2 encompasses not only quantitative predictor variables but also qualitative ones.\nWe use indicator variables that take on the values 0 and 1 to identify the classes of a qualitative variable. For example if length of hospital stay \\(Y\\) with \\(X_1\\) = age and \\(X_2\\) = gender of patient where:\n\n\\[\nX_2 =\n   \\left\\{\n   \\begin{array}{ll}\n     0 & \\text{female}\\\\\n     1 & \\text{male}\n  \\end{array}\n  \\right.\n\\]\n\nThe first-order regression model is (with variables defined above):\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\\] - The response function is\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2}\\]\n\nFor female patients, \\(X_2 = 0\\) and the response function becomes\n\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_{i1}\\] - For male patients, \\(X_2 = 1\\) and the response function becomes\n\\[E(Y_i) = (\\beta_0 + \\beta_2) + \\beta_1 X_{i1}\\]\n\nThese two response functions represent parallel straight lines with different intercepts.\nIn general, we represent a qualitative variable with \\(c\\) classes by means of \\(c - 1\\) indicator variables.\n\nPolynomial regression\n\nPolynomial regression models are special cases of the general linear regression model. They contain squared and higher-order terms of the predictor variable(s), making the response function curvilinear.\nThe following is a polynomial regres’sion model with one predictor variable:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_i1 + \\beta_2 X_i^2+ \\epsilon_i\\]\n\nDespite the curvilinear nature of the response function for regression model above, it is a special casee of general linear regression model Equation 5.2. If we let \\(X_{i1} = X_i\\) and \\(X_{i2} = X_i^2\\), we can write the above model in the same format as usual:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\\]\nTransformed variables\n\nModels with transformed variables involve complex, curvilinear response functions, yet still are special cases of the general linear regression model. Consider the following model with transformed \\(Y\\) variable\n\n\\[\\ln(Y_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\epsilon_i\\] - Here, the response surface is complex, yet this model can still be treated as a general linear regression model if we let \\(Y_i' = \\ln(Y_i)\\). Then just write the regression model as:\n\\[Y_i' = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\epsilon_i\\]\n\nThis has the same form as Equation 5.2, the response variable just happens to be the natural logarithm of \\(Y\\).\nMany models can be transformed into the g’enerallinear regression model. For example, take this model:\n\n\\[Y_i = \\frac{1}{\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i}\\]\n\nIt can be transformed to the general linear regression model by letting \\(Y_i' = 1/Y_i\\). Then we have\n\n\\[Y_i' = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\\]\nInteraction effects\n\nWhen the effects of the predictor variables on the response variable are not additive, the effect of one predictor variable depends on the levels of the other predictor variables.\nOur general linear regression model encompasses regression models with nonadditive or interacting effects. An example of a nonadditive regression model with two predictor variables \\(X_1\\) and \\(X_2\\) is the following:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\epsilon_i\\] - Here the response function is complex because of the interaction term \\(\\beta_3 X_{i1} X_{i2}\\). Yet this regression model is still a special case of the general linear regression model. Let \\(X_3i = X_{i1} X_{i2}\\) and then write the model as:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\epsilon_i\\]\nCombination of cases\n\nA regression model may combine several of the elements we have just noted and still be treated as a general linear regression model.\n\n\n\nExamples of complex regression surfaces for two predictors:\n\n Meaning of linear in general linear regression model\n\nIt should be clear from the various examples that general linear regression model Equation 5.2 is not restricted to linear response surfaces.\nThe term linear model refers to the fact that the model is linear in the parameters; it does-not refer to the shape of the response surface.\nWe say that a regression model is linear in the parameters when it can be written in the form:\n\n\\[Y_i = c_{i0} \\beta_0 + c_{i1} \\beta_1 + c_{i2} \\beta_2 + \\cdots + c_{i,p-1} \\beta_{p-1} + \\epsilon_i \\tag{5.3}\\]\n\nHere the terms \\(c_{i0}, c_{i1}, \\ldots\\) are coefficients involving the predictor variables. For example, take the first-order model with two variables:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\epsilon_i\\]\n\nThis is linear in parameters with \\(c_{i0} = 1\\), \\(c_{i1} = X_{i1}\\) and \\(c_{i2} = X_{i2}\\).\nAn example of a nonlinear regression model is the following:\n\n\\[Y_i = \\beta_0 \\mathrm{e}^{\\beta_1 X_i} + \\epsilon_i\\]\n\nThis is a nonlinear regression model because it cannot be expressed in the form of Equation 5.3.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#sec-main-mlr-demo",
    "href": "notes-multiple-regression-1.html#sec-main-mlr-demo",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.3 General linear regression model in matrix terms",
    "text": "5.3 General linear regression model in matrix terms\nIt is a remarkable property of matrix algebra that the results for the general linear regression model Equation 5.2 in matrix notation appear exactly as those for the simple linear regression model Equation 4.1. Only the degrees of freedom and other constants related to the number of \\(X\\) variables and the dimensions of some matrices are different. Hence, We are able to present the results very concisely.\nWe need the following matrices:\n\nEverything is just extended to \\(p\\) parameters and \\(p - 1\\) predictor variables.\nIn matrix terms, the general linear regression model Equation 5.2 is:\n\\[\\underset{n \\times 1}{\\boldsymbol{Y}} = \\underset{n \\times p}{\\boldsymbol{X}} \\, \\underset{p \\times 1}{\\boldsymbol{\\beta}} + \\underset{n \\times 1}{\\boldsymbol{\\epsilon}} \\hspace{10pt} \\text{and} \\hspace{10pt} \\underset{n \\times 1}{\\boldsymbol{E(Y)}} = \\underset{n \\times 1}{\\boldsymbol{X\\beta}} \\tag{5.4}\\]\n\n5.3.1 Estimation of regression coefficients, fitted values and residuals\nThis is also the exact same, just extended (see Section 4.3).\n\\(\\underset{p \\times 1}{\\boldsymbol{\\hat{\\beta}}} = (\\underset{p \\times p}{\\boldsymbol{X'X}})^{-1}\\underset{p \\times 1}{\\boldsymbol{X'Y}}\\)\n\n\nDemo\n\n\nR functions\nManual (matrices)\n\n\n\n(True) Population model: \\(Y = \\beta_0 + \\beta_1 Z_1 + \\beta_2 Z_2 + \\beta_3 Z_3 + \\beta_4 Z_4 + \\beta_5 Z_5 + \\epsilon_i\\)\nwhere \\(\\hspace{10pt} Z_1 = X_1, Z_2 = X_2, Z_3 = X_3, Z_4 = X_1^2, Z_5 = X_1 X_2 \\hspace{10pt} \\text{and} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0, \\sigma^2)\\)\n\n# NOTE: generating a single dataset (and fitting single model) to use for all of the following examples\n# -&gt; so incorporating lots of aspects into this model (polynomial, interaction, omitted predictor, unnecessary predictor)\n\n# generate predictor data\n# -&gt; num_x number of base predictors (assumed known; generated from same dist)\n# -&gt; num_noise unimportant predictors (placed at end)\n# -&gt; hardcoded 2 higher order / interaction terms (X1^2 and X1*X2)\nn &lt;- 50\nnum_x &lt;- 3\nnum_noise &lt;- 1\nX &lt;- sapply(1:(num_x+num_noise), function(i) {rnorm(n = n, mean = 0, sd = 3)})\nZ &lt;- cbind(rep(1, n), X[, c(1:num_x)], X[, 1]^2, X[, 1] * X[ , 2], X[, -c(1:num_x)])\n\n# set betas according to which ones are noise (and intercept)\nbetas &lt;- c(1, rep(2, num_x+2), rep(0, num_noise))\n\n# generate mean response and add random error\nY &lt;- Z %*% betas + rnorm(n, mean = 0, sd = 5)\n\n(Fitted) Population model: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_4 + \\beta_5 X_1^2 + \\beta_6 X_2 X_3\\)\n\n# create dataframe of sample data using above\ndata_sample &lt;- cbind(Y, X) %&gt;% data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\",1:(length(data_sample)-1)))\n\n# fit mlr regression model\n# -&gt; omitting X3, adding in unnecessary predictor X4, and adding in correct higher order terms\nmod_mlr &lt;- lm(Y ~ X1 + X2 + X4 + I(X1^2) + I(X1*X2), data = data_sample, x = TRUE)\n\n# view model summary\n# now using broom functions\n# -&gt; see expected results\nbroom::tidy(mod_mlr)\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.74      1.55      1.12  2.70e- 1\n2 X1             2.11      0.471     4.47  5.38e- 5\n3 X2             1.57      0.409     3.85  3.77e- 4\n4 X4             0.264     0.429     0.615 5.42e- 1\n5 I(X1^2)        2.06      0.137    15.1   5.40e-19\n6 I(X1 * X2)     1.95      0.206     9.47  3.50e-12\n\n\n\n\nNow comparing lots of things\n\n# extract design matrix from model for simplicity\ndesign &lt;- mod_mlr$x\n\n# estimate coefficients\n# -&gt; betas_hat = (X'X)^(-1) X'Y\nbetas_hat &lt;- solve(t(design) %*% design) %*% t(design) %*% Y\n\ncompare(coef(mod_mlr), betas_hat)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for current but not for target &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is numeric, current is matrix\"                             \n\n$comparison$`element-wise`\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n$`coef(mod_mlr)`\n(Intercept)          X1          X2          X4     I(X1^2)  I(X1 * X2) \n  1.7351776   2.1069293   1.5742790   0.2635299   2.0604271   1.9498712 \n\n$betas_hat\n                 [,1]\n(Intercept) 1.7351776\nX1          2.1069293\nX2          1.5742790\nX4          0.2635299\nI(X1^2)     2.0604271\nI(X1 * X2)  1.9498712\n\n# fitted values\ny_hat &lt;- design %*% betas_hat\n\ncompare(fitted(mod_mlr), as.numeric(y_hat), return_items = FALSE)\n\n$comparison\n[1] TRUE\n\n\n\n\n\n\n5.3.2 ANOVA\nSum of squares are the exact same as in Section 4.4, just the degrees of freedom are adjusted for the mean squares.\n\n\\(SSTO\\) has the usual \\(n - 1\\) df.\n\\(SSE\\) had \\(n - p\\) df due to \\(p\\) parameters needed to be estimated in the regression model Equation 5.4.\n\\(SSR\\) has \\(p - 1\\) df, representing the number of \\(X\\) variables \\(X_1, \\ldots, X_{p-1}\\) (one minus the number of parameters because the intercept doesn’t count)\n\n\\[MSR = \\frac{SSR}{p - 1} \\hspace{20pt} MSE = \\frac{SSE}{n - p}\\]\n\n\nDemo\n\n\nR functions\nManual (matrices)\n\n\n\nReturning to the demo in Section 5.3.\n\n# calculate anova table\n(at &lt;- mod_mlr %&gt;% anova %&gt;% broom::tidy())\n\n# A tibble: 6 × 6\n  term          df  sumsq  meansq statistic   p.value\n  &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 X1             1 14562. 14562.      211.   2.11e-18\n2 X2             1  2291.  2291.       33.2  7.49e- 7\n3 X4             1   852.   852.       12.4  1.03e- 3\n4 I(X1^2)        1 26138. 26138.      379.   3.00e-23\n5 I(X1 * X2)     1  6183.  6183.       89.7  3.50e-12\n6 Residuals     44  3035.    69.0      NA   NA       \n\n# calculate sum of squares\nssto &lt;- at %&gt;% summarize(sum(sumsq))\nsse &lt;- at %&gt;% filter(term == \"Residuals\") %&gt;% pull(sumsq)\nssr &lt;- ssto - sse\nssr &lt;- at %&gt;% filter(term != \"Residuals\") %&gt;% summarize(sum(sumsq))\n\n\n\n\n# calculate sum of squares using quadratic forms\n# -&gt; SSTO = Y'[I-(1/n)J]Y \nSSTO &lt;- t(Y) %*% (diag(x = 1, nrow = length(Y)) - (1 / length(Y)) * matrix(data = rep(1, length(Y)^2), nrow = length(Y), ncol = length(Y))) %*% Y\n\n# -&gt; SSE = Y'(I-H)Y\nhat_matrix &lt;- design %*% solve(t(design) %*% design) %*% t(design)\nSSE &lt;- t(Y) %*% (Matrix::diag(x = 1, nrow = length(Y)) - hat_matrix) %*% Y\n\ncompare(as.numeric(sse), as.numeric(SSE))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(sse)`\n[1] 3034.76\n\n$`as.numeric(SSE)`\n[1] 3034.76\n\n# -&gt; SSR = Y'[H-(1/n)J]Y \nSSR &lt;- t(Y) %*% (hat_matrix - (1 / length(Y)) * matrix(data = rep(1, length(Y)^2), nrow = length(Y), ncol = length(Y))) %*% Y\n\ncompare(as.numeric(ssr), as.numeric(SSR))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(ssr)`\n[1] 50027.51\n\n$`as.numeric(SSR)`\n[1] 50027.51\n\n\n\n\n\n\n5.3.3 F-test for regression relation\n\n\n\nR functions\nManual (matrices)\n\n\n\nReturning to the demo in Section 5.3.\n\n# get f-stat model summary\n# -&gt; looking for statistic\nbroom::glance(mod_mlr)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.943         0.936  8.30      145. 3.55e-26     5  -174.  361.  375.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nContinuing with sum of squares calculated from previous demo in Section 5.3.2.\n\n# calculate sum of squares using quadratic forms\n# -&gt; F = MSR / MSE = (SSR / df_R) / (SSE / df_E)\nMSR &lt;- SSR / (length(betas_hat) - 1)\nMSE &lt;- SSE / (length(Y) - length(betas_hat))\nF_stat &lt;- MSR / MSE\n\n# extract mse from model summary\n# -&gt; df = dr_R and df.residual = df_E -&gt; used to get p-value of F-stat\nmse &lt;- broom::glance(mod_mlr)$sigma^2\ncompare(as.numeric(mse), as.numeric(MSE))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(mse)`\n[1] 68.97183\n\n$`as.numeric(MSE)`\n[1] 68.97183\n\ncompare(broom::glance(mod_mlr)$statistic, as.numeric(F_stat))\n\n$comparison\n[1] TRUE\n\n$`broom::glance(mod_mlr)$statistic`\n   value \n145.0665 \n\n$`as.numeric(F_stat)`\n[1] 145.0665\n\n\n\n\n\n\n5.3.4 Coefficient of multiple determination\nThe coefficient of multiple determination \\(R^2\\) is the same as in SLR:\n\\[R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\]\nIt measures the proportionate reduction of total variation in \\(Y\\) associated with the use of the set of X variables \\(X_1, \\ldots , X_{p_1}\\).\nAdding more \\(X\\) variables to the regression model can only increase \\(R^2\\) and never reduce it, because \\(SSE\\) can never become larger with more \\(X\\) variables and \\(SSTO\\) is always the same for a given set of responses.\nTherefore, we often use the adjusted coefficient of multiple determination \\(R^2_{adj}\\). It adjusts \\(R^2\\) by dividing each sum of squares by its respective df.\n\\[R^2_{adj} = 1 - \\Big(\\frac{SSE}{n - p}\\Big) \\Big/ \\Big(\\frac{SSTO}{n - 1}\\Big) = 1 - \\big(\\frac{n - 1}{n - p}\\big) \\frac{SSE}{SSTO}\\]\nThis adjusted coefficient of multiple determination may actually become smaller when another \\(X\\) variable is introduced into the model, because any decrease in \\(SSE\\) may be more than offset by the loss of a degree of freedom in the denominator \\(n - p\\).\nNotes\n\nIt can be shown that the coefficient of multiple determination \\(R^2\\)can be viewed as a coefficient of simple determination between the responses \\(Y_i\\) and the fitted values \\(\\hat{Y}_i\\).\n\nA large value of \\(R^2\\) does not necessarily imply that the fitted model is a useful one.\n\nFor instance, observations may have been taken at only a few levels of the predictor variables. Despite a high \\(R^2\\) in this case, the fitted model may not be useful if most predictions require extrapolations outside the region of observations.\nAgain, even though \\(R^2\\) is large, \\(MSE\\) may still be too large for inferences to be useful when high precision is required.\n\n\n\n\n\nR functions\nManual (matrices)\n\n\n\nReturning to the demo in Section 5.3.\n\n# view model statistics\n# -&gt; looking at r.squared and adj.r.squared\nbroom::glance(mod_mlr)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.943         0.936  8.30      145. 3.55e-26     5  -174.  361.  375.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nContinuing with sum of squares calculated from a previous demo in Section 5.3.2.\n\n# calculate coefficient of determination\n# -&gt; R^2 = SSR / SSTO = 1 - SSE / SSTO\nR2 &lt;- SSR / SSTO\nR2 &lt;- 1 - SSE / SSTO\n\ncompare(as.numeric(broom::glance(mod_mlr)$r.squared), as.numeric(R2))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(broom::glance(mod_mlr)$r.squared)`\n[1] 0.9428076\n\n$`as.numeric(R2)`\n[1] 0.9428076\n\n# calculate adjusted coefficient of determination\n# -&gt; R^2 = 1 - (n - 1) / (n - p) SSE / SSTO\nR2_adj &lt;- 1 - (length(Y) - 1) / (length(Y) - length(betas_hat)) * SSE / SSTO\n\ncompare(as.numeric(broom::glance(mod_mlr)$adj.r.squared), as.numeric(R2_adj))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(broom::glance(mod_mlr)$adj.r.squared)`\n[1] 0.9363084\n\n$`as.numeric(R2_adj)`\n[1] 0.9363084\n\n\n\n\n\n\n5.3.5 Coefficient of multiple correlation\nThe coefficient of multiple correlation \\(R\\) is the positive square root of \\(R^2\\):\n\\[R = \\sqrt{R^2}\\]\nWhen there is only one variable in the model, thec oefficient of multiple correlation \\(R\\) equals in absolute value the correlation coefficient \\(r\\) in SLR.\n\n5.3.6 Inferences about regression parameters\n\n\nDemo\n\n\nIndividual test on \\(\\beta_k\\)\nIndividual CI on \\(\\beta_k\\) – R functions\nIndividual CI on \\(\\beta_k\\) – Manual\n\n\n\n\nReturning to the demo in Section 5.3.\n\n\n# view coefficient summaries\n# -&gt; looking at std.error, statistic and p.value columns\n(summ1 &lt;- broom::tidy(mod_mlr))\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.74      1.55      1.12  2.70e- 1\n2 X1             2.11      0.471     4.47  5.38e- 5\n3 X2             1.57      0.409     3.85  3.77e- 4\n4 X4             0.264     0.429     0.615 5.42e- 1\n5 I(X1^2)        2.06      0.137    15.1   5.40e-19\n6 I(X1 * X2)     1.95      0.206     9.47  3.50e-12\n\n# just going to recreate the p-value from given info\n(summ2 &lt;- broom::glance(mod_mlr))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.943         0.936  8.30      145. 3.55e-26     5  -174.  361.  375.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# verify t-stat = estimate / std.error\ncompare(as.numeric(summ1[2,4]), as.numeric(summ1[2,2] / summ1[2,3]))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(summ1[2, 4])`\n[1] 4.472731\n\n$`as.numeric(summ1[2, 2]/summ1[2, 3])`\n[1] 4.472731\n\n# calculate p-value from theoretical (beta_k-hat - beta_k)/ SE(\\beta_k-hat) ~ t_(n-p) \np_value &lt;- pt(q = as.numeric(summ1[2,4]), df = summ2$df.residual, lower.tail = FALSE)*2\n\ncompare(as.numeric(summ1[2,5]), p_value)\n\n$comparison\n[1] TRUE\n\n$`as.numeric(summ1[2, 5])`\n[1] 0.00005378785\n\n$p_value\n[1] 0.00005378785\n\n\n\n\n\n# calculate confidence interval for single coefficient\n# -&gt; remember these are all individual inferences, not simultaneous confidence intervals\nconfint(mod_mlr)[2, ]\n\n   2.5 %   97.5 % \n1.157568 3.056290 \n\n\n\n\n\n# calculate lower and upper bounds\nalpha &lt;- 0.05 \nbound_lower &lt;- as.numeric(summ1[2, 2]) + qt(alpha / 2, df = summ2$df.residual) * as.numeric(summ1[2,3])\nbound_upper &lt;- as.numeric(summ1[2, 2]) + qt(1 - alpha / 2, df = summ2$df.residual) * as.numeric(summ1[2,3])\n\ncompare(as.numeric(confint(mod_mlr)[2, ]), c(bound_lower, bound_upper))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(confint(mod_mlr)[2, ])`\n[1] 1.157568 3.056290\n\n$`c(bound_lower, bound_upper)`\n[1] 1.157568 3.056290\n\n\n\n\n\n\n\n\n\n5.3.7 Estimation of mean response and prediction of new observation",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#diagnostics-and-remedial-measures",
    "href": "notes-multiple-regression-1.html#diagnostics-and-remedial-measures",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.4 Diagnostics and remedial measures",
    "text": "5.4 Diagnostics and remedial measures\nDiagnostics play an impol1ant role in the development and evaluation of multiple regression models. Most of the diagnostic procedures and remedial meausures for simple linear regression that we described in Chapter 3 carry over directly to multiple regression.\nScatterplot matrix\n\nBox plots, sequence plots, and dot plots for each of the predictor variables and for the response variable can provide helpful, preliminary univariate information about these variables.\nScatter plots of the \\(Y\\) against each \\(X\\) can aid in determining the nature and strength of the bivariate relationships and in identifying gaps in the data points as well as outlying data points. Scatter plots of \\(X\\) against each of the other \\(X\\)s provide similar information.\nA complement to the scatter plot matrix that may be useful at times is the correlation matrix (matrix format as \\([Y, X]\\)). Note this is a symmetric matrix with 1 on the main diagonal, so often can just print the upper or lower traingle matrix.\n\n\nDemo\nReturning to the demo in Section 5.3.\n\n# scatterplot matrix (of just the Xs, not the transformed variables as well)\npairs(data_sample)\n\n\n\n\n\n\n# correlation matrix\n(r &lt;- cor(data_sample) %&gt;% round(3))\n\n        Y    X1     X2     X3     X4\nY   1.000 0.524  0.221  0.094 -0.076\nX1  0.524 1.000  0.026  0.023  0.086\nX2  0.221 0.026  1.000 -0.105  0.026\nX3  0.094 0.023 -0.105  1.000  0.003\nX4 -0.076 0.086  0.026  0.003  1.000\n\n# correlation plot\ncorrplot::corrplot(r)\n\n\n\n\n\n\n# upper triangle correlation matrix\nr[lower.tri(r)] &lt;- NA\nr\n\n    Y    X1    X2     X3     X4\nY   1 0.524 0.221  0.094 -0.076\nX1 NA 1.000 0.026  0.023  0.086\nX2 NA    NA 1.000 -0.105  0.026\nX3 NA    NA    NA  1.000  0.003\nX4 NA    NA    NA     NA  1.000\n\n\nResidual plots\n\nSame as for SLR, a plot of the residuals against the fitted values is useful for assessing the appropriateness of the multiple regression function and the constancy of the variance of the error terms, as well as for providing information about outliers.\nBox plots and normal probability plots of the residuals are useful for examining whether the error terms are reasonably normally distributed.\nIn addition, residuals should be plotted against each of the predictor variables. Each of these plots can provide further information about the adequacy of the regression function with respect to that predictor variable (e.g., whether a curvature effect is required for that variable) and about possible variation in the magnitude of the error variance in relation to that predictor variable.\nResiduals should also be plotted against important predictor variables that were omitted from the model, to see if the omitted variables have substantial additional effects on the response variable that have not yet been recognized in the regression model.\nAlso, residuals should be plotted against interaction terms for potential interaction effects not included in the regression model, such as against \\(X_1 X_2\\), \\(X_1 X_3\\), and \\(X_2 X_3\\), to see whether some or all of these interaction terms are required in the model.\n\nA plot of the absolute residuals or the squared residuals against the fitted values is useful for examining the constancy of the variance of the error terms.\n\nIf nonconstancy is detected, a plot of the absolute residuals or the squared residuals against each of the predictor vaIiables may identify one or several of the predictor variables to which the magnitude of the error variability is related.\n\n\n\nDemo\n\n# residual plots\nplot(mod_mlr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemedial measures\n\nThe remedial measures described previously are also applicable to multiple regression.\n\nWhen a more complex model is required to recognize curvature or interaction effects, the multiple regression model can be expanded to include these effects.\n\nFor example, \\(X_2^2\\) might be added as a variable to take into account a curvature effect of \\(X_2\\), or \\(X_1 X_3\\) might be added as a variable to recognize an interaction effect between \\(X_1\\) and \\(X_3\\) on the response variable.\n\n\n\nAlternatively, transformations on the response and/or the predictor variables can be made as discussed before.\n\nTransformations on the response variable may be helpful when the distributions of the error terms are quite skewed and the variance of the error terms is not constant.\nTransformations of some of the predictor variables may be helpful when the effects of these variables are curvilinear.\nIn addition, transformations on \\(Y\\) and or the predictor variables may be helpful in eliminating or substantially reducing interaction effects.\n\n\nAs with simple linear regression, the usefulness of potential transformations needs to be examined by means of residual plots and other diagnostic tools to determine whether the multiple regression model for the transformed data is appropriate.\nCan still do the Box Cox transformation (Section 3.7.3) like usual.\n\nDemo\n\n# run boxcox procedure\n# -&gt; plot MLE of lambda\nMASS::boxcox(mod_mlr)\n\nError in boxcox.default(mod_mlr): response variable must be positive\n\n# add constant to response variable so can run box-cox\n# -&gt; from simulation for 'effects of constants and scalars', adding constant to Y only changes the intercept -&gt; still valid model\nmod_mlr_prime &lt;- lm(Y+50 ~ X1 + X2 + X4 + I(X1^2) + I(X1*X2), data = data_sample)\n\n# try again\nMASS::boxcox(mod_mlr_prime)\n\n\n\n\n\n\n# lambda = 1 -&gt; no transformation is needed, as expected\n\n\n5.4.1 Residual plots demo for various misfitting models\n\n\nMissing important predictor\nMissing polynomial predictor\nMissing interaction predictor\n\n\n\nCreate residual plots against each \\(X\\) in data.\n\n# get residuals\ne &lt;- residuals(mod_mlr)\n\n# plot against each X\nnms_x &lt;- colnames(data_sample[, -1])\nmap2(data_sample[,-1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$X1\nNULL\n\n$X2\nNULL\n\n$X3\nNULL\n\n$X4\nNULL\n\n\nNote that there is no pattern for important predictors included in the model or for the unnecessary predictor. BUT for the important predictor \\(X_3\\) omitted from the model, there is clear systematic deviation!\nThis indicates that it should be in the model.\n\n\nCreate residual plots.\n\n# fit new model without polyomial predictor\nmod_no_poly &lt;- update(mod_mlr, . ~ . - I(X1^2))\nbroom::tidy(mod_no_poly)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)   16.8       2.92      5.73  0.000000789\n2 X1             5.04      1.05      4.78  0.0000192  \n3 X2             1.89      1.00      1.89  0.0655     \n4 X4             0.977     1.05      0.934 0.355      \n5 I(X1 * X2)     3.01      0.475     6.33  0.000000103\n\n# see residuals vs fitted\nplot(mod_no_poly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get residuals\ne &lt;- residuals(mod_no_poly)\n\n# plot against each X\nnms_x &lt;- colnames(data_sample[, -1])\nmap2(data_sample[,-1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$X1\nNULL\n\n$X2\nNULL\n\n$X3\nNULL\n\n$X4\nNULL\n\n\nNote that there is curvature in the residuals vs fitted. Then when viewing vs each predictor, it is clear that it comes from \\(X_1\\)!\nThis indicates that a polynomial term for \\(X_1\\) should be in the model.\nAlso note that missing the polynomial term hides the effect of missing \\(X_3\\) \\(\\Longrightarrow\\) Need to model the terms in there correctly first before looking at more variables.\n\n\nCreate residual plots.\n\n# fit new model without polyomial predictor\nmod_no_interaction &lt;- update(mod_mlr, . ~ . - I(X1*X2))\nbroom::tidy(mod_no_interaction)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.59     2.61     -0.609 5.46e- 1\n2 X1              2.14     0.812     2.63  1.15e- 2\n3 X2              1.79     0.703     2.54  1.47e- 2\n4 X4             -1.25     0.685    -1.82  7.48e- 2\n5 I(X1^2)         2.50     0.221    11.3   9.98e-15\n\n# see residuals vs fitted\nplot(mod_no_interaction)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get residuals\ne &lt;- residuals(mod_no_interaction)\n\n# plot against each X\nnms_x &lt;- colnames(data_sample[, -1])\nmap2(data_sample[,-1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$X1\nNULL\n\n$X2\nNULL\n\n$X3\nNULL\n\n$X4\nNULL\n\n\nNote that there is maybe a slight pattern with respect to \\(X_1\\) and \\(X_2\\), but definitely harder to detect.\nhardt o tell if should include an interaction or not, obviosuly it should be based on the known true model.\nAlso note that again this hides the effect of missing \\(X_3\\), just like the previous one.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-2.html#extra-sums-of-squares",
    "href": "notes-multiple-regression-2.html#extra-sums-of-squares",
    "title": "\n6  Multiple regression 2\n",
    "section": "\n6.1 Extra sums of squares",
    "text": "6.1 Extra sums of squares\n\n6.1.1 Overview\nBasic idea\n\n\nAn extra sum of squares measures the marginal reduction in the error sum of squares when one or several predictor variables are added to the regression model, given that other predictor variables are already in the model.\n\nSo it is the difference in \\(SSE\\) with the original variables in the model compared to when additional ones are added (remember \\(SSE\\) always decreases when more variables are added to the model).\n\n\nEquivalently, one can view an extra sum of squares as measuring the marginal increase in the regression sum of squares when one or several predictor variables are added to the regression model (likewise, \\(SSR\\) always increases with additional terms).\n\n6.1.2 Definitions\nUsing both perspectives, we can define\n\\[SSR(X_2 \\mid X_1) \\hspace{10pt} = \\hspace{10pt} SSE(X_1) - SSE(X_1, X_2)  \\hspace{10pt} = \\hspace{10pt} SSR(X_1, X_2) - SSR(X_1)\\]\nExtensions for three or more variables are straightforward.\n\\[SSR(X_3 \\mid X_1, X_2) \\hspace{10pt} = \\hspace{10pt} SSE(X_1, X_2) - SSE(X_1, X_2, X_3)  \\hspace{10pt} = \\hspace{10pt} SSR(X_1, X_2, X_3) - SSR(X_1, X_2)\\]\nNote that we can also switch the order, say if \\(X_2\\) enters the model first, then \\(X_1\\).\n\n6.1.3 Decomposition of \\(SSR\\) into extra sums of squares\nIn multiple regression, unlike simple linear regression, we can obtain a variety of decompositions of the regression sum of squares \\(SSR\\) into extra sums of squares.\n\n\n\nIt is obvious that the number of possible decompositions becomes vast as the number of \\(X\\) variables in the regression model increases.\n\n6.1.4 ANOVA table containing decomposition of \\(SSR\\)\n\nANOVA tables can be constructed containing decompositions of the regression sum of squares into extra sums of squares.\n\nNote that each extra sum of squares involving a single extra \\(X\\) variable has associated with it one degree of freedom. The resulting mean squares are constructed as usual.\n\\[MSR(X_2 \\mid X_1) = \\frac{SSR(X_2 \\mid X_1)}{1}\\]\nExtra sums of squares involving two extra \\(X\\) variables, such as \\(SSR(X_2, X_3 \\mid X_1)\\), have two degrees of freedom associated with them. This follows because we can express such an extra sum of squares as a sum of two extra sums of squares, each associated with one df. For example, by definition of the extra sums of squares, we have\n\\[SSR(X_2, X_3 \\mid X_1) = SSR(X_2 \\mid X_1) + SSR(X_3 \\mid X_1, X_2)\\]\nThe mean square is therefore obtained as follows:\n\\[MSR(X_2, X_3 \\mid X_1) = \\frac{SSR(X_2, X_3 \\mid X_1)}{2}\\]\nMany computer regression packages provide decompositions of \\(SSR\\) into single-degree-of-freedom extra sums of squares, usually in the order in which the \\(X\\) variables are entered into the model. Thus, if the \\(X\\) variables are entered in the order \\(X_1\\), \\(X_2\\), \\(X_3\\), the extra Sums of squares given in the output are:\n\\[\n\\begin{align*}\n&SSR(X_1)\\\\\n&SSR(X_2 \\mid X_1)\\\\\n&SSR(X_3 \\mid X_1, X_2)\\\\\n\\end{align*}\n\\]\nIf an extra sum of squares involving several extra \\(X\\) variables is desired, it can be obtained by summing appropriate single-degree-of-freedom extra sums of squares.\nThe reason why extra sums of squares are of interest is that they occur in a variety of tests about regression coefficients where the question of concern is whether certain \\(X\\) variables can be dropped from the regression model.\nDemo\n\n# NOTE: generating a single dataset to use for many of the following examples\n\n# generate predictor data\n# -&gt; num_x number of base predictors (assumed known; generated from same dist)\n# -&gt; num_noise unimportant predictors (placed at end)\nn &lt;- 50\nnum_x &lt;- 2\nnum_noise &lt;- 1\nX &lt;- sapply(1:(num_x+num_noise), function(i) {rnorm(n = n, mean = 0, sd = 3)}) %&gt;%\n  cbind(rep(1, n), .)\n\n# set betas according to which ones are noise (and intercept)\nbetas &lt;- c(1, rep(2, num_x), rep(0, num_noise))\n\n# generate mean response and add random error\nY &lt;- X %*% betas + rnorm(n, mean = 0, sd = 3)\n\nVerify the \\(SS\\) given in anova() are conditional.\n\n# combine data\ndata_sample &lt;- cbind(Y, X[, -1]) %&gt;% \n  data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\", 1:sum(num_x, num_noise)))\n\n# fit models\nmod_1 &lt;- lm(Y ~ X1, data_sample)\nmod_12 &lt;- lm(Y ~ X1 + X2, data_sample)\nmod_123 &lt;- lm(Y ~ X1 + X2 + X3, data_sample)\n\n# get anova table\nanova(mod_123) %&gt;% tidy\n\n# A tibble: 4 × 6\n  term         df    sumsq   meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 X1            1 2373.    2373.     230.      1.60e-19\n2 X2            1 1705.    1705.     165.      7.69e-17\n3 X3            1    0.139    0.139    0.0135  9.08e- 1\n4 Residuals    46  474.      10.3     NA      NA       \n\n# verify conditional sum of squares\n\n# -&gt; just X1 in the model\ncompare(mod_123 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(sumsq),\n        mod_1 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(sumsq))\n\n$comparison\n[1] TRUE\n\n$`mod_123 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(sumsq)`\n[1] 2372.823\n\n$`mod_1 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(sumsq)`\n[1] 2372.823\n\n# X2 in the model after X1 already in model\ncompare(mod_123 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq),\n        mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq))\n\n$comparison\n[1] TRUE\n\n$`mod_123 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq)`\n[1] 1705.173\n\n$`mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq)`\n[1] 1705.173",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple regression 2</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-2.html#use-of-extra-sums-of-squares-in-tests-for-regression-coefficients",
    "href": "notes-multiple-regression-2.html#use-of-extra-sums-of-squares-in-tests-for-regression-coefficients",
    "title": "\n6  Multiple regression 2\n",
    "section": "\n6.2 Use of extra sums of squares in tests for regression coefficients",
    "text": "6.2 Use of extra sums of squares in tests for regression coefficients\n\n\nThe \\(F^*\\) test statistic here to test whether or not \\(\\beta_k = 0\\) is called a partial F test statistic to distinguish it from the overall \\(F^*\\) statistic that Section 5.3.3 for testing whether all \\(\\beta_k = 0\\).\n\n\nNote\n\nFor testing whether a single \\(\\beta_k\\) equals zero, two equivalent test statistics are available: the \\(t^*\\) test statistic and the \\(F^*\\) general linear test statistic.\nWhen testing whether several \\(\\beta_k\\) equal zero, only the general linear test statistic \\(F^*\\) is available.\n\nDemo\n\nVerify \\((t^*)^2 = F^*\\)\n\n\n\n# verify t-stat^2 = F-stat for SAME order of model\nt_stat &lt;- mod_12 %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(statistic) # middle step cause compare() being weird\ncompare(t_stat %&gt;% raise_to_power(2),\n        mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(statistic))\n\n$comparison\n[1] TRUE\n\n$`t_stat %&gt;% raise_to_power(2)`\n[1] 168.9207\n\n$`mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(statistic)`\n[1] 168.9207",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple regression 2</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-2.html#summary-of-tests-concerning-regression-coefficients",
    "href": "notes-multiple-regression-2.html#summary-of-tests-concerning-regression-coefficients",
    "title": "\n6  Multiple regression 2\n",
    "section": "\n6.3 Summary of tests concerning regression coefficients",
    "text": "6.3 Summary of tests concerning regression coefficients\n\nVerify order doesn’t matter for estimates / t-tests, but order (in the sense of what is already included in the model does matter).\n\n# verify order doesn't matter for t-tests\n\n# fit model in different order\nmod_21 &lt;- lm(Y ~ X2 + X1, data_sample)\n\n# same estimate and t-test statistic\ncompare(mod_12 %&gt;% tidy %&gt;% arrange(term), mod_21 %&gt;% tidy %&gt;% arrange(term))\n\n$comparison\n[1] TRUE\n\n$`mod_12 %&gt;% tidy %&gt;% arrange(term)`\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.593     0.450      1.32 1.94e- 1\n2 X1             2.13      0.163     13.1  2.73e-17\n3 X2             1.91      0.147     13.0  3.57e-17\n\n$`mod_21 %&gt;% tidy %&gt;% arrange(term)`\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.593     0.450      1.32 1.94e- 1\n2 X1             2.13      0.163     13.1  2.73e-17\n3 X2             1.91      0.147     13.0  3.57e-17\n\n# just different decomposition of SS\ncompare(mod_12 %&gt;% anova %&gt;% tidy %&gt;% arrange(term),\n        mod_21 %&gt;% anova %&gt;% tidy %&gt;% arrange(term))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Component \\\"sumsq\\\": Mean relative difference: 0.2824361\"    \n[2] \"Component \\\"meansq\\\": Mean relative difference: 0.3145167\"   \n[3] \"Component \\\"statistic\\\": Mean relative difference: 0.3152953\"\n\n$comparison$`element-wise`\n     term   df sumsq meansq statistic p.value\n[1,] TRUE TRUE  TRUE   TRUE        NA      NA\n[2,] TRUE TRUE FALSE  FALSE     FALSE    TRUE\n[3,] TRUE TRUE FALSE  FALSE     FALSE    TRUE\n\n\n$`mod_12 %&gt;% anova %&gt;% tidy %&gt;% arrange(term)`\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Residuals    47  474.   10.1       NA  NA       \n2 X1            1 2373. 2373.       235.  6.50e-20\n3 X2            1 1705. 1705.       169.  3.57e-17\n\n$`mod_21 %&gt;% anova %&gt;% tidy %&gt;% arrange(term)`\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Residuals    47  474.   10.1       NA  NA       \n2 X1            1 1730. 1730.       171.  2.73e-17\n3 X2            1 2348. 2348.       233.  7.98e-20\n\n# verify what is included previpously in model matters for F tests\n\n# -&gt; same order model is necessary because F-test is different for X2 | X1 (second) vs X2 (first) --&gt; should be different\ncompare(mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(statistic),\n        mod_21 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(statistic))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Mean relative difference: 0.3770213\"\n\n$comparison$`element-wise`\n[1] FALSE\n\n\n$`mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(statistic)`\n[1] 168.9207\n\n$`mod_21 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(statistic)`\n[1] 232.6073\n\n# but X3 | X1, X2 should be same as X3 | X2, X1\nmod_213 &lt;- lm(Y ~ X2 + X1 + X3, data_sample)\n\ncompare(mod_123 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X3\") %&gt;% pull(statistic),\n        mod_213 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X3\") %&gt;% pull(statistic))\n\n$comparison\n[1] TRUE\n\n$`mod_123 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X3\") %&gt;% pull(statistic)`\n[1] 0.01347171\n\n$`mod_213 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X3\") %&gt;% pull(statistic)`\n[1] 0.01347171",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple regression 2</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-2.html#coefficient-of-partial-determination",
    "href": "notes-multiple-regression-2.html#coefficient-of-partial-determination",
    "title": "\n6  Multiple regression 2\n",
    "section": "\n6.4 Coefficient of partial determination",
    "text": "6.4 Coefficient of partial determination\nThe coefficient of multiple determination, \\(R^2\\), measures the proportionate reduction in the variation of \\(Y\\) achieved by the introduction of the entire set of \\(X\\) variables considered in the model.\nA coefficient of partial determination, in contrast, measures the marginal contribution of one \\(X\\) variable when all others are already included in the model.\n\nDemo\n\n# fit model with just X1\nmod_1 &lt;- lm(Y ~ X1, data_sample)\n\n# use model with X1 and X2 (already fit from above)\n\n# calculate R^2_{Y2|1} = SSR(X2|X1) / SSE(X1)\n# -&gt; reduction in SSE when X2 is added to the model relative to when only X1 in the model (just numerator in regression sum of squares perspective)\nSSR_X2_given_X1 &lt;- anova(mod_12) %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq) \nSSE_X1 &lt;- anova(mod_1) %&gt;% tidy %&gt;% filter(term == \"Residuals\") %&gt;% pull(sumsq) \n\n# another way to calculate (from the comments)\n# -&gt; adjust X2 and Y for the linear relationship to X1 to just get the added affect of X2\n\n# use mod_1\n\n# regress X2 on X1\nmod_X2X1 &lt;- lm(X2 ~ X1, data_sample)\n\n# regress residuals on each other\nmod_resids &lt;- lm(resid(mod_1) ~ resid(mod_X2X1))\n\n# extract R^2\ncompare(SSR_X2_given_X1 / SSE_X1, mod_resids %&gt;% glance %&gt;% pull(r.squared))\n\n$comparison\n[1] TRUE\n\n$`SSR_X2_given_X1/SSE_X1`\n[1] 0.7823275\n\n$`mod_resids %&gt;% glance %&gt;% pull(r.squared)`\n[1] 0.7823275\n\n# show added variable plot -&gt; strong linear relationship is evident after adjusting for X1\nplot(x = resid(mod_1), y = resid(mod_X2X1))",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple regression 2</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-2.html#sec-standardized-reg-model",
    "href": "notes-multiple-regression-2.html#sec-standardized-reg-model",
    "title": "\n6  Multiple regression 2\n",
    "section": "\n6.5 Standardized regression model",
    "text": "6.5 Standardized regression model\nNOTE: Writing notes here just to be aware of it, but not going to demo it cause doesn’t seem to be standard practice in R.\n\n6.5.1 Overview\nA standardized form of the general multiple regression model @ge-lin-mod is employed to control roundoff errors in normal equations calculations and to permit comparisons ofthe estimated regression coefficients in common units.\nRoundoff errors\n\nRoundoff errors tend to enter normal equations calculations primarily when the inverse of \\(\\boldsymbol{X'X}\\)is taken. These are magnified when calculating \\(\\boldsymbol{\\beta}\\) and other statistics.\nThe danger of serious roundoff errors in \\(\\boldsymbol{(X'X)}^{-1}\\) is particularly great when (1) \\(\\boldsymbol{X'X}\\) has a determinant that is close to zero and/or (2) the elements of \\(\\boldsymbol{X'X}\\) differ substantially in order of magnitude (say 10 and 10^6.\nThe transformation to obtain the standardized regression model, called the correlation transformation, makes all entries in the \\(\\boldsymbol{X'X}\\) matrix for the transformed variablesf all between -1 and 1 inclusive, so that the calculation of the inverse matrix becomes much less subject to roundoff errors due to dissimilar orders of magnitudes than with the original variables.\n\nAlthough, in order to avoid the computational difficulties inherent in inverting the \\(\\boldsymbol{X'X}\\) matrix, many statistical packages use an entirely different computational approach that involves decomposing the \\(\\boldsymbol{X}\\) matrix into a product of several matrices with special properties.\n\n\nSite explaining QR decomposition and how it relates to LSE.\n\n\n\nLack of comparability of regression coefficients\n\nA second difficulty with the nonstandardized multiple model is that ordinarily regression coefficients cannot be compared because of differences in the units involved. For example if \\(X_1\\) is in dollars and \\(X_2\\) is in thousands of dollars, simply comparing the relative magnitudes of \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta}_2\\) to get an idea of importance can be misleading.\n\nCorrelation transformation\n\n\nUse of the standardized variables above (the typical standardization process) without the correlation transformation modification above will lead to the same standardized regression coefficients resulting from the correlation-transformed variables. However, the elements of the \\(\\boldsymbol{X'X}\\) matrix will not then be bounded between -1 and 1.\n\nStandardized regression model\n\n\nSome more results about what the \\(\\boldsymbol{\\beta}\\) is and some intermediate calculations.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple regression 2</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-2.html#multicollinearity-and-its-effects",
    "href": "notes-multiple-regression-2.html#multicollinearity-and-its-effects",
    "title": "\n6  Multiple regression 2\n",
    "section": "\n6.6 Multicollinearity and its effects",
    "text": "6.6 Multicollinearity and its effects\n\n6.6.1 Setup\nIn multiple regression analysis, the nature and significance of the relations between the predictor or explanatory variables and the response variable are often of particular interest Some questions frequently asked are:\n\nWhat is the relative importance of the effects of the different predictor variables?\nWhat is the magnitude of the effect of a given predictor variable on the response variable?\nCan any predictor variable be dropped from the model because it has little or no effect on the response variable?\nShould any predictor variables not yet included in the model be considered for possible inclusion?\n\nIf the predictor variables included in the model are (l) uncorrelated among themselves and (2) uncorrelated with any other predictor variables that are related to the response variable but are omitted from the model, relatively simple answers can be given to these questions. Unfortunately in non-experimental studies this is often not the case.\nWhen the predictor variables are correlated among themselves, multicollinearity among them is said to exist. This creates a variety of problems, but we will first explore the uncorrelated case first.\n\n6.6.2 Uncorrelated predictor variables\nSuppose we have two predictor variables, \\(X_1\\) and \\(X_2\\), which are uncorrelated.\nObservation and results\n\n\nObservation: Estimated coefficient \\(\\hat{\\beta}_1\\) will be the same REGARDLESS of whether \\(X_2\\) is included in the model (in any order). Same for \\(\\hat{\\beta}_2\\) and \\(X_1\\).\n\nGeneralized result: When the predictor variables are uncorrelated, the effects ascribed to them by a first-order regression model are the same no matter which other of these predictor variables are included in the model.\nThis is a strong argument for controlled experiments whenever possible, since experimental control permits choosing the levels of the predictor variables so as to make these variables uncorrelated.\n\n\n\nObservation: The extra sum of squares \\(SSR(X_2 \\mid X_1)\\) equals the regression sum of squares \\(SSR(X_2)\\) when only \\(X_2\\) is in the regression model. Same for \\(X_1\\).\n\nGeneralized result: When two or more predictor variables are uncorrelated, the marginal contribution of one predictor variable in reducing the error sum of squares when the other predictor variables are in the model is exactly the same as when this predictor variable is in the model alone.\nNote that this can be seen in the formulas for the \\(\\hat{\\beta}_i\\)s.\n\n\n\nDemo\n\n\nPerfectly uncorrelated\nPractically uncorrelated\n\n\n\n\n# perfectly uncorrelated\n\n# not sure how to generate or transform to perfectly uncorrelated predictor variables\n\n# so using this found dataset (https://online.stat.psu.edu/stat501/lesson/12/12.2)\ndata_sample &lt;- tribble(\n  ~X1,  ~X2, ~Y,\n  -1,   1,  91,\n  -1,   2,  107,\n  -1,   3,  101,\n  -1,   4,  121,\n  -1,   5,  95,\n  0,    7,  84,\n  0,    8,  108,\n  0,    9,  102,\n  0,    10, 98,\n  1,    1,  73,\n  1,    2,  75,\n  1,    3,  102,\n  1,    4,  94,\n  1,    5,  113\n)\ndata_sample %&gt;% select(starts_with(\"X\")) %&gt;% cor\n\n   X1 X2\nX1  1  0\nX2  0  1\n\n\n\n# fit models and compare coefficients\nmod_1 &lt;- lm(Y ~ X1, data_sample)\nmod_12 &lt;- lm(Y ~ X1 + X2, data_sample)\nmod_2 &lt;- lm(Y ~ X2, data_sample)\n\ncompare(mod_1 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate),\n        mod_12 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate))\n\n$comparison\n[1] TRUE\n\n$`mod_1 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate)`\n[1] -5.8\n\n$`mod_12 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate)`\n[1] -5.8\n\ncompare(mod_2 %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(estimate),\n        mod_12 %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(estimate))\n\n$comparison\n[1] TRUE\n\n$`mod_2 %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(estimate)`\n[1] 1.360256\n\n$`mod_12 %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(estimate)`\n[1] 1.360256\n\n# compare extra sum of squares when variable included previously\ncompare(mod_2 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq),\n        mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq))\n\n$comparison\n[1] TRUE\n\n$`mod_2 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq)`\n[1] 206.176\n\n$`mod_12 %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"X2\") %&gt;% pull(sumsq)`\n[1] 206.176\n\n\n\n\n\n# practically uncorrelated\n\n# try to generate uncorrelated predictor variables\nn &lt;- 50\nmu &lt;- c(0, 0)\nsigma &lt;- 3 * matrix(data = c(1,0,0,1), ncol = 2)\nX &lt;- MASS::mvrnorm(n = n, mu = mu, Sigma = sigma)\ncor(X)\n\n             [,1]         [,2]\n[1,]  1.000000000 -0.001836028\n[2,] -0.001836028  1.000000000\n\nX %&lt;&gt;% cbind(rep(1, n), .)\n\n# set betas according to which ones are noise (and intercept)\nbetas &lt;- c(2, 2, 2)\n\n# generate mean response and add random error\nY &lt;- X %*% betas + rnorm(n, mean = 0, sd = 3)\n\n\n# combine data\ndata_sample &lt;- cbind(Y, X[, -1]) %&gt;% \n  data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\", 1:(length(betas)-1)))\n\n# fit models and compare coefficients\nmod_1 &lt;- lm(Y ~ X1, data_sample)\nmod_12 &lt;- lm(Y ~ X1 + X2, data_sample)\n\ncompare(mod_1 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate),\n        mod_12 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Mean relative difference: 0.00270964\"\n\n$comparison$`element-wise`\n[1] FALSE\n\n\n$`mod_1 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate)`\n[1] 1.602815\n\n$`mod_12 %&gt;% tidy %&gt;% filter(term == \"X1\") %&gt;% pull(estimate)`\n[1] 1.607158\n\n\n\n\n\n\n6.6.3 Perfectly correlated predictors\nResults of trivial example from book:\n\nFor a model with two predictors, there is an infinite set of response functions (surfaces) that generate the same set of fitted values where they intersect, but totally different ones when not.\n\nTwo implications of above example:\n\nThe perfect relation between \\(X_1\\) and \\(X_2\\) did not inhibit our ability to obtain a good fit to the data.\nSince many different response functions provide the same good fit, we cannot interpret anyone set of regression coefficients as reflecting the effects of the different predictor variables 9in terms of magnitude and direction).\n\nDemo\n\n\nPerfectly correlated\nPractically super correlated\n\n\n\n\n# generate a single vector and transform\nn &lt;- 50\nX &lt;- rnorm(n, mean = 0, sd = 3) %&gt;% data.frame(X1 = .)\nX %&lt;&gt;% mutate(X2 = 3*X1)\ncor(X)\n\n   X1 X2\nX1  1  1\nX2  1  1\n\nX %&lt;&gt;% cbind(rep(1, n), .)\n\n# set betas according to which ones are noise (and intercept)\nbetas &lt;- c(2, 2, 2)\n\n# generate mean response and add random error\nY &lt;- as.matrix(X) %*% betas + rnorm(n, mean = 0, sd = 3)\n\n\n# combine data\ndata_sample &lt;- cbind(Y, X[, -1]) %&gt;% \n  data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\", 1:(length(betas)-1)))\n\n# fit model with both variables\nmod_12 &lt;- lm(Y ~ X1 + X2, data_sample)\n\nsummary(mod_12)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_sample)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4289 -1.7612 -0.3936  2.0788  7.7564 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   2.2962     0.3998   5.744          0.000000616 ***\nX1            7.9718     0.1341  59.425 &lt; 0.0000000000000002 ***\nX2                NA         NA      NA                   NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.81 on 48 degrees of freedom\nMultiple R-squared:  0.9866,    Adjusted R-squared:  0.9863 \nF-statistic:  3531 on 1 and 48 DF,  p-value: &lt; 0.00000000000000022\n\n\nSecond coefficient doesn’t even get estimated.\n\n\n\n# generate a single vector and transform + slight noise\nn &lt;- 50\nX &lt;- rnorm(n, mean = 0, sd = 3) %&gt;% data.frame(X1 = .)\nX %&lt;&gt;% mutate(X2 = 3*X1)\nX$X2 &lt;- X$X2 + rnorm(n = n, mean = 0, sd = 0.01)\ncor(X)\n\n          X1        X2\nX1 1.0000000 0.9999994\nX2 0.9999994 1.0000000\n\nX %&lt;&gt;% cbind(rep(1, n), .)\n\n# set betas according to which ones are noise (and intercept)\nbetas &lt;- c(2, 2, 2)\n\n# generate mean response and add random error\nY &lt;- as.matrix(X) %*% betas + rnorm(n, mean = 0, sd = 3)\n\n\n# combine data\ndata_sample &lt;- cbind(Y, X[, -1]) %&gt;% \n  data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\", 1:(length(betas)-1)))\n\n# fit model with both variables\nmod_12 &lt;- lm(Y ~ X1 + X2, data_sample)\n\nsummary(mod_12)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_sample)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1601 -1.7987  0.1695  1.4131  5.6758 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)   2.0498     0.3699   5.541 0.00000132 ***\nX1           16.9964   126.0404   0.135      0.893    \nX2           -3.0002    42.0126  -0.071      0.943    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.609 on 47 degrees of freedom\nMultiple R-squared:  0.9852,    Adjusted R-squared:  0.9846 \nF-statistic:  1568 on 2 and 47 DF,  p-value: &lt; 0.00000000000000022\n\n\nSuper inflated standard errors and wildly different estimates than the \\(\\beta\\)s.\n\n\n\n\n6.6.4 Effects of multicollinearity\nSummary of related effects of multicollinearity\n\nMulticollinearity doesn’t affect prediction.\n\n\nThe fact that some or all predictor variables are correlated among themselves does not, in general, inhibit our ability to obtain a good fit nor does it tend to affect inferences about mean responses or predictions of new observations, provided these inferences are made within the region of observations.\n\\(MSE\\) still gets steadily reduced as more variables are added.\n\n\n\n\nAnd the precision of fitted values within the range of the observations on the predictor variables does not worsen with the addition of correlated predictor variables into the regression model (even despite the inflated standard errors of the \\(\\beta\\)s).\n\nThe essential reason for the stability is that the covariance between \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) is negative, which plays a strong counteracting influence to the increase in \\(s^2(\\hat{\\beta}_1)\\), in determining the value of \\(s^2(\\hat{Y}_h)\\) as given below:\n\n\n\n\n\nLarge standard errors in estimated regression coefficients.\n\n\nEstimated regression coefficients tend to have large sampling variability when the predictor variables are highly correlated \\(\\Longrightarrow\\) tend to vary widely from one sample to the next \\(\\Longrightarrow\\) only imprecise information may be available about the individual true regression coefficients.\n\n\n\nIndeed, many of the estimated regression coefficients individually may be statistically not significant even though a definite statistical relation exists between the response variable and the set of predictor variables.\n\n\nCannot interpret estimated regression coefficients.\n\n\nThe common interpretation of a regression coefficient as measuring the change in the expected value of the response variable when the given predictor variable is increased by one unit while all other predictor variables are held constant (aka the marginal effect) is not fully applicable when multicollinearity exists.\nIt may be conceptually feasible to think of varying one predictor variable and the others constant, but it may not be possible in practice to do so for predictor variables that are highly correlated. For example, in a regression model for predicting crop yield from amount of rainfall and hours of sunshine, the relation between the two predictor variables makes it unrealistic to consider varying one while holding the other constant.\n\n\nMagnitude (and sign) of estimated coefficients are conditional based on what is already in model.\n\n\nWhen predictor variables are correlated, the regression coefficient of anyone variable depends on which other predictor variables are included in the model and which ones are left out.\n\n\n\nThus, a regression coefficient does not reflect any inherent effect of the particular predictor variable on the response variable but only a marginal or partial effect, given whatever other correlated predictor variables are included in the model.\nEven have to be weary of predictors that are highly correlated with each other, even if some are omitted from the model.\n\n\nEffects of variables are conditional based on what is already in model.\n\n\nWhen predictor variables are correlated, the marginal contribution of anyone predictor variable in reducing the error sum of squares varies, depending on which other variables are already in the regression model, just as for regression coefficients.\n\nFor example, supppose \\(X_1\\) and \\(X_2\\) are highly correlated and \\(SSR(X_1) &gt;&gt; SSR(X_1 \\mid X_2)\\). The reason why \\(SSR(X_1 \\mid X_2)\\) is so small compared with \\(SSR(X1)\\) is that \\(X_1\\) and \\(X_2\\) are highly correlated with each other and with the response variable. Thus, when \\(X_2\\) is already in the regression model, the marginal contribution of \\(X_1\\) in reducing the error sum of squares is comparatively small because \\(X_2\\) contains much of the same information as \\(X_1\\).\n\nThis can also be seen in the coefficient of partial determinations, which also behave erratically sample-to-sample when multiollinearity is present.\n\n\n\n\n\nWhen predictor variables are correlated, there is no unique sum of squares that can be ascribed to anyone predictor variable as reflecting its effect in reducing the total variation in \\(Y\\).\n\n\nMismatching results from simulataneuous inferences vs overall inference on coefficients.\n\n\n\nUnstable model fitting.\n\n\nAs mentioned in {#sec-standardized-reg-model}, a near-zero determinant of \\(\\boldsymbol{X'X}\\) is a potential source of serious roundoff errors in normal equations calculations. Severe has the of making this determimant come close to zero.\nThus, under severe multicollineanty, the regression coefficients may be subject to large roundoff errors as well as large sampling variances \\(\\Longrightarrow\\) Should use correlation transformation or something akin.\n\n6.6.5 Need for more powerful diagnostics for multicollinearity\n\nAs we have seen, multicollinearity among the predictor variables can have important consequences for interpreting and using a fitted regression model.\nThe diagnostic tool considered here for identifying multicollinearity was the pairwise coefficients of simple correlation between the predictor variables; this is frequently helpful.\nOften, however, serious multicollinearity exists without being disclosed by the pairwise correlation coefficients. Later, we present a more powerful tool for identifying the existence of serious multicollinearity. Some remedial measures for lessening the effects of multicollinearity will be considered as well.\n\n6.6.6 Simulation\nSimualtion to demo the affects of varying levels of multicollinearity.\n\nCode# initialize simulation settings\n# -&gt; first the parameters of interest (that are variable)\n# -&gt; create all combos\n# -&gt; add in the constant settings\n# -&gt; arrange in a good order\n# -&gt; add simulation id column\n# -&gt; set row names equal to id (helps keep track of results later)\nparams &lt;- expand.grid(rho_x = c(0, 0.25, 0.5, 0.75, 0.95)) %&gt;% \n  mutate(n = 50,\n         sigma_x = 3,\n         beta_0 = 2,\n         beta_1 = 2,\n         beta_2 = 2,\n         sigma = 5) %&gt;%\n  arrange(rho_x) %&gt;% \n  bind_cols(data.frame(sim_id = 1:nrow(.)))\nrownames(params) &lt;- paste0(\"sim\", params$sim_id)\n\n# define function to run on each unique simulation setting\nrun_trial &lt;- function(vec, vec_names, m = 1000) {\n  \n  # redefine arguments for easier reference\n  names(vec) = vec_names\n  rho_x = as.numeric(vec[\"rho_x\"])\n  n = as.numeric(vec[\"n\"])\n  sigma_x = as.numeric(vec[\"sigma_x\"])\n  beta_0 = as.numeric(vec[\"beta_0\"])\n  beta_1 = as.numeric(vec[\"beta_1\"])\n  beta_2 = as.numeric(vec[\"beta_2\"])\n  sigma = as.numeric(vec[\"sigma\"])\n  sim_id = as.numeric(vec[\"sim_id\"])\n  \n  # initialize results vectors\n  beta0_hat = rep(NA, m)\n  beta1_hat = rep(NA, m)\n  beta2_hat = rep(NA, m)\n  se_beta1_hat = rep(NA, m)\n  se_beta2_hat = rep(NA, m)\n  beta1_t = rep(NA, m)\n  beta2_t = rep(NA, m)\n  MSE = rep(NA, m)\n  R2 = rep(NA, m)\n  R2_adj = rep(NA, m)\n\n\n  # loop to simulate m models and extract summaries\n  for (i in 1:m) {\n    \n    # generate X predictors according to specified correlation structure\n    Sigma = sigma_x * matrix(data = c(1,rho_x,rho_x,1), ncol = 2)\n    X = MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = Sigma)\n    X %&lt;&gt;% cbind(rep(1, n), .)\n    \n    # generate mean response and add random error\n    Y = as.matrix(X) %*% c(beta_0, beta_1, beta_2) + rnorm(n, mean = 0, sd = sigma)\n    \n    # combine data\n    data_sample = cbind(Y, X[, -1]) %&gt;% \n      data.frame\n    colnames(data_sample) = c(\"Y\", paste0(\"X\", 1:2))\n    \n    # fit model with both variables\n    mod = lm(Y ~ X1 + X2, data_sample)\n    \n    # get summaries\n    mod_summ &lt;- broom::tidy(mod)\n    mod_stats = broom::glance(mod)\n    beta0_hat[i] = mod_summ %&gt;% filter(term == \"(Intercept)\") %&gt;% pull(estimate)\n    beta1_hat[i] = mod_summ %&gt;% filter(term == \"X1\") %&gt;% pull(estimate)\n    beta2_hat[i] = mod_summ %&gt;% filter(term == \"X2\") %&gt;% pull(estimate)\n    se_beta1_hat[i] = mod_summ %&gt;% filter(term == \"X1\") %&gt;% pull(std.error)\n    se_beta2_hat[i] = mod_summ %&gt;% filter(term == \"X2\") %&gt;% pull(std.error)\n    beta1_t[i] = mod_summ %&gt;% filter(term == \"X1\") %&gt;% pull(statistic)\n    beta2_t[i] = mod_summ %&gt;% filter(term == \"X2\") %&gt;% pull(statistic)\n    MSE[i] = mod_stats$sigma^2\n    R2[i] = mod_stats$r.squared\n    R2_adj[i] = mod_stats$adj.r.squared\n\n  }\n  \n  return(cbind(beta0_hat, beta1_hat, beta2_hat, se_beta1_hat, se_beta2_hat, beta1_t, beta2_t, MSE, R2, R2_adj, sim_id = sim_id))\n  \n}\n\n\n# define function to run simulation and format results\nsimulation &lt;- function(params, sim_function, iterations = 100, format_results = FALSE) {\n  \n  # run simulation for each parameter settings\n  # -&gt; have to transpose so that now each column is a parameter settings and convert to dataframe so can pass to map(), which needs a list\n  results_raw = params %&gt;% \n    t %&gt;% \n    data.frame %&gt;% \n    map(\\(vec) sim_function(vec, vec_names = names(params), m = iterations), .progress = T)\n  \n  if (!format_results){\n    \n    return(results_raw)\n    \n  }else{\n    \n    # reformat results\n    # -&gt; convert each results matrix to a dataframe, then combine\n    # -&gt; attach the simulation settings to the results\n    # -&gt; pivot longer to have one column for the name of the summary measure and one for its value\n    # -&gt; convert parameters to factors for plot\n    results = results_raw %&gt;% \n      map(\\(x) data.frame(x)) %&gt;% \n      reduce(bind_rows) %&gt;% \n      left_join(params, by = \"sim_id\") %&gt;% \n      pivot_longer(cols = 1:(ncol(results_raw[[1]])-1),\n                   names_to = \"statistic\",\n                   values_to = \"value\") %&gt;% \n      mutate(across(1:ncol(params), as.factor))\n    return(results)\n    \n  }\n  \n}\n\n# define function to plot standard results\nplot_results &lt;- function(res, stats) {\n  \n  res %&gt;% \n  filter(statistic %in% stats) %&gt;% \n    ggplot(aes(x = value,\n               group = rho_x,\n               color = rho_x,\n               fill = rho_x),\n           data = .) + \n    geom_density(alpha = 0.2) + \n    facet_wrap(statistic ~ .,\n               scales = \"free\") \n  \n}\n\n\n\n# run simulation\nresults &lt;- simulation(params = params, sim_function = run_trial, iterations = 1000, format_results = TRUE)\nhead(results)\n\n# A tibble: 6 × 10\n  sim_id rho_x n     sigma_x beta_0 beta_1 beta_2 sigma statistic    value\n  &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 1      0     50    3       2      2      2      5     beta0_hat    2.55 \n2 1      0     50    3       2      2      2      5     beta1_hat    1.93 \n3 1      0     50    3       2      2      2      5     beta2_hat    1.98 \n4 1      0     50    3       2      2      2      5     se_beta1_hat 0.349\n5 1      0     50    3       2      2      2      5     se_beta2_hat 0.488\n6 1      0     50    3       2      2      2      5     beta1_t      5.53 \n\n\n\n# view results\nplot_results(results, stats = c(\"beta0_hat\", \"beta1_hat\", \"beta2_hat\"))\n\n\n\n\n\n\n\n\nAll point estimates are unbiased.\n\n\n# view results\nplot_results(results, stats = c(\"se_beta1_hat\", \"se_beta2_hat\"))\n\n\n\n\n\n\n\n\nLots more variable in standard errors of estimated coefficients for highly correlated predictors.\n\n\n# view results\nplot_results(results, stats = c(\"beta1_t\", \"beta2_t\"))\n\n\n\n\n\n\n\n\nMuch less likely to reject marginal tests when there is multicollinearity.\n\n\n# view results\nplot_results(results, stats = c(\"MSE\", \"R2\", \"R2_adj\"))\n\n\n\n\n\n\n\n\nMulticollinearity doesn’t affect \\(MSE\\).\nMulticollinearity does cause an inflation of \\(R^2\\) and \\(R^2_{adj}\\).",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple regression 2</span>"
    ]
  },
  {
    "objectID": "notes-reg-models-quan-and-qual.html#polynomial-regression-models",
    "href": "notes-reg-models-quan-and-qual.html#polynomial-regression-models",
    "title": "\n7  Regression models for quantitative and qualitative predictors\n",
    "section": "\n7.1 Polynomial regression models",
    "text": "7.1 Polynomial regression models\nWe first consider polynomial regression models for quantitative predictor variables. They are among the most frequently used curvilinear response models in practice because they are handled easily as a special case of the general linear regression model (@gen-lin-mod).\nNext, we discuss several commonly used polynomial regression models.\nThen we discuss some of the major issues encountered with polynomial regression models.\n\n7.1.1 Uses of polynomial models\nPolynomial regression models have two basic types of uses:\n\nWhen the true curvilinear response function is indeed a polynomial function.\nWhen the true curvilinear response function is unknown (or complex) but a polynomial function is a good approximation to the true function.\n\nThe second type of use, where the polynomial function is employed as an approximation when the shape of the true curvilinear response function is unknown, is very common. It may be viewed as a nonparametric approach to obtaining information about the shape of the response function.\nA main danger in using polynomial regression models is that extrapolations may be hazardous with these models, especially those with higher-order terms. Polynomial regression models may provide good fits for the data at hand, but may turn in unexpected directions when extrapolated beyond the range of the data.\n\n7.1.2 One predictor variable – Second order\nPolynomial regression models may contain one, two, or more than two predictor variables. Further, each predictor variable may be present in various powers. We begin with the simplest case: one predictor variable with second order.\n\\[Y_i \\hspace{10pt} = \\hspace{10pt} \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 \\hspace{10pt} = \\hspace{10pt} \\beta_0 + \\beta_1 x_i + \\beta_{11} x_i^2 + \\epsilon_i\\] where \\(x_i = X_i - \\bar{X}\\). This model has a quadratic response function (a parabola):\n\\[E(Y_i) = \\beta_0 + \\beta_1 x + \\beta_{11} x^2 \\]\n\nNotes\n\n\nThe predictor variable is centered, i.e. expressed as a deviation around its mean \\(\\bar{X}\\), and that the \\(i\\)th centered observation is denoted by \\(x_i\\).\n\nThe reason for using a centered predictor variable in the polynomial regression model is that \\(X\\) and \\(X^2\\) often will be highly correlated. Centering the predictor variable often reduces the multicollinearity substantially and tends to avoid computational difficulties.\nAn alternative to using centered variables in polynomial regression is to use orthogonal polynomials. This is what R uses in poly().\n\n\nCan use different subscripts on the \\(\\beta\\)s so they match up with the power of the corresponding \\(x\\) term better.\nThe danger of extrapolating a polynomial response function is illustrated by the response functions in the image above. If this function is extrapolated beyond \\(x = 2\\), it actually turns downward, might not be appropriate in a given case.\n\nOrthogonal polynomials demo\n\n7.1.3 One predictor variable – Higher orders\nThe previous model can easily be extended to higher orders. For example, here is a third order model:\n\\[Y_i = \\beta_0 + \\beta_1 x_i + \\beta_{11} x_i^2 + \\beta_{111} x_i^3 + \\epsilon_i\\]\n\nPolynomial models with the predictor variable present in higher powers than the third should be employed with special caution.\n\nThe interpretation of the coefficients becomes difficult for such models, and the models may be highly erratic for interpolations and even small extrapolations.\nNote that a polynomial model of sufficiently high order can always be found to fit data containing no repeat observations perfectly. For instance, the fitted polynomial regression function for one predictor variable of order \\(n - 1\\) will pass through all \\(n\\) observed \\(Y\\) values.\n\nOverfitting demo\n\n# generate data\nn &lt;- 10\nnum_x &lt;- 1\nX &lt;- sapply(1:(num_x), function(i) {runif(n = n, min = 0, max = 10)})\nX %&lt;&gt;% cbind(rep(1, n), .)\nbetas &lt;- c(1, rep(2, num_x))\nY &lt;- X %*% betas + rnorm(n, mean = 0, sd = 5)\n\n\n# combine data\ndata_sample &lt;- cbind(Y, X[, -1]) %&gt;% \n  data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\", 1:num_x))\n\n# fit highest order model\n# -&gt; note perfect fit\nmod_overfit &lt;- lm(Y ~ poly(X1, degree = n - 1), data_sample)\ntidy(mod_overfit)\n\n# A tibble: 10 × 5\n   term                      estimate std.error statistic p.value\n   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)                  9.87        NaN       NaN     NaN\n 2 poly(X1, degree = n - 1)1   19.1         NaN       NaN     NaN\n 3 poly(X1, degree = n - 1)2   -2.00        NaN       NaN     NaN\n 4 poly(X1, degree = n - 1)3    9.07        NaN       NaN     NaN\n 5 poly(X1, degree = n - 1)4   -6.83        NaN       NaN     NaN\n 6 poly(X1, degree = n - 1)5   -4.14        NaN       NaN     NaN\n 7 poly(X1, degree = n - 1)6   -2.22        NaN       NaN     NaN\n 8 poly(X1, degree = n - 1)7   -0.428       NaN       NaN     NaN\n 9 poly(X1, degree = n - 1)8   -0.383       NaN       NaN     NaN\n10 poly(X1, degree = n - 1)9    0.465       NaN       NaN     NaN\n\nglance(mod_overfit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1           NaN   NaN       NaN     NaN     9    Inf  -Inf  -Inf\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# plot response function\n# -&gt; add fitted values to dataset and extract them to plot (have to sort first)\ndata_sample %$% plot(X1, Y)\naugment(x = mod_overfit, data = data_sample) %&gt;%\n  arrange(X1) %$%\n  lines(x = X1, y = .fitted, col = \"red\")\n\n\n\n\n\n\n\n\n7.1.4 Two or more predictor variables – Second order\nAgain, can extend the previous models to now include a second predictor variable and beyond (now including cross terms (interaction terms), still considered second order). For example, here is a second order model with two predictor variables:\n\\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_{11} x_{i1}^2 + \\beta_{22} x_{i2}^2 + \\beta_{12} x_{i1} x_{i2} + \\epsilon_i\\]\nwhere \\(x_{i1} = X_{i1} - \\bar{X}_1\\) and \\(x_{i2} = X_{i2} - \\bar{X}_2\\). This model has a conic response function:\n\\[E(Y_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2 \\] \n\n7.1.5 Implementation of polynomial regression models\nFitting of polynomial models\n\nFitting of polynomial regression models presents no new problems since they are special cases of the usual general linear regression model.\n\nHiearchical approach to fitting\n\nWhen using a polynomial regression model as an approximation to the true regression function, statisticians will often fit a second-order or third-order model and then explore whether a lower-order model is adequate with partial \\(F\\) tests.\n\nWith the hierarchical approach, if a polynomial term of a given order is retained, then all related terms of lower order are also retained in the model.\n\nThus, one would not drop the quadratic term of a predictor variable but retain the cubic term in the model.\nSince the quadratic term is of lower order, it is viewed as providing more basic information about the shape of the response function; the cubic term is of higher order and is viewed as providing refinements in the specification of the shape of the response function.\n\n\n\nRegression function in terms of \\(X\\)\n\nAfter a polynomial regression model has been developed, we often wish to express the final model in terms of the original variables rather than keeping it in terms of the centered variables. This can be done readily (not showing because not Rs implementation).\nThe fitted values and residuals for the regression function in terms of \\(X\\) are exactly the SAME as for the regression function in terms of the centered values \\(x\\).\nThe estimated standard deviation however do not translate. They need to be found using alternate methods.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression models for quantitative and qualitative predictors</span>"
    ]
  },
  {
    "objectID": "notes-reg-models-quan-and-qual.html#interaction-regression-models",
    "href": "notes-reg-models-quan-and-qual.html#interaction-regression-models",
    "title": "\n7  Regression models for quantitative and qualitative predictors\n",
    "section": "\n7.2 Interaction regression models",
    "text": "7.2 Interaction regression models\n\n7.2.1 Interaction effects\n\n\n7.2.2 Interpretation of interaction regression models with linear effects\nInterpretation of regression coefficients\n\nThe regression model for two quantitative predictor variables with linear effects on \\(Y\\) and interacting effects of \\(X_1\\) and \\(X_2\\) on \\(Y\\) represented by a cross-product term is as follows:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\epsilon_i\\] - The meaning of the regression coefficients \\(\\beta_1\\) and \\(\\beta_2\\) here is not the same as that given earlier because of the interaction term \\(\\beta_3 X_{i1} X_{i2}\\). \\(\\beta_1\\) no longer represent mean change in \\(Y\\) for a one unit increase in \\(X_1\\) when all other predictors are held constant at any given level.\n\nIt can be easily shown that the change in the mean response with a unit increase in \\(X_1\\) when \\(X_2\\) is held constant is\n\n\\[\\beta_1 + \\beta_3 X_2\\]\n\nSimilarly the change in the mean response with a unit increase in \\(X_2\\) when \\(X_1\\) is held constant is\n\n\\[\\beta_2 + \\beta_3 X_1\\] - Thus, in the regression model above, both the effect of \\(X_1\\) for given level of \\(X_2\\) and the effect of \\(X_2\\) for given level of \\(X_1\\) depend on the level of the other predictor variable.\n\n\nBelow are conditional effects plots because they show the effects of \\(X_1\\) on the mean response conditional on different levels of the other predictor variable.\n\nFor additive models, the effect of \\(X_1\\) is the same for both levels of \\(X_2\\) (same slope) (i.e. \\(Y\\) increases by the same amount when \\(X_1\\) varies regardless of the level of \\(X_2\\).\nFor the reinforcement interaction, the effect of \\(X_1\\) becomes stronger at hifher levels of \\(X_2\\). This occurs if both the linear terms and the interaction terms have the same sign.\nIn the inteference plot, the effect of \\(X_1\\) is lessoned at higher levels of \\(X_2\\) because the linear terms and the interaction terms have opposing signs.\n\n\n\n\nSee interplot-vignette for explanations on interaction plots in R (they are a bit different than above).\n\n7.2.3 Implementation of interaction effects\n\n\nWhen interaction terms area dded to a regression model,high multicollinearities may exist between some of the predictor variables and spme of the interaction terms, as well as among some of the interaction terms.\n\nA partial remedy to improve computational accuracy is to center the predictor variables; i.e. \\(x_{ik} = X_{ik} - \\bar{X}_k\\)\n\n\n\n\nWhen the number of predictor variables in the regression model is large, the potential number of interaction terms can become very large.\n\nFor example, if eight predictor variables are present in the regression model in linear terms, there are potentially 28 pairwise interaction terms that could be added to the regression model. The dataset would need to be quite large before 36 \\(X\\) variables could be used in the regression model.\n\n\n\nIt is therefore desirable to identify in advance, whenever possible, those interactions that are most likely to influence the response variable in important ways.\n\nIn addition to utilizing a priori knowledge, one can plot the residuals for the additive regression model against the different interaction terms to determine which ones appear to be influential in affecting the response variable.\nWhen the number of predictor variables is large, these plots may need to be limited to intemction terms involving those predictor variables that appear to be the most important on the basis of the initial fit of the additive regression model.\n\n\n\n7.2.4 Demo\nNo interaction effect\n\n# generate data from additive model\nn &lt;- 30\nnum_x &lt;- 2\nX &lt;- sapply(1:(num_x), function(i) {rnorm(n = n, mean = 0, sd = 3)})\nX %&lt;&gt;% cbind(rep(1, n), .)\nbetas &lt;- c(1, rep(2, num_x))\nY &lt;- X %*% betas + rnorm(n, mean = 0, sd = 5)\n\n\n# combine data\ndata_sample &lt;- cbind(Y, X[, -1]) %&gt;% \n  data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\", 1:num_x))\n\n# fit interaction model\nmod_1 &lt;- lm(Y ~ X1 * X2, data_sample)\ntidy(mod_1)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.62     0.793       2.05 5.11e- 2\n2 X1             2.49     0.219      11.4  1.35e-11\n3 X2             2.37     0.325       7.27 1.00e- 7\n4 X1:X2         -0.132    0.0979     -1.35 1.88e- 1\n\n\n\n# interaction plot\n# -&gt; var1 = variable of interest\n# -&gt; var2 = conditioning variable\ninterplot::interplot(mod_1, var1 = \"X2\", var2 = \"X1\",\n                     point = T,\n                     stats_cp = \"ci\") + \n  labs(title = \"Non significant interaction\",\n       x = \"X2\",\n       y = \"Estimated coefficient for X1\")\n\n\n\n\n\n\n\n\nThe algorithm behind the plot is essentially fitting a bunch of models for each unique level of \\(X_2\\) (the conditioning variable) and plotting a CI for the resulting estimated coefficients for \\(X_1\\).\nReference line could be at \\(Y = 0\\), which obviously means non significant, but so does small values of the coefficient (relative to the standard error, have to make \\(t\\)-stats out of them).\nConfidence interval in bottom corner should however be interpreted with respect to 0; it is measuring something different (explained in the vignette). Can also look at the distribution of the conditioning variable with hist = T, which can help for effects that are significant over part of the range, but not all.\n\nInteraction effect\n\n# generate data from interaction model\nn &lt;- 30\nnum_x &lt;- 2\nX &lt;- sapply(1:(num_x), function(i) {rnorm(n = n, mean = 0, sd = 3)}) %&gt;%\n  data.frame %&gt;% \n  mutate(X1X2 = X1 * X2) %&gt;% \n  as.matrix\nX %&lt;&gt;% cbind(rep(1, n), .)\nbetas &lt;- c(1, rep(2, num_x+1))\nY &lt;- X %*% betas + rnorm(n, mean = 0, sd = 5)\n\n\n# combine data\ndata_sample &lt;- cbind(Y, X[, 2:3]) %&gt;% \n  data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\", 1:num_x))\n\n# fit interaction model\nmod_2 &lt;- lm(Y ~ X1 * X2, data_sample)\ntidy(mod_2)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.263     0.923    -0.285 7.78e- 1\n2 X1             2.13      0.287     7.42  6.98e- 8\n3 X2             2.58      0.341     7.58  4.76e- 8\n4 X1:X2          1.94      0.142    13.7   2.13e-13\n\n\n\n# interaction plot\n# -&gt; var1 = variable of interest\n# -&gt; var2 = conditioning variable\ninterplot::interplot(mod_2, var1 = \"X2\", var2 = \"X1\",\n                     stats_cp = \"ci\") + \n  labs(title = \"Significant interaction\",\n       x = \"X2\",\n       y = \"Estimated coefficient for X1\")",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression models for quantitative and qualitative predictors</span>"
    ]
  },
  {
    "objectID": "notes-reg-models-quan-and-qual.html#qualitative-predictors",
    "href": "notes-reg-models-quan-and-qual.html#qualitative-predictors",
    "title": "\n7  Regression models for quantitative and qualitative predictors\n",
    "section": "\n7.3 Qualitative predictors",
    "text": "7.3 Qualitative predictors\n\n7.3.1 Implementation of qualitative predictors\nAs mentioned in Section 5.2.1, qualitative, as well as quantitative, predictor variables can be used in regression models.\nA qualitative variable with \\(c\\) classes will be represented by \\(c - 1\\) indicator variables, each taking on the values 0 and 1.\n\nNeed one less predictor variable to avoid linear dependency with the intercept term (see book for explanation).\nAdditionally, we use indicator variables (rather than allocated codes (e.g. one variable with levels 1, 2, 3 for the three levels)), make no assumptions about the spacing of the classes and rely on the data to show the differential effects that occur.\n\nDemo\n\n# look at anova model (just to get the idea of how indicators are coded)\n\n# investigate data\n# -&gt; R dataset with two categorical variables (wool and tension) and one numeric response (breaks)\n# -&gt; confirm numeric and look at number of levels for each X\ndatasets::warpbreaks %&gt;% glimpse\n\nRows: 54\nColumns: 3\n$ breaks  &lt;dbl&gt; 26, 30, 54, 25, 70, 52, 51, 26, 67, 18, 21, 29, 17, 12, 18, 35…\n$ wool    &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,…\n$ tension &lt;fct&gt; L, L, L, L, L, L, L, L, L, M, M, M, M, M, M, M, M, M, H, H, H,…\n\nwarpbreaks %&gt;% map(class)\n\n$breaks\n[1] \"numeric\"\n\n$wool\n[1] \"factor\"\n\n$tension\n[1] \"factor\"\n\nwarpbreaks %&gt;% select(-1) %&gt;% map(levels)\n\n$wool\n[1] \"A\" \"B\"\n\n$tension\n[1] \"L\" \"M\" \"H\"\n\n# view replication\n# -&gt; it's a balanced design\nwarpbreaks %&gt;% select(-1) %&gt;% table\n\n    tension\nwool L M H\n   A 9 9 9\n   B 9 9 9\n\n# design matrix via lm()\n\n# specify model with only categorical predictor\nmod_cat &lt;- lm(breaks ~ wool, warpbreaks, x = TRUE)\ntidy(mod_cat)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    31.0       2.50     12.4  3.61e-17\n2 woolB          -5.78      3.54     -1.63 1.08e- 1\n\n# compare dataset to design matrix of indicator variables\n# -&gt; .$model this just gives the dataset where the variables are pulled from\ntmp &lt;- cbind(mod_cat$model, mod_cat$x[ , -1])\nhead(arrange(tmp, breaks), n = 10)\n\n   breaks wool mod_cat$x[, -1]\n1      10    A               0\n2      12    A               0\n3      13    B               1\n4      14    B               1\n5      15    A               0\n6      15    B               1\n7      15    B               1\n8      16    B               1\n9      16    B               1\n10     17    A               0\n\n# switch the default level\n# -&gt; just needs to be the first level\nmod_cat &lt;- warpbreaks %&gt;% \n  mutate(wool = factor(wool, levels = c(\"B\",\"A\"))) %&gt;% \n  {lm(breaks ~ wool, data = .)}\ntidy(mod_cat)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    25.3       2.50     10.1  7.37e-14\n2 woolA           5.78      3.54      1.63 1.08e- 1\n\n\n\n7.3.2 Interpretation of regression coefficients\nOne predictor – Two levels\n\nSuppose we have the regression model:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\\]\n\nwhere\n\n\\[\nX_{i1} = \\text{quantitative predictor};\\\\\nX_{i2} =\n  \\left\\{\n    \\begin{array}{ll}\n      0 & \\text{level 1}\\\\\n      1 & \\text{level 2}\n    \\end{array}\n  \\right.\n\\]\n\nThe response function for the model is\n\n\\[E(Y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\].\n\nTo understand the meaning of regression coefficients in this model, we have to look at the model for the different levels of the categorical predictor.\nSuppose we are looking at the model for level 1, where \\(X_2 = 0\\). The response function becomes:\n\n\\[E(Y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 (0) = \\beta_0 + \\beta_1 X_1 \\hspace{10pt} \\text{level 1}\\]\n\nThis response function is a straight line with \\(Y\\) intercept \\(\\beta_0\\) and slope \\(\\beta_1\\).\nFor a level 2 observation, \\(X_2 = 1\\). The response function becomes:\n\n\\[E(Y) = \\beta_0 + \\beta_1 X_1 + \\beta_2 (1) = (\\beta_0 + \\beta_2) + \\beta_1 X_1 \\hspace{10pt} \\text{level 2}\\]\n\nThis response function is a also straight line, with the same slope \\(\\beta_1\\), but with \\(Y\\) intercept \\(\\beta_0 + \\beta_1\\).\nThus \\(beta_2\\) measures the differential effect between level 2 and level 1,\n\n\\[\n\\begin{align*}\n\\beta_0 &= E(Y \\mid X_2 = 0)\\\\\n\\beta_2 &= E(Y \\mid X_2 = 1) - E(Y \\mid X_2 = 0)\n\\end{align*}\n\\]\n\nHere is a visual of the two response functions.\n\n\n\nIn general, \\(\\beta_1\\) shows how much higher (or lower) the mean response line is for the class coded 1 than the line for the class coded 0, for any given level of \\(X_1\\)\n\n\nOne predictor – More than two levels\n\nThe above logic and strategy can easily be extended to categorical predictors with more than 2 levels.\nSuppose we have the model:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_2 X_{i2} + \\epsilon_i\\]\n\\[\nX_{i1} = \\text{quantitative predictor};\n\\\\\nX_{i2} =\n  \\left\\{\n    \\begin{array}{ll}\n      1 & \\text{level 2}\\\\\n      0 & \\text{otherwise}\n    \\end{array}\n  \\right.;\n\\\\\nX_{i3} =\n  \\left\\{\n    \\begin{array}{ll}\n      1 & \\text{level 3}\\\\\n      0 & \\text{otherwise}\n    \\end{array}\n  \\right.\n\\]\n\nNow \\(\\beta_3 = E(Y \\mid X_2 = 0, X_3 = 1) - E(Y \\mid X_2 = 0, X_3 = 0)\\)\nWe may wish to estimate differential effects other than against the default zero level. This can be done by estimating differences between regression coefficients, say if there are 3 levels and we want to compare level 3 to level 2:\n\n\\[\\beta_3 - \\beta_2 = E(Y \\mid X_2 = 0, X_3 = 1) - E(Y \\mid X_2 = 1, X_3 = 0)\\]\n\nThis now measures the differential effect (how much higher or lower the response functioni is) of level 3 relative to level 2.\nThe point estimator of this quantity is, of course, \\(\\hat{\\beta}_3 - \\hat{\\beta}_2\\), and the estimated variance of this estimator is\n\n\\[s^2\\{\\hat{\\beta}_3 - \\hat{\\beta}_2\\} = s^2\\{\\hat{\\beta}_3\\} + s^2\\{\\hat{\\beta}_2\\} - 2 s\\{\\hat{\\beta}_3,\\hat{\\beta}_2\\}\\]\n\n(add variances and two times covariance, but minus). The needed variances and covariance can be readily ob’tained from the estimated variance-covariance matrix of the regression coefficients.\n\nDemo of design matrix intepretation\n\n# again, look at design matrix via lm() for more than two categories\n\n# specify model with only categorical predictor\nmod_cat2 &lt;- lm(breaks ~ tension, warpbreaks, x = TRUE)\ntidy(mod_cat2)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     36.4      2.80     13.0  8.30e-18\n2 tensionM       -10        3.96     -2.53 1.47e- 2\n3 tensionH       -14.7      3.96     -3.72 5.01e- 4\n\n# compare dataset to design matrix of indicator variables\n# -&gt; .$model this just gives the dataset where the variables are pulled from\ntmp2 &lt;- cbind(mod_cat2$model, mod_cat2$x[ , -1])\nhead(arrange(tmp2, breaks), n = 10)\n\n   breaks tension tensionM tensionH\n1      10       H        0        1\n2      12       M        1        0\n3      13       H        0        1\n4      14       L        0        0\n5      15       H        0        1\n6      15       H        0        1\n7      15       H        0        1\n8      16       M        1        0\n9      16       H        0        1\n10     17       M        1        0\n\n\nDemo of modeling with first order quantitative and qualitative predictors\n\n# now use data with continuous predictor\nmod_both &lt;- lm(Petal.Length ~ Petal.Width + Species, iris, x = TRUE)\n\n# plot three different response functions\n# -&gt; first get all points on there so zoom is correct\niris %$% plot(x = Petal.Width, y = Petal.Length, main = \"Constant slope, different intercepts\")\niris %&gt;% \n  filter(Species == \"setosa\") %$% \n  points(x = Petal.Width, y = Petal.Length, col = \"red\")\niris %&gt;% \n  filter(Species == \"versicolor\") %$% \n  points(x = Petal.Width, y = Petal.Length, col = \"green\")\niris %&gt;% \n  filter(Species == \"virginica\") %$% \n  points(x = Petal.Width, y = Petal.Length, col = \"blue\")\nabline(a = coef(mod_both)[1], b = coef(mod_both)[2], col = \"red\")\nabline(a = sum(coef(mod_both)[c(1,3)]), b = coef(mod_both)[2], col = \"green\")\nabline(a = sum(coef(mod_both)[c(1,4)]), b = coef(mod_both)[2], col = \"blue\")\n\n\n\n\n\n\n\nNow test on levels coefficients’ relative to other than the base level.\n\n# tests on coefficients (different intercepts)\ntidy(mod_both)\n\n# A tibble: 4 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           1.21    0.0652     18.6  2.88e-40\n2 Petal.Width           1.02    0.152       6.69 4.41e-10\n3 Speciesversicolor     1.70    0.181       9.38 1.17e-16\n4 Speciesvirginica      2.28    0.281       8.09 2.08e-13\n\n# both the latter two specied have different heights of there regression functions based on the t-tests of beta2 and beta3 (relative to beta1)\n\n# verify standard errors for t tests using matrix results\nX &lt;- mod_both$x\nvarcov_beta &lt;- glance(mod_both)$sigma^2 * solve(t(X) %*% X)\n\ncompare(mod_both %&gt;% tidy %&gt;% pull(std.error), varcov_beta %&gt;% diag %&gt;% sqrt)\n\n$comparison\n[1] TRUE\n\n$`mod_both %&gt;% tidy %&gt;% pull(std.error)`\n[1] 0.06524192 0.15224171 0.18094771 0.28132455\n\n$`varcov_beta %&gt;% diag %&gt;% sqrt`\n      (Intercept)       Petal.Width Speciesversicolor  Speciesvirginica \n       0.06524192        0.15224171        0.18094771        0.28132455 \n\n# test beta3 - beta2\nbeta3_beta2_hat &lt;- as.numeric(coef(mod_both)[4] - coef(mod_both)[3])\nbeta3_beta2_hat_se &lt;- sqrt(sum(diag(varcov_beta)[3:4]) - 2*varcov_beta[4,3])\nt_star &lt;- beta3_beta2_hat / beta3_beta2_hat_se\n\n# verify by changing the default level and checking t-stat\n# -&gt; need 2 to be the default\nmod_both2 &lt;- iris %&gt;% \n  mutate(Species = factor(Species, levels = c(\"versicolor\", \"setosa\", \"virginica\"))) %&gt;% \n  {lm(Petal.Length ~ Petal.Width + Species, .)}\n\ncompare(tidy(mod_both2) %&gt;% filter(term == \"Speciesvirginica\") %&gt;% pull(statistic), t_star)\n\n$comparison\n[1] TRUE\n\n$`tidy(mod_both2) %&gt;% filter(term == \"Speciesvirginica\") %&gt;% pull(statistic)`\n[1] 4.431538\n\n$t_star\n[1] 4.431538\n\n\nFitting one line instead of two\n\nFitting one line, as opposed to separate regression for each category, is the preferred method because of two reasons:\n\n\nSince the model assumes equal slopes and the same constant error term variance for each type of firm, the common slope \\(\\beta_1\\) can best be estimated by pooling the two levels.\nAlso, other inferences, such as for \\(\\beta_0\\) and \\(\\beta_2\\), can be made more precisely by working with one regression model containing an indicator variable since more degrees of freedom will then be associated with \\(MSE\\).\n\n\nMore is said about this strategy in Section 7.3.4.\n\n7.3.3 Modeling interactions between quantitative and qualitative predictors\nSetup\n\nSuppose we have the model below. Even though one of the predictor variables in the regression model is qualitative, interaction effects can still be introduced into the model in the usual manner, by including cross-product terms.\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} +\\epsilon_i\\]\n\nwhere\n\n\\[\nX_{i1} = \\text{quantitative predictor}\\\\\nX_{i2} =\n  \\left\\{\n    \\begin{array}{ll}\n      0 & \\text{level 1}\\\\\n      1 & \\text{level 2}\n    \\end{array}\n  \\right.\n\\]\n\nThe response function of the model is:\n\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2\\] Meaning of regression coefficients\n\nAgain, the meaning of the regression coefficients in response function above can best be understood by examining the nature of this function for each level.\nFor level 1, \\(X_2 = 0\\) and thus \\(X_1 X_2 = 0\\), so\n\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_1 + \\beta_2 (0) + \\beta_3 (0) = \\beta_0 + \\beta_1 X_1 \\hspace{20pt} \\text{level 1}\\]\n\nThis has intercept \\(\\beta_0\\) and slope \\(\\beta_1\\).\nFor level 2, \\(X_2 = 1\\) and thus \\(X_1 X_2 = X_1\\), so\n\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_1 + \\beta_2 (1) + \\beta_3 X_1 = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) X_1 \\hspace{20pt} \\text{level 2}\\]\n\nThis has intercept \\(\\beta_0 + \\beta_2\\) and slope \\(\\beta_1 + \\beta_3\\).\nWe see that \\(\\beta_2\\) here indicates how much greater (or smaller) is the \\(Y\\) intercept of the response function for the class coded 1 than that for the class coded O. Similarly, \\(\\beta_3\\) indicates how much greater (or smaller) is the slope of the response function for the class coded 1 than that for the class coded O.\nBecause both the intercept and the slope differ for the two classes in regression model, it is no longer true that \\(\\beta_2\\) indicates how much higher (or lower) one response function is than the other for any given level of \\(X_1\\).\nThus, when interaction effects are present, the effect of the qualitative predictor variable can be studied only by comparing the regression functions within the scope of the model for the different classes of the qualitative variable.\n\n\n\nWhen one of the predictor variables is qualitative and the other quantitative, nonparallel response functions that do not intersect within the scope of the model are sometimes said to represent an ordinal interaction. When the response functions intersect within the scope of the model, the interaction is then said to be a disordinal interaction.\n\nTesting interaction effects\n\nFitting the previous regression model yields the same response functions as would fitting separate regressions for level 1 and level 2. An advantage of using this model with an indicator variable is that one regression run will yield both fitted regressions.\n\nAnother advantage is that tests for comparing the regression functions for the different classes of the qualitative variable can be clearly seen to involve tests of regression coefficients in a general linear model.\n\nFor example we can do two tests: one on just the slopes and another of if the entire regression lines are identical.\n\n\n\n\\[\n\\begin{align*}\n  \\text{Test on slopes}\\\\\n  H_0&: \\beta_3 = 0\\\\\n  H_A&: \\beta_3 \\ne 0\\\\\n  \\text{Test on entire regression line}\\\\\n  H_0&: \\beta_2 = \\beta_3 = 0\\\\\n  H_A&: \\text{not both } \\beta_2 = 0 \\text{ and } \\beta_3 = 0\n\\end{align*}\n\\]\nDemo of design matrix interpretation and how constructed with interaction effects\n\n# specify each term individual -&gt; A + B + AB\nmod_cat3 &lt;- lm(breaks ~ wool + tension + wool : tension, data = warpbreaks, x = TRUE)\n\n# equivalent shorthand notation to cross factors\nmod_cat3 &lt;- lm(breaks ~ wool * tension, data = warpbreaks, x = TRUE)\n\n# extract design matrix\nX_lm &lt;- mod_cat3$x\n\n# check how to read indicators (ignoring replication)\nwarpbreaks %&gt;% select(-1) %&gt;% bind_cols(X_lm) %&gt;% unique\n\n   wool tension (Intercept) woolB tensionM tensionH woolB:tensionM\n1     A       L           1     0        0        0              0\n10    A       M           1     0        1        0              0\n19    A       H           1     0        0        1              0\n28    B       L           1     1        0        0              0\n37    B       M           1     1        1        0              1\n46    B       H           1     1        0        1              0\n   woolB:tensionH\n1               0\n10              0\n19              0\n28              0\n37              0\n46              1\n\n# -&gt; R has levels - 1 indicators for each categorical variable\n# -&gt; the \"dropped\" base level also isn't explicitly included in any interaction indicator terms\n# --&gt; so interactions are taken into account with zeros for actual interactions terms and 0 or 1 for the second factor\n\n\nAll higher order terms involving the base level are also dropped.\n\nDemo to visualize model with interactions\n\n# now use data with continuous predictor\nmod_both_int &lt;- lm(Petal.Length ~ Petal.Width * Species, iris)\ncoef(mod_both_int)\n\n                  (Intercept)                   Petal.Width \n                    1.3275634                     0.5464903 \n            Speciesversicolor              Speciesvirginica \n                    0.4537120                     2.9130892 \nPetal.Width:Speciesversicolor  Petal.Width:Speciesvirginica \n                    1.3228344                     0.1007691 \n\n# plot three different response functions\n# -&gt; first get all points on there so zoom is correct\niris %$% plot(x = Petal.Width, y = Petal.Length, main = \"Different regression lines (different intercepts and slopes)\")\niris %&gt;% \n  filter(Species == \"setosa\") %$% \n  points(x = Petal.Width, y = Petal.Length, col = \"red\")\niris %&gt;% \n  filter(Species == \"versicolor\") %$% \n  points(x = Petal.Width, y = Petal.Length, col = \"green\")\niris %&gt;% \n  filter(Species == \"virginica\") %$% \n  points(x = Petal.Width, y = Petal.Length, col = \"blue\")\nabline(a = coef(mod_both_int)[1], b = coef(mod_both_int)[2], col = \"red\")\nabline(a = sum(coef(mod_both_int)[c(1,3)]), b = sum(coef(mod_both_int)[c(2,5)]), col = \"green\")\nabline(a = sum(coef(mod_both_int)[c(1,4)]), b = sum(coef(mod_both_int)[c(2,6)]), col = \"blue\")\n\n\n\n\n\n\n# compare to ggplot\nggplot(data = iris,\n       aes(x = Petal.Width,\n           y = Petal.Length)) + \n  geom_point(aes(color = Species)) + \n  geom_smooth(aes(color = Species),\n              se = FALSE,\n              method = \"lm\",\n              fullrange = TRUE)\n\n\n\n\n\n\n\n\n# test if only slopes are different\nmod_full &lt;- mod_both_int\nmod_reduced &lt;- mod_both\nanova(mod_full, mod_reduced)\n\nAnalysis of Variance Table\n\nModel 1: Petal.Length ~ Petal.Width * Species\nModel 2: Petal.Length ~ Petal.Width + Species\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    144 18.816                                  \n2    146 20.833 -2   -2.0178 7.7213 0.0006525 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# test if entire regression lines are different\nmod_full &lt;- mod_both_int\nmod_reduced &lt;- update(mod_both, . ~ . -Species)\nanova(mod_full, mod_reduced)\n\nAnalysis of Variance Table\n\nModel 1: Petal.Length ~ Petal.Width * Species\nModel 2: Petal.Length ~ Petal.Width\n  Res.Df    RSS Df Sum of Sq      F                Pr(&gt;F)    \n1    144 18.816                                              \n2    148 33.845 -4   -15.029 28.755 &lt; 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n7.3.4 Miscellaneous topics with qualitative predictors\nQualitative predictors only\n\n\nRegression models containing only qualitative predictor variables can also be constructed.\n\nModels in which all explanatory variables are qualitative are called analysis of variance models (ANOVA).\nModels containing some quantitative and some qualitative explanatory variables, where the chief explanatory variables of interest are qualitative and the quantitative variables are introduced primarily to reduce the variance of the error terms, are called analysis of covariance models (ANCOVA).\n\n\n\nIndicator variables vs quantitative variables\n\n\nIndicator variables can be used even if the predictor variable is quantitative.\n\nFor instance, the quantitative variable age may be transformed by grouping ages into classes such as under 21, 21-34, 35-49, etc. Indicator variables are then used for the classes of this new predictor variable.\nThis is called discretizing the variable.\n\n\nAt first sight, this may seem to be a questionable approach because information about the actual ages is thrown away. Furthermore, additional parameters are placed into the model, which leads to a reduction of the degrees of freedom associated with \\(MSE\\). Nevertheless, there are occasions when replacement of a quantitative variable by indicator variables may be appropriate.\n\nFor large data sets, use of indicator variables can serve as an alternative to lowess and other nonparametric fits of the response function.\n\nCost: For say 1000 observations, the loss of 10 or 20 degrees of freedom is immaterial.\nGain: If you are very much in doubt about the shape of the regression function, which could be highly complex, you could utilize the indicator variable approach in order to obtain information about the shape ofthe response function without making any assumptions about its functional form (not assuming linear or quadratic for example).\n\n\n\nDemo\n\n# plot data\ndiamonds %$% plot(x = carat, y = price)\n\n# fit linear model to see\nmod_linear &lt;- lm(price ~ carat, diamonds)\nabline(mod_linear, col = \"red\")\n\n\n\n\n\n\n# think a more complex function would be better\n\n# discretize x variable and fit model with indicator variables\nmod_discretized &lt;- lm(diamonds$price ~ as.factor(cut_number(diamonds$carat, n = 10)))\nmod_discretized %&gt;% tidy\n\n# A tibble: 10 × 5\n   term                                   estimate std.error statistic   p.value\n   &lt;chr&gt;                                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                               655.       20.3     32.2  5.55e-226\n 2 as.factor(cut_number(diamonds$carat, …     95.4      31.5      3.03 2.44e-  3\n 3 as.factor(cut_number(diamonds$carat, …    267.       30.1      8.88 6.59e- 19\n 4 as.factor(cut_number(diamonds$carat, …    831.       30.6     27.2  1.70e-161\n 5 as.factor(cut_number(diamonds$carat, …   1435.       29.8     48.1  0        \n 6 as.factor(cut_number(diamonds$carat, …   2441.       28.8     84.9  0        \n 7 as.factor(cut_number(diamonds$carat, …   4384.       30.6    143.   0        \n 8 as.factor(cut_number(diamonds$carat, …   5170.       31.6    164.   0        \n 9 as.factor(cut_number(diamonds$carat, …   7289.       29.2    250.   0        \n10 as.factor(cut_number(diamonds$carat, …  12281.       31.4    391.   0        \n\n# fully discretize model\nmod_full_discretized &lt;- lm(price ~ as.factor(diamonds$carat), diamonds)\nmod_full_discretized %&gt;% tidy %&gt;% nrow\n\n[1] 273\n\n# add complex models to plot for comparison\ndiamonds %$% plot(x = carat, y = price)\nabline(mod_linear, col = \"red\")\ndiamonds %&gt;% \n  augment(x = mod_discretized, data = .) %&gt;% \n  arrange(carat) %$% \n  lines(x = carat, y = .fitted, col = \"blue\") # step function because x values in between cutoffs with same predicted y value\ndiamonds %$% lines(lowess(x = carat, y = price), col = \"green\")\ndiamonds %&gt;% \n  augment(x = mod_full_discretized, data = .) %&gt;% \n  arrange(carat) %$% \n  lines(x = carat, y = .fitted, col = \"yellow\") # when no replication, gets predicted exactly -&gt; but this is still a smooth curve because covers each unique value\n\n\n\n\n\n\n\nComparison of two or more regression functions\n\nTo formally test if these two regression lines are different (e.g. if data come from two different populations, such as males vs females), we can use indicator variables and partial \\(F\\)-tests.\n\nWe simply consider the different populations as classes of a predictor variable, define indicator variables for the different populations, and develop a regression model containing appropriate interaction terms\n\nIf just testing the heights, then just need additive indicator variables for the qualitative predictor.\nIf also want to consider different slopes (i.e. not assume a common slope among the two populations), then include interaction terms for the continuous predictor(s) and the indicator variable(s).\n\n\n\nAll of the above scenarios require assume that the populations have equal error term variances.\n\nIf the error vanances are not equal, transformations of the response variable may equalize them at least approximately.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression models for quantitative and qualitative predictors</span>"
    ]
  },
  {
    "objectID": "notes-building-reg-model-1.html#overview-of-the-model-building-process",
    "href": "notes-building-reg-model-1.html#overview-of-the-model-building-process",
    "title": "\n8  Building the regression model 1 – Model selection and validation\n",
    "section": "",
    "text": "Data collection and preparation\nReduction of explanatory or predictor variables (for exploratory observational studies)\nModel refinement and selection\nModel validation\n\n\n\n\n8.1.1 (1) Data collection\n\nThe data collection requirements for building a regression model vary with the nature of the study. It is useful to distinguish four types of studies.\n\nControlled experiments\n\n\nIn a controlled experiment, the experimenter controls the levels of the explanatory variables and assigns treatment, consisting of a combination of levels of the explanatory to each experimental unit and observes the response.\n\nIn controlled experiments, the explanatory variables are often called factors or control variables.\n\n\n\nThe data collection requirements for controlled experiments are straightforward, though not necessarily simple. Observations for each experimental unit are needed on the response variable and on the level of each of the control variables used for that experimental unit.\n\nThere may be difficult measurement and scaling problems for the response variable that are unique to the area of application.\n\n\n\nControlled experiments with covariates\n\nStatistical design of experiments uses supplemental information, such as characteristics of the experimental units, in designing the experiment so as to reduce the variance of the experimental error terms in the regression model.\n\nSometimes, however, it is not possible to incorporate this supplemental information into the design of the experiment. Instead, it may be possible for the experimenter to incorporate this information into the regression model and thereby reduce the error variance by including uncontrolled variables or covariates in the model.\n\nUse of covariates in the regression model make the analysis of the effects of the explanatory variables on the accuracy response more precise.\n\n\n\nConfirmatory observational studies\n\nThese studies, based on observational, not experimental, data, are intended to test (i.e., to confirm or not to confirm) hypotheses derived from previous studies or from hunches.\n\nFor these studies, data are collected for explanatory variables that previous studies have shown to affect the response variable, as well as for the new variable or variables involved in the hypothesis.\n\nIn this context, the explanatory variable(s) involved in the hypothesis are sometimes called the primary variables, and the explanatory variables that are included to reflect existing knowledge are called the control variables (known risk factors in epidemiology).\nThe control variables here are not controlled as in an experimental study, but they are used to account for known influences on the response variable.\n\n\n\nData collection for confirmatory observational studies involves obtaining observations on the response variable, the control variables, and the primary explanatory variable(s).\n\nHere, as in controlled experiments, there may be important and complex problems of measurement.\n\n\n\nExplanatory observational studies\n\nIn the social, behavioral, and health sciences, management, and other fields, it is often not possible to conduct controlled experiments. Furthermore, adequate knowledge for conducting confirmatory observational studies may be lacking.\nAs a result, many studies in these fields are exploratory observational studies where investigators search for explanatory variables that might be related to the response variable.\nTo complicate matters further, any available theoretical models may involve explanatory variables that are not directly measurable, such as a family’s future earnings ov the next 10 years.\nUnder these conditions, investigators are often forced to prospect for explanatory variables that could conceivably be related to the response variable under study. Obviously, such a set of potentially useful explanatory variables can be large.\n\nAfter a lengthy list of potentially useful explanatory variables has been compiled, some of these variables can be quickly screened out. An explanatory variable (1) may not be fundamental to the problem, (2) may be subject to large measurement errors, and/or (3) may effectively duplicate another explanatory variable in the list.\n\nExplanatory variables that cannot be measured may either be deleted or replaced by proxy variables that are highly correlated with them.\n\n\n\nThe number of cases to be collected for an exploratory observational regression study depends on the size of the pool of potentially useful explanatory variables available at this stage.\n\nMore cases are required when the pool is large than when it is small.\nA general rule of thumb states that there should be at least 6 to 10 cases for every variable in the pool.\n\n\nThe actual data collection for the pool of potentially useful explanatory variables and for the response variable again may involve important issues of measurement, just as for the other types of studies.\n\n8.1.2 (1) Data preparation\n\nOnce the data have been collected, edit checks should be performed and plots prepared to identify gross data errors as well as extreme outliers.\nDifficulties with data errors are especially prevalent in large data sets and should be corrected or resolved before the model building begins.\nWhenever possible, the investigator should carefully monitor and control the data collection process to reduce the likelihood of data errors.\n\n8.1.3 (2) Preliminary model investigation\n\nOnce the data have been properly edited, the formal modeling process can begin.\nA variety of diagnostics should be employed to identify (1) the functional forms in which the explanatory variables should enter the regression model and (2) important interactions that should be included in the model.\nScatter plots and residual plots are useful for determining relationships and their strengths.\nSelected explanatory variables can be fitted in regression functions to explore relationships, possible strong interactions and the need for transformations.\n\nWhenever possible, of course, one should also rely on the investigator’s prior knowledge and expertise to suggest appropriate transformations and interactions to investigate. This is particularly important when the number of potentially useful explanatory variables is large.\n\nIn this case, it may be very difficult to investigate all possible pairwise interactions, and prior knowledge should be used to identify the important ones.\n\n\n\n8.1.4 (2) Reduction of explanatory variables\n\nAgain this process looks different for the different types of studies.\n\nControlled experiments\n\nThe reduction of explanatory variables in the model-building phase is usually not an important issue for controlled experiments.\nThe experimenter has chosen the explanatory variables for investigation, and a regression model is to be developed that will enable the investigator to study the effects’of these variables on the response variable.\nAfter the model has been developed, including the use of appropriate functional forms for the variables and the inclusion of important interaction terms, the inferential procedures considered in previous chapters will be used to determine whether the explanatory variables have effects on the response variable and, if so, the nature and magnitude of the effects.\n\nControlled experiments can usually avoid many of the problems in exploratory observational studies discussed below.\n\nFor example, the effects of latent predictor variables are minimized by using randomization.\n\nIn addition, adequate ranges of the explanatory variables can be selected and correlations among the explanatory variables can be eliminated by appropriate choices of their levels.\n\n\nControlled experiments with covariates\n\nIn studies of controlled experiments with covariates, some reduction of the covariates may take place investigators often cannot be sure in advance that the selected covariates will be helpful in reducing the error variance.\nThe number of covariates considered in controlled experiments is usually small, so no special problems are encountered in determining whether some or all of the covariates should be dropped from the regression model.\n\nConfirmatory observational studies\n\nGenerally, no reduction of explanatory variables should take place in confirmatory observational studies.\nThe control variables were chosen on the basis of prior knowledge and should be retained for comparison with earlier studies even if some of the control variables tum out not to lead to any error variance reduction in the study at hand.\nThe primary variables are the ones whose influence on the response variable is to be examined and therefore need to be present in the model.\n\nExplanatory observational studies\n\nIn exploratory observational studies, the number of explanatory variables that remain after the initial screening typically is still large.\n\nFurther, many of these variables frequently will be highly intercorrelated. Hence, the investigator usually will wish to reduce the number of explanatory variables to be used in the final model. There are several reasons for this.\n\nA regression model with numerous explanatory variables may be difficult to maintain.\nFurther, regression models with a limited number of explanatory variables are easier to work with and understand.\nFinally, the presence of many highly intercorrelated explanatory variables may substantially increase the sampling variation of the regression coefficients, detract from the model’s descriptive abilities (falsely inflated \\(R^2\\)), and not improve, or even worsen, the model’s predictive ability.\nAn actual worsening of the model’s predictive ability can occur when explanatory variables are kept in the regression model that are not related to the response variable, given the other explanatory variables in the model (overfitting). In that case, the variances of the fitted values \\(\\sigma^2\\{\\hat{Y}_i\\}\\) tend to become larger with the inclusion of the useless additional explanatory variables.\n(Note that the multicollinearity effects were all confirmed with the simulation in Section 6.6.6)\n\n\n\nOnce the investigator has tentatively decided upon the functional form of the regression relations (whether given variables are to appear in linear form, quadratic form, etc.) and whether any interaction terms are to be included, the next step in many exploratory observational studies is to identify a few “good” subsets of \\(X\\) variables for further intensive study.\n\nThese subsets should include not only the potential explanatory variables first-order form but also any needed quadratic and other curvature terms and any necessary interaction terms.\n\n\n\nThe identification of “good” subsets of potentially useful explanatory variables to be included in the final regression model and the determination of appropriate functional and interaction relations for these variables usually constitute some of the most difficult problems in regression analysis.\n\nSince the uses of regression models vary, no one subset of explanatory vruiables may always be “best”.\nFor instance, a descriptive use of a regression model typically will emphasize precise estimation of the regression coefficients, whereas a predictive use will focus on the prediction errors.\nOften, different subsets of the pool of potential explanatory variables will best serve these varying purposes.\nEven for a given purpose, it is often found that several subsets are about equally “good” according to a given criterion, and the choice among these “good” subsets needs to be made on the basis of additional considerations.\n\n\n\nThe choice of a few appropriate subsets of explanatory variables for final consideration in exploratory observational studies needs to be done with great care. Elimination of key explanatory variables can seriously damage the explanatory power of the model and lead to biased estimates of regression coefficients, mean responses, and predictions of new observations, as well as biased estimates of the error variance.\n\nThe bias in these estimates is related to the fact that with observational data, the error terms in an underfitted regression model may reflect nonrandom effects of the explanatory variables not incorporated in the regression model. Important omitted explanatory variables are sometimes called latent explanatory variables.\nOn the other hand, if too many explanatory variables are included in the subset, then this overfitted model will often result in variances of estimated parameters that are larger than those for simpler models.\nAnother danger with observational data is that important explanatory variables may be observed only over narrow ranges. As a result, such important explanatory variables may be omitted just because they occur in the sample within a narrow range of values and therefore turn out to be statistically nonsignificant.\n\n\n\nA number of algorithms have been developed to help with variable selection.\n\nBut the process of developing a useful regression model must be pragmatic and needs to utilize large doses of subjective judgment.\nExplanatory variables that are considered essential should be included in the regression model before any automation is sought.\nFurther, algorithms that identify only a single subset of explanatory variables as “best” need to be supplemented so that additional subsets are also considered before the final regression mopel is decided upon.\n\n\n\nWRONG APPROACH\n\n\nFit full model and drop all unsignificant coefficients (according to \\(t\\)-test).\n\nReason for bad: This procedure can lead to the dropping of important intercorrelated explanatory variables (easier to fail to reject with high degree of multicollinearity).\nSo a good procedure needs to be able to take into account multicollinearity (i.e. not drop all of them).\n\n\n\n8.1.5 (3) Model refinement and selection\n\nAt this stage in the model-building process, the tentative regression model, or the several “good” regression models in the case of exploratory observational studies, need to checked in detail for curvature and interaction effects (this is the second check of higher order terms, one was done preliminarily in (2) Reduction of explanatory variables).\nResidual plots are helpful in deciding whether one model is to be preferred over another. In addition, the diagnostic checks to be described in the next chapter are useful for identifying influential outlying observations, multicollinearity, etc.\nThe selection of the ultimate regression model often depends greatly upon these diagnostic results. For example, one fitted model may be very much influenced by a single case, whereas another is not. Again, one fitted model may show correlations among the error terms, whereas another does not.\n\nWhen repeat observations are available, formal tests for lack of fit can be made. In any case, a variety of residual plots and analyses can be employed to identify any lack of fit, outliers, and influential observations.\n\nFor instance, residual plots against cross-product and/or power terms not included in the regression model can be useful in identifying ways in which the model fit can be improved further.\n\n\n\nWhen an automatic selection procedure is utilized for an exploratory observational study and only a single model is identified as “best,” other models should also be explored.\n\nOne procedure is to use the number of explanatory variables in the model identified as “best” as an estimate of the number of explanatory variables needed in the regression model.\nThen the investigator explores and identifies other candidate models with approximately the same number of explanatory variables identified by the automatic procedure.\n\n\nEventually, after thorough checking and various remedial actions, such as transformations, the investigator narrows the number of competing models to one or just a few.\nAt this point, it is good statistical practice to assess the validity of the remaining candidates through model validation studies. These methods can be used to help decide upon a final regression model, and to determine how well the model will perform in practice.\n\n8.1.6 (4) Model validation\n\nModel validity refers to the stability and reasonableness of the regression coefficients, the plausibility and usability of the regression function, and the ability to generalize inferences drawn from the regression analysis.\nValidation is a useful and necessary pan of the model-building process. Several methods of assessing model validity will be described at the end of this chapter.\n\n8.1.7 Overfitting simulation\n\n# overfitting simulation\n\n# simulation for this???\n\n\n8.1.8 Demo\n\nModeling process: Start with first order model with all predictors and assess diagnostics for intial problems\n\n\n# load data\ndata_surgery &lt;- ALSM::SurgicalUnit %&gt;% \n  select(-lny)\n\n# start with first order model with all predictors and assess diagnostics\nmod_start &lt;- lm(y ~ ., data_surgery)\nplot(mod_start, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee curvature and non constant variance and some slight issues with normality.\nTry linearizing transformation and fix the non constant variance with \\(Y' = \\ln(Y)\\).\n\n\n# fit model with transformed response\nmod_start_prime &lt;- lm(log(y) ~ ., data_surgery)\nplot(mod_start_prime, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransformation fixed most issues, now can continue to investigate the appropriateness of first-order additive terms.\n\n\n# correlation matrix and scatterplot matrix to assess strength of linear relationships (on the lookout for curvature)\n# -&gt; extra steps to get new response variable in there\ncorr &lt;- data_surgery %&gt;% \n  mutate(lny = log(y)) %&gt;% \n  select(-y) %&gt;% \n  cor %&gt;%\n  round(3)\ncorrplot::corrplot(corr)\n\n\n\n\n\n\ndata_surgery %&gt;% \n  mutate(lny = log(y)) %&gt;% \n  select(-c(x6, x7, x8, y)) %&gt;% \n  pairs\n\n\n\n\n\n\n\n\nNo apparent curvature for \\(X_k\\) with the response \\(\\ln(Y)\\).\nStrong linear associations with the response (\\(X_3, X_4\\) are the highest).\nMay be some multicollinearity (\\(X_4\\) is correlated with \\(X_1, X_2, X_3\\)).\n\n\n# check for significant interaction effects by plotting residuals against all possible interactions\n\n# fit fully crossed interaction model to extract design matrix\nmod_start_prime_crossed &lt;- lm(log(y) ~ .^2, data_surgery, x = TRUE)\n\n# get residuals of original model\ne &lt;- residuals(mod_start_prime)\n\n# extract interaction terms\n# -&gt; interaction symbol : gets recoded as .\nX_int &lt;- mod_start_prime_crossed$x %&gt;% \n  data.frame %&gt;% \n  select(contains(\".\"), -contains(\"Intercept\"))\n\n\n# plot residuals against all interaction terms\nnms_x_int &lt;- colnames(X_int)\nmap2(X_int, nms_x_int, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n# no obvious visual signs of significant interactions -&gt; maybe x3 and x4\n# -&gt; following textbook and ignoring these",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building the regression model 1 -- Model selection and validation</span>"
    ]
  },
  {
    "objectID": "notes-building-reg-model-1.html#criteria-for-model-selection",
    "href": "notes-building-reg-model-1.html#criteria-for-model-selection",
    "title": "\n8  Building the regression model 1 – Model selection and validation\n",
    "section": "\n8.2 Criteria for model selection",
    "text": "8.2 Criteria for model selection\n\n\nFrom any set of \\(p - 1\\) predictors, \\(2^{p-1}\\) alternative models can be constructed (this comes from the fact that each predictor can either be included or exlcuded from the model).\n\nFor example if \\(p = 3\\), then there are two predictors + intercept.\nCan have intercept only model (\\(Y_i = \\beta_0 + \\epsilon_i\\)), a one variable model (\\(Y_i = \\beta_0 + \\beta_1 X_1 + \\epsilon_i\\) or \\(Y_i = \\beta_0 + \\beta_1 X_2 + \\epsilon_i\\)) and the only two predictor model (\\(Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon_i\\)) for a total of \\(2^{3-1} = 4\\) models.\n\n\nThe number of possible models increases quickly, and will be impossible to examine every model in depth. Model selection procedures, also known as subset selection or variables selection procedures, have been developed to identify a small group of regression models that are “good” according to a specified criterion.\nA detailed examination can then be made of a limited number of the more promising or “candidate” models, leading to the selection of the final regression model to be employed. This limited number might consist of three to six “good” subsets according to the criteria specified, so the investigator can then carefully study these regression models for choosing the final model.\n\n\n8.2.1 \\(R^2_p\\) or \\(SSE_p\\) criterion\n\nWant to identify subset in which the coefficient of multiple determination \\(R^2\\) is high, or equivalently \\(SSE\\) is low (both of which are indexed by how many parameters are in the model = one less than the number of the predictors because of the intercept).\n\n\\[R^2_p = 1 - \\frac{SSE_p}{SSTO}\\]\n\nSince the denominator is constant for all models (just used \\(\\bar{Y}\\)), \\(R^2_p\\) and \\(SSE_p\\) vary inversely.\n\nNote that the \\(R^2_p\\) criterion is not intended to identify the subsets that maximize this criterion.\n\nWe know that \\(R^2_p\\) can never decrease as additional \\(X\\) variables are included in the model. Hence, \\(R^2_p\\) will be a maximum when all \\(P - 1\\) (\\(P\\) = total number of available predictors) potential \\(X\\) variables are included in the regression model.\nDimininshing return: The intent in using the \\(R^2_p\\) criterion is to find the point where adding more \\(X\\) variables is not worthwhile because it leads to a very small increase in \\(R^2_p\\).\n\n\n\n\n# pick random model to calculate each of the following statistics for (so can compare to book as well)\nmod_example &lt;- lm(log(y) ~ x1 + x2 + x3, data_surgery)\n\n# p\n# -&gt; including intercept\nmod_example %&gt;% coef %&gt;% length\n\n[1] 4\n\n# extract R^2_p and SSE_p\nmod_example %&gt;% glance %&gt;% pull(r.squared)\n\n[1] 0.7572331\n\nmod_example %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"Residuals\") %&gt;% pull(sumsq)\n\n[1] 3.10851\n\n# match book!\n\n# more ways\naugment(mod_example, data = data.frame(lny = log(data_surgery$y))) %&gt;% \n  yardstick::rsq_trad(data = ., truth = \"lny\", estimate = \".fitted\")\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 rsq_trad standard       0.757\n\nyardstick::rsq_trad_vec(truth = log(data_surgery$y),\n                        estimate = predict(mod_example))\n\n[1] 0.7572331\n\ncompare(mod_example %&gt;% glance %&gt;% pull(r.squared),\n        yardstick::rsq_trad_vec(truth = log(data_surgery$y),\n                        estimate = predict(mod_example)))\n\n$comparison\n[1] TRUE\n\n$`mod_example %&gt;% glance %&gt;% pull(r.squared)`\n[1] 0.7572331\n\n$`yardstick::rsq_trad_vec(truth = log(data_surgery$y), estimate = predict(mod_example))`\n[1] 0.7572331\n\n\n\n8.2.2 \\(R^2_{a,p}\\) or \\(MSE_p\\) criterion\n\nCan use \\(R^2_{adj} = R^2_{a,p} \\text{ (in this context)}\\) to take into account the number of parameters in the model through the degrees of freedom. We know that it can decrease as the number of parameters increases if the decrease in \\(MSE\\) isn’t enough to offset the loss of degrees of freedom.\n\n\\[R^2_{a,p} = 1 - \\Big(\\frac{n - 1}{n - p}\\Big) \\frac{SSE_p}{SSTO} = 1 - \\frac{MSE_p}{SSTO / (n - 1)}\\]\n\nThis only increases if \\(MSE_p\\) decreases since \\(SSTO / (n - 1)\\) is fixed for the given \\(Y\\) observations. Thus, \\(R^2_{a,p}\\) and \\(MSE_p\\) provide equivalent information.\n\nDemo\n\n# extract R^2_a.p and MSE_p\nmod_example %&gt;% glance %&gt;% pull(adj.r.squared)\n\n[1] 0.7426671\n\nmod_example %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2)\n\n[1] 0.06217021\n\n# match book!\n\n\n8.2.3 Mallow’s \\(C_p\\) criterion\nOverview\n\nThis criterion is concerned with the total mean squared error of the \\(n\\) fitted values for each subset regression model.\n\n\\[C_p = \\frac{SSE_p}{MSE(X_1, \\ldots, X_{P-1})} + (2p - n)\\]\nSpecifics\nInterpreting Mallow’s \\(C_p\\)\n\nGood models: When there is no bias in the regression model with \\(p - 1\\) \\(X\\) variables, then\n\n\\[E(C_p) \\approx p \\hspace{20pt} \\text{when }E(\\hat{Y}_i) \\approx \\mu_i\\]\n\nIf there are several good models, take the more parsimonious model.\nAcceptable values: \\(C_p &lt;\\approx p\\)\n\nInterpreting Mallow’s \\(C_p\\) plot\n\n\nWhen the \\(C_p\\) values for all possible regression models are plotted against \\(p\\), those models with little bias will tend to fall near the line \\(C_p = p\\).\n\nModels with substantial bias will tend to fall considerably above this line.\n\\(C_p\\) values below the line \\(C_p = p\\) are interpreted as showing no bias, being below the line due to sampling error.\n\n\n\nNotes\n\nEffective use of the \\(C_p\\) criterion requires careful development of the pool of \\(P-1\\) potential \\(X\\) variables, with the predictor variables expressed in appropriate form (linear, quadratic, transformed), and important interactions included, so that \\(MSE(X_1,\\ldots,X_{p-1})\\) provides an unbiased estimate of the error variance \\(\\sigma^2\\).\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# calculate mallow's Cp\n# -&gt; model is the candidate model and fullmodel has P - 1 predictors\nmod_example_full &lt;- lm(log(y) ~ x1 + x2 + x3 + x4, data_surgery)\nolsrr::ols_mallows_cp(model = mod_example, fullmodel = mod_example_full)\n\n[1] 3.387945\n\n# matches book!\n\n\n\n\n# Mallows Cp = SSE_candidate / MSE_full + (2p - n)\n# -&gt; calculate needed values\nSSE_p &lt;- mod_example %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"Residuals\") %&gt;% pull(sumsq)\nMSE_full &lt;- mod_example_full %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2)\np &lt;- mod_example %&gt;% coef %&gt;% length\nn &lt;- nobs(mod_example_full)\nmallows_cp &lt;- SSE_p / MSE_full + 2*p - n\n\ncompare(olsrr::ols_mallows_cp(model = mod_example, fullmodel = mod_example_full),\n        mallows_cp)\n\n$comparison\n[1] TRUE\n\n$`olsrr::ols_mallows_cp(model = mod_example, fullmodel = mod_example_full)`\n[1] 3.387945\n\n$mallows_cp\n[1] 3.387945\n\n# very close to book, going with roundoff error\n\n\n\n\n\n8.2.4 \\(AIC_p\\) and \\(BIC_p\\) criterion\n\nThese two criterion, in addition to \\(R^2_{a,p}\\) and \\(C_p\\), penalize models having large number of predictors.\n\\(AIC\\) = Akaike’s information criteria and \\(BIC\\) = Bayesian information criteria.\nGoal: Find models with small values of each:\n\n\\[\n\\begin{align*}\n  AIC &= n \\ln(SSE_p) - n \\ln(n) + 2p\\\\\n  BIC &= n \\ln(SSE_p) - n \\ln(n) + p\\ln(n)\n\\end{align*}\n\\]\n\nNotice the first terms \\(n \\ln(SSE_p)\\) decrease as \\(p\\) increases, the second term \\(n \\ln(n)\\) is fixed (for a given sample size of \\(n\\)), and the last term (the penalty term) increases as \\(p\\) increases.\nModels with small \\(SSE_p\\) do well with this criteria as long as the penalties aren’ too large. And for \\(n \\ge 8\\), the penalty for \\(BIC_p\\) is larger \\(\\Longrightarrow\\) \\(BIC_p\\) favors more parsimonious models.\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# different ways to calculate these statistics\n# -&gt; (taking into account something vs not taking into account something)\n# -&gt; https://stats.stackexchange.com/questions/43733/what-is-the-difference-between-aic-and-extractaic-in-r\n\n# one way\n\n# calculate AIC_p and BIC_p\nmod_example %&gt;% glance %&gt;% pull(AIC)\n\n[1] 9.08398\n\nmod_example %&gt;% AIC\n\n[1] 9.08398\n\nmod_example %&gt;% glance %&gt;% pull(BIC)\n\n[1] 19.0289\n\nmod_example %&gt;% BIC\n\n[1] 19.0289\n\n# does not match matches book... \n\n# another way\nextractAIC(mod_example, k = 2) # AIC used default k = 2\n\n[1]    4.0000 -146.1614\n\nmod_example %&gt;% extractAIC(k = log(nobs(.))) # BIC -&gt; just have a different k value = ln(n)\n\n[1]    4.0000 -138.2054\n\n# match book!\n\n# be careful comparing, need to compare like to like\n# -&gt; from now on always going to use the not book way because it has better compatibility across lm() and glm()\n\n\n\n\n# calculate needed items\nSSE_p &lt;- mod_example %&gt;% anova %&gt;% tidy %&gt;% filter(term == \"Residuals\") %&gt;% pull(sumsq)\np &lt;- mod_example %&gt;% coef %&gt;% length\nn &lt;- nobs(mod_example_full)\n\n# AIC = n ln(SSE_p) - n ln(n) + 2p\nAIC_p &lt;- n * log(SSE_p) - n * log(n) + 2*p\n\n# BIC = n ln(n) SSE_p - n ln(n) + p ln(n)\nBIC_p &lt;- n * log(SSE_p)- n * log(n) + p * log(n)\n\ncompare(extractAIC(mod_example)[2], AIC_p)\n\n$comparison\n[1] TRUE\n\n$`extractAIC(mod_example)[2]`\n[1] -146.1614\n\n$AIC_p\n[1] -146.1614\n\ncompare(extractAIC(mod_example, k = log(nobs(mod_example)))[2], BIC_p)\n\n$comparison\n[1] TRUE\n\n$`extractAIC(mod_example, k = log(nobs(mod_example)))[2]`\n[1] -138.2054\n\n$BIC_p\n[1] -138.2054\n\n\n\n\n\n\n8.2.5 \\(PRESS_p\\) criterion\n\nThe \\(PRESS_p\\) (prediction sum of squares) criterion is a measure of how well the use of the fitted values for a subset model (in terms of the training data) can predict the observed responses \\(Y_i\\). It is similar to \\(SSE = \\sum (Y_i - \\hat{Y}_i)^2\\) except each fitted value \\(\\hat{Y}_i\\) is obtained by:\n\n\nDeleting the \\(i\\)th case from the dataset.\nEstimating the regression function for the subset model from the remaining \\(n - 1\\) cases.\nThen using the fitted regression function to obtain the predicted value \\(\\hat{Y}_{i(i)}\\) for the \\(i\\)th case (note \\(\\hat{Y}_{i(i)}\\) = predicted value for the \\(i\\)th case when the \\((i)\\)th observation was omitted when the regression function was fit.\n\n\nThe prediction error for the \\(i\\)th case is then\n\n\\[Y_i - \\hat{Y}_{i(i)}\\]\n\nThe \\(PRESS_p\\) criterion is the sum of the squared prediction errors over all \\(n\\) cases:\n\n\\[\\sum_{i = 1}^n (Y_i - \\hat{Y}_{i(i)})^2\\]\n\nGood models: Models with small \\(PRESS_p\\) fit well in the sense of having small prediction errors and are considered candidate models.\n\\(PRESS_p\\) values can be calculated without requiring \\(n\\) separate regression runs, each time deleting one of the \\(n\\) cases using formulas in the next chapter.\nNote that \\(PRESS_p\\) values can also be used for model validation.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building the regression model 1 -- Model selection and validation</span>"
    ]
  },
  {
    "objectID": "notes-building-reg-model-1.html#automatic-search-procedures-for-model-selection",
    "href": "notes-building-reg-model-1.html#automatic-search-procedures-for-model-selection",
    "title": "\n8  Building the regression model 1 – Model selection and validation\n",
    "section": "\n8.3 Automatic search procedures for model selection",
    "text": "8.3 Automatic search procedures for model selection\n\nTwo common approaches for automating variable selection are “best” subsets regression and stepwise regression.\nWill not cover all options and variations of methods that are available.\nIt is essential that the specific features of the package employed be fully understood so that intelligent use of the package can be made.\nFor example, some variations allow variables to be considered in groups (such as all indicators for a categorical predictor) or to force variables into the model (if have a prioir beliefs).\n\nAll to say, there is no unique way of searching for “good” subsets of \\(X\\) variables, and subjective elements must play an important role in the search process.\n\nJudgment needs to play an important role in model building for exploratory studies. Some explanatory variables may be known to be more fundamental than others and therefore should be retained in the regression model if the primary purpose is to develop a good explanatory model.\nFor example, keep all indicator variables for a categorical predictor, keep lower order terms if a higher order term is included, etc.\n\n\n\nAn important issue in exploratory model building that we have not will be considered is the bias in estimated regression coefficients and in estimated mean responses.\n\nThis is where model validation comes into play (don’t get lost in finding the best fit)\n\n\n\nDemo\n\n# calculate PRESS\nolsrr::ols_press(mod_example)\n\n[1] 3.914315\n\n# matches book!\n\n\n8.3.1 “Best” subsets algorithms\n\nThese algorithms provide the best subsets according to the specified criterion, as well as identifying several “good” subsets for each possible number of \\(X\\) variables in the model.\nWhen the pool of potential \\(X\\) variables is very large, say greater than 30 or 40, even the “best” subset algorithms may require excessive computer time. Under these conditions, one of the stepwise regression may need to be used.\nUse several different criterion when evaluating the “best” subsets. This is one way to get multiple candidate models.\nOnce the investigator has identified a few “good” subsets for intensive examination, a final choice of the model variables must be made with the help of residual analyses, industry knowledge, and finally then confirmed through model validation.\n\nStrengths\n\nExhaustive search of subsets.\nResults in several good candidate models.\n\n\n\nWeaknesses\n\nComputationally infeasible when there is a large number of predictors.\n\n\n\nDemo\n\n\nleaps::regsubsets\nolsrr::ols_step_best_subset\n\n\n\n\n# define function to format results for plotting\nformat_results &lt;- function(best_subs) {\n  \n  # get summary statistics\n  summ = summary(best_subs)\n  \n  # combine to single dataframe and rename columns\n  results = summ[2:6] %&gt;% reduce(bind_cols)\n  colnames(results) = names(summ)[2:6]\n  \n  # append number of predictor variables\n  results = results %&gt;% mutate(p = sort(rep(1:best_subs$np, best_subs$nbest))[1:nrow(summ$outmat)], .before = 1)\n  \n  return(results)\n  \n}\n\n# define function to plot results\nplot_results &lt;- function(res, var = \"rsq\") {\n  \n  # plot point for each model at each p\n  # -&gt; add line connecting optimal models at each p\n  p = ggplot(data = res,\n         aes(x = p,\n             y = .data[[var]])) + \n    geom_point(color = \"grey70\") + \n    geom_line(data = res %&gt;% group_by(p) %&gt;% slice_head(n = 1))\n  \n  # conditionally add Cp = p line\n  if (identical(var, \"cp\")) {\n    p = p + \n      geom_abline(intercept = 0,\n                  slope = 1)\n  }\n  \n  return(p)\n  \n}\n\n# perform best subsets regression and save necessary output\n# -&gt; note there are options to force variables in and out\n# -&gt; nbest = nmax does ALL possible models\nbest_subs &lt;- leaps::regsubsets(formula(mod_start_prime), data = data_surgery, \n                               nbest = 1, nvmax = 8, method = \"exhaustive\")\n\n# format and results\nresults &lt;- format_results(best_subs)\nmap(colnames(results)[-1], \\(var) plot_results(results, var))\n\n[[1]]\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n\n# this gives the single best model for number of predictor variables\n# -&gt; can use this to most easily get which variables are in the single best model_p\nbest_subs &lt;- olsrr::ols_step_best_subset(mod_example_full)\n\n# plot results also works, just plots only the best\nbest_subs %&gt;% rename(p = n) %&gt;% {map(colnames(best_subs)[4:10], \\(var) plot_results(., var))}\n\n[[1]]\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n\n\n8.3.2 Stepwise regression methods\nMotivation\n\nIn those occasional cases when the pool of potential \\(X\\) variables contains 30 to 40 or even more variables, use of a “best” subsets algorithm may not be feasible. An automatic search procedure that develops the “best” subset of \\(X\\) variables sequentially may then be helpful.\n\nOverview\n\nThis search method develops a sequence of regression models, at each step adding or deleting an \\(X\\) variable (iterative procedure).\nCan use one of several different criterion for adding or deleting an \\(X\\) variable, such as: reduction in error sum of squares, coefficient of partial correlation, \\(t^*\\) statistic, or \\(F^*\\) statistic.\n\nStrengths\n\nComputational more efficient than evaluating ALL possible subsets.\n\n\n\nWeaknesses\n\nExperience has shown that each of the stepwise search procedures can sometimes err by identifying a suboptimal regression model as “best.”\nEnd result is only a SINGLE “best” model. So it hides potentially other “good” models, whose “goodness” need to be evaluated using a variety of diagnostics.\n\n\n\nStrategy then\n\nWe should use the subset identified by the automatic search procedure as a starting point for searching for other “good” subsets.\nOne possibility is to treat the number of \\(X\\) variables in the regression model identified by the automatic search procedure as being about the right subset size and then use the “best” subsets procedure for subsets of this and nearby sizes.\n\n\n\nForward stepwise regression\n\nStep 0: Start with intercept-only model.\n\nStep 1: Fit all one variable models and evaluate criteria. Find the best. For example, the largest \\(\\lvert t^* \\rvert\\) or equivalently smallest \\(p\\)-value.\n\nSince the degrees of freedom associated with \\(MSE\\) vary depending on the number of \\(X\\) variables in the model, and since repeated tests on the same data are undertaken, fixed \\(t^*\\) limits for adding or deleting a variable have no precise probabilistic meaning. For this reason, software programs often favor the use of predetermined \\(\\alpha\\)-limits.\n\n\n\n\\[t^*_{k1} = \\sqrt{\\frac{\\hat{\\beta}_{k1}}{s\\{\\hat{\\beta}_{k1}\\}}} \\Longrightarrow p\\text{-value}\\]\n\nStep 2: Start with the variable from the previous step and fit all 2 variable models. Find the best second variable and see if it meets the keep criteria.\n\n\\[t^*_{k1} = \\sqrt{\\frac{MSR(X_{k2} \\mid X_{k1})}{MSE(X_{k1}, X_{k2})}} \\Longrightarrow p\\text{-value}\\]\n\nStep 3: Check to see if a variable should be deleted. Fit model with all predictors currently kept and see if one variable should be dropped (i.e. see if criteria is on wrong side of the keep criteria).\nStep 4: Continue adding and checking to see if previous variable should be dropped until adding a variable doesn’t improve the model and dropping a variable doesn’t improve the model. Then algorithm is done.\n\nNote that the stepwise regression algorithm allows an \\(X\\) variable, brought into the model at an earlier stage, to be dropped subsequently if it is no longer helpful in conjunction with variables added at later stages.\n\ni.e. The order in which variables enter the regression model does not reflect their importance.\n\n\n\nChoice of keep criteria in terms of \\(\\alpha\\)\n\n\nThe choice of \\(\\alpha\\) to-enter and \\(\\alpha\\)-to-remove values essentially represents a balancing of opposing tendencies.\n\nSimulation studies have shown that for scenarios with large pools of 1) uncorrelated predictors 2) that are not related to the response, larger \\(\\alpha\\)-to-enter values results in models that allow too many variables. Conversely, a small \\(\\alpha\\)-to-enter values results in models that are often underspecified, resulting in \\(\\sigma^2\\) being badly overestimated and the procedure being too conservative.\nThe max \\(\\alpha\\)-to-enter &lt; min \\(\\alpha\\)-to-remove. If not, will get an endless loop.\n\n\n\nDemo\n\n\nstats::step\nolsrr\n\n\n\n\n# perform stepwise regression (forward)\n\n# specify starting and ending models\nmod_null &lt;- lm(log(y) ~ 1, data_surgery) # starting with intercept model\nmod_full &lt;- mod_start_prime # just need formula\nformula(mod_full)\n\nlog(y) ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n\n# run procedure\n# -&gt; criterion is based on AIC from extractAIC()\nmod_step_aic &lt;- step(object = mod_null, scope = formula(mod_full), direction = \"both\", k = 2)\n\nStart:  AIC=-75.72\nlog(y) ~ 1\n\n       Df Sum of Sq     RSS      AIC\n+ x3    1    5.4708  7.3337 -103.811\n+ x4    1    5.3967  7.4079 -103.268\n+ x2    1    2.8303  9.9742  -87.205\n+ x8    1    1.7808 11.0238  -81.802\n+ x1    1    0.7770 12.0275  -77.096\n+ x6    1    0.6889 12.1156  -76.703\n&lt;none&gt;              12.8045  -75.716\n+ x5    1    0.2694 12.5351  -74.864\n+ x7    1    0.2067 12.5978  -74.595\n\nStep:  AIC=-103.81\nlog(y) ~ x3\n\n       Df Sum of Sq     RSS      AIC\n+ x2    1    3.0209  4.3129 -130.479\n+ x4    1    2.2018  5.1319 -121.089\n+ x1    1    1.5512  5.7825 -114.644\n+ x8    1    1.1386  6.1951 -110.922\n&lt;none&gt;               7.3337 -103.811\n+ x6    1    0.2582  7.0755 -103.747\n+ x5    1    0.2390  7.0947 -103.600\n+ x7    1    0.0659  7.2679 -102.298\n- x3    1    5.4708 12.8045  -75.716\n\nStep:  AIC=-130.48\nlog(y) ~ x3 + x2\n\n       Df Sum of Sq    RSS      AIC\n+ x8    1    1.4709 2.8420 -151.002\n+ x1    1    1.2044 3.1085 -146.161\n+ x4    1    0.6979 3.6150 -138.011\n+ x7    1    0.2280 4.0849 -131.412\n+ x5    1    0.1648 4.1481 -130.583\n&lt;none&gt;              4.3129 -130.479\n+ x6    1    0.0822 4.2306 -129.518\n- x2    1    3.0209 7.3337 -103.811\n- x3    1    5.6613 9.9742  -87.205\n\nStep:  AIC=-151\nlog(y) ~ x3 + x2 + x8\n\n       Df Sum of Sq    RSS      AIC\n+ x1    1    0.6642 2.1778 -163.376\n+ x4    1    0.4658 2.3761 -158.669\n+ x6    1    0.1372 2.7048 -151.674\n&lt;none&gt;              2.8420 -151.002\n+ x5    1    0.0709 2.7711 -150.367\n+ x7    1    0.0241 2.8179 -149.462\n- x8    1    1.4709 4.3129 -130.479\n- x2    1    3.3531 6.1951 -110.922\n- x3    1    4.9403 7.7823  -98.605\n\nStep:  AIC=-163.38\nlog(y) ~ x3 + x2 + x8 + x1\n\n       Df Sum of Sq    RSS      AIC\n+ x6    1    0.0966 2.0812 -163.826\n&lt;none&gt;              2.1778 -163.376\n+ x5    1    0.0760 2.1018 -163.293\n+ x4    1    0.0415 2.1363 -162.415\n+ x7    1    0.0224 2.1554 -161.935\n- x1    1    0.6642 2.8420 -151.002\n- x8    1    0.9307 3.1085 -146.161\n- x2    1    2.9891 5.1670 -118.722\n- x3    1    5.4459 7.6237  -97.717\n\nStep:  AIC=-163.83\nlog(y) ~ x3 + x2 + x8 + x1 + x6\n\n       Df Sum of Sq    RSS     AIC\n+ x5    1    0.0769 2.0043 -163.86\n&lt;none&gt;              2.0812 -163.83\n- x6    1    0.0966 2.1778 -163.38\n+ x7    1    0.0219 2.0593 -162.40\n+ x4    1    0.0163 2.0649 -162.25\n- x1    1    0.6236 2.7048 -151.67\n- x8    1    0.9754 3.0567 -145.07\n- x2    1    2.8287 4.9099 -119.48\n- x3    1    5.0742 7.1554  -99.14\n\nStep:  AIC=-163.86\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n\n       Df Sum of Sq    RSS      AIC\n&lt;none&gt;              2.0043 -163.858\n- x5    1    0.0769 2.0812 -163.826\n- x6    1    0.0975 2.1018 -163.293\n+ x7    1    0.0326 1.9718 -162.743\n+ x4    1    0.0022 2.0021 -161.919\n- x1    1    0.6284 2.6327 -151.133\n- x8    1    0.9011 2.9054 -145.810\n- x2    1    2.7644 4.7688 -119.052\n- x3    1    5.0752 7.0795  -97.716\n\n# confirm from extractAIC()\nextractAIC(mod_step_aic)\n\n[1]    7.0000 -163.8583\n\n# repeat for BIC criteria -&gt; k = log(nobs(mod))\nmod_step_bic &lt;- step(object = mod_null, scope = formula(mod_full), direction = \"both\", k = log(nobs(mod_null)), trace = 0) # criterion is based on BIC from extractAIC()\nextractAIC(mod_step_bic, k = log(nobs(mod_step_bic)))\n\n[1]    5.000 -153.431\n\n# compare models\n# -&gt; BIC is more conservative, which makes sense\ncompare(formula(mod_step_aic), formula(mod_step_bic))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"formulas differ in contents\"\n\n$comparison$`element-wise`\nNULL\n\n\n$`formula(mod_step_aic)`\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n\n$`formula(mod_step_bic)`\nlog(y) ~ x3 + x2 + x8 + x1\n\n\n\n\n\n# perform stepwise regression (forward)\n# -&gt; based on AIC\nmod_step_aic2 &lt;- olsrr::ols_step_both_aic(mod_full, progress = TRUE, details = TRUE)\n\nStepwise Selection Method \n-------------------------\n\nCandidate Terms: \n\n1 . x1 \n2 . x2 \n3 . x3 \n4 . x4 \n5 . x5 \n6 . x6 \n7 . x7 \n8 . x8 \n\n Step 0: AIC = 79.52928 \n log(y) ~ 1 \n\n\nVariables Entered/Removed: \n\n                         Enter New Variables                       \n-------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS      R-Sq     Adj. R-Sq \n-------------------------------------------------------------------\nx3            1    51.434     5.471     7.334    0.427        0.416 \nx4            1    51.977     5.397     7.408    0.421        0.410 \nx2            1    68.040     2.830     9.974    0.221        0.206 \nx8            1    73.443     1.781    11.024    0.139        0.123 \nx1            1    78.149     0.777    12.028    0.061        0.043 \nx6            1    78.543     0.689    12.116    0.054        0.036 \nx5            1    80.381     0.269    12.535    0.021        0.002 \nx7            1    80.651     0.207    12.598    0.016       -0.003 \n-------------------------------------------------------------------\n\n- x3 added \n\n\n Step 1 : AIC = 51.43434 \n log(y) ~ x3 \n\n                        Enter New Variables                       \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx2            1    24.767     8.492    4.313    0.663        0.650 \nx4            1    34.156     7.673    5.132    0.599        0.583 \nx1            1    40.602     7.022    5.783    0.548        0.531 \nx8            1    44.323     6.609    6.195    0.516        0.497 \nx6            1    51.499     5.729    7.075    0.447        0.426 \nx5            1    51.645     5.710    7.095    0.446        0.424 \nx7            1    52.947     5.537    7.268    0.432        0.410 \n------------------------------------------------------------------\n\n- x2 added \n\n\n Step 2 : AIC = 24.76682 \n log(y) ~ x3 + x2 \n\n                    Remove Existing Variables                     \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx2            1    51.434     5.471    7.334    0.427        0.416 \nx3            1    68.040     2.830    9.974    0.221        0.206 \n------------------------------------------------------------------\n\n                        Enter New Variables                       \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx8            1     4.243     9.963    2.842    0.778        0.765 \nx1            1     9.084     9.696    3.109    0.757        0.743 \nx4            1    17.234     9.190    3.615    0.718        0.701 \nx7            1    23.834     8.720    4.085    0.681        0.662 \nx5            1    24.663     8.656    4.148    0.676        0.657 \nx6            1    25.727     8.574    4.231    0.670        0.650 \n------------------------------------------------------------------\n\n- x8 added \n\n\n Step 3 : AIC = 4.243223 \n log(y) ~ x3 + x2 + x8 \n\n                    Remove Existing Variables                     \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx8            1    24.767     8.492    4.313    0.663        0.650 \nx2            1    44.323     6.609    6.195    0.516        0.497 \nx3            1    56.640     5.022    7.782    0.392        0.368 \n------------------------------------------------------------------\n\n                        Enter New Variables                       \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx1            1    -8.131    10.627    2.178    0.830        0.816 \nx4            1    -3.424    10.428    2.376    0.814        0.799 \nx6            1     3.572    10.100    2.705    0.789        0.772 \nx5            1     4.879    10.033    2.771    0.784        0.766 \nx7            1     5.783     9.987    2.818    0.780        0.762 \n------------------------------------------------------------------\n\n- x1 added \n\n\n Step 4 : AIC = -8.130569 \n log(y) ~ x3 + x2 + x8 + x1 \n\n                    Remove Existing Variables                     \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx1            1     4.243     9.963    2.842    0.778        0.765 \nx8            1     9.084     9.696    3.109    0.757        0.743 \nx2            1    36.524     7.638    5.167    0.596        0.572 \nx3            1    57.529     5.181    7.624    0.405        0.369 \n------------------------------------------------------------------\n\n                        Enter New Variables                       \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx6            1    -8.580    10.723    2.081    0.837        0.821 \nx5            1    -8.048    10.703    2.102    0.836        0.819 \nx4            1    -7.170    10.668    2.136    0.833        0.816 \nx7            1    -6.689    10.649    2.155    0.832        0.814 \n------------------------------------------------------------------\n\n- x6 added \n\n\n Step 5 : AIC = -8.580332 \n log(y) ~ x3 + x2 + x8 + x1 + x6 \n\n                    Remove Existing Variables                     \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx6            1    -8.131    10.627    2.178    0.830        0.816 \nx1            1     3.572    10.100    2.705    0.789        0.772 \nx8            1    10.176     9.748    3.057    0.761        0.742 \nx2            1    35.768     7.895    4.910    0.617        0.585 \nx3            1    56.105     5.649    7.155    0.441        0.396 \n------------------------------------------------------------------\n\n                        Enter New Variables                       \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx5            1    -8.613    10.800    2.004    0.843        0.823 \nx7            1    -7.151    10.745    2.059    0.839        0.819 \nx4            1    -7.005    10.740    2.065    0.839        0.818 \n------------------------------------------------------------------\n\n- x5 added \n\n\n Step 6 : AIC = -8.612898 \n log(y) ~ x3 + x2 + x8 + x1 + x6 + x5 \n\n                    Remove Existing Variables                     \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx5            1    -8.580    10.723    2.081    0.837        0.821 \nx6            1    -8.048    10.703    2.102    0.836        0.819 \nx1            1     4.113    10.172    2.633    0.794        0.773 \nx8            1     9.435     9.899    2.905    0.773        0.749 \nx2            1    36.193     8.036    4.769    0.628        0.589 \nx3            1    57.529     5.725    7.080    0.447        0.390 \n------------------------------------------------------------------\n\n                        Enter New Variables                       \n------------------------------------------------------------------\nVariable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq \n------------------------------------------------------------------\nx7            1    -7.497    10.833    1.972    0.846        0.823 \nx4            1    -6.673    10.802    2.002    0.844        0.820 \n------------------------------------------------------------------\n\n\nNo more variables to be added or removed.\n\nFinal Model Output \n------------------\n\n                        Model Summary                         \n-------------------------------------------------------------\nR                       0.918       RMSE               0.207 \nR-Squared               0.843       Coef. Var          3.211 \nAdj. R-Squared          0.823       MSE                0.043 \nPred R-Squared          0.784       MAE                0.162 \n-------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression     10.800         6          1.800    42.209    0.0000 \nResidual        2.004        47          0.043                     \nTotal          12.805        53                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                   \n---------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n---------------------------------------------------------------------------------------\n(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 \n         x3     0.015         0.001        0.653    10.909    0.000     0.012    0.018 \n         x2     0.014         0.002        0.473     8.051    0.000     0.010    0.017 \n         x8     0.351         0.076        0.280     4.597    0.000     0.197    0.505 \n         x1     0.072         0.019        0.233     3.839    0.000     0.034    0.109 \n         x6     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 \n         x5    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 \n---------------------------------------------------------------------------------------\n\nplot(mod_step_aic2)\n\n\n\n\n\n\n# extract formula from ending model\nmod_step_aic2_formula &lt;- mod_step_aic2$predictors %&gt;% {paste0(\"log(y) ~ \", paste0(., collapse = \" + \"))} %&gt;% as.formula\n\ncompare(formula(mod_step_aic), mod_step_aic2_formula)\n\n$comparison\n[1] TRUE\n\n$`formula(mod_step_aic)`\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n\n$mod_step_aic2_formula\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n&lt;environment: 0x7fd67710dcb8&gt;\n\n# confirm using AIC()\n# -&gt; this result is found in the printed output!\nAIC(lm(mod_step_aic2_formula, data_surgery))\n\n[1] -8.612898\n\n# perform stepwise regression (forward)\n# -&gt; based on p-values, using default alpha-to-enter and alpha-to-remove\nmod_step_p &lt;- olsrr::ols_step_both_p(mod_full, pent = 0.1, prem = 0.3)\nmod_step_p\n\n\n                             Stepwise Selection Summary                               \n-------------------------------------------------------------------------------------\n                     Added/                   Adj.                                       \nStep    Variable    Removed     R-Square    R-Square      C(p)        AIC       RMSE     \n-------------------------------------------------------------------------------------\n   1       x3       addition       0.427       0.416    117.4780    51.4343    0.3755    \n   2       x2       addition       0.663       0.650     50.4920    24.7668    0.2908    \n   3       x8       addition       0.778       0.765     18.9010     4.2432    0.2384    \n   4       x1       addition       0.830       0.816      5.7340    -8.1306    0.2108    \n-------------------------------------------------------------------------------------\n\n\n\n\n\n\n8.3.3 Other stepwise procedures\nForward selection\n\nThe forward selection search procedure is a simplified version of forward stepwise regression, omitting the test whether a variable once entered into the model should be dropped.\n\nDemo\n\n# perform forward selection regression\n# -&gt; based on AIC (could do BIC same way for step())\nmod_forward &lt;- step(object = mod_null, scope = formula(mod_full), direction = \"forward\", k = 2, trace = 0)\n\nmod_forward2 &lt;- olsrr::ols_step_forward_aic(mod_full)\nmod_forward2_formula &lt;- mod_forward2$predictors %&gt;% {paste0(\"log(y) ~ \", paste0(., collapse = \" + \"))} %&gt;% as.formula\n\ncompare(formula(mod_forward), mod_forward2_formula)\n\n$comparison\n[1] TRUE\n\n$`formula(mod_forward)`\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n\n$mod_forward2_formula\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n&lt;environment: 0x7fd6757d0ab8&gt;\n\ncompare(formula(mod_step_aic), formula(mod_forward))\n\n$comparison\n[1] TRUE\n\n$`formula(mod_step_aic)`\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n\n$`formula(mod_forward)`\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n\n# olsrr based on p-values\n# -&gt; default enter seems high\nolsrr::ols_step_forward_p(mod_full, pent = 0.3)\n\n\n                            Selection Summary                             \n-------------------------------------------------------------------------\n        Variable                  Adj.                                       \nStep    Entered     R-Square    R-Square      C(p)        AIC       RMSE     \n-------------------------------------------------------------------------\n   1    x3            0.4273      0.4162    117.4783    51.4343    0.3755    \n   2    x2            0.6632      0.6500     50.4918    24.7668    0.2908    \n   3    x8            0.7780      0.7647     18.9015     4.2432    0.2384    \n   4    x1            0.8299      0.8160      5.7340    -8.1306    0.2108    \n   5    x6            0.8375      0.8205      5.5282    -8.5803    0.2082    \n   6    x5            0.8435      0.8235      5.7725    -8.6129    0.2065    \n-------------------------------------------------------------------------\n\n\nBackward elimation\n\nThe backward elimination search procedure is the opposite of forward selection. It begins with the model containing all potential \\(X\\) variables and identifies the one with the largest \\(p\\)-value and determines if it should be dropped. Then repeat with the remaining \\(P-2\\) \\(X\\) variables.\nThis process continues until no further \\(X\\) variables can be dropped.\nA stepwise modification can also be adapted that allows variables eliminated earlier to be added later: this modification is called the backward stepwise regression procedure.\n\nFor small and moderate numbers of variables in the pool of potential \\(X\\) variables, some statisticians argue for backward stepwise search over forward stepwise search.\n\nA potential disadvantage of the forward stepwise approach is that the \\(MSE\\) (and hence s{_k}$) will lend to be inflated during the initial steps, because important predictors have been omitted \\(\\Longrightarrow\\) \\(t^*\\) test statistics that are too small.\nFor the backward stepwise procedure, \\(MSE\\) values tend to be more nearly unbiased because important predictors are retained at each step.\nAn argument in favor of the backward stepwise procedure can also be made in situations where it is useful as a first step to look at each \\(X\\) variable in the regression function adjusted for all the other \\(X\\) variables in the pool.\n\n\n\n\n# perform backward selection regression\n# -&gt; based on AIC (could do BIC same way for step())\nmod_backward &lt;- step(object = mod_full, scope = formula(mod_full), direction = \"backward\", k = 2, trace = 0)\n\nmod_backward2 &lt;- olsrr::ols_step_backward_aic(mod_full)\nmod_backward2_formula &lt;- mod_forward2$predictors %&gt;% {paste0(\"log(y) ~ \", paste0(., collapse = \" + \"))} %&gt;% as.formula\n\ncompare(formula(mod_backward), mod_backward2_formula) # same models, different order\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"formulas differ in contents\"\n\n$comparison$`element-wise`\nNULL\n\n\n$`formula(mod_backward)`\nlog(y) ~ x1 + x2 + x3 + x5 + x6 + x8\n\n$mod_backward2_formula\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n&lt;environment: 0x7fd6a36465b8&gt;\n\ncompare(formula(mod_step_aic), formula(mod_backward)) # same models, different order\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"formulas differ in contents\"\n\n$comparison$`element-wise`\nNULL\n\n\n$`formula(mod_step_aic)`\nlog(y) ~ x3 + x2 + x8 + x1 + x6 + x5\n\n$`formula(mod_backward)`\nlog(y) ~ x1 + x2 + x3 + x5 + x6 + x8\n\n# olsrr based on p-values\nolsrr::ols_step_backward_p(mod_full, prem = 0.3, progress = TRUE, details = TRUE)\n\nBackward Elimination Method \n---------------------------\n\nCandidate Terms: \n\n1 . x1 \n2 . x2 \n3 . x3 \n4 . x4 \n5 . x5 \n6 . x6 \n7 . x7 \n8 . x8 \n\nWe are eliminating variables based on p value...\n\n- x4 \n\nBackward Elimination: Step 1 \n\n Variable x4 Removed \n\n                        Model Summary                         \n-------------------------------------------------------------\nR                       0.920       RMSE               0.207 \nR-Squared               0.846       Coef. Var          3.220 \nAdj. R-Squared          0.823       MSE                0.043 \nPred R-Squared          0.781       MAE                0.162 \n-------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression     10.833         7          1.548    36.103    0.0000 \nResidual        1.972        46          0.043                     \nTotal          12.805        53                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                   \n---------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n---------------------------------------------------------------------------------------\n(Intercept)     4.037         0.236                 17.096    0.000     3.562    4.513 \n         x1     0.071         0.019        0.233     3.824    0.000     0.034    0.109 \n         x2     0.014         0.002        0.468     7.900    0.000     0.010    0.017 \n         x3     0.015         0.001        0.655    10.901    0.000     0.012    0.018 \n         x5    -0.004         0.003       -0.084    -1.429    0.160    -0.009    0.002 \n         x6     0.087         0.058        0.089     1.503    0.140    -0.029    0.203 \n         x7     0.058         0.067        0.059     0.872    0.388    -0.076    0.192 \n         x8     0.388         0.087        0.309     4.437    0.000     0.212    0.564 \n---------------------------------------------------------------------------------------\n\n\n- x7 \n\nBackward Elimination: Step 2 \n\n Variable x7 Removed \n\n                        Model Summary                         \n-------------------------------------------------------------\nR                       0.918       RMSE               0.207 \nR-Squared               0.843       Coef. Var          3.211 \nAdj. R-Squared          0.823       MSE                0.043 \nPred R-Squared          0.784       MAE                0.162 \n-------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression     10.800         6          1.800    42.209    0.0000 \nResidual        2.004        47          0.043                     \nTotal          12.805        53                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                   \n---------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n---------------------------------------------------------------------------------------\n(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 \n         x1     0.072         0.019        0.233     3.839    0.000     0.034    0.109 \n         x2     0.014         0.002        0.473     8.051    0.000     0.010    0.017 \n         x3     0.015         0.001        0.653    10.909    0.000     0.012    0.018 \n         x5    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 \n         x6     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 \n         x8     0.351         0.076        0.280     4.597    0.000     0.197    0.505 \n---------------------------------------------------------------------------------------\n\n\n\nNo more variables satisfy the condition of p value = 0.3\n\n\nVariables Removed: \n\n- x4 \n- x7 \n\n\nFinal Model Output \n------------------\n\n                        Model Summary                         \n-------------------------------------------------------------\nR                       0.918       RMSE               0.207 \nR-Squared               0.843       Coef. Var          3.211 \nAdj. R-Squared          0.823       MSE                0.043 \nPred R-Squared          0.784       MAE                0.162 \n-------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression     10.800         6          1.800    42.209    0.0000 \nResidual        2.004        47          0.043                     \nTotal          12.805        53                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                   \n---------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper \n---------------------------------------------------------------------------------------\n(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 \n         x1     0.072         0.019        0.233     3.839    0.000     0.034    0.109 \n         x2     0.014         0.002        0.473     8.051    0.000     0.010    0.017 \n         x3     0.015         0.001        0.653    10.909    0.000     0.012    0.018 \n         x5    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 \n         x6     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 \n         x8     0.351         0.076        0.280     4.597    0.000     0.197    0.505 \n---------------------------------------------------------------------------------------\n\n\n\n                          Elimination Summary                           \n-----------------------------------------------------------------------\n        Variable                  Adj.                                     \nStep    Removed     R-Square    R-Square     C(p)       AIC       RMSE     \n-----------------------------------------------------------------------\n   1    x4             0.846      0.8226    7.0288    -7.4974    0.2070    \n   2    x7            0.8435      0.8235    5.7725    -8.6129    0.2065    \n-----------------------------------------------------------------------",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building the regression model 1 -- Model selection and validation</span>"
    ]
  },
  {
    "objectID": "notes-building-reg-model-1.html#model-validation-1",
    "href": "notes-building-reg-model-1.html#model-validation-1",
    "title": "\n8  Building the regression model 1 – Model selection and validation\n",
    "section": "\n8.4 Model validation",
    "text": "8.4 Model validation\n\n8.4.1 Overview\n\nThe final step in the model-building process is the validation of the selected regression models. Model validation usually involves checking a candidate model against independent data. Three basic ways of validating a regression model are:\n\n\nCollection of new data to check the model and its predictive ability.\nComparison of results with theoretical expectations, earlier empirical results, and simulation results.\nUse of a holdout sample to check the model and its predictive ability.\n\nOther types of studies\n\n\nWhen a regression model is used in a controlled experiment, a repetition of the experiment and its analysis serves to validate the findings in the initial study if similar results for the regression coefficients, predictive ability, and the like are obtained. Same for confirmatory observational studies, just use other data and repeat analyses.\n\nThis is because there is no issue with variable selection.\nAdditionally, if the model is to be used for making predictions over the entire range of the \\(X\\) observations, a possibility is to include data points that are uniformly distributed over the \\(X\\) space.\n\n\n\nOn the contrast, definitely need to validate for exploratory observational studies because they often start with large pools of \\(X\\) variables which is narrowed down significantly.\n\nFor these studies, validation of the regression model involves also the appropriateness of the variables selected, as well as the magnitudes of the regression coefficients, the predictive ability of the model, and the like.\n\n\n\n8.4.2 Collection of new data to check model\n\n\nThe best means of model validation is through the collection of new data. The purpose of collecting new data is to be able to examine whether the regression model developed from the earlier data is still applicable for the new data.\n\nIf so, one has assurance about the applicability of the model to data beyond those on which the model is based.\n\n\n\nMethods of checking validity\n\nThere are a variety of methods of examining the validity of the regression model against the new data.\n\n\n\nOne validation method is to reestimate the model form chosen earlier using the new data.\n\nThe estimated regression coefficients and various characteristics of the fitted model are then compared for consistency to those of the regression model based on the em’lier data.\nIf the results are consistent, they provide strong support that the chosen regression model is applicable under broader circumstances than those related to the original data.\n\n\n\nA second validation method is designed to calibrate the predictive capability of the selected regression model.\n\nWhen a regression model is developed from given data, it is inevitable that the selected model is chosen, at least in large part, because it fits well the data at hand.\nFor a different set of random outcomes, one may likely have arrived at a different model in terms of the predictor valiables selected and/or their functional forms and interaction terms present in the model.\nA result of this model development process is that the \\(MSE\\) will tend to understate the inherent variability in making future predictions from the selected model.\nA means of measuring the actual predictive capability of the selected regression model is to use this model to predict each case in the new data set and then to calculate the mean squared prediction error \\(MSPR\\):\n(where \\(Y_i\\) is the value of the response valiable in the \\(i\\)th validation case; \\(\\hat{Y}_i\\) is the predicted value for the \\(i\\)th validation case based on the model-building dataset; and \\(n^*\\) is the number of cases in the validation set)\n\n\n\n\\[MSPR = \\frac{1}{n^*}\\sum_{i = 1}^{n^*} (Y_i - \\hat{Y}_i)^2\\]\n\n\nIf the \\(MSPR\\) is fairly close to \\(MSE\\) based on the regression fit to the model-building data set, then the \\(MSE\\) for the selected regression model is not seriously biased and gives an appropriate indication of the predictive ability of the model.\n\nIf the \\(MSPE\\) is much larger than \\(MSE\\), one should rely on the \\(MSPE\\) as an indicator of how well the selected regression model will predict in the future.\n\n\n\nDifficulties in replicating a study\n\nDifficulties often arise when new data are collected to validate a regression model, especially with observational studies.\nEssentially, hard to get exactly identical conditions. In observational studies, things like settings and time frequently change.\n\nEven though perfect replication is impossible, all validation studies are useful because No single study is fully useful until we know how much the results of the study can be generalized.\n\nIf a replication study for which the conditions of the setting differ only slightly from those of the initial study substantially different regression results, then we learn that the results of the initial study cannot be readily generalized.\nOn the other hand, if the conditions differ substantially and the regression results are still similar, we find that the regression results can be generalized to apply under substantially varying conditions.\nStill another possibility is that the regression results for the replication study differ substantially from those of the initial study, the differences being related to changes in the setting. This information may be useful for enriching the regression model by including new explanatory variables that make the model more widely applicable.\n\n\n\n8.4.3 Comparison with theory, empirical evidence, or simulation results\n\nIn some cases, theory, simulation rdmlts, or previous empirical results may be helpful in determining whether the selected model is reasonable.\nComparisons of regression coefficients and predictions with theoretical expectations, previous empirical results, or simulation results should be made.\nUnfortunately, there is often little theory that can be used to validate regression models.\n\n8.4.4 Data splitting\nOverview\n\nBy far the preferred method to validate a regression model is through the collection of new data. Often, however, this is neither practical nor feasible.\n\nAn alternative is to use a training and testing set, which in effect an attempt to simulate replication of the study. The validation data set is used for validation in the same way as when new data are collected.\n\nThe regression coefficients can be reestimated for the selected model and then compareed for consistency with the coefficients obtained from the model-building data set.\nAlso, predictions can be made for the testing data from the model built on the training data in order to calibrate the predictive ability of this regression model for the new data.\nWhen the calibration data set is large enough, one can also study how the “good” models considered in the model selection phase fare with the new data.\n\n\n\nHow to split\n\n\\(n_{\\text{train}} &gt;&gt; n_{\\text{test}}\\), can even go 50/50 if large enough dataset.\nSplits of the data can be made at random or stratified to ensure certain spread of cases (roughly equal representation of \\(X\\) levels) in the training and testing datasets.\n\nCan also do \\(k\\)-fold cross validation. The \\(k\\) estimates of prediction error are then combined to produce a \\(k\\)-fold cross-validation estimate.\n\nNote that when \\(k = n\\), this criteria is equal to \\(PRESS_p\\) \\(\\Longrightarrow\\) \\(k\\)-fold cross validation estimates are approximations to \\(PRESS_p\\).\nCan also use a variation of \\(PRESS_p\\) where \\(m\\) observations are held out rather than 1, and the \\(n-m\\) observations are used to fit the models.\n\n\n\nIf a dataset for an exploratory observational study is very large, it can be divided into three parts.\n\nThe first part is used for model training, the second part for cross-validation and model selection, and the third part for testing and calibrating the final model.\nThis approach avoids any bias resulting from estimating the regression parameters from the same data set used for developing the model.\nBut it still uses less data to fit the model, so less precise standard errors of estimated coefficients.\n\n\n\nDrawbacks\n\nA possible drawback of data splitting is that the variances of the estimated regression coefficients developed from the model-building data set will usually be larger than those that would have been obtained from the fit to the entire data set.\nIf the model-building data set is reasonably large, however, these variances generally will not be that much larger than those for the entire data set.\nIn any case, once the model has been validated, it is customary practice to use the entire data set for estimating the final regression model.\n\nResults\n\n\nWhen regression models built on observational data do not predict well outside the range of the \\(X\\) observations in the data set (extrapolation), the usual reason is the existence of multicollinearity among the \\(X\\) variables.\n\nRidge regression or other biased estimation techniques are possible solutions to this.\n\n\n\n8.4.5 Demo – Data splitting\n\n\ntidymodels\ncaret\ndplyr\n\n\n\n\n# random split 2-way\n# -&gt; make train / test split\ndata_split &lt;- data_surgery %&gt;% rsample::initial_split(prop = .7)\ndata_train &lt;- data_split %&gt;% rsample::training()\ndata_test &lt;- data_split %&gt;% rsample::testing()\n\n# stratified split\n# -&gt; https://www.tidymodels.org/start/case-study/\n# -&gt; based on response variable (ensures representative training and testing)\n# -&gt; if numeric, does based on binned quantiles\n# -&gt; if categorical, does it based on proportion of levels\ndata_split_strat &lt;- data_surgery %&gt;% rsample::initial_split(prop = .7, strata = y)\ndata_train2 &lt;- data_split_strat %&gt;% rsample::training()\ndata_test2 &lt;- data_split_strat %&gt;% rsample::testing()\n\n# random split 3-way\n# -&gt; make train / validation / test split\ndata_split_diamonds &lt;- diamonds %&gt;% rsample::initial_validation_split(prop = c(0.6, 0.2))\ndata_train_diamonds &lt;- data_split_diamonds %&gt;% rsample::training()\ndata_validation_diamonds  &lt;- data_split_diamonds %&gt;% rsample::validation()\ndata_test_diamonds  &lt;- data_split_diamonds %&gt;% rsample::testing()\n\n# k folds\n# -&gt; https://rsample.tidymodels.org/articles/Working_with_rsets.html\n# -&gt; access analysis and holdout data for the first fold\n# -&gt; setup to map() a function to perform on each fold split\nkfolds &lt;- rsample::vfold_cv(data = data_surgery, v = 10, repeats = 1, strata = y)\nkfolds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [48/6]&gt; Fold01\n 2 &lt;split [48/6]&gt; Fold02\n 3 &lt;split [48/6]&gt; Fold03\n 4 &lt;split [48/6]&gt; Fold04\n 5 &lt;split [48/6]&gt; Fold05\n 6 &lt;split [48/6]&gt; Fold06\n 7 &lt;split [48/6]&gt; Fold07\n 8 &lt;split [50/4]&gt; Fold08\n 9 &lt;split [50/4]&gt; Fold09\n10 &lt;split [50/4]&gt; Fold10\n\ndata_analysis1 &lt;- rsample::analysis(kfolds$splits[[1]])\ndata_holdout1 &lt;- rsample::assessment(kfolds$splits[[1]])\n\n# IMPORTANT NOTE: in practice would do the resampling method on the TRAINING data, then get the final model performance on the testing set\n\n\n\n\n# random split\n\n# get indices in training data\n# then subset data to training and testing data\nin_train &lt;- caret::createDataPartition(data_surgery$y, p = 0.7)\ndata_train &lt;- data_surgery[in_train$Resample1, ]\ndata_test &lt;- data_surgery[-(in_train$Resample1), ]\n\n\n\n\n# random split\n\n# sample a proportion for training set and get all non matches for testing\n# -&gt; requires ID variable (saving new object to not overwrite original data)\ndata_surgery_tmp &lt;- data_surgery %&gt;% mutate(id = row_number())\ndata_train &lt;- data_surgery_tmp %&gt;% slice_sample(prop = 0.7)\ndata_test &lt;- anti_join(data_surgery_tmp, data_train, by = join_by(id))\n\n\n\n\n\n8.4.6 Demo – Get best model from train\n\n# random split 2-way\n# -&gt; make train / test split\nset.seed(2)\ndata_split &lt;- data_surgery %&gt;% rsample::initial_split(prop = .7)\ndata_train &lt;- data_split %&gt;% rsample::training()\ndata_test &lt;- data_split %&gt;% rsample::testing()\n\n\n# get final best model\n\n# perform best subsets regression and save necessary output\n# -&gt; note there are options to force variables in and out\n# -&gt; nbest = nmax does ALL possible models\nbest_subs &lt;- leaps::regsubsets(formula(mod_start_prime), data = data_train, \n                               nbest = 8, nvmax = 8, method = \"exhaustive\")\n\n# format and results\nresults &lt;- format_results(best_subs)\nmap(colnames(results)[-1], \\(var) plot_results(results, var))\n\n[[1]]\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n# criteria -&gt; best model\n# rsq -&gt; 4\n# rss -&gt; 4\n# adjr2 -&gt; 3\n# cp -&gt; 4\n# bic -&gt; 3\n\n# going with best 4 parameter model (3 Xs)\nsummary(best_subs)$which %&gt;% data.frame %&gt;% filter(row.names(.) %in% c(\"X3\"))\n\n   X.Intercept.    x1   x2   x3    x4    x5    x6    x7   x8\nX3         TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE\n\n# for sake of simplicity, not evaluating multiple models and skipping some criteria like PRESSp\n\n# fit final model using best model\n# -&gt; keeping x7 so indicator variables x7 and x8 are together\nmod_train &lt;- lm(log(y) ~ x2 + x3 + x7 + x8, data_train)\nmod_train %&gt;% tidy\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.32     0.181      23.9   5.71e-22\n2 x2            0.0156   0.00195     8.00  3.93e- 9\n3 x3            0.0133   0.00157     8.46  1.16e- 9\n4 x7            0.0466   0.0796      0.586 5.62e- 1\n5 x8            0.344    0.0986      3.49  1.44e- 3\n\nmod_train %&gt;% glance\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.821         0.799 0.207      36.7 1.55e-11     4   8.44 -4.88  4.79\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n8.4.7 Demo – Validate model\n\n\nR functions\nManual\n\\(k\\)-fold cross validation \\(RMSE\\) – Semi manual\n\\(k\\)-fold cross validation \\(RMSE\\) – Full tidymodels\n\n\n\n\n# make predictions on testing data\npreds &lt;- predict(mod_train, newdata = data_test)\n\n# calculate rmse\nyardstick::rmse_vec(truth = log(data_test$y), estimate = preds)\n\n[1] 0.3088084\n\n# calculate MSPE\nyardstick::rmse_vec(truth = log(data_test$y), estimate = preds)^2\n\n[1] 0.09536261\n\n# compare regression coefficients fit on training data to testing data for consistency\nmod_test &lt;- lm(formula(mod_train), data_test)\ntidy(mod_test)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)  4.30      0.521       8.26  0.00000270\n2 x2           0.00977   0.00595     1.64  0.126     \n3 x3           0.0194    0.00446     4.35  0.000952  \n4 x7          -0.0948    0.217      -0.437 0.670     \n5 x8           0.555     0.278       2.00  0.0691    \n\ntidy(mod_train)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.32     0.181      23.9   5.71e-22\n2 x2            0.0156   0.00195     8.00  3.93e- 9\n3 x3            0.0133   0.00157     8.46  1.16e- 9\n4 x7            0.0466   0.0796      0.586 5.62e- 1\n5 x8            0.344    0.0986      3.49  1.44e- 3\n\n# compare MSEp and MSE\ncompare(mod_test %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2),\n        mod_train %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Mean relative difference: 0.4767689\"\n\n$comparison$`element-wise`\n[1] FALSE\n\n\n$`mod_test %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2)`\n[1] 0.08199894\n\n$`mod_train %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2)`\n[1] 0.0429044\n\nvar(log(data_surgery$y))\n\n[1] 0.2415945\n\n\nConclusions\n\nCoefficients of the testing model are all within a standard error or two of the training model (only concern in x7 switches signs)\n\\(MSE\\)s are close, especially considering scale of \\(var(ln(y))\\) for reference. So it does perform well for predictive accuracy.\n\n\n\n\n# calculate MSPE\nsum((log(data_test$y) - preds)^2) / nrow(data_test)\n\n[1] 0.09536261\n\n# calculate RMSE = sqrt(MSPE)\n(sum((log(data_test$y) - preds)^2) / nrow(data_test)) %&gt;% sqrt\n\n[1] 0.3088084\n\n\n\n\n\n# k folds\nkfolds &lt;- rsample::vfold_cv(data = data_surgery, v = 10, repeats = 1, strata = y)\n\n# function to:\n# 1) fit model on analysis data\n# 2) make predictions on holdout data\n# ... will be the model formula\nholdout_results &lt;- function(splits, ...) {\n  \n  # fit the model to the analysis data (90%: 9 of the 10 folds)\n  mod_kfold = lm(..., data = rsample::analysis(splits))\n  \n  # save the holdout data (10%: last fold)\n  data_holdout = rsample::assessment(splits)\n  \n  # `augment` will save the predictions with the holdout data set\n  preds = augment(mod_kfold, newdata = data_holdout)\n  \n  return(preds)\n  \n}\n\n# fit models for k-fold cross validation\nkfolds$results &lt;- kfolds$splits %&gt;% map(holdout_results, formula(mod_train))\n\n# calculate rmse for each fold\nkfolds$rmse &lt;- kfolds$results %&gt;% map_dbl(\\(df) yardstick::rmse_vec(truth = log(df$y),estimate = df$`.fitted`))\n\n# calculate final cross validation rmse\nkfolds$rmse %&gt;% mean\n\n[1] 0.2440127\n\n\n\n\nSLIDES\n\n# fit a single linear regression model on entire data for practice\n# -&gt; https://www.tidymodels.org/start/models/\n\n# make the parsnip model \nspec_linreg &lt;- parsnip::linear_reg()\n\n# train the model\nfit_linreg &lt;- mod_linreg %&gt;% \n  parsnip::fit(log(y) ~ ., data = data_surgery)\n\nError in eval(expr, envir, enclos): object 'mod_linreg' not found\n\ntidy(fit_linreg)\n\nError in eval(expr, envir, enclos): object 'fit_linreg' not found\n\n\n\n# fit models for all resamples\n# -&gt; https://www.tidymodels.org/start/resampling/\n\n# use kfolds from above\n\n# define workflow\nwf_linreg &lt;- workflows::workflow() %&gt;% \n  workflows::add_model(spec_linreg) %&gt;% \n  workflows::add_formula(formula(mod_train))\nwf_linreg\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nlog(y) ~ x2 + x3 + x7 + x8\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n# fit cross validation models \nrs_fit_linreg &lt;- wf_linreg %&gt;% \n  tune::fit_resamples(object = .,\n                      resamples = kfolds,\n                      metrics = yardstick::metric_set(yardstick::rmse))\n\n# get final cross validation measures\nworkflowsets::collect_metrics(rs_fit_linreg)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.244    10  0.0325 Preprocessor1_Model1\n\ncompare(kfolds$rmse %&gt;% mean,\n        workflowsets::collect_metrics(rs_fit_linreg) %&gt;% pull(mean))\n\n$comparison\n[1] TRUE\n\n$`kfolds$rmse %&gt;% mean`\n[1] 0.2440127\n\n$`workflowsets::collect_metrics(rs_fit_linreg) %&gt;% pull(mean)`\n[1] 0.2440127",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building the regression model 1 -- Model selection and validation</span>"
    ]
  },
  {
    "objectID": "notes-inference.html",
    "href": "notes-inference.html",
    "title": "2  Inference",
    "section": "",
    "text": "2.1 Inferences concerning \\(\\beta_1\\)\nOverview\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-building-reg-model-1.html",
    "href": "notes-building-reg-model-1.html",
    "title": "\n8  Building the regression model 1 – Model selection and validation\n",
    "section": "",
    "text": "8.1 Overview of the model building process\nThe figure below presents a strategy for the building of a regression model. This strategy involves three or, sometimes, four phases:\nHere we will overview each phase, then dive deeper later.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building the regression model 1 -- Model selection and validation</span>"
    ]
  },
  {
    "objectID": "notes-slr.html",
    "href": "notes-slr.html",
    "title": "1  Simple linear regression",
    "section": "",
    "text": "1.1 Regression overview\nRegression overview\nRegression models and their uses\nCode# create regression data frame to show different the deterministic and statistical relationships (assuming SLR normal error model for demonstration so can visualize)\n# generate\n# -&gt; X values (not from a random dist)\n# -&gt; f(X) = E(Y | X) = beta_0 + beta_1 X demonstration\n# -&gt; random error = epsilon ~ N(0,1)\n# -&gt; Y = f(X) + epsilon\nbeta_0 &lt;- 3\nbeta_1 &lt;- 1\ndata_ships &lt;- data.frame(X = seq(from = 10, to = 19, by = 1)) %&gt;% \n  mutate(f_X = beta_0 + beta_1 * X,\n         epsilon = rnorm(n = nrow(.), mean = 0, sd = 1),\n         Y = f_X + epsilon)\n\n# display data\ndata_ships %&gt;% display_nice(col.names = c(\"X\", \"f(X) = E(Y | X)\", \"$epsilon ~ N(0,1)\", \"Y = f(X) + epsilon\"))\n\n\n\n X \n    f(X) = E(Y | X) \n    $epsilon ~ N(0,1) \n    Y = f(X) + epsilon \n  \n\n\n 10 \n    13 \n    0.060 \n    13.060 \n  \n\n 11 \n    14 \n    -0.166 \n    13.834 \n  \n\n 12 \n    15 \n    1.026 \n    16.026 \n  \n\n 13 \n    16 \n    -0.418 \n    15.582 \n  \n\n 14 \n    17 \n    0.444 \n    17.444 \n  \n\n 15 \n    18 \n    -0.674 \n    17.326 \n  \n\n 16 \n    19 \n    0.060 \n    19.060 \n  \n\n 17 \n    20 \n    -0.459 \n    19.541 \n  \n\n 18 \n    21 \n    1.115 \n    22.115 \n  \n\n 19 \n    22 \n    1.122 \n    23.122 \n  \n\n\n\nCode# plot (X, Y)\n# -&gt; and add points and line for E(Y | X)\n# --&gt; population E(Y | X)) because using known betas (not estimating from sample data)\ndata_ships %$% \n  plot(x = X, y = Y, type = \"p\", main = \"Deterministic vs statistical relationship\")\ndata_ships %$% \n  points(x = X, y = f_X, col = \"red\", pch = 5)\ndata_ships %$% \n  lines(x = X, y = f_X, col = \"red\")\ntext(x = 18.5, y = 20, labels = \"f(X) = E(Y|X)\", col = \"red\")\nConstruction of statistical regression models\nUses of statistical regression models\nTypical strategy for regression analysis",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html",
    "href": "notes-diagnostics-and-remedial-measures.html",
    "title": "3  Diagnostics and remedial measures",
    "section": "",
    "text": "3.1 Diagnostics for predictor variable\nOverview\nPlots\nDemo\n# generate several samples of X values\nx_1 &lt;- extraDistr::rdunif(n = 15, min = 5, max = 15)\nx_2 &lt;- extraDistr::rdunif(n = 50, min = 5, max = 15)\nx_3 &lt;- runif(n = 1000, min = 5, max = 15)\n\n# create simple plots for diagnostics of X variable\n# NOTE -&gt; just doing simple base R plots \n\n# dot plot for small samples\nstripchart(x_1, method = \"stack\")\n\n\n\n\n\n\n# sequence plot for \"sequential\" data\n# -&gt; data is random, so will be a random walk\nplot(x_1, type = \"b\")\n\n\n\n\n\n\n# boxplot for larger samples\n# -&gt; add mark for mean\nboxplot(x_2, horizontal = TRUE)\npoints(x = mean(x_2), y = 1, pch = 4)\n\n\n\n\n\n\n# histogram for really large sample\nhist(x_3)",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  }
]