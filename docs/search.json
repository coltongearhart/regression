[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression",
    "section": "",
    "text": "Overview\nThese course notes introduce regression models and accompanying assumptions. In addition, they cover the theory of least squares to understand the statistical inference procedures behind linear models. Extensive applications of modeling in R will be demonstrated, with a goal of being able to understand strengths and limitations of models. These will be applied to predict and perform inference, while developing sensitivity to important model assumptions.\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/regression.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "part1-slr.html",
    "href": "part1-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "This section contains notes from Part 1: Simple Linear Regression.",
    "crumbs": [
      "Simple linear regression"
    ]
  },
  {
    "objectID": "notes-slr.html#regression-overview",
    "href": "notes-slr.html#regression-overview",
    "title": "1  Simple linear regression",
    "section": "\n1.1 Regression overview",
    "text": "1.1 Regression overview\nRegression overview\n\nGoal is to determine if and how one variable is related to a set of other variables.\n\nVariables\n\nResponse variable, denoted \\(Y\\), represents an outcome whose variation is being studied.\nExplanatory variable, denoted \\(X\\), represents the causes (i.e. potential reasons for variation).\n\n\n\nTwo types of relationships\n\nFunctional (deterministic) → There is an exact relation between two variables (have the form \\(y = ax+ b\\)).\nStatistical (probabilistic) → There is not an exact relation because there are other variables that affect the relationship (have the form \\(y = ax + b + \\epsilon\\)).\n\n\n\nRegression models and their uses\n\nStatistical models quantify the relationship between a response variable (i.e. a random variable) and explanatory variables, which are usually assumed to be deterministic (i.e. known exactly).\n\nElements of a statistical regression model\n\n\nIn general, observations do not fall directly on the curve of a relationship.\n\n\\(Y \\mid X\\) has a probability distribution.\n\\(E(Y \\mid X)\\) varies deterministically with \\(X\\).\n\n\n\n\nSo the statistical model is:\n\\[\n\\begin{align*}\n  Y &= E(Y \\mid X) + \\epsilon \\\\\n    &= f(X) + \\epsilon, \\hspace{20pt} \\text{where $\\epsilon$ has some distribution}\n\\end{align*}\n\\]\n\n\n\n\nTwo components of a statistical model:\n\n\\(f(X) = E(Y \\mid X)\\) → Defines relationship between \\(Y\\) and \\(X\\); explains the average behavior of the response.\n\\(\\epsilon\\) → An element of randomness (i.e. error). This contains the variation that \\(f(X)\\) cannot explain and/or that is of no interest.\n\n\nThis means \\(f(X) = E(Y \\mid X)\\) will be the same for all samples with the same \\(X\\) values. The only thing that changes is the random error \\(\\epsilon\\) and as a result \\(Y\\). Example \\(Y = 3 + 1X + \\epsilon\\):\n\n\nCode# create regression data frame to show different the deterministic and statistical relationships (assuming SLR normal error model for demonstration so can visualize)\n# generate\n# -&gt; X values (not from a random dist)\n# -&gt; f(X) = E(Y | X) = beta_0 + beta_1 X demonstration\n# -&gt; random error = epsilon ~ N(0,1)\n# -&gt; Y = f(X) + epsilon\nbeta_0 &lt;- 3\nbeta_1 &lt;- 1\ndata_ships &lt;- data.frame(X = seq(from = 10, to = 19, by = 1)) %&gt;% \n  mutate(f_X = beta_0 + beta_1 * X,\n         epsilon = rnorm(n = nrow(.), mean = 0, sd = 1),\n         Y = f_X + epsilon)\n\n# display data\ndata_ships %&gt;% display_nice(col.names = c(\"X\", \"f(X) = E(Y | X)\", \"$epsilon ~ N(0,1)\", \"Y = f(X) + epsilon\"))\n\n\n\n X \n    f(X) = E(Y | X) \n    $epsilon ~ N(0,1) \n    Y = f(X) + epsilon \n  \n\n\n 10 \n    13 \n    1.844 \n    14.844 \n  \n\n 11 \n    14 \n    -0.728 \n    13.272 \n  \n\n 12 \n    15 \n    -0.110 \n    14.890 \n  \n\n 13 \n    16 \n    0.444 \n    16.444 \n  \n\n 14 \n    17 \n    0.697 \n    17.697 \n  \n\n 15 \n    18 \n    -0.757 \n    17.243 \n  \n\n 16 \n    19 \n    0.408 \n    19.408 \n  \n\n 17 \n    20 \n    0.734 \n    20.734 \n  \n\n 18 \n    21 \n    -0.604 \n    20.396 \n  \n\n 19 \n    22 \n    0.329 \n    22.329 \n  \n\n\n\nCode# plot (X, Y)\n# -&gt; and add points and line for E(Y | X)\n# --&gt; population E(Y | X)) because using known betas (not estimating from sample data)\ndata_ships %$% \n  plot(x = X, y = Y, type = \"p\", main = \"Deterministic vs statistical relationship\")\ndata_ships %$% \n  points(x = X, y = f_X, col = \"red\", pch = 5)\ndata_ships %$% \n  lines(x = X, y = f_X, col = \"red\")\ntext(x = 18.5, y = 20, labels = \"f(X) = E(Y|X)\", col = \"red\")\n\n\n\n\n\n\n\nConstruction of statistical regression models\n\n\nSelection of predictor variables (how to decide which ones?).\n\nUse of outside information, historical knowledge, and/or experience.\nExploratory data analysis.\nVariable selection techniques → Find a subset of important variables (i.e. practical and easy to find).\n\n\n\nFunctional form of the regression relation (what is form of \\(f(X)\\)?).\n\n&lt; based on same info as (1) &gt;\nIf there is an abundance of data, maybe start with more complex models and then simplify.\n\n\n\nScope of model (when is the model useful?).\n\nWhen the model best predicts or describes the relationship between response and predictor variables.\n\n\n\nUses of statistical regression models\n\nDetermining whether an \\(X\\) “affects” \\(Y\\) or not.\nEstimation of impact of a given \\(X\\) on the \\(Y\\).\nEstimation of the mean of \\(Y\\) for a given \\(X\\) value.\nPrediction of a single value of \\(Y\\) for a given \\(X\\) value.\n\nTypical strategy for regression analysis\n\n\n1.1.1 Simple linear regression model (SLR)\nGoal of SLR\n\nInvestigate the relationship between \\(Y\\) and a single numeric independent variable \\(X\\), assuming that, in the population, the mean of \\(Y\\) is linearly related to the value of \\(X\\).\nPopulation relationship → \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).\nSample relationship → \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nData structure\n\nBoth \\(X\\) and \\(Y\\) on a random sample of \\(n\\) individuals are collected from the population of interest.\nThe resulting data has the form \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nHolds for all \\(n\\) values in the random sample of \\(n\\) pairs of values, \\((X_i, Y_i), \\, i = 1, \\ldots, n\\).\n\\(Y_i\\) → Dependent (or response) variable value. These are independent, but not identically distributed.\n\\(X_i\\) → Independent (or predictor) variable value. These are not random variables, rather known constants.\n\\(\\epsilon_i\\) → Random error term, assumed to have mean zero and variance \\(\\sigma^2\\). \\(\\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = \\mathrm{Corr}(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j : i \\ne j\\). Often, the \\(\\epsilon_i\\) are assumed to be independent and identically distributed \\((iid)\\), i.e. all have the same distribution with the same mean and variance.\n\\(\\beta_0\\) and \\(\\beta_1\\) → Fixed but unknown regression parameters that need to be estimated.\n\\(\\sigma^2\\) → Another parameter that needs estimated, but it is technically not a “regression” parameter since it does not determine the relationship between \\(Y\\) and \\(X\\) (i.e. it only deals with randomness).\nNote that \\(Y_i\\) and \\(\\epsilon_i\\) are random variables and therefore have distributions. Thus, discussing their mean and variances are appropriate.\nAlternate (equivalent) version of regression model\n\n\n\nResults\nDerivation\n\n\n\nModel statement\n\nCan use the deviation \\(X_i - \\bar{X}\\) as the predictor instead of \\(X_i\\). This leads to:\n\n\\[\nY_i = \\beta_0^* + \\beta_1 (X_i - \\bar{X}) + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\beta_0^* = \\beta_0 + \\beta_1 \\bar{X}\n\\]\n\nThis model can be useful in some derivations.\n\nLater results (jumping ahead)\n\n\nEstimated coefficients\n\nLSE for \\(\\beta_1\\) is the same as before.\nNow the LSE for \\(\\beta_0^*\\) can be found using \\(\\hat{\\beta}_0\\) :\n\n\n\n\\[\n\\hat{\\beta}_0^* = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X} = (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 \\bar{X} = \\bar{Y}\n\\]\n\nEstimated regression function\n\n\\[\n\\hat{Y} = \\bar{Y} - \\hat{\\beta}_1 (X - \\bar{X})\n\\]\n\n\nProperty of all regression functions\n\nUsing this alternative model, it is easy to see that all regression functions pass through the point \\((\\bar{X}, \\bar{Y})\\).\n\n\n\n\n\n\n\n\n\nSome implications of above\n\n\nResults\nDerivation\n\n\n\n\nMean of \\(Y_i\\) for given \\(X_i\\) → \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\)\nVariance of \\(Y_i\\) for given \\(X_i\\) → \\(V(Y_i) = \\sigma^2\\)\n\n\n\n\n\n\n\nInterpretation of regression parameters \\((\\beta_0, \\beta_1)\\)\n\n\n\\(\\beta_0\\)\n\n\\(Y\\)-intercept of the regression line and gives \\(Y\\)’s mean when \\(X = 0\\) → \\(E(Y \\mid X = 0) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\)\n\nOnly makes sense to interpret when \\(X=0\\) is within the scope of the model. So, ask two questions when deciding whether or not to interpret:\n\nDoes it make sense to interpret in context?\nDo we have data in the proximity of zero?\n\n\nIf answer “no” to either of these, then no need to interpret the intercept.\n\n\n\n\\(\\beta_1\\)\n\nSlope of the regression line and indicates the change in \\(Y\\)’s mean when \\(X\\) increases by one unit → \\(E(Y \\mid X = x^* + 1) - E(Y \\mid X = x^*) = [\\beta_0 + \\beta_1 (x^* + 1)] - [\\beta_0 + \\beta_1 x^*] = \\beta_1\\)\nDetermines whether a relationship exists between \\(Y\\) and \\(X\\).\nNote that regression does not substantiate or prove a cause-effect relationship. Rather it gives evidence that \\(Y\\) and \\(X\\) are related (but not that \\(X\\) “causes” the value of \\(Y\\)).\n\n\n\nModel scope (and the dangers of extrapolation)\n\nThe structural form of the relationship may not be the same as you move away from the observed predictor space. Therefore, using \\(\\hat{Y}\\) for estimation / prediction at \\(X\\) values outside of model scope would be extrapolation and results are not reliable.\nShould collect data from the entire region of predictor values of research interest.\n\n\nEstimators\n\n\nPurpose\n\nEstimators help us make inferences about population parameters.\nEach estimator corresponds to a population parameter (e.g. \\(\\hat{\\beta}_1 \\rightarrow \\beta_1\\)).\n\n\n\nEstimator / estimate distinction\n\nEstimators are based on formulas (e.g. \\(\\bar{X} = \\frac{1}{n} \\sum X_i\\)).\nEstimates are values computed from estimators.\n\n\n\nGood estimators\n\nBias of an estimator is the difference between the estimator’s expected value and the true value of the parameter being estimated:\n\nIf the bias equals zero, the estimator is said to be unbiased. \\[\\text{Bias} = E(\\hat{\\theta}) - \\theta\\]\n\ne.g. \\(\\bar{X}\\) and \\(\\mu\\) → \\(E(\\bar{X}) \\overset{\\surd} = \\mu\\).\nAnother way to think about it → \\(E(\\bar{X})\\) will be the center of the sampling distribution of \\(\\bar{X}\\) (which will be approximately normal because of the central limit theorem), and we want this to equal \\(\\mu\\).\n\n\n\nThe standard deviation of an estimator is referred to as the standard error of said estimator.\n\nIdeally, estimators will have relatively small standard deviations, in which case they are said to be efficient.\n\n\n\n\n\n1.1.2 Estimation of the regression function\nSetup\n\nFor each point we have an observed value \\(Y_i\\), a fitted value \\(\\hat{Y}_i\\) and a residual \\(\\hat{\\epsilon}_i\\).\nFitted regression function → \\(\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\), where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_0\\) are estimators of \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\nGoal\n\nGoal is to estimate the two “regression” parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nThere are several methods to do this.\n\n1.1.3 Method of least squares\nOverview\n\nThe method of least squares is one way to find “good” estimators of the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nFor each observation \\((X_i, Y_i)\\), this method considers the model error term, which is the deviation of \\(Y_i\\) from its expected value:\n\n\\[\n\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\n\\]\n\nThen we minimize the sum of some function of these errors:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\text{function of } \\epsilon_i \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - E(Y_i)\\big) \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - (\\beta_0 + \\beta_1 X_i)\\big) \\quad\\quad \\text{&lt; for SLR &gt;}\n\\end{align*}\n\\]\n\nFor least squares method specifically, we consider the sum of the \\(n\\) squared errors (deviations). Thus we have:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\epsilon_i^2 \\\\\n    &= \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\end{align*}\n\\]\n\n\nAccording to the method of least squares, the point estimators (functions of the random sample that estimate population quantities) of \\(\\beta_0\\) and \\(\\beta_1\\) are those values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), respectively, that minimize the criterion \\(Q\\) for the sample observations \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\nOnce data is collected, the estimates (realized values) for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are calculated based on the observed sample \\((x_1, y_1), \\ldots, (x_n, y_n)\\). (Note: no good notation for this, maybe \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\); similar idea for means: \\(\\mu\\), \\(\\bar{X}\\) and \\(\\bar{x}\\))\n\nLeast squares estimators\n\n\nThese can be found in two ways.\n\nNumerical methods (optimization) → Search procedures to systematically evaluate the least squares criterion \\(Q\\) for different estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) until the ones that minimize \\(Q\\) are found.\nAnalytical methods → Using calculus. The analytical approach is feasible when the regression model is not mathematically complex.\n\n\n\nThe general process for deriving the least squares estimators of any model is:\n\nWrite \\(Q\\) as a function of the model error term \\(\\epsilon_i\\) as shown above.\nTake derivative with respect to the desired parameter \\(\\beta_i\\).\nSet derivative equal to zero (notation: substituting population term \\(\\beta_i\\) for its estimator \\(\\hat{\\beta}_i\\)) and solve for \\(\\hat{\\beta}_i\\).\nCheck second derivative test to ensure global minimum (take derivative from result of step 2)\n\n\nUsing the analytical approach with the SLR model, we have the following results:\n\n\n\nResults\nDerivation\n\n\n\n\nThe estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are found by solving the simultaneous equations (called the normal equations):\n\n\\[\n\\begin{align*}\n  \\sum Y_i &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum X_i \\\\\n  \\sum X_i Y_i &= \\hat{\\beta}_0 \\sum X_i + \\hat{\\beta}_1 \\sum X_i^2\n\\end{align*}\n\\]\n\nThis leads to\n\n\\[\n\\begin{align*}\n  \\text{Intercept} \\hspace{10pt} \\hat{\\beta}_0 \\hspace{10pt} &= \\hspace{10pt} \\frac{1}{n}\\sum Y_i + \\hat{\\beta}_1 \\frac{1}{n} \\sum X_i \\hspace{10pt} = \\hspace{10pt} \\bar{Y}- \\hat{\\beta}_1 \\bar{X} \\\\\n  \\text{Slope} \\hspace{10pt} \\hat{\\beta_1} \\hspace{10pt} &= \\hspace{10pt} \\frac{\\sum X_i Y_i -\\frac{1}{n} \\sum X_i Y_i}{\\sum X_i^2 - \\frac{1}{n}(\\sum X_i)^2} \\hspace{10pt} = \\hspace{10pt} \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \\hspace{10pt} = \\hspace{10pt} \\frac{S_{XY}}{S_{XX}}\n\\end{align*}\n\\]\n\nNote: We did not have to assume any distribution of the error term. These are the LSE estimators for any SLR model.\n\n\n\n\n\n\n\n\n\nDemo:\n\nFirst we can generate the sample data \\((x_1, y_1), \\ldots, (x_n, y_n)\\).\n\n\n\n\nCode# simulate sample regression dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = B0 + B1X + E\n# -&gt; Y | X ~ Uniform(B0 + B1*X - a, B0 + B1*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nmin &lt;- -2\nmax &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate error terms\n# -&gt; assumption for LSE is E(epsilon) = 0, so symmetric uniform about zero works\nepsilon &lt;- runif(n = n, min = min, max = max)\n\n# calculate y terms as function of x and error terms\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n\n\nCode# display observed sample data\ndata.frame(x, y) %&gt;% \n  head(10) %&gt;% \n  display_nice(col.names = c(\"x\", \"y\"))\n\n\n\n x \n    y \n  \n\n\n 9.858 \n    20.797 \n  \n\n 8.013 \n    16.761 \n  \n\n 7.225 \n    14.231 \n  \n\n 6.607 \n    13.135 \n  \n\n 6.325 \n    14.399 \n  \n\n 14.171 \n    27.823 \n  \n\n 12.959 \n    27.244 \n  \n\n 8.075 \n    18.832 \n  \n\n 12.601 \n    24.244 \n  \n\n 7.384 \n    15.549 \n  \n\n\n\nCode# plot sample data\nplot(x = x, y = y)\n\n\n\n\n\n\n\n\nThen we can fit the model and visualize it on the scatter plot.\n\n\nCode# add regression line to plot\nplot(x = x, y = y)\nabline(lm(formula = y ~ x), col = \"red\")\ntext(x = 14, y = 22, labels = bquote(hat(Y) * \" = \" * hat(beta)[0] + hat(beta)[1] * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nFinally we can get the estimates of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\n\n\nR Functions\nManual\n\n\n\n\n# fit SLR model\n# -&gt; lm() calls lm.fit() behind the scenes --&gt; this performs a QR decomposition to fit the model, which is a more stable way to solve for LSE\nmod_slr &lt;- lm(formula = y ~ x)\n\n# display estimated coefficients\nmod_slr$coefficients\n\n(Intercept)           x \n   1.518714    1.937210 \n\n\n\n# or can do\ncoefficients(mod_slr)\ncoef(mod_slr) # alias for coefficients()\n\nThese are estimates of the population coefficients, so can compare above values to \\(\\beta_0\\) = 1 and \\(\\beta_1\\) = 2.\nProgramming note → Beautiful sites ((1) and (2)) explaining QR decomposition and how it relates to LSE; must go through and recreate at some point.\n\n\n\n# calculate estimates for B0 and B1 using LSE formulas\n# -&gt; b_1 = S_XY / S_XX\n# -&gt; b_0 = Y-bar - b_1 X-bar\ns_xy &lt;- sum((x - mean(x)) * (y - mean(y)))\ns_xx &lt;- sum((x - mean(x))^2)\nb_1 &lt;- s_xy / s_xx\nb_0 &lt;- mean(y) - b_1 * mean(x)\n\n\nCode# compare to results from lm()\ncompare(c(b_0, b_1), mod_slr$coefficients)\n\n$comparison\n[1] TRUE\n\n$`c(b_0, b_1)`\n[1] 1.518714 1.937210\n\n$`mod_slr$coefficients`\n(Intercept)           x \n   1.518714    1.937210 \n\n\n\n\n\nLSE for other models\n\nCan also derive LSE estimators for other models using the same process.\n\nExample 1\n\nRegression through the origin: \\(Y = \\beta X + \\epsilon\\).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X + a, B*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -5, max = 5)\n\n# Y | X ~ Uniform(min = BX + a, max = BX + b)\ny &lt;- runif(n = n, min = beta * x + min, max = beta * x + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 3, y = -5, labels = bquote(hat(Y) * \" = \" * hat(beta) * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model without intercept\nmod_noint &lt;- lm(formula = y ~ -1 + x)\n\n# display estimated coefficient\ncoefficients(mod_noint)\n\n       x \n2.093513 \n\n\n\n\n\n# calculate estimate B using LSE formula\nb &lt;- sum(x * y) / sum(x^2)\n\n\nCode# compare to results from lm()\ncompare(b, coefficients(mod_noint))\n\n$comparison\n[1] TRUE\n\n$b\n[1] 2.093513\n\n$`coefficients(mod_noint)`\n       x \n2.093513 \n\n\n\n\n\n\n\nExample 2\n\nRegression through the origin with squared predictor variable: \\(Y = \\beta_1 X^2 + \\epsilon\\).\nSometimes a transformation can make deriving estimators easier. This way we can get the model in a familiar form and derive like usual (simpler). Then we just have to make the appropriate substitutions at the end in order to get the estimators for the original models.\nThis strategy lessens the number of models we have to know how to derive (note this is just for deriving; no need to do it when fitting models because it’s an extra middle step that doesn’t change anything, leads to equivalent results).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin with squared predictor dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X^2 + a, B*X^2 + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -1, max = 5)\n\n# Y | X ~ Uniform(min = BX^2 + a, max = BX^2 + b)\ny &lt;- runif(n = n, min = beta * x^2 + min, max = beta * x^2 + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 4, y = 15, labels = bquote(hat(Y) * \" = \" * hat(beta) * X^2) , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model with no intercept and squared X\n# -&gt; use asis function I() to treat `^` as an arithmetic operator and not a formula operator\n# -&gt; save design matrix for later demo\nmod_noint_squared &lt;- lm(formula = y ~ -1 + I(x^2), x = TRUE)\n\n# display estimated coefficient\ncoef(mod_noint_squared)\n\n  I(x^2) \n1.964126 \n\n\n\n\n\n# calculate estimates B using LSE formula\nb &lt;- sum(x^2 * y) / sum(x^4)\n\n\nCode# compare to results from lm()\ncompare(b, coef(mod_noint_squared))\n\n$comparison\n[1] TRUE\n\n$b\n[1] 1.964126\n\n$`coef(mod_noint_squared)`\n  I(x^2) \n1.964126 \n\n\n\n# demonstrating that transformed model way is equivalent\n# -&gt; square predictor variable\nx_star &lt;- x^2\nmod_transformed &lt;- lm(formula = y ~ -1 +  x_star, x = TRUE)\n\n# compare design matrices of two models\ncompare(head(mod_noint_squared$x), head(mod_transformed$x))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; Component \\\"dimnames\\\": Component 2: 1 string mismatch &gt;\"\n\n$comparison$`element-wise`\nNULL\n\n\n$`head(mod_noint_squared$x)`\n      I(x^2)\n1  0.4701793\n2  7.8662515\n3  2.8222557\n4 16.0135448\n5  4.6882342\n6  1.7800644\n\n$`head(mod_transformed$x)`\n      x_star\n1  0.4701793\n2  7.8662515\n3  2.8222557\n4 16.0135448\n5  4.6882342\n6  1.7800644\n\n# calculate estimates B using LSE formula\nb_star &lt;- sum(x_star * y) / sum(x_star^2)\n\n# compare betas from usual method and transformed model way\ncompare(b, b_star)\n\n$comparison\n[1] TRUE\n\n$b\n[1] 1.964126\n\n$b_star\n[1] 1.964126\n\n\n\n\n\nProperties of least squares estimators\n\n\nGauss-Markov theorem: If the conditions of the SLR model hold (i.e. \\(Y_i= \\beta_0 + \\beta_1 X_i + \\epsilon_i\\), where \\(\\epsilon_i\\) have mean zero, variance \\(\\sigma^2\\) and are uncorrelated), then the LSE \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the Best Linear Unbiased Estimators (BLUE) (note that \\(\\epsilon_i\\) do not have to be normal).\n\nLinear estimators → Through lots of algebra (shown later), \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta}_1\\) can be written as be written as a linear combination of the \\(Y_i\\): \\(\\sum k_i Y_i\\), where \\(k_i\\) are constant. Thus they are linear estimators.\nUnbiased estimators → Their expected values are \\(\\beta_0\\) and \\(\\beta_1\\), respectively. So neither estimator tends to overestimate or underestimate systematically.\nBest → More precise than all other linear unbiased estimators of \\(\\beta_0\\) and \\(\\beta_1\\) (smaller variance of the sampling distributions).\n\n\n\n1.1.4 Point estimation of the mean response\n\nFor the population relationship \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\), we have the regression function \\(E(Y) = \\beta_0 + \\beta_1 X\\) (because \\(E(\\epsilon) = 0\\)). This is estimated with \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nTerms and what they represent:\n\n\\(Y\\) is a value of the response variable; the observed value.\n\\(E(Y)\\) the mean response. So it is the center of the probability distribution of \\(Y\\) corresponding to the level \\(X\\) of the predictor variable (so technically a conditional expected value \\(E(Y \\mid X)\\).\n\\(\\hat{Y}\\) is a point estimator of the mean response when the level of the predictor variable is \\(X\\); the fitted value.\n\n\n\nApplication of Gauss-Markov theorem\n\n\n\\(\\hat{Y}\\) is the BLUE for \\(E(Y)\\) with \\(V(\\hat{Y}) = V(\\epsilon) = \\sigma^2\\).\n\n\n\n1.1.5 Residuals and model errors\n\nModel error term \\(\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\\) → Measures the difference between an observation and its expected value (unknown true regression line). It is unknown / unobservable.\nResidual \\(\\hat{\\epsilon}_i = e_i = Y_i - \\hat{Y}_i\\) → This is a known, observable estimate of the unobservable model error. Measures the deviation of the observed value from the fitted regression function.\n\n\n\nResiduals are very useful for studying whether the given regression model is appropriate for the data.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#estimation-of-error-terms-variance",
    "href": "notes-slr.html#estimation-of-error-terms-variance",
    "title": "1  Simple linear regression",
    "section": "\n1.2 Estimation of error terms variance",
    "text": "1.2 Estimation of error terms variance\nNeed to estimate the variance \\(\\sigma^2\\) of the error terms \\(\\epsilon_i\\) in a regression model to get an indication of the variability of the probability distributions of \\(Y\\). Also several inferences for models require an estimate of \\(\\sigma^2\\).\nMotivation\n\nFor a single population, the variance \\(\\sigma^2\\) is estimated by the sample variance \\(S^2\\), which is an unbiased estimator. We find \\(S^2\\) by taking the sum of the squared deviations of the observed value and the estimated mean (sum of squares) and then dividing it by the degrees of freedom \\(n - 1\\) (one df is lost when estimating \\(\\mu\\); mean square) .\n\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (Y_i - \\bar{Y})^2}{n - 1}\n\\]\nRegression model\n\nSame logic as above, except we use the residuals as the deviations because each \\(Y_i\\) comes from a different probability distribution with different \\(X\\) (depends on the \\(X_i\\) level).\n\n\\[\n\\text{Error (residual) sum of squares} \\hspace{10pt} SSE = \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 = \\sum_{i = 1}^n e_i^2\n\\]\n\nThen divide by the \\(df = n - 2\\) to the mean square (two dfs are lost when because \\(\\beta_0\\) and \\(\\beta_1\\) need to be estimated when getting the estimated means \\(\\hat{Y_i}\\)).\n\n\\[\n\\text{Error (residual) mean square} \\hspace{10pt} S^2 = MSE = \\frac{SSE}{n - 2} = \\frac{\\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2}{n - 2} = \\frac{\\sum_{i = 1}^n e_i^2}{n - 2}\n\\]\n\nIt will be shown later that \\(MSE\\) is an unbiased estimator for \\(\\sigma^2\\) → \\(E(MSE) = \\sigma^2\\).\nAn estimator of the standard deviation is simply \\(S = \\sqrt{MSE}\\).\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# simulate dataset (using uniform errors)\n# initialize items, generate X values, errors and y values\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; min &lt;- -2; max &lt;- 2\nx &lt;- runif(n = n, min = 5, max = 15)\nepsilon &lt;- runif(n = n, min = min, max = max)\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# display model summary -&gt; looking for residual standard error = S = sqrt(MSE)\n# -&gt; save to object to extract it\n(summ &lt;- summary(mod))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.74939 -1.03730  0.07757  1.06263  1.96274 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  1.27612    0.81392   1.568               0.128    \nx            1.96528    0.07683  25.580 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.241 on 28 degrees of freedom\nMultiple R-squared:  0.959, Adjusted R-squared:  0.9575 \nF-statistic: 654.3 on 1 and 28 DF,  p-value: &lt; 0.00000000000000022\n\n# display S and MSE = S^2\nsumm$sigma\n\n[1] 1.241054\n\nsumm$sigma^2\n\n[1] 1.540216\n\n\nThese are estimates of population quantities for the error terms, and when simulating the data we assumed \\(\\epsilon_i \\sim \\text{Uniform}\\,(a, b) \\Longrightarrow \\sigma^2 = V(\\epsilon_i) = \\frac{(b - a)^2}{12}\\). For \\(a\\) = -2 and \\(b\\) = 2, \\(V(\\epsilon_i)\\) = 1.333.\n\n\n\n# SIDENOTE -&gt; showing multiple equivalent ways to do the same thing (just keep the last way)\n\n# get fitted values using functions\ny_hat &lt;- mod$fitted.values\ny_hat &lt;- fitted(mod)\ny_hat &lt;- fitted.values(mod) # alias for fitted()\ny_hat &lt;- predict(mod) # discussed more later\n\n# extract residuals using functions (or some manual calculations)\ne &lt;- residuals(mod)\ne &lt;- resid(mod) # alias for residuals()\ne &lt;- mod$residuals\ne &lt;- y - y_hat\n\n# calculate SSE and df\nsse &lt;- sum(e^2)\ndf &lt;- mod$df.residual # extract from model object\ndf &lt;- df.residual(mod)\ndf &lt;- summ$df[2] # extract from model summary -&gt; error df is the second item (even if have MLR model)\ndf &lt;- n - 2 # for SLR (with intercept)\n\n# calculate MSE\nmse &lt;- sse / df\n\n# calculate residual standard deviation\ns &lt;- sqrt(mse)\n\n# compare to results from summary(lm())\n# -&gt; s\ncompare(summ$sigma, s)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma`\n[1] 1.241054\n\n$s\n[1] 1.241054\n\n# -&gt; MSE\ncompare(summ$sigma^2, mse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2`\n[1] 1.540216\n\n$mse\n[1] 1.540216\n\n# -&gt; SSE = MSE * df\ncompare(summ$sigma^2 * summ$df[2], sse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2 * summ$df[2]`\n[1] 43.12605\n\n$sse\n[1] 43.12605",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#normal-error-regression-model",
    "href": "notes-slr.html#normal-error-regression-model",
    "title": "1  Simple linear regression",
    "section": "\n1.3 Normal error regression model",
    "text": "1.3 Normal error regression model\nLeast squares results\n\nNo matter what may be the form of the distribution of the error terms \\(\\epsilon_i\\) (and thus of the \\(Y_i\\)), the LSE provides unbiased point estimators of \\(\\beta_0\\) and \\(\\beta_1\\), that have minimum variance among all unbiased linear estimators (BLUEs).\n\nAssumptions on error term distribution\n\nThese assumptions on \\(\\epsilon_i\\) are needed to set up interval estimates and make tests.\nThe standard assumption is that the error terms are normally distributed. This greatly simplifies the theory of regression analysis and is justifiable in many real-world situations where regression analysis is applied.\n\nNew regression model\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\n\\]\n\nThis is the same regression model as before, just with specified error distribution now (\\(iid\\) = independent and identically distributed).\nBecause this model assumes normal errors, the assumption of uncorrelatedness of the original (unspecified error) model now becomes an independence assumption (\\(\\perp \\!\\!\\! \\perp\\Longrightarrow \\mathrm{Corr}(\\epsilon_i,\\epsilon_j) = 0\\). So the outcome in anyone trial has no effect on the error term for any other trial (in terms of positive or negative, small or large).\nThis model means \\(Y_i \\overset{\\perp \\!\\!\\! \\perp}\\sim \\text{Normal}\\,\\) with \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\) and \\(V(Y_i) = \\sigma^2\\).\n\n\nJustification of the normality assumption.\n\nError terms frequently represent the effects of factors omitted from the model that affect the response to some extent and that vary at random without reference to the variable \\(X\\).\nThese random effects have a degree of mutual independence, the composite error term representing all these factors tends to normal as the number of factors becomes large (by the CLT).\nAlso, the estimation and testing procedures shown later are based on the \\(t\\) distribution and are usually only sensitive to large departures from normality. So, unless the departures from normality are serious, particularly with respect to skewness, the actual confidence coefficients and risks of errors will be close to the levels for exact normality.\n\n\n1.3.1 Estimation of paramters by method of maximum likelihood\nOverview\n\n\nThe method of maximum likelihood chooses as estimates values of the parameters that are most consistent with the sample data. The measure of consistency is the product of densities and is called the likelihood value \\(L(\\mu)\\).\n\nIf the value of \\(\\mu\\) is consistent with the sample data \\(\\Longrightarrow\\) densities relatively large \\(\\Longrightarrow\\) large likelihood value. If not, both will be small.\n\n\n\n\n\nJust like the LSE, there are two ways to find MLEs: a systematic numerical search and by use of an analytical solution.\n\nThe product of the densities viewed as a function of the unknown parameters is called the likelihood function.\n\nIf the likelihood function is relatively peaked in the neighborhood of the maximum, then the MLE estimate is precise because values of \\(\\mu\\) not near the MLE are much less consistent with the data.\nWhen it is relatively flat in a fairly wide region around the MLE, many values of the parameter are almost as consistent with the sample data as the MLE and therefore the MLE is relatively imprecise.\n\n\n\nSteps to MLEs\n\n\nFor multivariate parameter vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_{k})\\), write the likelihood function (i.e. joint density function) and the log-likelihood function\n\n\n\n\\[\nL(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\prod_{i = 1}^n f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\hspace{20pt} \\rightarrow \\hspace{20pt} \\ell(\\boldsymbol{\\theta}) = \\ln[L(\\boldsymbol{\\theta} \\mid \\mathbf{y})]\n\\]\n\nOptimize the log-likelihood function by taking the partial derivatives with respect to each parameters of interest \\(\\theta_1, \\ldots, \\theta_{k}\\).\n\n\nSet to zero and solve for each parameter of interest.\n\n\\[\n\\text{For } j = 1, \\ldots, k \\hspace{20pt} \\ell'(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\theta_j} \\ell(\\boldsymbol{\\theta}) = 0 \\hspace{20pt} \\rightarrow \\hspace{20pt} \\hat{\\boldsymbol{\\theta}} = (\\hat{\\theta}_1, \\ldots, \\hat{\\theta}_{k}) = \\text{potential MLE}\n\\]\n\nVerify that the global maximum of the log-likelihood function occurs at \\(\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}\\).\nFind the second derivative of the log-likelihood function, then plug in \\(\\hat{\\boldsymbol{\\theta}}\\) and see if less than zero.\n\nMLEs for normal error regression model\n\nGenerally\n\n\n\nResults\nDerivation\n\n\n\n\n\n\n\n\n\nParameter\nMLE\n\n\n\n\\(\\beta_0\\)\n\n\\(\\hat{\\beta}_0 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\beta_1\\)\n\n\\(\\hat{\\beta}_1 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\sigma^2\\)\n\\(\\displaystyle \\hat{\\sigma}^2 = \\frac{\\sum (Y_i - \\hat{Y_i})^2}{n}\\)\n\n\n\nNotes\n\n\nProperties of estimators\n\nSince MLEs for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the same as the LSE estimators, they have the same properties: BLUE (unbiased and minimum variance in class of unbiased linear estimators).\nAnd because they are MLEs, we can also say they are: consistent (\\(\\hat{\\beta}_i \\overset{p}\\rightarrow \\beta_i\\), converge in probability to their respective parameters); sufficient (captures all of the information about \\(\\beta_i\\) contained in the sample); minimum variance in class of unbiased estimators (linear or otherwise).\n\n\n\n\n\nMLE \\(\\hat{\\sigma}^2\\) and MSE\n\nRelationship \\(S^2 = MSE = \\frac{n}{n - 2} \\hat{\\sigma}^2 \\hspace{20pt} \\Longleftrightarrow \\hspace{20pt} \\hat{\\sigma}^2 = \\frac{n - 2}{n} MSE\\). However, for large \\(n\\), the difference is small.\nMLE \\(\\hat{\\sigma}^2\\) is biased → \\(E(\\hat{\\sigma}^2) = \\frac{n - 2}{n} \\sigma^2\\), which is an underestimation of \\(\\sigma^2\\). Because of this, MSE is generally used because it is unbiased. However, \\(\\hat{\\sigma}^2\\) is asymptotically unbiased (as \\(n \\rightarrow \\infty\\)) and has a smaller variance than MSE, which may be preferred in some scenarios (tradeoff between bias and precision).\n\n\nGenerally MLE gives the same results as LSE.\n\n\n\n\n\n\n\nCan also derive MLEs for transformed models using the same strategy as with LSE.\n\nExample: Suppose we have \\(\\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\). We can transform \\(Y_i^* = \\beta_0^* + \\beta_1^* X_i + \\epsilon_i\\), derive like usual, then substitute at end to get untransformed estimators.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#inferences-concerning-beta_1",
    "href": "notes-inference.html#inferences-concerning-beta_1",
    "title": "2  Inference",
    "section": "\n2.1 Inferences concerning \\(\\beta_1\\)\n",
    "text": "2.1 Inferences concerning \\(\\beta_1\\)\n\nOverview\n\n\nTest on slope \\(\\beta_1\\) and the implications\n\nWe often want to make inferences about \\(\\beta_1\\). A common test on \\(\\beta_1\\) has the form below.\nIf \\(\\beta_1 = 0\\) \\(\\Longrightarrow\\) Regression line in horizontal, which means there is no linear association between \\(Y\\) and \\(X\\), and even more no relation of any type because all probability distributions of \\(Y\\) are identical at all levels of \\(X\\): normal with \\(E(Y) = \\beta_0 + (0) X = \\beta_0\\) and variance \\(\\sigma^2\\).\n\n\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\n\n2.1.1 Sampling distribution of \\(\\hat{\\beta}_1\\)\n\n\n\nResults\nDerivation\n\n\n\nSampling distribution of \\(\\hat{\\beta}_1\\)\n\nRefers to distribution of \\(\\hat{\\beta}_1\\) from repeated sampling when the levels of the predictor variable \\(X\\) are held constant from sample to sample.\nRecall \\(\\displaystyle \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\); this is the point estimator.\nDistribution of \\(\\hat{\\beta}_1\\) is Normal with mean and variance:\n\n\\[\n\\begin{align*}\n   E(\\hat{\\beta}_1) &= \\beta_1 \\\\\n  V (\\hat{\\beta}_1) &= \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2}\n\\end{align*}\n\\]\n\nThen we can estimate the variance by replacing the parameter \\(\\sigma^2\\) with \\(MSE\\), the unbiased estimator of \\(\\sigma^2\\). This gives us \\(S^2_{\\hat{\\beta}_1}\\), which is an unbiased estimator for the variance of the sampling distribution of \\(\\hat{\\beta}_1\\). And we can take the positive square root to give us \\(S_{\\hat{\\beta}_1}\\), which is the point estimator of \\(\\sigma_{\\hat{\\beta}_1}\\).\n\n\\[\n  S^2_{\\hat{\\beta}_1} = \\frac{MSE}{\\sum(X_i - \\bar{X})^2} = \\frac{MSE}{S_{XX}} \\hspace{20pt} \\longrightarrow \\hspace{20pt} s_{\\hat{\\beta}_1} = \\sqrt{\\frac{MSE}{S_{XX}}} = \\frac{S}{\\sqrt{S_{XX}}}\n  \\]\n\n\nThus, \\(S^2_{\\hat{\\beta}_1}\\) is an unbiased estimator for the variance of the sampling distribution of \\(\\hat{\\beta}_1\\) and\n\nThis is also called the standard error (another way to think about it is the standard deviation of the sampling distribution) → \\(S_{\\hat{\\beta}_1} = SE(\\hat{\\beta}_1)\\)\n\n\n\n\n\n\nMain result for normal, mean and variance\n\n\n\\(\\hat{\\beta}_1\\) can be written as a linear combination of the observations \\(Y_i\\):\n\n\\[\n\\hat{\\beta_1}=\\sum k_i Y_i, \\hspace{20pt} \\text{where} \\hspace{10pt} k_i = \\frac{X_i - \\bar{X}}{\\sum(X_i - \\bar{X})^2}\n\\]\n\n\\(\\sum \\text{Normal} \\sim \\text{Normal}\\,\\), so we know \\(\\hat{\\beta}_1\\) must be Normal. Then properties of the coefficients \\(k_i\\) can be used to show the mean and variance.\nNote that \\(k_i\\)’s are fixed quantities because they are a function of only \\(X_i\\)’s (which are fixed quantities). Therefore, \\(\\hat{\\beta}_1\\) is a linear combination of \\(Y_i\\), where the coefficients are solely a function of the fixed \\(X_i\\) (this is why \\(\\hat{\\beta}_1\\) is a linear estimator).\n\nProperties of coefficients \\(k_i\\):\n\nProof of linear combination:\n\nProofs of properties of \\(k_i\\):\n\nProof of mean:\n\nProof of variance:\n\nCan also prove that \\(\\hat{\\beta}_1\\) has minimum variance among all unbiased linear estimators.\n\n\n\n\n\n2.1.2 Sampling distribution of \\((\\hat{\\beta}_1 - \\beta_1) / S_{\\hat{\\beta}_1}\\)\n\n\n\nResults\nDerivation\n\n\n\n\\[\n\\frac{\\hat{\\beta}_1 - \\beta_1}{S_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sqrt{MSE / S_{XX}}} \\sim \\text{t}\\,_{n - 2}\n\\]\n\n\nDeriving the distribution of the standardized slope → \\(\\displaystyle \\frac{\\hat{\\beta}_1 - E(\\hat{\\beta}_1)}{SE(\\hat{\\beta}_1)}\\)\n\nTechnically when standardizing with an estimated standard deviation it is referred to as studentized statistic.\n\n\n\n\n\n\n\n2.1.3 Confidence interval for \\(\\beta_1\\)\n\nForming interval\n\\[\n\\begin{align*}\n  & P(-t_{\\alpha/2, n-2} \\le \\frac{\\hat{\\beta}_1 - \\beta_1}{S_{\\hat{\\beta}_1}} \\le t_{\\alpha/2, n-2}) = 1 - \\alpha \\\\\n  & \\Longleftrightarrow \\hspace{20pt} P(\\hat{\\beta}_1 - t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1} \\le \\frac{ - \\beta_1}{S_{\\hat{\\beta}_1}} \\le \\hat{\\beta}_1 + t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1}) = 1 - \\alpha \\\\\n  & \\Longleftrightarrow \\hspace{20pt}  100(1 - \\alpha)\\% \\text{ CI } = \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1} \\hspace{10pt} = \\hspace{10pt} \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{MSE / S_{XX}}\n\\end{align*}\n\\]\nInterpretation\n\nWith &lt; \\(100(1 - \\alpha)\\) &gt;% confidence, we estimate that the average &lt; \\(Y\\) context &gt; increases by between &lt; lower bound &gt; and &lt; upper bound &gt; for each additional unit increase in &lt; \\(X\\) context &gt;.\nRemember the scope of the regression model is restricted to some range of values of the predictor variable → May not be reasonable to use these slope estimates outside this range as the regression relation may not be linear then.\n\nDemo:\n\n\nR functions\nManual\n\n\n\n\n# simulate sample dataset for normal error regression model\n# -&gt; E ~ Normal(0, sigma^2)\n# -&gt; Y = B0 + B1*X + E\n# -&gt; Y | X ~ Normal(B0 + B1*X, sigma^2)\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 50\nbeta_0 &lt;- 2\nbeta_1 &lt;- 5\nsigma2 &lt;- 10\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 10)\n\n# generate error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sqrt(sigma2))\n      \n# calculate observations Y | X ~ Normal(B0 + B1*X, sigma^2)\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# plot sample data with regression line\nplot(x = x, y = y)\nabline(lm(formula = y ~ x ), col = \"red\")\n\n\n\n\n\n\n# display error terms\nx_plot &lt;- seq(from = -3 * sqrt(sigma2), to = 3 * sqrt(sigma2), by = 0.01)\nhist(epsilon, freq = FALSE, xlab = bquote(epsilon * \"~ Normal (\" * mu * \",\" * sigma^2 * \")\"), main = \"\")\nlines(x = x_plot,\n      y = dnorm(x = x_plot, mean = 0, sd = sqrt(sigma2)))\n\n\n\n\n\n\n\n\n# fit model\nmod_normal &lt;- lm(y ~ x)\n\n# display only coefficient summaries of slope\nsummary(mod_normal)$coefficients[2,] # Estimate is the middle of the interval\n\n                      Estimate                     Std. Error \n 4.983704923403577957685683941  0.300016865499538709638471801 \n                       t value                       Pr(&gt;|t|) \n16.611415878589134820231265621  0.000000000000000000001552572 \n\n# calculate confidence interval for slope\nalpha &lt;- 0.05\nconfint(mod_normal, parm = \"x\", level = 1 - alpha)\n\n     2.5 %   97.5 %\nx 4.380481 5.586929\n\n\n\n\n\n# using R functions to get the needed values (can reference how to calculate these manually above)\n\n# extract / calculate needed items\n# -&gt; point estimate = beta1-hat\n# -&gt; critical value = t_alpha/2, n-2\n# -&gt; SE(beta1-hat) = MSE / S_XX = S / sqrt(S_XX)\npe &lt;- as.numeric(coef(mod_normal)[\"x\"]) # SIDENOTE -&gt; as.numeric() just to remove the named number data type\nt_crit &lt;- abs(qt(p = alpha / 2, df = mod_normal$df.residual))\ns &lt;- summary(mod_normal)$sigma\ns_xx &lt;- sum((x - mean(x))^2)\nse_beta1_hat &lt;- s / sqrt(s_xx)\n\n# compare to Std. Error from summary of model coefficients\ncompare(summary(mod_normal)$coefficients[\"x\",\"Std. Error\"], se_beta1_hat)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"Std. Error\"]`\n[1] 0.3000169\n\n$se_beta1_hat\n[1] 0.3000169\n\n# calculate confidence interval for beta1\nci_limits &lt;- c(lower = pe - t_crit * se_beta1_hat, upper = pe + t_crit * se_beta1_hat)\n\n# compare to results from confint(lm())\ncompare(confint(mod_normal, parm = \"x\", level = 1 - alpha), ci_limits)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for target but not for current &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is matrix, current is numeric\"                             \n\n$comparison$`element-wise`\nNULL\n\n\n$`confint(mod_normal, parm = \"x\", level = 1 - alpha)`\n     2.5 %   97.5 %\nx 4.380481 5.586929\n\n$ci_limits\n   lower    upper \n4.380481 5.586929 \n\n\n\n\n\n\n2.1.4 Tests concerning \\(\\beta_1\\)\n\nOverview\n\nThe test shown below is called a test of utility of the model.\nIf reject → We conclude that \\(X\\) does contribute information for the prediction of \\(Y\\) when using the straight-line model.\n\nIf fail to reject → Then we conclude there is no linear relationship between \\(Y\\) and \\(X\\) (horizontal model). But keep in mind:\n\nAdditional data might indicate that \\(\\beta_1\\) differs from zero.\nA more complex relationship between \\(Y\\) and \\(X\\) may exist, which would require fitting a model other than the straight-line model.\n\n\nAll assumptions about the error terms (\\(\\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\\)) should be satisfied.\n\nTwo-tailed test (most common)\n\nHypotheses\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\nTest statistic\n\n\\[\nTS = t^* = \\frac{\\hat{\\beta}_1 - 0}{S_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1}{\\sqrt{MSE / S_{XX}}}\n\\]\n\nRejection region and p-value\n\n\\[\n\\begin{align*}\n  RR &= \\{\\lvert t^* \\rvert &gt; t_{\\alpha/2, n - 2}\\} \\\\\n  p\\text{-value} &= 2 \\cdot P(t_{n-2} \\ge \\lvert t^* \\rvert)\n\\end{align*}\n\\]\n\n\nDecision\n\nReject \\(H_0\\) and conclude \\(H_A\\) if \\(\\hspace{10pt}\\) \\(TS \\in RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} \\le \\alpha\\)\nFail to reject \\(H_0\\) if \\(\\hspace{10pt}\\) \\(TS \\notin RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} &gt; \\alpha\\)\nCan also look at the \\(100(1 - \\alpha)\\%\\) CI for \\(\\beta_1\\) to see if contains 0.\n\n\n\nConclusion / Interpretation\n\nAt the \\(\\alpha\\) significance level, we &lt; have / do not have &gt; sufficient evidence of a significant linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;. &lt; if yes… &gt; This is a &lt; positive / negative &gt; linear relationship, indicating that as &lt; \\(X\\) context &gt; increases, &lt; \\(Y\\) context &gt; &lt; increases / decreases &gt;, on average.\n\n\n\nOther tests\n\n\nOne-tailed tests\n\n\\(H_A: \\beta_1 &lt; 0 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} RR = \\{t^*&lt; t_{\\alpha, n - 2}\\} \\hspace{10pt} \\text{and} \\hspace{10pt} p\\text{-value} = P(t_{n-2} \\le t^*)\\)\n\\(H_A: \\beta_1 &gt; 0 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} RR = \\{t^* &gt; t_{\\alpha, n - 2}\\} \\hspace{10pt} \\text{and} \\hspace{10pt} p\\text{-value} = P(t_{n-2} \\ge t^*)\\)\n\n\n\nTests against specified nonzero value of \\(\\beta_{1,0}\\)\n\n\\(TS = t^* = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{S_{\\hat{\\beta}_1}}\\)\nRR, \\(p\\)-value and decisions are the same, just based on new \\(t^*\\).\n\n\n\nDemo:\n\n\nR functions\nManual\nSimulation\n\n\n\n\n# using same model from the confidence interval for beta1 demo\n\n# display model summary, focusing on coefficient summaries for slope\n# -&gt; looking for t value (if doing traditional method test with RR) and Pr(&gt;|t|) = p-value\nsummary(mod_normal) # compare p-value to alpha\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0335 -2.2447  0.1801  2.0837  6.1565 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)    1.956      2.372   0.825               0.414    \nx              4.984      0.300  16.611 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.983 on 48 degrees of freedom\nMultiple R-squared:  0.8518,    Adjusted R-squared:  0.8487 \nF-statistic: 275.9 on 1 and 48 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n# calculate test statistic t* = beta1-hat / SE(beta1-hat)\nbeta1_hat &lt;- as.numeric(coef(mod_normal)[2])\nse_beta1_hat &lt;- summary(mod_normal)$coefficients[\"x\",\"Std. Error\"]\nt_star &lt;- beta1_hat / se_beta1_hat\n\n# compare to result from summary(lm())\ncompare(summary(mod_normal)$coefficients[\"x\",\"t value\"], t_star)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"t value\"]`\n[1] 16.61142\n\n$t_star\n[1] 16.61142\n\n# calculate p-value = 2 * P(t_(n-2) &gt;= |t*|)\np_value &lt;- 2 * pt(q = t_star, df = df.residual(mod_normal), lower.tail = FALSE)\n\n# compare to results from summary(lm())\ncompare(summary(mod_normal)$coefficients[\"x\",\"Pr(&gt;|t|)\"], p_value)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"Pr(&gt;|t|)\"]`\n[1] 0.000000000000000000001552572\n\n$p_value\n[1] 0.000000000000000000001552572\n\n\n\n\nSimulation to determine how the magnitude of \\(\\beta_1\\) and \\(\\sigma\\) affect the resulting significance of the \\(X\\) variable in SLR when simulating data. For example, suppose \\(\\beta_1 = 1a\\) and \\(\\sigma = 3a\\), where \\(a = 1, 2, 3\\). Is there the same resulting significance for all values of \\(a\\)?\n\n# initialize simulation settings\n# -&gt; first the parameters of interest (that are variable)\n# -&gt; create all combos\n# -&gt; add in the constant settings\n# -&gt; arrange in a good order\n# -&gt; add simulation id column\n# -&gt; set row names equal to id (helps keep track of results later)\nparams &lt;- expand.grid(beta_1 = c(1,5,10),\n                      ratio = c(1:3,5)) %&gt;% \n  mutate(sigma = beta_1 * ratio) %&gt;% \n  mutate(n = 30,\n         beta_0 = 2) %&gt;%\n  arrange(beta_1, ratio) %&gt;% \n  bind_cols(data.frame(sim_id = 1:nrow(.)))\nrownames(params) &lt;- paste0(\"sim\", params$sim_id)\n\n# define function to run on each unique simulation setting\nsimulation &lt;- function(vec, vec_names, m = 1000) {\n  \n  # set names of parameter vector for clearer reference\n  names(vec) = vec_names\n  \n  # initialize results vector\n  results = rep(NA, m)\n  \n  # loop to simulate m models and extract summaries\n  for (i in 1:m) {\n    \n    # generate data\n    x = runif(n = vec[\"n\"], min = 5, max = 10)\n    y = rnorm(n = vec[\"n\"], mean = vec[\"beta_0\"] + vec[\"beta_1\"] * x, sd = vec[\"sigma\"])\n    \n    # fit model\n    mod = lm(y ~ x)\n    \n    # get summaries\n    # -&gt; just want t-stat\n    results[i] = summary(mod)$coefficients[\"x\", \"t value\"]\n    \n  }\n  \n  return(results)\n  \n}\n\n# run simulation for each parameter settings\n# -&gt; have to transpose so that now each column is a parameter settings and convert to dataframe so can pass to map(), which needs a list\nresults_raw &lt;- params %&gt;% \n  t %&gt;% \n  data.frame %&gt;% \n  map(\\(vec) simulation(vec, vec_names = names(params), m = 1000), .progress = T)\n\n# reformat results\n# -&gt; combine results into a dataframe\n# -&gt; convert to long so have one column for which simulation settings and one for the results\n# -&gt; attach the simulation settings to the results\n# --&gt; for the plot, want all parameters as factors -&gt; so before joining convert all\nresults &lt;- results_raw %&gt;% \n  bind_rows %&gt;% \n  pivot_longer(cols = 1:nrow(params),\n               names_to = \"sim_id\",\n               values_to = \"t\") %&gt;% \n  mutate(sim_id = as.factor(str_sub(sim_id, start = 4))) %&gt;% \n  left_join(mutate(params, across(everything(), as_factor)), by = \"sim_id\") # levels are slightly out of order....\n\n# create a plot facetted by beta value, with density curves for the sampling distributions of the t-stats for each value of the sigma / beta1 ratio\n# -&gt; add reference line for significance cutoff\n\n# set more informative labels for the facets\nlabels_beta &lt;- c(\"1\" = \"beta1 = 1\",\n                 \"5\" = \"beta1 = 5\",\n                 \"10\" = \"beta1 = 10\")\nggplot() + \n  geom_density(aes(x = t,\n                   group = ratio,\n                   color = ratio,\n                   fill = ratio),\n               data = results,\n               alpha = 0.2) + \n  geom_vline(aes(xintercept = t),\n             data = data.frame(t = abs(qt(0.025, df = params[1,\"n\"]-1))),\n             color = \"darkgrey\") + \n  facet_grid(beta_1 ~ .,\n             labeller = as_labeller(labels_beta)) + \n  guides(\n    fill = guide_legend(bquote(sigma *  \"/\" * beta[1] * \" ratio\")),\n    color = \"none\")\n\n\n\n\n\n\n\nBased on the sampling distributions, it appears that the relative magnitude is what is important. And if it is the same even for different values, then will get similar results.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#some-considerations-when-making-inferences",
    "href": "notes-inference.html#some-considerations-when-making-inferences",
    "title": "2  Inference",
    "section": "\n2.2 Some considerations when making inferences",
    "text": "2.2 Some considerations when making inferences\nEffects of departures from normality:\n\nIf the probability distributions of \\(Y\\) are not exactly normal but do not depart seriously \\(\\Longrightarrow\\) Sampling distribution of \\(\\hat{\\beta_1}\\) \\(\\approx\\) normal \\(\\Longrightarrow\\) Using the \\(t\\) distribution will provide \\(\\approx\\) \\((100 - \\alpha)\\%\\) CIs and \\(\\alpha\\)-level tests.\nThe estimator \\(\\hat{\\beta_1}\\) generally has the property of asymptotic normality (i.e. it’s distribution approaches normality under very general conditions as the sample size increases). So with a sufficiently large sample size, inference procedures are still valid even if distributions of \\(Y\\) have large departures from normality. Can switch from \\(t\\)-based procedures to \\(Z\\)-based with large \\(n\\).\n\nInterpretation of confidence coefficient and risks of errors\n\nOur regression model assumes that the \\(X_i\\) are known constants. So the confidence coefficient and risks of errors are interpreted with respect to taking repeated samples where the \\(X\\) observations are kept at the same levels as in the observed sample.\nFor example, confidence interval for \\(\\beta_1\\) with confidence coefficient 95% → If many independent samples are taken where the levels of \\(X\\) are the same as in the dataset, approximately 95% of the constructed confidence intervals would capture the true value of \\(\\beta_1\\).\n\nSpacing of the \\(X\\) levels\n\nFor a given \\(n\\) and \\(\\sigma^2\\), the variance of \\(\\hat{\\beta_1}\\) is affected by the spacing of the \\(X\\) levels in the observed data. As the spread in the \\(X\\) levels increases, \\(S_{xx}\\) increases and therefore \\(V(\\hat{\\beta_1})\\) decreases.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#interval-estimation-of-ey_h",
    "href": "notes-inference.html#interval-estimation-of-ey_h",
    "title": "2  Inference",
    "section": "\n2.3 Interval estimation of \\(E(Y_h)\\)\n",
    "text": "2.3 Interval estimation of \\(E(Y_h)\\)\n\nOverview\n\nA common objective is to estimate the mean of one or more probability distributions of \\(Y\\).\n\nSetup\n\nLet \\(X_h\\) = level of \\(X\\) that we wish to estimate the mean response for (may be a value which occurred in the sample, or some other value within the scope of the model).\nMean response when \\(X = X_h\\) is \\(E(Y_h)\\); this is what we are estimating.\n\n\n\n\n2.3.1 Sampling distribution of \\(\\hat{Y_h}\\)\n\n\n\nResults\nDerivation\n\n\n\nSampling distribution of \\(\\hat{Y_h}\\)\n\nAgain, this refers to the different of \\(\\hat{Y_h}\\) from repeated sampling when the levels of the predictor variable \\(X\\) are held constant from sample to sample.\nPoint estimator for \\(E(Y_h)\\) → \\(\\hat{Y_h} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_h\\)\nDistribution of \\(\\hat{Y_h}\\) is Normal with mean and variance:\n\n\\[\n  \\begin{align*}\n    E(\\hat{Y_h}) &= E(Y_h) \\\\\n    V(\\hat{Y_h}) &= \\sigma^2 \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]\n  \\end{align*}\n\\]\n\nEstimate variance (and standard deviation) by substituting \\(MSE\\). This gives us:\n\n\\[\n  S^2_{\\hat{Y_h}} = MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg] \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{\\hat{Y_h}} = \\sqrt{S^2_{\\hat{Y_h}}} = S \\sqrt{\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{S_{XX}}}\n\\]\nNotes\n\nThe variability of the sampling distribution of \\(Y_h\\) is affected by how far \\(X_h\\) is from \\(\\bar{X}\\) (from the numerator of second term in \\(V(\\hat{Y_h})\\)) \\(\\Longrightarrow\\) \\(V(\\hat{Y_h})\\) increases the further \\(X_h\\) is from \\(\\bar{X}\\).\n\nIntuitive / visual explanation of this affect\n\nIn the picture below, two regression lines are assumed to go through the same \\((\\bar{X}, \\bar{Y})\\) point to isolate the effect of variation in the estimated slope \\(\\hat{\\beta}_1\\) from sample to sample.\n\n\n\nWe see the difference between estimated responses is much smaller when \\(X\\) is near the mean \\(\\bar{X}\\). So the variation in slope from sample to sample has a much more pronounced effect for \\(X\\) levels far from the mean.\n\n\nWhen \\(X_h = 0\\) \\(\\Longrightarrow\\) \\(V(\\hat{Y_h})\\) reduces to \\(V(\\hat{\\beta}_0)\\) (likewise for the estimated variances). This is because when \\(X_h = 0\\), \\(\\hat{Y_h} = \\hat{\\beta}_0 + \\hat{\\beta}_1 (0) = \\hat{\\beta}_0\\).\n\n\n\n\n\n\n\n\n2.3.2 Sampling distribution of \\((\\hat{Y_h} - E(Y_h)) / S_{\\hat{Y_h}}\\)\n\n\\[\n\\frac{\\hat{Y_h} - E(Y_h)}{S_{\\hat{Y_h}}} = \\frac{\\hat{Y_h} - E(Y_h)}{\\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}} \\sim \\text{t}\\,_{n - 2}\n\\]\n\n2.3.3 Confidence interval for \\(E(Y_h)\\)\n\n\nGoal → Estimate the mean value of \\(Y\\) for a given value of \\(X\\).\nResults → Using the same general format / derivation of a \\(t\\) interval, we have\n\n\\[\n100(1 - \\alpha)\\% \\text{ CI } = \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{Y_h}} \\hspace{10pt} = \\hspace{10pt} \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}\n\\]\n\nInterpretation → With &lt; \\(1 - \\alpha\\) &gt;% confidence, we estimate that the true mean value of &lt; \\(Y\\) context &gt; for all individuals with an &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\n\nNotes\n\nInterpretations → Same interpretation rules about repeated sampling for constant \\(X\\) levels (because \\(X_i\\) are known constants in the regression model).\n\nConfidence interval width is the smallest when \\(X_h = \\bar{X}\\) (assuming everything else remains equal).\n\nDesign of experiments → Thus in an experiment to estimate the mean response at a particular level \\(X_h\\), the precision of the estimate will be best if (everything else remaining equal) the observations on \\(X\\) are spaced so that \\(\\bar{X} = X_h\\).\n\n\nTests → Can use the CI to perform a two-sided test as well.\n\nRobust → Confidence limits shown here for \\(\\hat{Y_h}\\) are not sensitive to moderate departures from the assumption that the error terms are normally distributed. If there is substantial departures from normality, still not sensitive if large \\(n\\); this robustness comes from robustness of CIs for \\(\\beta_0\\) and \\(\\beta_1\\).\n\nCan also think of these CIs as robust because they are only concerned with the center (location) of the distribution of \\(Y_h\\).\n\n\nMultiple intervals → ** CIs apply when a estimating a single mean response from the study. Will show later how to adjust when estimating several mean responses. **\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# simulate sample dataset for normal error regression model\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30\nbeta_0 &lt;- 2\nbeta_1 &lt;- 5\nsigma &lt;- 10 \n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate response Y | X ~ Normal(B0 + B1*X, sigma^2)\n# -&gt; specify the conditional means and then incorporate the random error\n# -&gt; rnorm() can take a vector of means and iterates through them\n# --&gt; if there n &gt; length(means), then the means get recycled from the start\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# specify new X level (X range is 5 - 15)\n# -&gt; newdata frame should have columns with same name as data for the model\nx_h &lt;- data.frame(x = 12)\n\n# calculate just the point estimate of estimated E(Y_h)\n# -&gt; technically using the predict() function for class lm(), so just calling predict() is masking predict.lm() --&gt; other classes like predict.glm() have slightly different argument options\n# -&gt; by default predict() returns the fits (regression line) for all of the obs (X values) used in the model fit, can specify new data points with newdata = &lt; data_frame &gt;\npredict(mod, newdata = x_h)\n\n       1 \n59.27135 \n\n# calculate lower and upper bounds of interval estimate for E(Y_h)\n# -&gt; by default does 95% CI and returns the point estimate (fit) as well\npredict(mod, newdata = x_h, interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 59.27135 54.74569 63.79701\n\n# show items related to standard error of estimation se(Y_h-hat)\n# -&gt; residual.scale = S = sqrt(MSE)\npredict(mod, newdata = x_h, se.fit = TRUE)\n\n$fit\n       1 \n59.27135 \n\n$se.fit\n[1] 2.209356\n\n$df\n[1] 28\n\n$residual.scale\n[1] 9.230822\n\n# alternate way from ALSM package\n# -&gt; this is the package that goes with the textbook; provides some functions that are nowhere else, and some alternatives to common functions\n# --&gt; the implementation of this function is essentially what is done in the \"manual\" section\n# --&gt; type = \"m\" gives CI for mean observation; by default does CL of 95%, but specifies alpha\n# -&gt; SIDENOTE -&gt; formatting: original result is a dataframe which displays poorly, so convert to matrix\nALSM::ci.reg(mod, newdata = x_h, type = \"m\", alpha = 0.05) %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 59.27135   54.74569   63.79701\n\n\n\n\n\n# calculate point estimate using estimated coefficients\n# -&gt; hat(Y_h) = hat(beta_0) + hat(beta_1) X_h\nb &lt;- as.numeric(coef(mod))\npe &lt;- b[1] + b[2] * as.numeric(x_h)\n\n# calculate critical value\n# -&gt; use lower.tail = FALSE to get the positive version of t_alpha/2\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate se(Y_h-hat) = MSE (1/n (X_h - X-bar)^2 / S_XX) %&gt;% sqrt OR S * sqrt(1/n + S_XX)\nn &lt;- length(x)\nx_bar &lt;- mean(x)\ns_xx &lt;- sum((x - x_bar)^2)\nmse &lt;- (summary(mod)$sigma)^2\nmse &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$residual.scale\nse_yh_hat &lt;- sqrt(mse * (1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\nse_yh_hat &lt;- summary(mod)$sigma * sqrt((1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\n\n# calculate CI for E(Y_h)\nci_limits &lt;- c(lower = pe - t_crit * se_yh_hat, upper = pe + t_crit * se_yh_hat)\n\n# compare results to predict(lm(), type = \"confidence\")\n# -&gt; Y_h-hat and interval bounds\ncompare(predict(mod, newdata = x_h, interval = \"confidence\"), c(\"point estimate\" = pe, ci_limits))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for target but not for current &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is matrix, current is numeric\"                             \n\n$comparison$`element-wise`\nNULL\n\n\n$`predict(mod, newdata = x_h, interval = \"confidence\")`\n       fit      lwr      upr\n1 59.27135 54.74569 63.79701\n\n$`c(`point estimate` = pe, ci_limits)`\npoint estimate          lower          upper \n      59.27135       54.74569       63.79701 \n\n# -&gt; se of estimation\n# --&gt; by default predict uses s = sqrt(mse) = residual standard error to get the se.fit (and the df as well = df.residual(mod)) ---&gt; assumes future obs have same error variance as originals used for fitting\n# --&gt; can specify a different variance to use for future obs with pred.var if desired\ncompare(predict(mod, newdata = x_h, se.fit = TRUE)$se.fit, se_yh_hat)\n\n$comparison\n[1] TRUE\n\n$`predict(mod, newdata = x_h, se.fit = TRUE)$se.fit`\n[1] 2.209356\n\n$se_yh_hat\n[1] 2.209356\n\n# can return fit, bounds, and standard error info from predict using type = \"terms\"\npredict(mod, newdata = x_h, type = \"terms\", interval = \"conf\")\n\n$fit\n         x\n1 9.989412\nattr(,\"constant\")\n[1] 49.28194\n\n$se.fit\n        x\n1 1.42863\n\n$lwr\n         x\n1 7.062995\nattr(,\"constant\")\n[1] 49.28194\n\n$upr\n         x\n1 12.91583\nattr(,\"constant\")\n[1] 49.28194\n\n$df\n[1] 28\n\n$residual.scale\n[1] 9.230822\n\n# -&gt; for the fit and bounds, for some reason this actually uses the alternate regression model -&gt; Y_h-hat = bar(y) + beta_1-hat (X_h - bar(x))\n# -&gt; results: constant = beta_1-hat (X_h - X-bar) and attr = Y-bar\n# -&gt; same process to get upper and lower too, add to Y-bar\n(x_star &lt;- predict(mod, newdata = x_h, type = \"terms\"))\n\n         x\n1 9.989412\nattr(,\"constant\")\n[1] 49.28194\n\ncompare(pe, mean(y) + b[2] * (as.numeric(x_h) - mean(x)))\n\n$comparison\n[1] TRUE\n\n$pe\n[1] 59.27135\n\n$`mean(y) + b[2] * (as.numeric(x_h) - mean(x))`\n[1] 59.27135\n\n\n\n\n\n# compare width of confidence intervals at two X levels\n\n# specify new X levels (X range is 5 - 15)\n# -&gt; optimal width will be at X = X-bar\nx_h_mean &lt;- data.frame(x = mean(x))\nx_h2 &lt;- data.frame(x = 8)\n\n# calculate width of intervals\npredict(mod, newdata = x_h_mean, interval = \"confidence\")[2:3] %&gt;% diff\n\n[1] 6.904401\n\npredict(mod, newdata = x_h2, interval = \"confidence\")[2:3] %&gt;% diff\n\n[1] 8.512111",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#prediction-of-new-observation",
    "href": "notes-inference.html#prediction-of-new-observation",
    "title": "2  Inference",
    "section": "\n2.4 Prediction of new observation",
    "text": "2.4 Prediction of new observation\nOverview\n\nGoal → Predict a new observation \\(Y\\) for a given \\(X\\) value. This new observation is viewed as the result of a new trial, independent of the trials the model is based on.\n\nSetup\n\nAgain, \\(X_h\\) is the \\(X\\) level for the new trial. Still assuming the underlying regression model is appropriate for the new observation.\nThe new observation on \\(Y\\) is \\(Y_{h(new)}\\); this is what we are estimating.\n\n\n\nDistinction between (1) estimation of the mean response \\(E(Y_h)\\) and (2) prediction of a new response \\(Y_{h(new)}\\)\n\nWe estimate the mean of the distribution of \\(Y\\).\nWe predict an individual outcome drawn from the distribution of \\(Y\\). Obviously, most outcomes deviate from the mean response; so this must be taken into account when predicting \\(Y_{h(new)}\\).\n\n\nPrediction has extra variability \\(\\Longrightarrow\\) Less precision.\n\n\n2.4.1 Prediction interval for \\(Y_{h(new)}\\) when parameters are known\nDemonstration of prediction intervals\n\nFor a simple example, assume the relevant parameters of the regression model are known:\n\n\\[\n\\beta_0 = 0.10, \\, \\beta_1 = 0.95, \\, \\sigma = 0.12 \\hspace{10pt} \\longrightarrow \\hspace{10pt} E(Y) = 0.10 + 0.95 X\n\\]\n\nIf we have a new observation with \\(X_h = 3.425\\) → \\(E(Y_h) = 0.10 + 0.95 (3.5) = 3.425\\) (so we know the center of the normal distribution of \\(Y_h\\)). Thus, using the empirical rule we have the following prediction interval:\n\n\\[\n99.7\\% \\text{ CI } = E(Y_h) \\pm 3 \\sigma \\hspace{10pt} = \\hspace{10pt} 3.425 \\pm 3 (0.12) \\hspace{10pt} \\Longrightarrow \\hspace{10pt} 3.065 \\le Y_{h(new)} \\le 3.785\n\\]\n\nBasic idea of prediction used here\n\nChoose a range in the distribution of \\(Y\\) where most of the observations will fall and then declare that the next observation will fall in this range.\nThe usefulness of the prediction interval depends on the width of the interval and the needs for precision by the user.\n\nGeneralizing for this simple scenario\n\nWhen the regression parameters of normal error regression model are known:\n\n\\[\n100(1 - \\alpha)\\% \\text{ PI for } Y_{h(new)} = E(Y_h) \\pm z_{\\alpha / 2} \\cdot \\sigma\n\\]\n\nCentering the limits around \\(E(Y_h)\\) results in the narrowest interval consistent with the specified probability of a correct prediction.\n\n2.4.2 Prediction interval for \\(Y_{h(new)}\\) when parameters are unknown\nOverview and demo\n\n\nWhen the regression parameters are unknown, they must be estimated.\n\nThe mean of the distribution of \\(Y\\) is estimated by \\(\\hat{Y_h}\\) as usual, and the variance of the distribution of \\(Y\\) is estimated with \\(MSE\\).\nHowever, we cannot simply use the previous PI with the parameters replaced by the corresponding point estimators. Here’s a demo of why:\nThe picture below shows two possible probability distributions of \\(Y\\), corresponding to the lower and upper limits of a CI for \\(E(Y_h)\\). In other words, the distribution of \\(Y\\) could be located as far left as the one shown, as far right as the other one shown, or anywhere in between.\n\n\n\n\n\n\nResults\n\nSince we do not know the mean \\(E(Y_h)\\) and only estimate it by a confidence interval, we cannot be certain of the location of the distribution of \\(Y\\).\nBecause of this uncertainty, PIs for \\(Y_{h(new)}\\) clearly must take into account two elements:\n\n\nVariation in possible location of the distribution of \\(Y\\).\nVariation within the probability distribution of \\(Y\\).\n\n\n\nPrediction interval for \\(Y_{h(new)}\\)\n\n\nSampling distribution\n\nNote that this studentized statistic uses the point estimator \\(\\hat{Y_h}\\) in the numerator rather than the true mean \\(E(Y_h)\\) because the true mean is unknown and cannot be used in making a prediction.\nFor \\(E(Y_h)\\) CIs shown earlier, we used \\([\\hat{Y_h} - E(Y_h)] / S_{\\hat{Y_h}}\\), which was okay because the only unknown was \\(E(Y_h)\\) and it is what we are estimating. But now there are two layers of uncertainty (variability). So the reference value in the numerator is also an estimate, not the true value.\n\n\n\n\\[\n\\frac{Y_{h(new)} - \\hat{Y_h}}{S_{pred}} \\sim \\text{t}\\,_{n-2}\n\\]\n\n\nPrediction interval\n\nInterpretation → With &lt; \\(1-\\alpha\\) &gt;% confidence, we predict that the true value of &lt; \\(Y\\) context &gt; for a single (or the next) &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\n\n\n\n\\[\n100(1 - \\alpha)\\% \\text{ PI for } Y_{h(new)} = \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{pred}\n\\]\nStandard deviation of prediction \\(\\sigma^2_{pred}\\)\n\nThe numerator of the studentized statistic represents how far the new observation will deviate from the estimated mean (based on the original \\(n\\) cases in the study). This difference can be viewed as the prediction error, with \\(\\hat{Y_h}\\) serving as the best point estimate of the value of the new observation \\(Y_{h(new)}\\).\nWe can easily find the variance of this difference (because of independence of the new \\(Y_{h(new)}\\) and original \\(n\\) cases on which \\(\\hat{Y_h}\\) is based).\n\n\\[\n\\sigma^2_{pred} = V(Y_{h(new)} - \\hat{Y_h}) = V(Y_{h(new)}) + V(\\hat{Y_h}) = \\sigma^2 + \\sigma^2_{\\hat{Y_h}}.\n\\]\n\n\nThis has two components:\n\nVariance of the distribution of \\(Y\\) at \\(X = X_h\\) → \\(\\sigma^2\\)\nVariance of the sampling distribution of \\(\\hat{Y_h}\\) → \\(\\sigma^2_{\\hat{Y_h}}\\)\n\n\nThe unbiased estimator of \\(\\sigma^2_{pred}\\) is\n\n\\[\nS^2_{pred} = MSE + S^2_{\\hat{Y_h}} = MSE + MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg] = MSE \\bigg[1 + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum (X_i - \\bar{X})^2}\\bigg] \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{pred} = \\sqrt{S^2_{pred}}\n\\]\nNotes about prediction intervals\n\n\nAnalogous results / interpretations to estimation\n\nInterpretation → Again, the confidence coefficient refers to taking repeated samples based on the same set of \\(X\\) values, and calculating prediction limits for \\(Y_{h(new)}\\) for each sample.\nPrecision → PI width is the smallest when \\(X_h = \\bar{X}\\) (assuming everything else remains equal).\nOne interval → PIs apply for a single prediction based on the sample data.\n\n\nNot robust → PIs (unlike CIs for the mean response) are sensitive to departures from normality of the error terms distribution. Can think of this non-robustness a result of having to take into account the center of the distribution of \\(Y_h\\) (just like with CIs) AND also the tails (spread) of the distribution.\n\nPrecision → Even if PIs are too wide for useful predictions, they can still be informative for control / modelling purposes, specifically the estimated variance of prediction.\n\n\\(S^2_{pred} = MSE + S^2_{\\hat{Y_h}} \\hspace{10pt}\\) has two pieces: (1) \\(MSE\\) measures \\(X\\)-to-\\(X\\) variation within the probability distribution for \\(Y\\) (different response values for observations with same \\(X\\) level) and (2) \\(S^2_{\\hat{Y_h}}\\) measures sample-to-sample variation (mean response of samples with overall same \\(X\\) levels).\nSo if \\(MSE\\) is very large compared to \\(S^2_{\\hat{Y_h}}\\), e.g. \\(\\frac{MSE}{S^2_{pred}} \\ge 0.8 \\text{ or } 0.9\\), then the majority of the variation is from sample-to-sample. This could reflect other factors that aren’t being taken into account by the model. So perhaps a multiple linear regression model should be used, which could result in more useful predictions. NOT SURE HOW THIS WORKS\n\n\n\nEstimation vs Prediction\n\nLocation → For a particular \\(X_h\\), CIs and PIs have the same point estimate \\(\\hat{Y_h}\\), which is the estimate of the mean \\(E(Y_h)\\).\n\nPrecision\n\nThe difference between CIs and PIs then lies in the relative accuracy of the interval.\nCIs are narrower than PIs at the same \\(X_h\\).\nThe only way to obtain more accurate prediction for a new value of \\(Y\\) is to reduce the standard deviation of the regression model. This can be accomplished by using a curvilinear model, adding new independent variables, etc. or by collecting more data (width of both intervals decrease when the sample size increases).\n\n\nConceptual difference → CIs resemble PIs, except: A CI represents an inference on a parameter and is an interval that is intended to cover the value of the parameter; and a PI is a statement about the value to be taken by a random variable, the new observation \\(Y_{h(new)}\\).\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate just the point estimate of predicted Y_h(new)\n# -&gt; this is the same as the PE for a CI of E(Y_h) \npredict(mod, newdata = x_h)\n\n       1 \n59.27135 \n\n# calculate lower and upper bounds of prediction interval Y_h(new)\n# -&gt; by default does 95% CI and returns the point estimate (fit) as well\npredict(mod, newdata = x_h, interval = \"prediction\")\n\n       fit      lwr      upr\n1 59.27135 39.82881 78.71389\n\npredict(mod, newdata = x_h, interval = \"confidence\")\n\n       fit      lwr      upr\n1 59.27135 54.74569 63.79701\n\n# show items related to standard error of ESTIMATION se(Y_h-hat)\n# -&gt; ALWAYS returns this, even if specify interval = \"pred\"...\n# --&gt; suppose it implicitly adds the extra MSE term (from residual.scale) to get the prediction lwr and upr\npredict(mod, newdata = x_h, se.fit = TRUE)\n\n$fit\n       1 \n59.27135 \n\n$se.fit\n[1] 2.209356\n\n$df\n[1] 28\n\n$residual.scale\n[1] 9.230822\n\n# alternative way using ALSM\n# -&gt; type = \"n\" gives PI for single new observation\nALSM::ci.reg(mod, newdata = x_h, type = \"n\") %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 59.27135   39.82881   78.71389\n\n\n\n\n\n# calculate point estimate and critical value\n# -&gt; same as for confidence interval\nb &lt;- as.numeric(coef(mod))\npe &lt;- b[1] + b[2] * as.numeric(x_h)\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate standard error in prediction se(pred) = sqrt(MSE + var(Y-h-hat))\n# -&gt; save se(estimation = fit), shown earlier, then have to add in the extra MSE term\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nse_pred &lt;- sqrt(summary(mod)$sigma^2 + se_fit^2)\nse_pred &lt;- sqrt(summary(mod)$sigma^2 + se_fit^2)\n\n# OR calculate se(pred) using expanded formula se(pred) = MSE * (1 + 1/n (X_h - X-bar)^2 / S_XX) %&gt;% sqrt OR s * sqrt(...)\nn &lt;- length(x)\nx_bar &lt;- mean(x)\ns_xx &lt;- sum((x - x_bar)^2)\ns &lt;- (summary(mod)$sigma)\nse_pred &lt;- s * sqrt((1 + 1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\n\n# calculate PI for Y_h(new)\npi_limits &lt;- c(lower = pe - t_crit * se_pred, upper = pe + t_crit * se_pred)\n\n# compare results to predict(lm(), type = \"prediction)\n# -&gt; Y_h-hat and interval bounds\ncompare(predict(mod, newdata = x_h, interval = \"prediction\"), c(\"point estimate\" = pe, pi_limits))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for target but not for current &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is matrix, current is numeric\"                             \n\n$comparison$`element-wise`\nNULL\n\n\n$`predict(mod, newdata = x_h, interval = \"prediction\")`\n       fit      lwr      upr\n1 59.27135 39.82881 78.71389\n\n$`c(`point estimate` = pe, pi_limits)`\npoint estimate          lower          upper \n      59.27135       39.82881       78.71389 \n\n\n\n\n\n# compare width of prediction intervals at two X levels\n\n# specify new X levels (X range is 5 - 15)\n# -&gt; optimal width will again be at X = X-bar\nx_h_mean &lt;- data.frame(x = mean(x))\nx_h2 &lt;- data.frame(x = 8)\n\n# calculate width of intervals\npredict(mod, newdata = x_h_mean, interval = \"prediction\")[2:3] %&gt;% diff\n\n[1] 38.44208\n\npredict(mod, newdata = x_h2, interval = \"prediction\")[2:3] %&gt;% diff\n\n[1] 38.76311\n\n# compare widths of CI vs PI at the same X level\n\n# calculate width of intervals\npredict(mod, newdata = x_h, interval = \"conf\")[2:3] %&gt;% diff\n\n[1] 9.051321\n\npredict(mod, newdata = x_h, interval = \"pred\")[2:3] %&gt;% diff\n\n[1] 38.88508\n\n\n\n\n\n\n\n2.4.3 Prediction of mean of \\(m\\) observations for given \\(X_h\\)\n\nOverview\n\nGoal → Predict the mean of \\(m\\) new observations on \\(Y\\) for a given level of the predictor variable.\nSetup → \\(\\bar{Y}_{h(new)}\\) represents the mean of the new \\(Y\\) observations to be predicted.\n\nResults\n\n\n\\(100(1 - \\alpha)\\%\\) Prediction interval for \\(\\bar{Y}_{h(new)}\\) (assuming the new observations are independent):\n\n\\[\n\\hat{Y_h} \\pm t_{(1 - alpha /2, n - 2)} \\cdot S_{predmean}\n\\]\n\nStandard deviation in prediction of a mean \\(S_{predmean}\\)\n\n\n\\[\nS^2_{predmean} = \\frac{MSE}{m} + S^2_{\\hat{Y_h}}  = MSE \\bigg[\\frac{1}{m} + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{S_{XX}}\\bigg]  \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{predmean} = \\sqrt{S^2_{predmean}}\n\\]\n\n\nThis has two components:\n\nVariance of the distribution mean of \\(m\\) observations from the probability distrubtion of \\(Y\\) at \\(X = X_h\\) → \\(\\sigma^2 / m\\)\nVariance of the sampling distribution of \\(\\hat{Y_h}\\) → \\(\\sigma^2_{\\hat{Y_h}}\\)\n\n\n\nNotes\n\nInterpretation → With &lt; \\(1 - \\alpha\\) &gt;% confidence, we predict that the true value of &lt; \\(Y\\) context &gt; for &lt; \\(m\\) &gt; &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\nInterval still has the same center as when estimating \\(E(Y_h)\\) and predicting a single \\(Y_{h(new)}\\).\nThis prediction interval is narrower than when predicting for a single observation (because it involves the prediction of the mean for a group), but still wider than the confidence interval.\n\nWe can then obtain the prediction interval for the total of the \\(m\\) observations by multiplying each limit by \\(m\\).\n\ne.g) For \\(m = 4\\), if \\(5 \\le \\bar{Y}_{h(new)} \\le 15 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} 5(4) = 20 \\le \\sum Y_{h(new)} \\le 15(4)= 60\\)\n\n\n\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate the predicted mean for m observations of Y_h(new)\n# -&gt; type = \"nm\" is gives PR for mean of m new observations at X_h\nALSM::ci.reg(mod, newdata = x_h, type = \"nm\") %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 59.27135   39.82881   78.71389\n\n\n\n\n\n# set number of observations to predict at X_h\nm &lt;- 4\n\n# calculate just the point estimate of predicted Y-bar_h(new) and critical value\n# -&gt; PE is the same as previous CI and PI\npe &lt;- as.numeric(predict(mod, newdata = x_h))\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate standard error in prediction se(pred) = sqrt(MSE / m + var(Y-h-hat))\n# -&gt; save se(fit), shown earlier, then have to add in the extra MSE / m term\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nse_pred &lt;- sqrt(summary(mod)$sigma^2 / m + se_fit^2)\n\n# calculate PI for Y-bar_h(new)\n(pi_limits &lt;- c(lower = pe - t_crit * se_pred, upper = pe + t_crit * se_pred))\n\n   lower    upper \n48.78974 69.75297 \n\n# compare to (widths) of previous types of intervals\n# -&gt; order from most to least precise: CI for E(Y_h), PI for Y-bar_h(new), PI for single Y_h-hat\npredict(mod, newdata = x_h, interval = \"conf\")\n\n       fit      lwr      upr\n1 59.27135 54.74569 63.79701\n\npredict(mod, newdata = x_h, interval = \"pred\")\n\n       fit      lwr      upr\n1 59.27135 39.82881 78.71389\n\n# interval for sum (total) of m predictions at X_h\n# -&gt; depends on context if this is meaningful\npi_limits * m\n\n   lower    upper \n195.1589 279.0119",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#confidence-band-for-regression-line",
    "href": "notes-inference.html#confidence-band-for-regression-line",
    "title": "2  Inference",
    "section": "\n2.5 Confidence band for regression line",
    "text": "2.5 Confidence band for regression line\nOverview\n\nThe goal is to obtain a confidence band for the entire regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\).\nThis band enables us to see the region in which the entire regression line lies and is particularly useful for determining the appropriateness of a fitted regression function.\n\nResults\n\n\nThis confidence band formula below the same form as the CI formula for \\(E(Y_h)\\), the mean response at \\(X_h\\) (\\(\\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{Y_h}}\\)), except it uses a different multiplier to adjust for multiple comparisons \\(\\Longrightarrow\\) Same point estimate and standard error.\n\nSome alternative procedures for developing confidence bands have been developed.\nThe one shown below is the Working-Hotelling confidence band (more will be said about this method later).\nThe simplicity of this method is that it is a direct extension of the confidence limits for a single mean response \\(E(Y_h)\\) shown earlier.\n\n\nThe \\(100(1 - \\alpha)\\%\\) confidence band for the regression line has boundaries at any level \\(X_h\\):\n\n\\[\n\\hat{Y_h} \\pm W \\cdot S_{\\hat{Y_h}} = \\hat{Y_h} \\pm W \\cdot \\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}\n\\]\n\n(Initial steps of deriving CI) → We are finding the two values such that \\(P(\\text{lower} \\le \\beta_0 + \\beta_1 X_h \\le \\text{upper}) = 1- \\alpha\\); And \\(\\hat{Y_h}\\) is the point estimator for \\(E(Y_h) = \\beta_0 + \\beta_1 X_h\\).\n\\(W\\) multiplier\n\n\\[\nW^2 = 2 \\cdot F_{(1-\\alpha; \\, 2, n-2)}\n\\]\n\nWe multiply by two for SLR becuase there are two estimated coefficients.\nThe \\(W\\)-multiplier is larger than the \\(t\\) multiplier because the confidence band must encompass the entire regression line, whereas the confidence limits for \\(E(Y_h)\\) at \\(X_h\\) apply only at the single level \\(X_h\\).\n\nNotes\n\n\nConfidence band confidence level\n\nInterpretation → Indicates the proportion of time that the estimating procedure will yield a band that covers the entire line, in a long series of samples in which the \\(X\\) observations are kept at the same level as in the actual study.\nTechnical application → The confidence band applies to the entire regression line over all real-numbered values of \\(X\\) from \\(-\\infty\\) to \\(\\infty\\).\nPractical application → In practice, the confidence band is ignored for that part of the regression line which is not of interest, so the confidence coefficient for this limited segment is somewhat higher than \\(1-\\alpha\\). Thus, \\(1-\\alpha\\) serves as a lower bound to the confidence coefficient.\n\n\nThings to look for → Can see if the slope is clearly positive or negative, can look at the levels of the regression line at different levels of \\(X\\) to gauge relative precision, etc.\n\nPrecision\n\nWith the somewhat wider limits for the entire regression line, we are able to draw conclusions about any and all mean responses for the entire regression line and not just about the mean response at a given \\(X\\) level.\nAlthough, generally confidence band lines at any value \\(X_h\\) often are not substantially wider than the confidence limits for the mean response at that single \\(X_h\\) level.\nJust like with the CI for \\(E(Y_h)\\), the boundary points of the confidence band for the regression line are wider apart the further \\(X_h\\) is from the mean \\(\\bar{X}\\) of the \\(X\\) observations.\n\n\nFormula → The lower and upper bounds together actually define a hyperbola.\n\nDemo\n\n\nR functions\nManual\nOther functions\nIn practice\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate confidence band limits across scope of model\n# -&gt; initialize many x_h values covering min to max of original sample\nx_h &lt;- data.frame(x = seq(from = min(x), to = max(x), length = 20))\n\n# calculate confidence bands\n# -&gt; type = \"w\" uses the Working-Hotelling method with the W multiplier\nconf_band &lt;- ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05)\n\n# display results\nkable(conf_band[1:5,], format = \"html\", digits = 3) %&gt;% \n  kable_styling(full_width = FALSE,\n                position = \"left\")\n\n\n\nx\nFit\nLower.Band\nUpper.Band\n\n\n\n5.039\n27.101\n17.817\n36.386\n\n\n5.553\n29.474\n20.955\n37.994\n\n\n6.066\n31.848\n24.069\n39.626\n\n\n6.580\n34.221\n27.152\n41.290\n\n\n7.093\n36.595\n30.194\n42.995\n\n\n\n\n# compare width when estimating a single E(X_h) to the confidence bands\nc(x_h[1,], predict(mod, newdata = data.frame(x = x_h[1,]), interval = \"conf\")) %&gt;% \n  set_names(c(\"x\", \"fit\", \"lwr\", \"upr\"))\n\n        x       fit       lwr       upr \n 5.039184 27.101050 19.743006 34.459094 \n\n# plot fitted line and confidence bands on scatterplot\n# -&gt; also add reference line for X-bar showing where the most precision is\nconf_band %$% \n  matplot(x = x, y = cbind(Fit, Lower.Band, Upper.Band), type =  \"l\", lty = c(1,2,2), col = c(\"red\", \"blue\", \"blue\"), xlab = expression(X[h]), ylab = expression(hat(Y)[h]), main = bquote(.(100 * (1 - alpha)) * \"% Confidence band\"))\npoints(x, y)\nabline(v = mean(x), lty = 2, col = \"lightgrey\")\ntext(x = mean(x) + 0.5, y = min(y) + 1, labels = expression(bar(X)) , col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\n\n# calculate W multiplier = 2 F_crit(regression = of coefficients, residual)\n# degrees of freedom\n# -&gt; numerator -&gt; = df regression (# of coefficients) = 2 for SLR\n# -&gt; denominator -&gt; df residual (n - # of coefficients) = n - 2 for SLR\n# probability -&gt; F is right-tailed, so need to use 1 - alpha now\n# -&gt; no dividing by two or taking absolute value\nalpha &lt;- 0.05\n(W &lt;- sqrt(2 * qf(1 - alpha, df1 = length(coef(mod)), df2 = df.residual(mod))))\n\n[1] 2.584719\n\n# compare W to t at a specific X_h\nabs(qt(alpha / 2, df = df.residual(mod)))\n\n[1] 2.048407\n\n# calculate confidence bands\n# -&gt; use seq of x values from before\n# -&gt; get the fits and se(estimation) for each new x\n# -&gt; calculate lower and upper confidence band limits = Y_h-hat +- W * se(estimation)\nfit &lt;- as.numeric(predict(mod, newdata = x_h))\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nlower &lt;- fit - W * se_fit\nupper &lt;- fit + W * se_fit\n\n# combine above info\ndata_conf_band &lt;- data.frame(x_h = x_h$x, fit, se_fit, lower, upper, width = upper - lower) \n\n# illustrate process of confidence bands over X range\ndata_conf_band %&gt;% \n  display_nice(col.names = c(\"X_h\", \"fit = Y_h-hat\", \"SE(fit) = S_{Y_h-hat}\", \"LB = Y_h-hat - W x S_{Y_h-hat}\", \"UB = Y_h-hat + W x S_{Y_h-hat}\", \"Width\"))\n\n\n\n X_h \n    fit = Y_h-hat \n    SE(fit) = S_{Y_h-hat} \n    LB = Y_h-hat - W x S_{Y_h-hat} \n    UB = Y_h-hat + W x S_{Y_h-hat} \n    Width \n  \n\n\n 5.039 \n    27.101 \n    3.592 \n    17.817 \n    36.386 \n    18.569 \n  \n\n 5.553 \n    29.474 \n    3.296 \n    20.955 \n    37.994 \n    17.039 \n  \n\n 6.066 \n    31.848 \n    3.009 \n    24.069 \n    39.626 \n    15.557 \n  \n\n 6.580 \n    34.221 \n    2.735 \n    27.152 \n    41.290 \n    14.138 \n  \n\n 7.093 \n    36.595 \n    2.476 \n    30.194 \n    42.995 \n    12.802 \n  \n\n 7.607 \n    38.968 \n    2.240 \n    33.179 \n    44.757 \n    11.578 \n  \n\n 8.120 \n    41.341 \n    2.032 \n    36.089 \n    46.594 \n    10.505 \n  \n\n 8.634 \n    43.715 \n    1.864 \n    38.897 \n    48.533 \n    9.635 \n  \n\n 9.148 \n    46.088 \n    1.746 \n    41.575 \n    50.601 \n    9.026 \n  \n\n 9.661 \n    48.462 \n    1.689 \n    44.095 \n    52.828 \n    8.733 \n  \n\n\n\n# compare results to ci.reg(type = \"w)\ncompare(head(ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05)), head(data_conf_band[,c(1,2,4,5)]))\n\n$comparison\n[1] TRUE\n\n$`head(ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05))`\n         x      Fit Lower.Band Upper.Band\n1 5.039184 27.10105   17.81653   36.38557\n2 5.552726 29.47445   20.95476   37.99414\n3 6.066267 31.84785   24.06920   39.62650\n4 6.579809 34.22125   27.15236   41.29013\n5 7.093350 36.59464   30.19384   42.99544\n6 7.606892 38.96804   33.17919   44.75689\n\n$`head(data_conf_band[, c(1, 2, 4, 5)])`\n       x_h      fit    lower    upper\n1 5.039184 27.10105 17.81653 36.38557\n2 5.552726 29.47445 20.95476 37.99414\n3 6.066267 31.84785 24.06920 39.62650\n4 6.579809 34.22125 27.15236 41.29013\n5 7.093350 36.59464 30.19384 42.99544\n6 7.606892 38.96804 33.17919 44.75689\n\nhead(data_conf_band)\n\n       x_h      fit   se_fit    lower    upper    width\n1 5.039184 27.10105 3.592081 17.81653 36.38557 18.56904\n2 5.552726 29.47445 3.296177 20.95476 37.99414 17.03938\n3 6.066267 31.84785 3.009477 24.06920 39.62650 15.55730\n4 6.579809 34.22125 2.734875 27.15236 41.29013 14.13777\n5 7.093350 36.59464 2.476400 30.19384 42.99544 12.80160\n6 7.606892 38.96804 2.239643 33.17919 44.75689 11.57770\n\n\n\n\n\n# demo to verify what geom_smooth() gives\n\n# results\n# -&gt; lm regression lines line up as expected, BUT...\n# -&gt; the confidence bands using se = TRUE are actually the lower and upper **pointwise** confidence interval around the mean\n# -&gt; ** this means it is plotting ALL of the INDIVIDUAL CIs for E(Y_h), whereas Working-Hotelling confidence bands represent a confidence \"interval\" for the ENTIRE regression line\n\n# create dataframe of original observations the model was built on\ndata_original &lt;- data.frame(x, y)\n\n# set t multiplier for making single interval estimates of E(Y_h)\nt_crit &lt;- abs(qt(alpha / 2, df = df.residual(mod)))\n\n# create confidence band demo plotting dataset\n# -&gt; rename current lower and upper to have an indication of W multiplier\n# -&gt; calculate new lower and upper bounds based on t multiplier\n# -&gt; remove unneeded columns for plot\n# -&gt; reshaoe to long with to long with and columns for type of bound and value\n# -&gt; create indicator variable for type of multiplier used in calculating the lower and upper bounds (check what suffix is of bound) and then remove multiplier indication from bound column (just take off suffix)\ndata_conf_band_plot &lt;- data_conf_band %&gt;% \n  rename(lower_w = lower,\n         upper_w = upper) %&gt;% \n  mutate(lower_t = fit - t_crit * se_fit,\n         upper_t = fit + t_crit * se_fit) %&gt;% \n  select(-c(width, se_fit)) %&gt;% \n  pivot_longer(cols = starts_with(c(\"lower\", \"upper\")),\n               names_to = \"bound\",\n               values_to = \"value\") %&gt;% \n  mutate(multiplier = \n           case_when(\n             str_sub(bound, start = -1) == \"t\" ~ \"t\",\n             TRUE ~ \"W\"),\n         bound = str_sub(bound, end = -3))\n\n# create demo plot\n# layer:  geom_smooth()\n# -&gt; this adds regression line\n# -&gt; se + TRUE adds the shaded ribbon representing \"confidence bands\"\n# layer: geom_line() first one\n# -&gt; add manually calculated regression line, should line up exactly with above\n# layer: geom_line() second and third ones\n# -&gt; confidence bands for both multipliers\nggplot() + \n  geom_smooth(aes(x = x,\n                  y = y),\n              data = data_original,\n              method = \"lm\",\n              formula = y ~ x,\n              level = 1 - alpha,\n              se = TRUE,\n              color = \"yellow\",\n              linewidth = 1,\n              alpha = 0.5) + \n  geom_line(aes(x = x_h,\n                y = fit),\n            data = data_conf_band_plot,\n            color = \"red\",\n            linewidth = 0.5) + \n  geom_line(aes(x = x_h,\n                y = value,\n                color = multiplier),\n            data = filter(data_conf_band_plot, bound == \"lower\"),\n            linetype = 2) + \n  geom_line(aes(x = x_h,\n                y = value,\n                color = multiplier),\n            data = filter(data_conf_band_plot, bound == \"upper\"),\n            linetype = 2) + \n  scale_color_manual(name = \"Multiplier\", values = c(t = \"purple\", W = \"blue\")) + \n  labs(title = \"Demo of confidence bands\",\n       x = expression(X[h]),\n       y = expression(hat(Y)[h]))\n\n\n\n\n\n\n\nWhere the multipliers are \\(t_{\\alpha / 2, n - p}\\) and \\(W = 2 \\cdot F_{1 - \\alpha; p, n - p}\\).\n\n\nDemo to verify confidence bands and prediction bands from predict().\n\n# create two datasets of should be equivalent confidence limits (from different functions)\ndata_bands_plot1 &lt;-predict(mod, newdata = x_h, interval = \"conf\") %&gt;% \n  data.frame() %&gt;% \n  mutate(fun = \"pred\") %&gt;% \n  bind_rows(\n    ALSM::ci.reg(mod, newdata = x_h, type = \"m\") %&gt;% \n      data.frame %&gt;% \n      mutate(fun = \"ci.reg\") %&gt;% \n      select(fit = Fit, lwr = Lower.Band, upr = Upper.Band, fun)\n  ) %&gt;% \n  bind_cols(bind_rows(x_h, x_h), .)\n\nggplot(data = data_bands_plot1) + \n  geom_line(aes(x = x,\n                y = fit)) + \n  geom_line(aes(x = x,\n                y = lwr, \n                color = fun,\n                linetype = fun)) + \n  geom_line(aes(x = x,\n                y = upr,\n                color = fun,\n                linetype = fun))\n\n\n\n\n\n\n# SAME\n\n# try again for predictions\n# create two datasets of should be equivalent prediction limits (from different functions)\ndata_bands_plot2 &lt;-predict(mod, newdata = x_h, interval = \"pred\") %&gt;% \n  data.frame() %&gt;% \n  mutate(fun = \"pred\") %&gt;% \n  bind_rows(\n    ALSM::ci.reg(mod, newdata = x_h, type = \"n\") %&gt;% \n      data.frame %&gt;% \n      mutate(fun = \"ci.reg\") %&gt;% \n      select(fit = Fit, lwr = Lower.Band, upr = Upper.Band, fun)\n  ) %&gt;% \n  bind_cols(bind_rows(x_h, x_h), .)\n\nggplot(data = data_bands_plot2) + \n  geom_line(aes(x = x,\n                y = fit)) + \n  geom_line(aes(x = x,\n                y = lwr, \n                color = fun,\n                linetype = fun)) + \n  geom_line(aes(x = x,\n                y = upr,\n                color = fun,\n                linetype = fun))\n\n\n\n\n\n\n# SAME\n\nSo can just use predict() with dataset spread over range of predictor to form pointwise confidence / prediction bands. Assuming this is still valid because limits of the confidence band for the regression line at any value \\(X_h\\) are not substantially wider than the confidence limits for the mean response at that single \\(X_h\\) level.\n\n# create dataset of both pointwise confidence and prediction limits\nconfs &lt;- predict(mod, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod, newdata = x_h, interval = \"pred\")\nplot(x = data_original$x, y = data_original$y, type = \"p\")\nlines(x = x_h$x, y = confs[,\"fit\"],  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = confs[,\"lwr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = confs[,\"upr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = preds[,\"lwr\"],  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = preds[,\"upr\"],  type = \"l\", col = \"blue\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#analysis-of-variance-approach-to-regression",
    "href": "notes-inference.html#analysis-of-variance-approach-to-regression",
    "title": "2  Inference",
    "section": "\n2.6 Analysis of variance approach to regression",
    "text": "2.6 Analysis of variance approach to regression\n\nThis approach is very useful for multiple linear regression and other types of linear statistical models.\n\n\n2.6.1 Partitioning of total sum of squares\nOverview\n\nIn a regression setting, analysis of variance (ANOVA) allows us to capture the different sources of variability in the model.\nWe do this by partitioning the sums of squares and degrees of freedom associated with the the response variable \\(Y\\).\n\n\nTypes of sum of squares\n\nTotal sum of squares → Measured in terms of the deviations of the \\(Y_i\\) around their mean \\(\\bar{Y}\\).\n\n\\[\nSSTO = \\sum (Y_i - \\bar{Y})^2 = S_{YY}\n\\]\n\nMeasures the total variation of \\(Y\\), which tells us the uncertainty related to \\(Y\\) when the predictor variable \\(X\\) is not taken into account.\nIf all observations are the same, then \\(SSTO = 0\\). More variation in \\(Y_i\\), the larger \\(SSTO\\) is.\nSum of squares error → Measured in terms of the deviations of the \\(Y_i\\) around the fitted regression line \\(\\hat{Y_i}\\) (i.e. the residuals \\(e_i\\)).\n\n\\[\nSSE = \\sum (Y_i - \\hat{Y_i})^2\n\\]\n\nMeasures the remaining variation / uncertainty in the \\(Y_i\\)’s after we utilize the predictor variable \\(X\\) (i.e. “unexplained” variation).\nIf all observations fall on the fitted line, \\(SSE = 0\\). More variation in \\(Y\\) around the fitted line, the larger \\(SSE\\) is.\nSum of squares regression → Measured in terms of the deviations of the fitted \\(\\hat{Y_i}\\) around their mean \\(\\bar{Y}\\).\n\n\\[\nSSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\n\\]\n\nMeasures the variation in the \\(Y_i\\)’s that is associated with the regression line.\nIn other words, it measures the variation in the \\(Y_i\\)’s that is accounted for by the relationship between \\(Y\\) and \\(X\\) (i.e. “explained” variation). We are essentially upgrading our original prediction for \\(Y\\) from \\(\\bar{Y}\\) to now \\(\\hat{Y}\\); so this \\(\\approx\\) (conceptually) measures how much better the predictions become.\nSo, the larger \\(SSR\\) is in relation to \\(SSTO\\), the greater is the effect of the regression relation in accounting for the total variation in the \\(Y_i\\) observations.\n\n\n\nResults\nDerivation\n\n\n\n\nPartitioning individual deviations\n\n\n\n\nThe two components are:\n\nThe deviation of the fitted value \\(\\hat{Y_i}\\) around the mean \\(\\bar{Y}\\).\nThe deviation of the observation \\(Y_i\\) around the fitted regression line.\n\n\nThis relationship holds for the sum of the squared deviations as well:\n\n\\[\n\\begin{align*}\n  \\sum (Y_i - \\bar{Y})^2 &= \\sum (\\hat{Y_i} - \\bar{Y})^2 + \\sum (Y_i - \\hat{Y_i})^2\\\\\n  SSTO &= SSR + SSE\n\\end{align*}\n\\]\n\n\n\nPartitioning total sum of squares\n\n\n\nThe formulas for \\(SSTO\\), \\(SSE\\) and \\(SSR\\) above are best for computations. But an alternate form of \\(SSR\\) is useful for deriving analytical results is shown below:\n\n\\[\n\\begin{align*}\n  SSR &= \\sum [\\hat{Y_i} - \\bar{Y}]^2 \\\\\n      &= \\sum [(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i) - \\bar{Y}]^2 \\\\\n      &= \\sum [(\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 X_i - \\bar{Y}]^2 \\\\\n      &= \\hat{\\beta}_1^2 \\sum [X_i - \\bar{X}]^2\n\\end{align*}\n\\]\n\n\n\n\n2.6.2 Breakdown of degrees of freedom\nDegrees of freedom for each sum of square\n\n\n\\(SSTO\\) → Has \\(n-1\\) degrees of freedom associated with it.\n\nOne degree of freedom is lost because the deviations \\(Y_i - \\bar{Y}\\) are subject to one constraint: they must sum to zero (\\(\\sum (Y_i - \\bar{Y}) = 0\\)).\nEquivalently, one degree of freedom is lost because the sample mean \\(\\bar{Y}\\) is used to estimate the population mean.\n\n\n\n\\(SSE\\) → Has \\(n-2\\) degrees of freedom associated with it.\n\nTwo degrees of freedom are lost because the two parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated in obtaining the fitted values \\(\\hat{Y_i}\\).\n\n\n\n\\(SSR\\) → Has 1 degrees of freedom associated with it.\n\nAlthough there are \\(n\\) deviations \\(\\hat{Y_i} - \\bar{Y}\\), all fitted values are calculated from the same estimated regression line. So the regression line just has two degrees of freedom (corresponding to the slope and intercept, for SLR \\(p = 2\\))…\nBUT then, one degree of freedom is lost because again we have to estimate the mean in order to calculate \\(\\sum (\\hat{Y_i} - \\bar{Y})^2\\), thus \\(p - 1\\) \\(\\Longleftrightarrow\\) the deviations \\(\\hat{Y_i} - \\bar{Y}\\) are subject to one constraint: they must sum to zero.\n\n\n(Note: The constraints on the deviations come from the properties of the fitted LSE line.)\n\nProperty of degrees of freedom\n\nDfs are additive\n\n\\[\n\\begin{align*}\n  n - 1 &= 1 + (n-2) \\\\\n  df_{TO} &= df_{R} +df_{E}\n\\end{align*}\n\\]\n\n2.6.3 Mean squares\n\nGeneral definition → A sum of squares divided by its associated degrees of freedom is called a mean square (\\(MS\\)).\n\nTwo mean squares\n\nMean square regression:\n\n\\[MSR = \\frac{SSR}{df_R} = \\frac{SSR}{1}\\]\n\nMean square error:\n\n\\[MSE = \\frac{SSR}{df_E} = \\frac{SSE}{n-2}\\]\n\nNote → Mean squares are not additive \\(\\Longrightarrow\\) \\(MSR + MSE \\ne MSTO\\)\n\n2.6.4 ANOVA table\n\nThe breakdowns of the total sum of squares and associated degrees of freedom are displayed in the form of an analysis of variance table.\nBelow shows an extra column for expected mean squares, which will be needed for inference (usual ANOVA tables have everything except this column).\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\n\\(df\\)\n\\(SS\\)\n\\(MS\\)\n\\(E(MS)\\)\n\\(F\\)\n\n\\(p\\)-value\n\n\n\nRegression\n\\(df_R = 1\\)\n\n\\(SSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\\) 1\n\\(MSR = \\frac{SSR}{1}\\)\n\\(\\sigma^2 + \\beta_1 \\sum (X_i - \\bar{X})^2\\)\n\\(\\frac{MSR}{MSE}\\)\n\\(P(F_{(df_R, df_E)} &gt; F)\\)\n\n\nError\n\\(df_E = n - 2\\)\n\\(SSE = \\sum (Y_i - \\hat{Y_i})^2\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\\(\\sigma^2\\)\n\n\n\n\nTotal\n\\(df_{TO} = n -1\\)\n\\(SSTO = \\sum (Y_i - \\bar{Y})^2\\)\n\n\n\n\n\n\n\n\nCould convert \\(SSTO = S_{YY}\\) to a mean square as well by dividing by \\(df_{TO} = n - 1\\) → This gives us \\(\\frac{1}{n - 1}{\\sum (Y_i - \\bar{Y})^2}\\) = Sample variance of \\(Y_i\\) (totally unrelated to the regression line; just the variance of a set of numbers, which is technically a mean square).\nThis is different than the regression \\(S^2 = MSE\\) which is an estimate of the error variance which is found by taking the deviations relative to the fitted line (not \\(bar{Y}\\)).\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 3; sigma &lt;- 15 \n\n# generate data\nx &lt;- runif(n = n, min = 5, max = 15)\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# now introducing another function to fit models and the different results / outputs from each\n\n# fit equivalent models\n# -&gt; lm() fits linear models\n# --&gt; also one-way anova (ancova) models, but aov() works better with anova analyses\nmod_lm &lt;- lm(y ~ x)\nmod_aov &lt;- aov(y ~ x)\n\n# the main difference between lm() and aov() is mainly in the form of the output\n\n# calling (printing) the model object\n# -&gt; for lm object gives estimated coefficients\nmod_lm\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -8.139        4.165  \n\n# -&gt; for aov object it gives breakdown of SS and df for each variable and sigma estimate\nmod_aov\n\nCall:\n   aov(formula = y ~ x)\n\nTerms:\n                       x Residuals\nSum of Squares  3688.611  5299.585\nDeg. of Freedom        1        28\n\nResidual standard error: 13.75758\nEstimated effects may be unbalanced\n\n# summary() function\n# -&gt; for lm it gives regression-style output, i.e. regression coefficients with standard errors and t-tests\nsummary(mod_lm)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.513  -8.663  -3.052   8.726  30.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -8.1391     8.8030  -0.925 0.363086    \nx             4.1651     0.9435   4.415 0.000137 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.76 on 28 degrees of freedom\nMultiple R-squared:  0.4104,    Adjusted R-squared:  0.3893 \nF-statistic: 19.49 on 1 and 28 DF,  p-value: 0.0001371\n\n# -&gt; for aov it gives anova table,which is the same information but represented as sums of squares estimates with F ratios\nsummary(mod_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx            1   3689    3689   19.49 0.000137 ***\nResiduals   28   5300     189                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# can switch between the two summaries by calling a summary method\n# -&gt; get anova table from lm object\nsummary.aov(mod_lm)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx            1   3689    3689   19.49 0.000137 ***\nResiduals   28   5300     189                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# -&gt; get regression-style output from aov object\nsummary.lm(mod_aov)\n\n\nCall:\naov(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.513  -8.663  -3.052   8.726  30.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -8.1391     8.8030  -0.925 0.363086    \nx             4.1651     0.9435   4.415 0.000137 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.76 on 28 degrees of freedom\nMultiple R-squared:  0.4104,    Adjusted R-squared:  0.3893 \nF-statistic: 19.49 on 1 and 28 DF,  p-value: 0.0001371\n\n# alternative (more straight-forward) way to get anova table \n# -&gt; use anova() on lm object &lt;==&gt; anova.lm() -&gt; this is one of the two main uses of this function\n# -&gt; looking for Df, Sum Sq, and Mean Sq\n# --&gt; gives sequential SS (will look into more later)\nanova(mod_lm) %&gt;% as.matrix # could do anova(mod_aov), but makes for sense just to do summary() if already have aov object\n\n          Df   Sum Sq   Mean Sq  F value       Pr(&gt;F)\nx          1 3688.611 3688.6107 19.48853 0.0001370854\nResiduals 28 5299.585  189.2709       NA           NA\n\n# save usual lm model as another object to simplify notation\nmod &lt;- mod_lm\n\n\n\n\n# recreating each value in the anova table output\n\n# degrees of freedom \n# -&gt; regression: each individual coefficient gets 1 df, then lose 1; so for SLR df = 2 - 1 = 1\n# -&gt; error: df = n - # of predictors (p) as usual\ndf_e &lt;- mod$df.residual\ndf_r &lt;- length(coef(mod)) - 1\n\n# sums of squares\n# -&gt; SSR = explained error (improved prediction) -&gt; Y-hat - Y-bar\n# -&gt; SSE = still unexplained error (residuals) -&gt; Y - Y-hat\nssr &lt;- sum((fitted(mod) - mean(y))^2)\nsse &lt;- sum((y - fitted(mod))^2)\n\n# mean squares = respective SS / df\nmsr &lt;- ssr / df_r\nmse &lt;- sse / df_e\n\n# combine (organize) into anova table layout for comparison\n# -&gt; fill matrix by column\nat &lt;- matrix(data = c(c(df_r, df_e), c(ssr, sse), c(msr, mse)), nrow = 2, byrow = FALSE)\ncolnames(at) &lt;- c(\"df\", \"SS\", \"MS\")\nrownames(at) &lt;- c(\"x\", \"error\")\n\n# compare results to relevant pieces of anova(lm())\ncompare(anova(mod_lm)[,1:3], at)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Modes: list, numeric\"                                                \n[2] \"Lengths: 3, 6\"                                                       \n[3] \"Attributes: &lt; Names: 2 string mismatches &gt;\"                          \n[4] \"Attributes: &lt; Component 1: Modes: character, numeric &gt;\"              \n[5] \"Attributes: &lt; Component 1: target is character, current is numeric &gt;\"\n[6] \"Attributes: &lt; Component 2: Modes: character, list &gt;\"                 \n[7] \"Attributes: &lt; Component 2: target is character, current is list &gt;\"   \n[8] \"current is not list-like\"                                            \n\n$comparison$`element-wise`\nNULL\n\n\n$`anova(mod_lm)[, 1:3]`\n          Df Sum Sq Mean Sq\nx          1 3688.6  3688.6\nResiduals 28 5299.6   189.3\n\n$at\n      df       SS        MS\nx      1 3688.611 3688.6107\nerror 28 5299.585  189.2709\n\n\n\n\n\n# demonstrate additive df and SS\n\n# df total = n - 1 (have to estimate pop mean, so lose 1)\ndf_to &lt;- length(y) - 1\n\n# SSTO = total deviation (S_YY) -&gt; Y - Y-bar\nssto &lt;- sum((y - mean(y))^2)\n\n# compare pieces from anova to calculated totals\ncompare(c(sum(anova(mod)[, \"Df\"]), sum(anova(mod)[, \"Sum Sq\"])), c(df_to, ssto))\n\n$comparison\n[1] TRUE\n\n$`c(sum(anova(mod)[, \"Df\"]), sum(anova(mod)[, \"Sum Sq\"]))`\n[1]   29.000 8988.196\n\n$`c(df_to, ssto)`\n[1]   29.000 8988.196\n\n\n\n\n\n\n2.6.5 Expected mean squares\nGoal → In order to make inferences based on the analysis of variance approach, we need to know the expected value of each of the mean squares.\n\n\nResults\nDerivation\n\n\n\n\nThe expected value of a mean square is the mean of its sampling distribution and tells us what is being estimated by the mean square.\nIt can be shown that:\n\n\\[\n\\begin{align*}\n  E(MSE) &= \\sigma^2 \\\\\n  E(MSR) &= \\sigma^2 + \\beta_1^2 \\sum (X_i - \\bar{X})^2\n\\end{align*}\n\\]\n\nNote that the first result goes with earlier statement that \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nImportant implications\n\nThe mean of the sampling distribution of \\(MSE\\) is \\(\\sigma^2\\) whether or not \\(Y\\) and \\(X\\) are linearly related (i.e. whether or not \\(\\beta_1 = 0\\)).\nThe mean of the sampling distribution of \\(MSR\\) is also \\(\\sigma^2\\) when \\(\\beta_1 = 0\\).\n\n\nThus, when \\(\\beta_1 = 0\\), the sampling distributions of \\(MSE\\) and \\(MSR\\) are located identically and \\(MSE\\) and \\(MSR\\) will tend to be relatively close to each other.\nBut, when \\(\\beta_1 \\ne 0\\), the mean of the sampling distribution of \\(MSR\\) will be greater than \\(\\sigma^2\\) (because \\(\\beta_1^2 \\sum (X_i - \\bar{X})^2\\) then must be positive) and therefore located to the right of that of \\(MSE\\). So, \\(MSR\\) will tend to be larger than \\(MSE\\).\nThese results suggest that a comparison of \\(MSR\\) and \\(MSE\\) is useful for testing whether or not \\(\\beta_1 = 0\\).",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#f-test-of-beta_1-0-vs-beta_1-ne-0",
    "href": "notes-inference.html#f-test-of-beta_1-0-vs-beta_1-ne-0",
    "title": "2  Inference",
    "section": "\n2.7 \\(F\\) test of \\(\\beta_1 = 0\\) vs \\(\\beta_1 \\ne 0\\)\n",
    "text": "2.7 \\(F\\) test of \\(\\beta_1 = 0\\) vs \\(\\beta_1 \\ne 0\\)\n\nOverview\n\nThe analysis of variance approach lets us perform very useful test for regression models (and other linear statistical models).\n\n\n\nResults\nDerivation\n\n\n\nTest on slope\n\nHypotheses → For the SLR, ANOVA gives us a test for:\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\nTest statistic → For ANOVA, the test statistic \\(F^*\\) compares \\(MSR\\) and \\(MSE\\)\n\n\n\\[\nTS = F^* = \\frac{MSR}{MSE}\n\\]\n\n(see derivation) Under \\(H_0: \\beta_1 = 0\\) → \\(F^* \\sim \\text{F}\\,_{(1, n-2)}\\)\n\nRejection region and p-value\n\n\n\\(F^*\\) values near 1 support \\(H_0\\) and large \\(F^*\\) values support \\(H_A\\) (if the model is useful, we expect \\(MSR\\) to be large compared to \\(MSE\\)) \\(\\Longrightarrow\\) Upper-tailed test.\n\n\n\n\\[\n\\begin{align*}\n  RR &= \\{F^* &gt; F_{(1 - \\alpha;\\, 1, n - 2)}\\} \\\\\n  p\\text{-value} &= P(F_{(1, n-2)} \\ge F^*)\n\\end{align*}\n\\]\n\nNote → \\(F_{(1-\\alpha;\\,1,n-2)}\\) is the \\(100 (1-\\alpha)\\) percentile of the appropriate \\(F\\) distribution (different notation meaning than \\(t_{\\alpha / 2}\\) because not a symmetric distribution now).\n\nDecision → Same rules as usual, now just with \\(F\\)-distribution\n\nReject \\(H_0\\) and conclude \\(H_A\\) if \\(\\hspace{10pt}\\) \\(TS \\in RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} \\le \\alpha\\); Fail to reject \\(H_0\\) if previous not true.\n\n\n\nConclusion / Interpretation\n\nAt the \\(\\alpha\\) significance level, we &lt; have / do not have &gt; sufficient evidence of a significant linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;.\n\n\n\nEquivalence of \\(F\\) test and \\(t\\) test\n\n\nThe \\(F\\)-test is algebraically equivalent to the two-tailed \\(t\\) test → \\(F^* = (t^*)^2\\)\n\nSame relationship for the critical values when defining the rejection region / p-value → \\(F_{(1-\\alpha; \\, 1, n-2)} = t_{\\alpha/2, n-2}^2\\).\n\n\nWill get same conclusion either way, but the \\(t\\) test is more flexible because it can be used for one-sided alternatives involving \\(\\beta_1\\), while the \\(F\\) test cannot.\n\n\n\nSampling distribution of \\(F^*\\)\n\nGoal → In order to be able to construct a statistical decision rule and examine its properties, we need to know the sampling distribution of \\(F^*\\).\n\nDerivation → Start by considering the sampling distribution of \\(F^*\\) under \\(H_0: \\beta_1 = 0\\). We will use the following theorem:\n\n\nCochran’s theorem → Let \\(Y_1, \\ldots, Y_{n}\\) represent a random sample from the same normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Suppose \\(SSTO = \\sum (Y_i - \\bar{Y})^2\\) is partitioned into \\(k\\) sums of squares \\(SS_r\\), each with degrees of freedom \\(df_r\\). If \\(\\displaystyle \\sum_{r=1}^k df_{r} = n - 1\\), then each of the \\(\\frac{SS_r}{\\sigma^2}\\) terms are independent \\(\\chi^2\\) random variables with \\(df_r\\) degrees of freedom.\n\n\n\n\n\n\nUnder \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\\(F^* \\sim \\text{Non-central F}\\,_{(1, n - 2)}\\) with non-centrality parameter \\(\\lambda\\).\nStill \\(SSR \\perp \\!\\!\\! \\perp SSE\\) and \\(SSE / \\sigma^2 \\sim \\chi^2_{n-2}\\). But the condition that both \\(SSR / \\sigma^2\\) and \\(SSE / \\sigma^2\\) are \\(\\chi^2\\) random variables requires \\(\\beta_1 = 0\\).\n\n\n\nEquivalence of \\(F\\) test and \\(t\\) test\n\n\n\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# continuing previous anova table demo\n\n# show anova table\n# -&gt; looking for F value` and Pr(&gt;F)\nanova(mod) %&gt;% as.matrix\n\n          Df   Sum Sq   Mean Sq  F value       Pr(&gt;F)\nx          1 3688.611 3688.6107 19.48853 0.0001370854\nResiduals 28 5299.585  189.2709       NA           NA\n\n# equivalence of F test and two sided t-test\ncompare(anova(mod)[\"x\",\"F value\"], summary(mod)$coefficients[\"x\",\"t value\"]^2)\n\n$comparison\n[1] TRUE\n\n$`anova(mod)[\"x\", \"F value\"]`\n[1] 19.48853\n\n$`summary(mod)$coefficients[\"x\", \"t value\"]^2`\n[1] 19.48853\n\n\n\n\n\n# continuing to recreate each value in the anova table output\n\n# calculate F and p-value\n# -&gt; TS F* = MSR / MSE\n# -&gt; p-value = P(F(df reg, df error) &gt; F*)\nF_star &lt;- msr / mse\np_value &lt;- pf(q = F_star, df1 = df_r, df2 = df_e, lower.tail = FALSE)\n\n# compare results to relevant pieces of anova(lm())\ncompare(anova(mod_lm)[1,4:5], c(F_star, p_value))\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Modes: list, numeric\"                                \n[2] \"Attributes: &lt; Modes: list, NULL &gt;\"                   \n[3] \"Attributes: &lt; Lengths: 2, 0 &gt;\"                       \n[4] \"Attributes: &lt; names for target but not for current &gt;\"\n[5] \"Attributes: &lt; current is not list-like &gt;\"            \n\n$comparison$`element-wise`\nNULL\n\n\n$`anova(mod_lm)[1, 4:5]`\n  F value    Pr(&gt;F)    \nx  19.488 0.0001371 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`c(F_star, p_value)`\n[1] 19.4885259006  0.0001370854",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#general-linear-test-approach",
    "href": "notes-inference.html#general-linear-test-approach",
    "title": "2  Inference",
    "section": "\n2.8 General linear test approach",
    "text": "2.8 General linear test approach\nOverview\n\n\nThe ANOVA \\(F\\) test above is an example of a General Linear Test (GLT) (also called a global or an omnibus test) for a statistical model, which is an approach that can be used for highly complex tests of linear statistical models, as well as for simple tests.\n\nFor SLR, the global test (the significance of a model test), the ANVOA \\(F\\) test, and the \\(t\\) test for the linear impact are all equivalent.\n\n\n\nIt has three basic steps, which are described in more detail below:\n\nFit the full model and obtain the error sum of squares \\(SSE(F)\\).\nFit the reduced model under Ho and obtain the error sum of squares \\(SSE(R)\\).\nUse test statistic / p-value to make decision.\n\n\n\nFull model\n\n\nStart with the model considered to be appropriate for the data (or the model with all available predictors); this is called the full / unrestricted model.\n\nFor SLR, the full model is just the normal error regression model:\n\n\n\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{Full model}\n\\]\n\nWe need to fit the full model and get the error sum of squares, denoted \\(SSE(F)\\) (deviations of \\(Y_i\\) and its estimated expected value \\(\\hat{Y_i}\\), which is the fitted regression line). For the full model, we have:\n\n\\[\nSSE(F) = \\sum (Y_i - \\hat{Y_i})^2 = \\sum [Y_i - (\\hat{\\beta}_0  + \\hat{\\beta}_1 X_i)]^2 = SSE\n\\]\n\n\nWith a SLR full model, the error sum of squares is the usual \\(SSE\\).\n\nReduced model\n\nNext we consider \\(H_0\\). For SLR, we have:\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\hspace{20pt} \\text{Reduced model is appropriate} \\\\\n  H_A &: \\beta_1 \\ne 0 \\hspace{20pt} \\text{Full model is appropriate}\n\\end{align*}\n\\]\n\nThe model under \\(H_0\\) is called the reduced / restricted model. When \\(\\beta_1 = 0\\), the full model reduces to:\n\n\\[\nY_i = \\beta_0 + 0 \\cdot X_i + \\epsilon_i = \\beta_0 + \\epsilon_i \\hspace{20pt} \\text{Reduced model}\n\\]\n\n\nThen we fit the reduced model and again get the error sum of squares, now denoted \\(SSE(R)\\).\n\n\nFor this particular (a SLR) reduced model, it can easily be shown that the LSE and MLE estimator of \\(\\beta_0\\) is \\(\\bar{Y}\\). Thus the estimated expected value of each observation is \\(\\hat{\\beta}_0 = \\bar{Y}\\) and we can get the error sum of squares with:\n\n\n\n\\[\nSSE(R) = \\sum (Y_i - \\hat{Y_i})^2 = \\sum (Y_i - \\hat{\\beta}_0)^2 = \\sum (Y_i - \\bar{Y})^2 = SSTO\n\\]\n\nThus, for any intercept-only reducted model \\(\\Longrightarrow\\) \\(SSE(R) = SSTO\\).\n\nTest statistic and decision\n\n\nLogic → Now we compare the two sum of squares from the full and reduced model using the fact that the reduced \\(SSE\\) is always greater than or equal to the full \\(SSE\\).\n\n\n\\(SSE(R) \\ge SSE(F)\\) → More parameters in the model ALWAYS leads to a better fit (i.e. less unexplained variability = more explained variability) \\(\\Longrightarrow\\) Smaller are the deviations around the fitted regression function.\n\n\n\nComparison scenarios\n\n\n\\(SSE(F)\\) are close \\(SSE(R)\\)\n\n\n\nUsing the full model does not account for much more unexplained variability than does the reduced model \\(\\Longrightarrow\\) Added parameters in the full model do not really help to reduce the unexplained variation \\(\\Longrightarrow\\) Reduced model is adequate and \\(H_0\\) holds.\n\n\n\\(SSE(R) &lt;&lt; SSE(F)\\)\n\n\nThe additional parameters in the full model do help to substantially reduce the unexplained variability in \\(Y_i\\), which means \\(H_A\\) holds.\n\n\nTest statistic is a function of the difference in two \\(SSE\\)s, relative to the full \\(SSE\\):\n\n\\[\nTS = F^* = \\frac{SSE(R) - SSE(F)}{df_R - df_F} \\Big/ \\frac{SSE(F)}{df_F} = \\frac{SSE(R) - SSE(F)}{df_R - df_F}  \\Big/ MSE(F)\n\\]\n\nUnder \\(H_0: \\beta_1 = 0 \\hspace{10pt} \\text{Reduced model}\\) → \\(F^* \\sim \\text{F}\\,_{(df_R - df_F, df_F)}\\)\n\nRejection region and p-value\n\nAgain, we reject for large values of \\(F^*\\) (large difference \\(SSE(R) - SSE(F)\\) supports \\(H_A\\)).\n\n\n\n\\[\n\\begin{align*}\n  RR &= \\{F^* &gt; F_{(1 - \\alpha;\\, df_R - df_F, \\, df_F)}\\} \\\\\n  p\\text{-value} &= P(F_{(df_R - df_F, \\, df_F} \\ge F^*)\n\\end{align*}\n\\]\n\nFor SLR (testing whether or not \\(\\beta_1 = 0\\)), we have:\n\n\\[\n\\begin{align*}\n  SSE(R) &= SSTO \\hspace{20pt} SSE(F) = SSE \\\\\n  f_R &= n - 1 \\hspace{45pt} df_F = n - 2\n\\end{align*}\n\\]\n\nSo the test statistic becomes\n\n\\[\nF^* = \\frac{SSTO - SSE}{(n - 1) - (n - 2)} \\Big/ \\frac{SSE}{n - 2} = \\frac{SSR}{1} \\Big/ \\frac{SSE}{n - 2} = \\frac{MSR}{MSE}\n\\]\n\nThis is equivalent to the ANOVA \\(F\\) test shown earlier.\n\n\n\nR functions\nManual\n\n\n\n\n# using the same data as anova table demos\n\n# fit full model\n# -&gt; for SLR, full model is E(Y) = B0 + B1 X\nmod_full &lt;- lm(y ~ x)\n\n# fit reduced model\n# -&gt; for SLR, full model is E(Y) = B0\n# -&gt; to fit an intercept-only model, specify 1 on the RHS\nmod_reduced &lt;- lm(y ~ 1)\n\n# perform general linear test of reduced vs full model\n# -&gt; testing H0: beta_1 = 0 (reduced model) vs HA: beta_1 != 0 (full model)\n# function call\n# -&gt; anova() on multiple lm objects &lt;==&gt; anova.lmlist() -&gt; this is the second main uses of this function\n# -&gt; typically will supply models smallest to largest (this makes for a natural interpretation of the results); but it works regardless\n# -&gt; models MUST BE nested AND fit on the same dataset in order for results to make statistical sense\n# results\n# -&gt; sequentially gives the change in dfs and SS from mod 1 to mod 2 (then from mod 2 to mod 3, and so on...)\n# -&gt; so it is df_1 - df_2 and SS_1 - SS_2 ==&gt; if nested from smallest to largest model, results will all be positive and interpreted as \"additional reductions\"\n# -&gt; by default, performs F test comparing models in the order specified (again sequentially)\nanova(mod_reduced, mod_full, test = \"F\") %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq        F       Pr(&gt;F)\n1     29 8988.196 NA        NA       NA           NA\n2     28 5299.585  1  3688.611 19.48853 0.0001370854\n\n# demo with more than two models\nmod_squared &lt;- lm(y ~ x + I(x^2))\nanova(mod_reduced, mod_full, mod_squared) %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq          F       Pr(&gt;F)\n1     29 8988.196 NA        NA         NA           NA\n2     28 5299.585  1 3688.6107 19.3794314 0.0001516424\n3     27 5139.082  1  160.5029  0.8432593 0.3665972510\n\n# demo showing how the sequential comparison works\n# -&gt; above compared mod 1 to mod 2 and then mod 2 to mod 3\n# -&gt; now comparing mod 1 directly to mod 3\nanova(mod_reduced, mod_squared) %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq        F       Pr(&gt;F)\n1     29 8988.196 NA        NA       NA           NA\n2     27 5139.082  2  3849.114 10.11135 0.0005276865\n\n\n\n\n\n# general linear test of reduced vs full model\n\n# using the same data as anova table demos\n\n# for full model -&gt;  get the corresponding SSE and df \n# -&gt; for SLR, full model is E(Y) = B0 + B1 X\nsse_f &lt;- resid(mod_full)^2 %&gt;% sum\ndf_f &lt;- df.residual(mod_full)\n\n# for reduced model -&gt; get the corresponding SSE and df\nsse_r &lt;- resid(mod_reduced)^2 %&gt;% sum\ndf_r &lt;- df.residual(mod_reduced)\n\n# calculate F stat and p-value\n# -&gt; using the shortcut for second term: F* = [(SSE(R) - SSE(F)) / (df_R - df_F)] / MSE(F)\nmse_f &lt;- summary(mod_full)$sigma^2\nF_star &lt;- ((sse_r - sse_f) / (df_r - df_f)) / mse_f\np_value &lt;- pf(q = F_star, df1 = df_r - df_f, df2 = df_f, lower.tail = FALSE)\n\n# combine (organize) into anova table layout for comparison\nat &lt;- matrix(data = c(c(df_r, df_f), c(sse_r, sse_f), c(NA, df_r - df_f), c(NA, sse_r - sse_f), c(NA, F_star), c(NA, p_value)), nrow = 2, byrow = FALSE)\ncolnames(at) &lt;- c(\"df_E Original\", \"SSE Original\", \"Change df\", \"Change SSE\", \"F*\", \"p-value\")\nrownames(at) &lt;- c(\"Mod 1 - Reduced\", \"Mod 2 - Full\")\n\n# compare to results from anova(reduced mod, full model)\ncompare(anova(mod_reduced, mod_full), at)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Modes: list, numeric\"                                                \n[2] \"Lengths: 6, 12\"                                                      \n[3] \"Attributes: &lt; Names: 2 string mismatches &gt;\"                          \n[4] \"Attributes: &lt; Length mismatch: comparison on first 2 components &gt;\"   \n[5] \"Attributes: &lt; Component 1: Modes: character, numeric &gt;\"              \n[6] \"Attributes: &lt; Component 1: target is character, current is numeric &gt;\"\n[7] \"Attributes: &lt; Component 2: Modes: character, list &gt;\"                 \n[8] \"Attributes: &lt; Component 2: target is character, current is list &gt;\"   \n[9] \"current is not list-like\"                                            \n\n$comparison$`element-wise`\nNULL\n\n\n$`anova(mod_reduced, mod_full)`\nAnalysis of Variance Table\n\nModel 1: y ~ 1\nModel 2: y ~ x\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     29 8988.2                                  \n2     28 5299.6  1    3688.6 19.488 0.0001371 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$at\n                df_E Original SSE Original Change df Change SSE       F*\nMod 1 - Reduced            29     8988.196        NA         NA       NA\nMod 2 - Full               28     5299.585         1   3688.611 19.48853\n                     p-value\nMod 1 - Reduced           NA\nMod 2 - Full    0.0001370854\n\n# for SLR ==&gt; equivalent to F test on beta 1\ncompare(anova(mod_reduced, mod_full)[2, \"F\"], anova(mod_full)[\"x\", \"F value\"])\n\n$comparison\n[1] TRUE\n\n$`anova(mod_reduced, mod_full)[2, \"F\"]`\n[1] 19.48853\n\n$`anova(mod_full)[\"x\", \"F value\"]`\n[1] 19.48853",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#descriptive-measures-of-linear-association-between-x-and-y",
    "href": "notes-inference.html#descriptive-measures-of-linear-association-between-x-and-y",
    "title": "2  Inference",
    "section": "\n2.9 Descriptive measures of linear association between \\(X\\) and \\(Y\\)\n",
    "text": "2.9 Descriptive measures of linear association between \\(X\\) and \\(Y\\)\n\nOverview\n\nThere is no one single measure to completely describe the usefulness of a regression model for a particular application.\nIf the goal is estimation of parameters and means and predicting new observations, usefulness of estimates or predictions depends upon the width of the interval and the user’s needs for precision. This can vary from one application to another.\nRather than making inferences, goals could be to describe the degree of linear association between \\(Y\\) and \\(X\\). Again the usefulness of which measure and its value depend on the application.\n\n\n2.9.1 Coefficient of determination\nOverview\n\nA very common measure because of its simplicity is the coefficient of determination \\(R^2\\), which is a measure of the effect of \\(X\\) in reducing the uncertainty in predicting \\(Y\\). This reduction in sum of squares (\\(SSTO - SSE = SSR\\)) gets expressed as a proportion:\n\n\\[\nR^2 = \\frac{SSR}{SSTO} = 1-\\frac{SSE}{SSTO}, \\hspace{20pt} \\text{range:} \\hspace{10pt} 0 \\le R^2 \\le 1\n\\] Interpretation\n\n\n&lt; \\(R^2 *100\\) &gt;% of the variation in &lt; \\(Y\\) context &gt; can be explained by the linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;.\n\nSo, the larger \\(R^2\\) is, the more the total variation of \\(Y\\) is reduced by introducing the predictor variable \\(X\\) \\(\\Longleftrightarrow\\) greater degree of linear association between \\(Y\\) and \\(X\\).\n\n\nPractically, this indicates the quality of the fit by measuring the proportion of variability explained by the fitted model.\n\nLimiting values\n\nWhen the fitted regression line is horizontal (\\(\\hat{\\beta}_1 = 0\\) and \\(\\hat{Y_i} = \\bar{Y}\\)) \\(\\Longrightarrow\\) \\(SSE = SSTO\\) and \\(R^2 = 0\\).\nWhen there is a perfect fit (all of the points lie on the fitted regression line) \\(\\Longrightarrow\\) \\(SSE = 0\\) and \\(R^2 = 1\\).\nIn practice, unlikely to be exactly equal to either of these. Also note that context of data (scientific field of the application) has a big impact on general values of \\(R^2\\) and consequently what is interpreted as “strong”.\nIf \\(R^2\\) is small, we can consider adding other independent variables that can explain a significant portion of the remaining unexplained variability in the model.\n\n\n\nLimitations of \\(R^2\\)\n\n\nUsefulness in prediction → A high coefficient of determination does not necessarily indicate that useful (precise) predictions can be made.\n\nThis is because \\(R^2\\) measures only a relative reduction from \\(SSTO\\) and provides no information about absolute precision for estimating a mean response or predicting a new observation.\n\n\n\nQuality of fit → A high coefficient of determination does not necessarily indicate that the estimated regression line is a good fit and similarly an \\(R^2\\) near zero does not necessarily indicate that \\(Y\\) and \\(X\\) are not related.\n\nThis is because \\(R^2\\) measures the degree of linear association between \\(Y\\) and \\(X\\), whereas the actual regression relation may be curvilinear. So make sure to look at the scatterplot.\n\n\n\n\nInflating \\(R^2\\)\n\n\n\\(X\\) level spacing → The value taken by \\(R^2\\) in a given sample tends to be affected by the spacing of the \\(X\\) observations: Wider spacing in \\(X_i\\) \\(\\Longrightarrow\\) higher \\(R^2\\). Here’s why:\n\nWider spacing (larger spread) in \\(X_i\\) in the sample when \\(\\hat{\\beta}_1 \\ne 0\\) \\(\\Longrightarrow\\) larger spread of the observed \\(Y_i\\) around \\(\\bar{Y}\\) \\(\\Longrightarrow\\) Larger \\(SSTO\\).\nAnd since \\(SSE\\) is unaffected (\\(V(Y_i) = \\sigma^2\\) for all \\(X_i\\)), \\(SSR\\) has to increase. Then we can see from either representation \\(R_2 = SSR/ SSTO = 1 - SSE / SSTO\\), that \\(R^2\\) will increase.\n\nOverfitting → \\(R^2\\) can be artificially inflated by including additional model terms (adding extra predictors).\n\nThis is because \\(SSR\\) always increases with more predictors, even if they are completely unrelated to the response variable.\nLater \\(R^2_{adj}\\) will be discussed which corrects for the inclusion of extra predictors.\n\n\n\n\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 5; sigma &lt;- 10\n\n# generate data\nx &lt;- runif(n = n, min = 5, max = 15)\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# display summary\n# -&gt; looking for Multiple R-squared\nsummary(mod)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.1251  -4.6996  -0.8081   4.5217  21.4896 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)   3.3090     6.2154   0.532         0.599    \nx             4.9094     0.5851   8.391 0.00000000398 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.233 on 28 degrees of freedom\nMultiple R-squared:  0.7155,    Adjusted R-squared:  0.7053 \nF-statistic: 70.41 on 1 and 28 DF,  p-value: 0.000000003976\n\n\n\n# plot x and y with regression line\n# -&gt; add annotation for R^2 value\nplot(x = x, y = y)\nabline(mod, col = \"red\")\ntext(x = 14, y = 30, labels = bquote(R^2 * \" = \" * .(round(summary(mod)$r.squared,3))), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n# get anova table for model\nat &lt;- anova(mod)\nas.matrix(at)\n\n          Df   Sum Sq    Mean Sq  F value            Pr(&gt;F)\nx          1 6001.815 6001.81492 70.40678 0.000000003976141\nResiduals 28 2386.856   85.24484       NA                NA\n\n# calculate R^2 from sums of squares from anova table\n# R^2 = SSR / SSTO = SSR / (SSR + SSE) = 1 - SSE / SSTO\nssr &lt;- at[\"x\", \"Sum Sq\"]\nsse &lt;- at[\"Residuals\", \"Sum Sq\"]\nr_squared &lt;- ssr / (ssr + sse)\nr_squared &lt;- 1 - sse / (ssr + sse)\n\n# compare results from summary(lm())\ncompare(summary(mod)$r.squared, r_squared)\n\n$comparison\n[1] TRUE\n\n$`summary(mod)$r.squared`\n[1] 0.7154668\n\n$r_squared\n[1] 0.7154668\n\n\n\n\n\n# demo to show how more spacing of X levels increases R^2 value\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 3; sigma &lt;- 5 \n\n# generate two datasets\n# -&gt; generate common error terms, so the only difference is the spacing of the X levels\n# -&gt; generate two X vectors, one with larger spread of X values\n# -&gt; calculate Y\n# -&gt; add indicator for spread and then combine into one dataframe\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\ndata_samples &lt;- list(runif(n = n, min = 8, max = 12),\n                     runif(n = n, min = 2, max = 16)) %&gt;% \n  map(\\(x) data.frame(x = x,\n                      y = beta_0 + beta_1 * x + epsilon)) %&gt;% \n  map2(c(\"smaller\", \"larger\"), \\(df, spread) mutate(df, spread = spread)) %&gt;% \n  bind_rows\n\n# confirm same error terms\n# -&gt; calculate epsilon as Y - E(Y) and then add observation number column to sort by\ndata_samples %&gt;% \n  mutate(epsilon = y - (beta_0 + beta_1 * x)) %&gt;% \n  mutate(observation = 1:n, .by = spread) %&gt;% \n  arrange(observation) %&gt;% \n  head(n = 6) %&gt;% \n  display_nice\n\n\n\n x \n    y \n    spread \n    epsilon \n    observation \n  \n\n\n 9.243 \n    30.679 \n    smaller \n    0.951 \n    1 \n  \n\n 9.429 \n    31.238 \n    larger \n    0.951 \n    1 \n  \n\n 9.426 \n    21.439 \n    smaller \n    -8.837 \n    2 \n  \n\n 4.938 \n    7.975 \n    larger \n    -8.837 \n    2 \n  \n\n 10.484 \n    34.348 \n    smaller \n    0.897 \n    3 \n  \n\n 14.458 \n    46.271 \n    larger \n    0.897 \n    3 \n  \n\n\n\n# calculate R^2 for each spread dataset\ndata_samples %&gt;% \n  split(.$spread) %&gt;% \n  map_dbl(\\(df) summary(lm(y ~ x, data = df))$r.squared) %&gt;% \n  round(3)\n\n larger smaller \n  0.827   0.295 \n\n# create plot of both sets of points and regression line\n# -&gt; add reference line for Y-bar for each dataset (to show how SSTO is calculated)\nggplot(aes(x = x,\n           y = y,\n           color = spread),\n       data = data_samples) + \n  geom_point() + \n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE,\n              fullrange = TRUE) + \n  geom_hline(aes(yintercept = y_bar,\n                 color = spread),\n             data = summarize(data_samples, y_bar = mean(y), .by = spread),\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n2.9.2 Coefficient of correlation\n\nA measure of linear association between \\(Y\\) and \\(X\\) when both \\(Y\\) and \\(X\\) are random is the coefficient of correlation. This is the signed square root of \\(R^2\\):\n\n\\[\nr = \\pm \\sqrt{R^2}\n\\]\n\nA plus or minus sign is attached to this measure according to whether the slope of the fitted regression line is positive or negative. Thus, \\(-1 \\le r \\le 1\\).\nONLY in SLR can the coefficient of determination \\(R^2\\) be computed as the square of the correlation coefficient \\(r^2\\).\n\n\n# continuing previous example\n\n# calculate correlation\nr &lt;- cor(x, y)\n\n# compare to squared-correlation to R^2\ncompare(summary(mod)$r.squared, r^2)\n\n$comparison\n[1] TRUE\n\n$`summary(mod)$r.squared`\n[1] 0.7154668\n\n$`r^2`\n[1] 0.7154668\n\n\n\nNote → Regression models do not contain a parameter to be estimated by \\(R^2\\) or \\(r\\). These are simply descriptive measures of the degree of linear association between \\(Y\\) and \\(X\\) in the sample observations that may, or may not, be useful in any instance.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#considerations-in-applying-regression-analysis",
    "href": "notes-inference.html#considerations-in-applying-regression-analysis",
    "title": "2  Inference",
    "section": "\n2.10 Considerations in applying regression analysis",
    "text": "2.10 Considerations in applying regression analysis\nReminders when implementing regression models\n\nFrequently, regression analysis is used to make inferences for the future (e.g. a school board wants to predict future enrollments by using a regression model containing several demographic variables as predictor variables). In these situations, the validity of the regression application depends on whether basic conditions in the future will be similar to those at the time the regression analysis is based on.\nIn predicting new observations on \\(Y\\), the predictor variable \\(X\\) itself often has to be predicted. Therefore, predictions are dependent upon the correctness of the population projection (i.e. they are conditional predictions).\nBe careful of extrapolation. We cannot be sure that the regression function that fits the past data is appropriate over a wider range of the predictor variable.\nA statistical test that concludes \\(\\beta_1 \\ne 0\\) does not establish a cause-and-effect relation between the predictor and response variables. With nonexperimental data, both the \\(X\\) and \\(Y\\) variables may be simultaneously influenced by other variables not in the regression model. On the other hand, the existence of a regression relation in controlled experiments is often good evidence of a cause-and-effect relation.\nWe frequently wish to estimate several mean responses or predict several new observations for different levels of the predictor variable; this causes some special problems to arise. The confidence coefficients for the limits for estimating a mean response and for the prediction limits for a new observation only for a single level of \\(X\\) for a given sample.\nWhen observations on the predictor variable \\(X\\) are subject to measurement errors, the resulting parameter estimates are generally no longer unbiased.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-predictor-variable",
    "href": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-predictor-variable",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.1 Diagnostics for predictor variable",
    "text": "3.1 Diagnostics for predictor variable\nOverview\n\n\nWe need diagnostic information about the predictor variable in order to:\n\nSee if there are any outlying \\(X\\) values that could influence the appropriateness of the fitted regression function or if the \\(X\\) distribution is skewed, which can impact some aspects of the model behavior / performance.\nGet info about the range and concentration of the \\(X\\) levels in the study. This is useful for figuring out the range of validity of the regression analysis.\nCan use simple graphical tools to help.\n\n\n\nPlots\n\n\nDot plot → Good when the sample size is not large.\n\nCan look at min and max; are observations spread throughout this interval? any far outlying observations? replication at different \\(X\\) levels?.\n\n\n\nSequence plot → Should be utilized whenever data are obtained in a sequence, such as over time or for adjacent geographic areas.\n\nShould look like a random walk (i.e. no obvious pattern).\nFor example, if one level of \\(X\\) occurs way more frequently in the beginning and then another occurs way more at end, this info can be very helpful when looking at diagnostics later for how appropriate the model is.\n\n\n\nBox plots → Good when have a larger sample size.\n\nCan look at min and max, location of IQR, outliers, shape (skewness).\n\n\n\nHistograms → Good with really big sample sizes.\n\nCan look for same characteristics as boxplots, as well as modality.\n\n\n\nDemo\n\n# generate several samples of X values\nx_1 &lt;- extraDistr::rdunif(n = 15, min = 5, max = 15)\nx_2 &lt;- extraDistr::rdunif(n = 50, min = 5, max = 15)\nx_3 &lt;- runif(n = 1000, min = 5, max = 15)\n\n# create simple plots for diagnostics of X variable\n# NOTE -&gt; just doing simple base R plots \n\n# dot plot for small samples\nstripchart(x_1, method = \"stack\")\n\n\n\n\n\n\n# sequence plot for \"sequential\" data\n# -&gt; data is random, so will be a random walk\nplot(x_1, type = \"b\")\n\n\n\n\n\n\n# boxplot for larger samples\n# -&gt; add mark for mean\nboxplot(x_2, horizontal = TRUE)\npoints(x = mean(x_2), y = 1, pch = 4)\n\n\n\n\n\n\n# histogram for really large sample\nhist(x_3)",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#residuals",
    "href": "notes-diagnostics-and-remedial-measures.html#residuals",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.2 Residuals",
    "text": "3.2 Residuals\nOverview\n\nGoal of residual analysis → Assess the aptness of a statistical model.\n\nWhy use residuals\n\nDirect diagnostic plots for the response variable \\(Y\\) are ordinarily not too useful in regression analysis because the response variable observations are a function of the level of predictor variable.\nSo, instead we look at diagnostics for \\(Y\\) indirectly by examining the residuals.\n\n\n\nResiduals and model error\n\nRecall residual \\(e_i = Y_i - \\hat{Y_i}\\) is an estimate of the unobservable model error \\(\\epsilon = Y_i - E(Y_i)\\).\n** For our regression model, we assume \\(\\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,\\,(0, \\sigma^2\\)). So if the model is appropriate for the data at hand, the residuals should reflect these properties.\nThis is the basic idea of a residual analysis, which is a highly useful way to check if a model is appropriate.\n\n\n\n\n3.2.1 Properites of residuals\n\n\nMean\n\n\nThe mean of the \\(n\\) residuals for the SLR is shown below.\n\nSince this is always 0, it doesn’t give any info as to whether the true errors \\(\\epsilon_i\\) have expected value \\(E(\\epsilon_i) = 0\\).\n\n\n\n\n\n\\[\n\\bar{e} = \\sum e_i = 0\n\\]\n\n\nVariance\n\n\nThe variance of the \\(n\\) residuals for the SLR is shown below.\n\nAgain, if the model is appropriate \\(MSE\\) is an unbiased estimator of the variance of the error terms \\(\\sigma^2\\)\n\n\n\n\n\n\n\\[\nS^2 = \\frac{\\sum (e_i - \\bar{e})^2}{n - 2} = \\frac{\\sum e_i^2}{n - 2} = \\frac{SSE}{n - 2} = MSE\n\\]\n\n\nNonindependence\n\nThe residuals \\(e_i\\) are not independent random variables because they involve the fitted values \\(\\hat{Y_i}\\), which are computed on the same \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nAs a result, the residuals are subject to two constraints: \\(\\sum e_i = 0\\) and \\(\\sum X_i e_i = 0\\).\nHowever, when the sample size \\(n\\) is much larger than the number of parameters \\(p\\), we can ignore the minor dependence.\n\n\n\n3.2.2 Semistudentized residuals\n\nAt times, it is helpful to standardize the residuals for residual analysis. Since the standard deviation of the error terms \\(\\epsilon_i\\) is \\(\\sigma\\), which is estimated by \\(\\sqrt{MSE}\\), we can naturally standardize with\n\n\\[\ne_i^* = \\frac{e_i - \\bar{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}}\n\\]\n\nStudentized → If \\(\\sqrt{MSE}\\) were an estimate of the standard deviation of the residual \\(e_i\\), we would call \\(e_i^*\\) a studentized residual. But it is not…\nSemistudentized → \\(\\sigma_{e_i}\\) is complex and varies for the different residuals \\(e_i\\), and \\(\\sqrt{MSE}\\) is only an approximation of the standard deviation of \\(\\sigma_{e_i}\\). So, instead it is a semistudentized residual.\n\n3.2.3 Departures from model to be studied by residuals\n\n\nWe are going to use residuals to study the following departures from the SLR model with normal errors:\n\nThe regression function is not linear → Is the functional form of the model appropriate?\nThe error terms do not have constant variance.\nThe error terms are not independent.\nThe model fits all but one or a few outlier observations → Do any of the data points have a disproportionate influence on the parameter estimates?\nThe error terms are not normally distributed.\nOne or several important predictor variables have been omitted from the model.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-residuals",
    "href": "notes-diagnostics-and-remedial-measures.html#diagnostics-for-residuals",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.3 Diagnostics for residuals",
    "text": "3.3 Diagnostics for residuals\nTypes of plots\n\nHere are the informal diagnostic plots of residuals we will use to see if any of the above 6 departures from the SLR model are present,\n\n\nResiduals against predictor variable.\nAbsolute or squared residuals against predictor variable.\nResiduals against fitted values.\nResiduals against time or other sequence.\nResiduals against omitted predictor variables.\nBoxplot of residuals.\nNormal probability plot of residuals.\n\nDemo\n\nHow to make plots and what they look like when assumptions are met.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# view diagnostic plots (base R)\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# NOT RUN -&gt; note this works inline\n# view ggplot versions of diagnostic plots\nggplot2::autoplot(mod)\n\n\nNext section\n\nNow we will go through how to visually assess each assumption to see if it is met and diagnosis (identify) each violation if present.\nAt then end, we will discuss the impacts of each departure on things such as \\(MSE\\), slope standard errors, prediction accuracy, etc.\n\n\n3.3.1 Nonlinearity of regression function\n\n\nContent\nIdeal\n\n\n\nOverview\n\nWe can look at the following plots to check if a linear regression function is appropriate:\n\n\n\nResidual plot against the fitted values → This is the preferred plot for this assumption check.\n\nWhen a linear regression model is appropriate, the residuals then fall within a horizontal band centered around 0, displaying no systematic tendencies to be positive and negative (randomly scattered around 0).\nWhen the linearity assumption is violated, there are systematic deviations.\n\n\n\n\n\n\nResidual plot against the predictor variable\n\nFor SLR only, this shows the same info as the residuals vs fitted values because the \\(\\hat{Y_i}\\) are a linear function of the \\(X_i\\).\nSo the basic pattern of the plotted points is not affected whether the residual plot is against the \\(X_i\\) or the \\(\\hat{Y_i}\\) (only the \\(X\\) scale values are affected).\nFor curvilinear regression and multiple regression, separate plots of the residuals against the fitted values and against the predictor variable(s) are usually helpful.\n\n\nScatterplot of \\(Y\\) vs \\(X\\) → If the actual trend in the data in linear, then it makes sense to fit a straight-line model to the data.\n\n\nBut, not always as effective as the residual plots. Residual plots are preferred over scatteplot, because it has two advantages:\n\n\n\nCan also easily be used for checking other assumptions.\nThere are occasions when the scaling of the scatter plot places the \\(Y_i\\) observations close to the fitted values \\(\\hat{Y_i}\\), for instance, when there is a steep slope. It then becomes more difficult to study the appropriateness of a linear regression function from the scatterplot, while the residual plot can clearly show any systematic pattern in the deviations around the fitted regression line under these conditions.\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_ideal &lt;- lm(y ~ x)\n\n\n\nResiduals vs fitted plot\n\nR also adds a trend line, which we want to be approximately horizontal.\n\n\n\n\n# residual vs fitted plot\nplot(mod_ideal, which = 1)\n\n# verify smoothing line that plot.lm() performs (see discussion in nonconstant variance ideal section)\nplot(mod_ideal, which = 1)\nlines(x = lowess(fitted(mod_ideal), y = resid(mod_ideal)), col = \"blue\")\n\n\n\n\n\n\n\n\n\nResiduals vs \\(X\\) plot\n\nFor SLR → Shows same pattern as above, different scale on \\(X\\) axis.\n\n\n\n\n# create residuals vs X plot\n# -&gt; add smoothing line\nplot(x = x, y = residuals(mod_ideal), ylab = \"residuals\", main = \"Residuals vs X\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\nlines(lowess(x = x, y = resid(mod_ideal)), col = \"red\")\n\n\n\n\n\n\n\n\n\nDemo for difference when in MLR\n\nResults → Can see different pattern (even if only slightly based on values chosen) pattern in each.\n\n\n\n\n# initialize new items\nbeta_2 &lt;- 3\n\n# generate new X sample\nx_2 &lt;- runif(n = n, min = 0, max = 5)\n\n# calculate new Y observations using same errors, but with both xs\ny_2 &lt;- beta_0 + beta_1 * x + beta_2 * x_2  + epsilon\n\n# fit new MLR model\nmod_mlr &lt;- lm(y_2 ~ x + x_2)\n\n\n# plot residuals vs fitted, X and X2\n# -&gt; create dataset so can display all three plots\n# -&gt; also add smoothing line to make differences in patterns more noticeable\ndata.frame(X1 = x,\n           X2 = x_2,\n           fitted = fitted(mod_mlr),\n           residuals = residuals(mod_mlr)) %&gt;%\n  pivot_longer(1:3,\n               names_to = \"X_axis\",\n               values_to = \"value\") %&gt;% \n  ggplot(aes(x = value,\n             y = residuals),\n         data = .) + \n  geom_point() + \n  geom_smooth(se = FALSE,\n              method = \"loess\",\n              formula = y ~ x) + \n  facet_grid(. ~ X_axis,\n             scales = \"free\") + \n  geom_hline(yintercept = 0,\n             col = \"grey\",\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\nScatterplot\n\n\n# scatterplot of y vs x with regression line added\nplot(x = x, y = y, main = \"Scatterplot of Y vs X\")\nabline(mod_ideal, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nPolynomial\nNonlinear regression model\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\text{Normal}(0, \\sigma^2)\\)\n\nNote that this is still a linear model (even though there is a higher order term), becuase the model in linear in the parameters.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; sigma &lt;- 5; beta_0 &lt;- 1; beta_1 &lt;- 2;\nbeta_2 &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x  + beta_2 * x^2 + epsilon\n\n# fit model (only on SLR x with linear term)\nmod_squared_x &lt;- lm(y ~ x)\n\n\nResiduals vs fitted plot\n\n\n# residual vs fitted plot\nplot(mod_squared_x, which = 1)\n\n\n\n\n\n\n\n\nScatterplot\n\n\n# scatterplot of y vs x with regression line added\nplot(x = x, y = y, main = \"Squared predictor\")\nabline(mod_squared_x, col = \"red\")\n\n\n\n\n\n\n\n\n\nDemo to show how steep slope can hide lack of fit in scatterplot, but residual plot still picks it up.\n\nResults → For a true quadratic model, the scatterplot of \\(Y\\) vs \\(X\\) looks just linear (not noticeable curvature) when there is a large \\(\\beta_1\\) term (coefficient of linear term). But this is misleading, especially after seeing the residual plot which shows the missing evidence of curvature.\n\n\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 50; beta_0 &lt;- 1; sigma &lt;- 5\n\n# SMALL slope of linear term\nbeta_1 &lt;- 3\nbeta_2 &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x  + beta_2 * x^2 + epsilon\n\n# fit model (only on SLR x with linear term)\nmod_squared &lt;- lm(y ~ x)\n\n# scatterplot of y vs x with regression line added\n# -&gt; can see curvature\nplot(x = x, y = y, main = \"Samll slope\")\nabline(mod_squared, col = \"red\")\n\n\n\n\n\n\n# residuals vs fitted plot\n# -&gt; curvature very obvious here\nplot(mod_squared, which = 1)\n\n\n\n\n\n\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 50; beta_0 &lt;- 1; sigma &lt;- 5\n\n# LARGE slope of linear term\nbeta_1 &lt;- 10\nbeta_2 &lt;- 1\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x  + beta_2 * x^2 + epsilon\n\n# fit model (only on SLR x with linear term)\nmod_squared &lt;- lm(y ~ x)\n\n# scatterplot of y vs x with regression line added\n# -&gt; curvature is hidden more, regression line looks okay\nplot(x = x, y = y, main = \"Large slope\")\nabline(mod_squared, col = \"red\")\n\n\n\n\n\n\n# residuals vs fitted plot\n# -&gt; curvature still visible here\nplot(mod_squared, which = 1)\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 \\, \\mathrm{e}^{\\beta_1 \\, X_i} + \\text{Normal}(0, \\sigma^2)\\)\n\nNonlinear because \\(Y_i\\) cannot be expressed as a linear combination of the \\(\\beta_i\\)s.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 0.1; beta_1 &lt;- 0.5; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 * exp(beta_1 * x) + epsilon\n\n# fit model\nmod_nonlinear &lt;- lm(y ~ x)\n\n\n\nScattterplot of \\(Y\\) vs \\(X\\) and residual plot\n\nCan see obvious nonlinear pattern in scatterplot and systematic deviations in residual plot.\n\n\n\n\n# scatterplot of y vs x with regression line\n# -&gt; obviously not a linear pattern\nplot(x = x, y = y, main = \"Scatterplot of Y vs X\")\nabline(mod_nonlinear, col = \"red\")\n\n\n\n\n\n\n# residual vs fitted plot\n# -&gt; definite pattern to residuals\nplot(mod_nonlinear, 1)\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Nonconstant variance\n\n\nContent\nIdeal\n\n\n\nOverview\n\nWe can look at the following plots to check if the error terms have constant variance:\n\n\n\nResidual plot against the fitted values\n\nAgain for SLR, this shows the same info as residuals vs \\(X\\) plot, but in MLR we want to use the fitted values.\nWhen there is a constant error variance, points again should fall within a horizontal band. So there is a constant spread of the residuals as move across the scope of fitted (or \\(X\\)) values.\n“Tipped over tornado” effect of the points indicates a non-constant variance (i.e. as the fitted values increase, the residuals vary more, or vice versa). Reverse megaphone (decreasing variance with increasing levels of \\(X\\) or \\(\\hat{Y}\\)) is possible as well or varying in some other complex fashion. A nonconstant variance in called heteroscedasticity (the assumption is a homoscedastic error variance).\n\n\n\n\n\n\nScale-location plot → Refined version of above, preferred plot for this assumption.\n\nPlots standardized residuals against fitted values.\nThere are a few ways to transform the residuals for this type of plot, all of which get at the same purpose (options include: absolute value, square root of absolute value, squared, standardized (studentized, semistudentized), square root of standardized).\nBy doing this, it places all of the information on changing magnitudes of the residuals above the horizontal zero line, which makes it easier to see whether the magnitude of the residuals is changing with the level \\(\\hat{Y}\\) (\\(\\pm\\) is not important for this condition).\nThe scale-location plot specifically uses the standardized residuals (which have equal variance by assumption), and are given by \\(e_i^* = \\frac{e_i}{\\sqrt{MSE (1 - h_{ii})}}\\), where the leverages \\(h_{ii}\\) are the diagonal entries of the hat matrix. It then takes the absolute value and square root \\(\\sqrt{\\lvert e_i^* \\rvert}\\) to diminish the skewness because under \\(\\text{Normal}(0, \\sigma^2)\\), \\(\\sqrt{\\lvert e_i^* \\rvert}\\) is much less skewed than \\(\\lvert e_i^* \\rvert\\).\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nResiduals vs fitted plot\n\nLooking residuals to fall within horizontal bands on either side of 0.\n\n\n\n\n# using ideal model from linearity assumption demo\n\n# again, same plot as to check linearity\nplot(mod_ideal, which = 1)\n\n\n\n\n\n\n\n\n\nScale-location plot\n\nNow we want a flat trend line.\n\n\n\n\n# scale-location plot\nplot(mod_ideal, which = 3)\n\n\n\n\n\n\n\n\nDemo of different ways to standardize residuals (including the scale-location plot)\n\n\n# scale-location plot\nplot(mod_ideal, which = 3)\n\n# demo of the standardized residuals used above\n# -&gt; check help page for plot.lm() for more details on methods behind diagnostic plots\n\n# standardize the residuals\n# -&gt; manual calculation using the formula for e* or r functions\nmse &lt;- summary(mod_ideal)$sigma^2\ne_star &lt;- resid(mod_ideal) %&gt;% divide_by(sqrt(mse * (1 - hatvalues(mod_ideal)))) \ne_star &lt;- rstandard(mod_ideal)\n\n# view final transformation of residuals\ndata.frame(e = resid(mod), e_star, f_e_star = sqrt(abs(e_star))) %&gt;% \n  head(n = 5) %&gt;% \n  display_nice(col.names = c(\"Residuals $e_i$\", \"Standardized residuals $e_i^*$\", \"$f(e_i^*) = \\\\sqrt{\\\\lvert e_i^* \\\\rvert}$\"))\n\n\n\n Residuals $e_i$ \n    Standardized residuals $e_i^*$ \n    $f(e_i^*) = \\sqrt{\\lvert e_i^* \\rvert}$ \n  \n\n\n 1.991 \n    -1.374 \n    1.172 \n  \n\n -3.361 \n    -1.563 \n    1.250 \n  \n\n 5.149 \n    -0.696 \n    0.834 \n  \n\n -7.380 \n    -0.204 \n    0.452 \n  \n\n -0.855 \n    0.978 \n    0.989 \n  \n\n\n\n# overlay function of standardized residuals to scale-location plot to confirm they line up \npoints(x = fitted(mod_ideal), y = sqrt(abs(e_star)), col = \"blue\", pch = 3)\n\n\n\n\n\n\n\n\n# demo different residual plots AND different smoothers\n\n# create dataset of different standardized residuals so can display all\n# 0) residuals\n# 1) f(e*) from above (sqrt of abs of standardized residuals from scale-location)\n# 2) semistudentized residuals = e / sqrt(MSE)\n# 3) abs(residuals)\n# 4) sqrt(abs(residuals))\n# 5) residuals^2\ne &lt;- resid(mod_ideal)\ndata_plot &lt;- data.frame(\"fitted\" = fitted(mod_ideal),\n                        e,\n                        f_e_star = sqrt(abs(e_star)),\n                        e_semi = e / sqrt(mse),\n                        e_abs = abs(e),\n                        e_abs_sqrt = sqrt(abs(e)),\n                        e_squared = e^2) %&gt;%\n  pivot_longer(c(starts_with(\"e\"), \"f_e_star\"),\n               names_to = \"type\",\n               values_to = \"residuals\")\n\n# create dataset for lowess smoothers of each residual type\ndata_lowess &lt;- data_plot %&gt;% \n  split(.$type) %&gt;% map2(names(.), \\(df, type) data.frame(lowess(x = df$fitted, y = df$residuals), type = type)) %&gt;% \n  reduce(bind_rows)\n\n# set more informative labels for the facets\nlabels_residuals &lt;- c(e = \"residuals\",\n                      f_e_star = \"f(standardized residuals\",\n                      e_semi = \"residuals / sqrt(MSE)\",\n                      e_abs = \"abs(residuals)\",\n                      e_abs_sqrt = \"sqrt(abs(residuals))\",\n                      e_squared = \"residuals^2\")\n\n# then plot with different smoothing lines\n# smoothing line\n# -&gt; geom_smooth() default method is loess smoother, which is a local polynomial regression fit\n# -&gt; plot() uses lowess smoother, which uses locally-weighted polynomial regression \n# -&gt; summary -&gt; similar methods, slight difference\n# --&gt; lowess is for adding a smooth curve to a scatterplot, i.e., for univariate smoothing, while loess is for fitting a smooth surface to multivariate data\ndata_plot %&gt;% \n  ggplot(aes(x = fitted,\n           y = residuals),\n       data = .) + \n  geom_point() + \n  geom_smooth(se = FALSE,\n              method = \"loess\",\n              formula = y ~ x) + \n    geom_line(aes(x = x,\n                y = y),\n            data = data_lowess,\n            col = \"red\") + \n  facet_wrap(. ~ type,\n             scales = \"free\",\n             labeller = as_labeller(labels_residuals)) + \n  geom_hline(yintercept = 0,\n             col = \"grey\",\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nIncreasing variance\nComplex variance\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2_i = 5 X_i)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# specify a vector of variances, which is a function of X\nsigma &lt;- 5 * x\n\n# generate response Y\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n# fit model\nmod_increasing_error_variance &lt;- lm(y ~ x)\n\n\n\nScatterplot\n\nCan see more variation around ftted line as \\(X\\) increases.\n\n\n\n\n# view scatterplot with regression line\nplot(x = x, y = y)\nabline(mod_increasing_error_variance, col = \"red\")\n\n\n\n\n\n\n\n\n\nResiduals vs fitted plot\n\nLooking for a pattern that doesn’t follow horizontal bands as move left to right, trend line may still be horizontal though.\n\n\n\n\n# residual plot\nplot(mod_increasing_error_variance, which = 1)\n\n\n\n\n\n\n\n\n\nScale-location plot\n\nNow trend line will not be horizontal after transforming residuals, should increase with an increasing variance.\n\n\n\n\n# scale-location plot\nplot(mod_increasing_error_variance, which = 3)\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\text{ decreasing then increasing } \\sigma^2_i)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# -&gt; specify a vector of variances instead so it sequentially increases\nsigma &lt;- c(seq(from = 20, to = 1, length.out = n / 2), seq(from = 1, to = 20, length.out = n / 2))\n\n# generate X values\n# -&gt; need pattern of variances to follow as X increases, so sort Xs after\nx &lt;- runif(n = n, min = 5, max = 15) %&gt;% sort\n\n# generate response Y\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n# fit model\nmod_complex_error_variance &lt;- lm(y ~ x)\n\n\n# view scatterplot with regression line\n# -&gt; now see values are closer to line in middle of plot\nplot(x = x, y = y)\nabline(mod_complex_error_variance, col = \"red\")\n\n\n\n\n\n\n# residual plot\n# -&gt; see ribbon-like pattern\nplot(mod_complex_error_variance, which = 1)\n\n\n\n\n\n\n# scale-location plot\n# -&gt; see U kinda pattern\nplot(mod_complex_error_variance, which = 3)\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Presence of unusual observations\n\n\nContent\nIdeal\n\n\n\nOverview\n\n\nUnusual observations can create much difficulty when fitting models. When present, they can lead to a misleading fit because the line was estimated by minimizing the squared deviations.\n\nThus, a fitted line may be pulled disproportionately toward an unusual observation. Unusual observations far from \\(\\bar{X}\\) have a larger impact on the model and estimates than those near \\(\\bar{X}\\) (this is the idea of leverage / influence).\n\n\n\n\n\n\nThere are two types unusual observations and how we define them, check for them, and their impact is different.\n\n\nOutliers → Extreme values of the response (outliers in \\(Y\\)).\n\n\nA rough rule of thumb → When the sample size is large, semistudentized (or standardized) residuals with absolute value more than 3 or 4 can be considered outliers (i.e. outliers if \\(\\lvert \\frac{e_i}{\\sqrt{MSE}} \\text{ or } e_i^* \\rvert \\ge 3 \\text{ or } 4\\)).\nWe can look at the following plots to check if residual outliers are present:\n\n\n\nResidual plot against the fitted values (or \\(X\\))\n\n\nJust looking for points far away from the pattern of the rest. In SLR, can also look at the scatterplot of \\(Y\\) vs \\(X\\) for points far from overall pattern.\nPlotting of semistudentized residuals is helpful for distinguishing outiers because it’s easy to identify residuals that lie many standard deviations from zero.\n\n\n\n\n\n\n\nBoxplots, histograms, Normal QQ of the residuals (or standardized residuals)\n\nCan also look for outliers in the usual way based on these plots. Note that different visuals / rules will give more or less evidence for outliers.\n\n\n\n\n\n\nHigh leverage points → Observations whose predictor values are far from the center of the predictor space (extreme values (outliers) of \\(X\\) or unusual combination of \\(X\\)s).\n\nPlots → Residuals vs Leverage plot (or Cook’s distance plot, Cook’s dist vs Leverage (1 - Leverage) plot) → These use measures not covered yet.\n\n\n\n(3.) Influential points → High leverage points that actually influence the slope of the regression line (outlier in \\(X\\) AND \\(Y\\)).\n\nPlots → In order to determine if a point is influential, visualize the regression line with and without the point. Does the slope of the line change considerably? If so, then the point is influential. If not, then it’s not an influential point.\n\n\nRemoving unusual observations\n\nBecause unusual observations can create lots of difficulty, when we find one, we initially think that the observation resulted from a mistake or some extraneous effect. So it should be removed.\nOn the other hand, unusual observations may convey significant information (such as if it occurs because of an interaction with another predictor variable omitted from the model).\nA safe rule → Discard an unusual observation only if there is direct evidence that it represents an error in recording, a miscalculation, a malfunctioning of equipment, or a similar type of circumstance.\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nResiduals vs fitted plot\n\nLooking for observations with residuals far below or above 0.\n\n\n\n\n# using ideal model from linearity assumption demo\n# -&gt; randomness may result in an outlier or two\n\n# again, same plot as to check linearity and nonconstant variance\nplot(mod_ideal, which = 1)\n\n\n\n\n\n\n# by default, on all the diagnostic plots R labels the 3 observations with the most extreme residuals \n# -&gt; this says nothing about being potential outliers, just the top three\n# -&gt; can change how many get labeled and what the label is (by default it is the observation number)\nplot(mod_ideal, which = 1, id.n = 10, labels.id = paste(\"x = \", round(x, 3), \"\\n y = \", round(y, 3)))\n\n\n\n\n\n\n# could also add lines for 3 * sqrt(MSE) to get the same comparison as below, just in units of the original residuals\n# -&gt; NOTE: bounds are off plot cause not any outliers\nplot(mod_ideal, which = 1)\nmse &lt;- summary(mod_ideal)$sigma^2\nabline(h = c(-3 * sqrt(mse), 3 * sqrt(mse)), col = \"orange\")\n\n\n\n\n\n\n\n\n\nStandardized residuals vs fitted plot\n\nLooking for observations beyond with residuals 3 or 4 away from zero after standardizing.\nBoth ways of standardizing get essentially the same outcomes \\(\\Longrightarrow\\) An outlier in one will be an outlier in the other, so can just use the default R\n\n\n\n\n# show how to check the standardized residual plots and compare different versions\n\n# create dataset of two different standardized residuals for variation of residual plot\n# 1) semistudentized residuals\n# 2) e* from above (standardized residuals from scale-location)\ndata_plot &lt;- data.frame(\"fitted\" = fitted(mod_ideal),\n                        e_semi = resid(mod_ideal) / sqrt(mse),\n                        e_star = rstandard(mod_ideal)) %&gt;%\n  pivot_longer(starts_with(\"e\"),\n               names_to = \"type\",\n               values_to = \"residuals\")\n               \n# create residual plot with smoothing line for both types of residuals\n# -&gt; add bands at +/- 3 \"standardized units\" from zero\ndata_plot %&gt;% \n  ggplot(aes(x = fitted,\n             y = residuals,\n             color = type),\n         data = .,\n         alpha = 0.01) + \n  geom_point() + \n  geom_smooth(se = FALSE,\n              method = \"loess\",\n              formula = y ~ x) + \n  geom_hline(yintercept = c(0,-3,3),\n             col = c(\"grey\", \"orange\", \"orange\"),\n             linetype = c(\"dashed\", \"solid\", \"solid\")) + \n  scale_color_manual(name = \"Type of residual\",\n                     values = c(e_semi = \"#F8766D\", e_star = \"#00BFC4\"), # use hexcode of default colors when two values\n                     labels = c(e_semi = \"semistudentized\", e_star =  \"standardized\"))\n\n\n\n\n\n\n\n\n\nBoxplot, histogram and Normal QQ plot of residuals\n\nShould not see any observations far away from the general patterns.\n\n\n\n\n# boxplot of residuals\nboxplot(resid(mod_ideal), horizontal = TRUE,  main = \"Boxplot of residuals\")\n\n\n\n\n\n\n# histogram of residuals\nhist(resid(mod_ideal), main = \"Histogram  of residuals\")\n\n\n\n\n\n\n# normal qq plot\nplot(mod_ideal, which = 2)\n\n\n\n\n\n\n\n\nResiduals vs Leverage plot, Cook’s distance plot and Cook’s dist vs Leverage / (1 - Leverage) plot\n\n\n# diagnostic plots of: residuals vs lev, cooks dist, and cooks dist vs lev\n# NOT RUN -&gt; note works inline\nautoplot(mod_ideal, which = c(5,4,6))\n\nError in `autoplot()`:\n! Objects of class &lt;lm&gt; are not supported by autoplot.\nℹ have you loaded the required package?\n\n# residuals vs lev plot uses standardized residuals (and not transforming them with sqrt(abs(.)) like the scale-location plot)\n# -&gt; so could add the +/- 3 lines to this plot (and for now ignore the leverage lines)\n# note -&gt; lines are off plot cause no outliers\nplot(mod_ideal, which = 5)\nabline(h = c(-3, 3), col = \"orange\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nOutlier\nHigh-leverage point\nInfluential point\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2), \\hspace{20pt} \\text{except} \\hspace{10pt} Y_{n+1} = \\beta_0^* + \\beta_1^* X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 4; sigma &lt;- 5\n\n# generate X values and y values in a dataframe\n# (SIDENOTE -&gt; needs to be a separate mutate() statement so y uses the x's just generated and not ones from a previous vector)\ndata_without &lt;- data.frame(x = runif(n = n, min = 5, max = 15)) %&gt;% \n  mutate(y = rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma))\n\n\n# introduce outlier\n# -&gt; new point follows different population regression model, but with typical X value\n# --&gt; X ranges from 5 to 15 from above\ndata_new &lt;- data.frame(x = 10) %&gt;% \n  mutate(y = rnorm(n = 1, mean = (beta_0 + 4) + (beta_1 + 4) * x, sd = sigma))\ndata_outlier &lt;- bind_rows(data_without, data_new)\n\n\n\nDiagnostic plots with outlier (typical \\(X\\) but extreme \\(Y\\) \\(\\Longrightarrow\\) Outlier in \\(Y\\))\n\nCan see a point with unusually large residuals (outlier in boxplot, way above / below in residual plot).\n\n\n\n\n# scatterplot of data with outlier\ndata_outlier %$% plot(x = x, y = y)\n\n\n\n\n\n\n# fit model with outlier\nmod_outlier &lt;- data_outlier %$% lm(y ~ x)\n\n# boxplot of residuals\nboxplot(resid(mod_outlier), horizontal = TRUE, main  = \"Boxplot of residuals\")\n\n\n\n\n\n\n# if there is an outlier in the boxplot, can extract it\n# -&gt; then find the corresponding (x,y) pair\n(outlier &lt;- boxplot(resid(mod_outlier), horizontal = TRUE)$out)\n\n\n\n\n\n\n\n       17        26        31 \n-15.64046  12.46312  38.35688 \n\ndata_outlier[which(mod_outlier$residuals == outlier),] %&gt;% display_nice\n\n\n\n   \n    x \n    y \n  \n\n 26 \n    8.227 \n    47.184 \n  \n\n\n# residual vs fitted plot with reference lines to confirm outlier\nplot(mod_outlier, which = 1)\nmse &lt;- summary(mod_outlier)$sigma^2\nabline(h = c(-3 * sqrt(mse), 3 * sqrt(mse)), col = \"orange\")\n\n\n\n\n\n\n# get standardized residual value of the identified outlier\nrstandard(mod_outlier)[31]\n\n      31 \n4.158885 \n\n\n\n# view regression lines with and without outlier\ndata_outlier %$% plot(x = x, y = y, main = \"Outlier\")\nabline(mod_outlier, col = \"red\")\ndata_outlier[-31,] %$% abline(lm(y ~ x), col = \"purple\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2), \\hspace{20pt} \\text{except} \\hspace{10pt} Y_{n+1} = \\beta_0 + \\beta_1 X_i^* + \\text{Normal}(0, \\sigma^2)\\)\n\n# introduce high-leverage point\n# -&gt; new point follows same population regression model, just with extreme X value\ndata_new &lt;- data.frame(x = 20) %&gt;% \n  mutate(y = rnorm(n = 1, mean = beta_0 + beta_1 * x, sd = sigma))\ndata_high_leverage &lt;- bind_rows(data_without, data_new)\n\n\n\nDiagnostic plots with high-leverage point (extreme \\(X\\) but follow pattern of \\(Y\\) \\(\\Longrightarrow\\) Outlier in \\(X\\))\n\nResidual of high-leverage point will be typical, but far away horizontally from the rest of the points.\n\n\n\n\n# scatterplot of data with high-leverage point\ndata_high_leverage %$% plot(x = x, y = y)\n\n\n\n\n\n\n# fit model with high-leverage point\nmod_high_leverage &lt;- data_high_leverage %$% lm(y ~ x)\n\n# residual vs leverage plot with reference lines to confirm high-leverage (outlier in X) and also outlier (in Y)\nplot(mod_high_leverage, which = 5)\nabline(h = c(-3,3), col = \"orange\")\n\n\n\n\n\n\n\n\n# view regression lines with and without outlier\ndata_high_leverage %$% plot(x = x, y = y, main = \"High-leverage point\")\nabline(mod_high_leverage, col = \"red\")\ndata_high_leverage[-31,] %$% abline(lm(y ~ x), col = \"purple\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2), \\hspace{20pt} \\text{except} \\hspace{10pt} Y_{n+1} = \\beta_0^* + \\beta_1^* X_i^* + \\text{Normal}(0, \\sigma^2)\\)\n\n# introduce influential point\n# -&gt; new point follows different population regression model AND extreme X value\ndata_new &lt;- data.frame(x = 20) %&gt;% \n  mutate(y = rnorm(n = 1, mean = (beta_0 + 4) + (beta_1 + 4) * x, sd = sigma))\ndata_influential &lt;- bind_rows(data_without, data_new)\n\n\n\nDiagnostic plots with influential point (extreme \\(X\\) and does NOT follow patter of \\(Y\\) \\(\\Longrightarrow\\) Outlier in \\(X\\) and \\(Y\\))\n\nResidual of influential point will be extreme AND far away horizontally from the rest of the points.\n\n\n\n\n# scatterplot of data with influential point\ndata_influential %$% plot(x = x, y = y, main = \"Influential point\")\n\n\n\n\n\n\n# fit model with influential point\nmod_influential &lt;- data_influential %$% lm(y ~ x)\n\n# residual vs leverage plot with reference lines to confirm high-leverage and not outlier\nplot(mod_influential, which = 5)\nabline(h = c(-3,3), col = \"orange\")\n\n\n\n\n\n\n# view regression lines with and without outlier\ndata_influential %$% plot(x = x, y = y, main = \"Influential point\")\nabline(mod_influential, col = \"red\")\ndata_influential[-31,] %$% abline(lm(y ~ x), col = \"purple\")\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 Nonindependence of error terms\n\n\nContent\nIdeal\n\n\n\nOverview\n\nIdeally, any potential source of dependence is handled at the experimental design stage (or the sampling scheme), so that it is either eliminated by randomization or explicitly included in the data and we have one observation per subject.\n\nWhenever data are obtained in a time sequence or some other type of sequence, such as for adjacent geographic areas, we can examine the potential dependence of error terms using a sequence plot of residuals.\n\nThis is used to see if there is any correlation between error terms that are near each other in the sequence.\n\n\n\nSequence plot of residuals\n\nWe want to plot the residuals against time, collection order, spatial coordinates, or some other indicator of the sequence that we think might affect the data.\nIf errors are independent, residuals hould look like a random walk around the base line zero (i.e. no obvious pattern).\nDependence (lack of randomness) can appear as a trend or cyclical pattern\n\n\n\n\nModelling with nonindependence or apparent nonindependence\n\nIt can be useful to view the problem of nonindependence of the error terms as one in which and important variable has been omitted from the model (whether it be time, observation number, etc.).\nMore subtle dependencies can be difficult to detect, especially if the information needed to detect them has not been included with the dataset.\nWhen the residuals are plotted against \\(X\\), the plot may not appear to be random. But the basic problem could be from a poorly fitting regression function rather than a lack of independence of the error terms. Example, where a quadratic term should be included:\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nSequence plot of residuals vs observation number\n\nLooking for random walk.\n\n\n\n\n# using ideal model from linearity assumption demo\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_ideal)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Sequence plot of residuals vs collection order\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nDependent errors\nLinear trend effect\nCyclical nonindependence\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\sim \\text{Normal}\\,(\\mu = \\epsilon_{i - 1}, \\sigma^2)\\)\n\nError terms are dependent now because the mean for \\(\\epsilon_i\\) mean is based (dependent) on the previous error term \\(\\epsilon_{i - 1}\\).\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate dependent normal error terms\n# -&gt; setup to first generate the deviations from rnorm()\n# -&gt; the first deviation is from mean = 0, the second will now be from (centered on) the first deviation\n# --&gt; so just add the first deviation and the new second deviations together, which will give the effect of a new mean from which rnorm() generated the next deviation\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma) %&gt;% cumsum\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_dependent_error &lt;- lm(y ~ x)\n\n\n# scatterplot of y vs x with regression line\n# -&gt; nothing looks amiss\nplot(x = x, y = y, main = \"Scatterplot - Dependent errors\")\nabline(mod_dependent_error, col = \"red\")\n\n\n\n\n\n\n# residual vs fitted plot\n# -&gt; again, seems fine\nplot(mod_dependent_error, 1, \"Residuals vs fitted - Dependent errors\")\n\n\n\n\n\n\n\n\n\nSequence plot with dependent errors (\\(\\epsilon_i\\) are no longer iid).\n\nNow there is not a random walk around zero.\n\n\n\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_dependent_error)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Dependent errors\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\sim \\text{Normal}\\,(\\text{increasing } \\mu, \\sigma^2)\\)\n\nDependent now because successive error terms are more likely to be greater.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate dependent normal error terms\n# -&gt; increasing means\nepsilon &lt;- rnorm(n = n, mean = seq(from = -20, to = 20, length.out = n), sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_linear_trend_error &lt;- lm(y ~ x)\n\n\n\nSequence plot with linear trend effect (still dependent).\n\nClear increase from in residuals with collection order → Early points more have negative residuals and later ones are positive.\n\n\n\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_linear_trend_error)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Linear trend effect\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\sim \\text{Normal}\\,(\\text{cyclical } \\mu, \\sigma^2)\\)\n\nDependent now because adjacent errors are more likely to be closer to each other.\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; sigma &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate dependent normal error terms\n# -&gt; cyclical (decreasing, then increasing, then decreasing, repeat) means\nepsilon &lt;- rnorm(n = n, mean = c(seq(from = -20, to = 20, length.out = n / 3),\n                                 seq(from = 20, to = -20, length.out = n / 3),\n                                 seq(from = -20, to = 20, length.out = n / 3)),\n                 sd = sigma)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_cyclical_error &lt;- lm(y ~ x)\n\n\n\nSequence plot with cyclical pattern in errors (still dependent).\n\nCyclical pattern in residuals against collection order.\n\n\n\n\n# sequence plot of residuals vs observation number\ne &lt;- resid(mod_cyclical_error)\nplot(x = 1:length(e), y = e, type = \"b\", main = \"Cyclical nonindependence\", xlab = \"obs #\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n3.3.5 Nonnormality of errors\n\n\nContent\nIdeal\n\n\n\nOverview\n\nSmall departures from normality do not create any serious problems, but major departures should be of concern.\nWe can check the normality of error terms in a variety of errors:\n\n\n\nDistribution plots of residuals → This is one of the two preferred plots.\n\nBoxplots are helpful for seeing if residuals are symmetric and if there are any possible outliers.\nHistograms can also be used for the same purpose, looking for roughly normal. But the sample size needs to be reasonably large for this plot to convey reliable information about the shape of the distribution of error terms (there can be lots of fluctuation in the shape with small samples) \\(\\Longrightarrow\\) Moderate departures from normality do not imply a serious violation of this assumption.\nFor both of these plots, as long as there is not severe departures, it is okay.\n\n\n\nComparison of relative frequencies → Use the empirical rule to compare observed relative frequencies to expected under normality.\n\nUsing \\(\\sqrt{MSE}\\) as an estimate for \\(\\sigma\\), for large sample size \\(n\\) we expect approximately:\n\n\n68% of residuals to fall within \\(\\pm \\sqrt{MSE}\\).\n95% of residuals to fall within \\(\\pm 2 \\sqrt{MSE}\\).\n99.7% of residuals to fall within \\(\\pm 3 \\sqrt{MSE}\\).\n\n\nIf the sample size is moderately large, then we can the corresponding \\(t_{n-1}\\) critical values (multipliers).\nJust looking for relative consistency with these rules.\n\n\n\n\n\n\nNormal probability (QQ) plot of residuals → This is the other preffered plot.\n\nEach standardized residual is plotted against its theoretical percentile (aka quantile, which gives us the expected value) under normality. Note that there are a few ways to make the QQ plots (different ways of standardizing residuals and different algorithms for theoretical quantiles), but none of these variations affect the nature of the plot.\nA plot that is nearly linear suggests agreement with normality, whereas a plot that departs substantially from linearity suggests that the error distribution is not normal.\nAgain a moderate departure from normality is of little concern, only extreme departures are of note.\n\n\n\n\nDifficulties in assessing normality\n\n\nThe analysis for model departures regarding normality is often more difficult than departures of other types because…\n\n\nRandom variation can be particularly mischievous when studying the nature of a probability distribution unless the sample size is quite large.\n\nEven worse, other types of departures can and do affect the distribution of the residuals.\ne.g. Residuals may appear to be not normally distributed because an inappropriate regression function is used or because the error variance is not constant.\n\n\nSo, it is usually a good strategy to investigate these other types of departures first, before assessing the normality of the error terms.\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Normal}(0, \\sigma^2)\\)\n\n\nDistribution plots\n\nLooking roughly symmetric boxplot and a roughly symmetric histogram, both with no extreme outliers.\n\n\n\n\n# using ideal model from linearity assumption demo\n\n# again, same distribution plots used to check residuals for outliers\n# boxplot of residuals\nboxplot(resid(mod_ideal), horizontal = TRUE,  main = \"Boxplot of residuals\")\n\n\n\n\n\n\n# histogram of residuals\nhist(resid(mod_ideal), main = \"Histogram  of residuals\")\n\n\n\n\n\n\n\n\n\nComparison of relative frequencies of residuals\n\nPercentages should match the empirical rule.\n\n\n\n\n# create relative frequency table for interval probabilities of residuals\n# -&gt; assuming here 30 is large enough to use Z multipliers\n# -&gt; pass z multipliers to map() statement calculating proportion of abs value residuals less than multiplier * sqrt(MSE) -&gt; (gets interval rel freq because centered around zero)\nmse &lt;- summary(mod_ideal)$sigma^2\ne &lt;- resid(mod_ideal)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n        0.700         0.967         1.000 \n\n\n\n\nNormal QQ plot of residuals\n\nWant standard residuals to follow the trend line.\n\n\n\n\n# normal qq plot\nplot(mod_ideal, which = 2)\n\n\n\n\n\n\n\n\n\n\nDepartures\n\n\nSkewed errors\nLight-tailed errors\nHeavy-tailed errors\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Exponential}(\\lambda = 1/5)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# generate right-skewed error terms from exponential distribution\nepsilon &lt;- rexp(n = n, rate = 1/5)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_skewed_error &lt;- lm(y ~ x)\n\n\n\n\nComparison of frequencies, distribution plots and QQ plot of residuals\n\nCan see relative frequencies don’t match the empirical rule; right skew in boxplot (with high outliers) and histogram; concave up pattern in qq plot.\n\n\n\n\n# comparison of frequencies\nmse &lt;- summary(mod_skewed_error)$sigma^2\ne &lt;- resid(mod_skewed_error)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n         0.80          0.98          0.98 \n\n# boxplot of residuals\nboxplot(resid(mod_skewed_error), horizontal = TRUE,  main = \"Right-skewed error\")\n\n\n\n\n\n\n# histogram of residuals\nhist(resid(mod_skewed_error), freq = FALSE, main = \"Right-skewed error\")\nx_plot &lt;- seq(from = -3, to = 3, by = 0.01)\nlines(x = x_plot, dnorm(x = x_plot))\n\n\n\n\n\n\n# normal qq plot\nplot(mod_skewed_error, which = 2)\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{Uniform}(a = -2, b = 2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# generate light-tailed error terms from uniform distribution\nepsilon &lt;- runif(n = n, min = -2, max = 2)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_light_tailed_error &lt;- lm(y ~ x)\n\n\nSame methods → Relative frequencies don’t line up; likely no outliers on boxplot; the histogram may show more data closer to the center; S-shape in QQ plot.\n\n\n# comparison of frequencies\nmse &lt;- summary(mod_light_tailed_error)$sigma^2\ne &lt;- resid(mod_light_tailed_error)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n         0.58          1.00          1.00 \n\n# boxplot of residuals\nboxplot(resid(mod_light_tailed_error), horizontal = TRUE,  main = \"Light-tailed error\")\n\n\n\n\n\n\n# histogram of standardized residuals (so can overlay Z density curve)\nhist(rstandard(mod_light_tailed_error), freq = FALSE, breaks = 20, main = \"Light-tailed error\")\nx_plot &lt;- seq(from = -3, to = 3, by = 0.01)\nlines(x = x_plot, dnorm(x = x_plot))\n\n\n\n\n\n\n# normal qq plot\nplot(mod_light_tailed_error, which = 2)\n\n\n\n\n\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\text{t}_4\\)\n\n# initialize items\n# -&gt; sample size, population parameters\nn &lt;- 50; beta_0 &lt;- 2; beta_1 &lt;- 5\n\n# generate X values\nx &lt;- runif(n = n, min = 1, max = 15)\n\n# generate heavy-tailed error terms from t-distribution\n# -&gt; df = 4 so not the most extreme\nepsilon &lt;- rt(n = n, df = 4)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod_heavy_tailed_error &lt;- lm(y ~ x)\n\n\nSame methods → Again, relative frequencies don’t line up; likely outliers on both sides of boxplot; the histogram will may show more data away from the center; flipped S-shape in QQ plot.\n\n\n# comparison of frequencies\nmse &lt;- summary(mod_heavy_tailed_error)$sigma^2\ne &lt;- resid(mod_heavy_tailed_error)\nc(1:3) %&gt;% \n  set_names(c(\"± sqrt(MSE)\", \"± 2 sqrt(MSE)\",\"± 3 sqrt(MSE)\")) %&gt;% \n  map_dbl(\\(z) round(mean(abs(e) &lt;= z * sqrt(mse)), 3))\n\n  ± sqrt(MSE) ± 2 sqrt(MSE) ± 3 sqrt(MSE) \n         0.74          0.94          0.98 \n\n# boxplot of residuals\nboxplot(resid(mod_heavy_tailed_error), horizontal = TRUE,  main = \"Heavy-tailed error\")\n\n\n\n\n\n\n# histogram of standarized residuals (so can overlay Z density curve)\nhist(rstandard(mod_heavy_tailed_error), freq = FALSE, breaks = 20, main = \"Heavy-tailed error\")\nx_plot &lt;- seq(from = -3, to = 3, by = 0.01)\nlines(x = x_plot, dnorm(x = x_plot))\n\n\n\n\n\n\n# normal qq plot\nplot(mod_heavy_tailed_error, which = 2)\n\n\n\n\n\n\n\n\n\n\n\n3.3.6 Omission of important predictor variables\n\n\nContent\nIdeal\n\n\n\nOverview\n\n\nResiduals should also be plotted against variables omitted from the model that might have important effects on the response.\n\nAllows us to see whether or not the residuals tend to vary systematically with the level of the additional predictor variable (can be qualitative or quantitative).\n\n\n\n\nOnly a few of the factors operating on any response variable \\(Y\\) in real-world situations can be included explicitly in a regression model (depends on what data was collected).\n\nSo when using residual analysis to identify other important predictors, we are simply testing the adequacy of the model and seeing if it could be improved materially by adding one or more predictor variables.\nIn doing so, we can provide important additional descriptive and predictive power to the model.\n\n\n\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_{1,i} + 0 \\, X_{2,i} + \\text{Normal}(0, \\sigma^2)\\)\n\nSimplifies to: \\(Y_i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2 X_{2,i} + \\epsilon_i \\hspace{20pt} \\text{with unrelated } X_{2,i}\\)\n\n\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 3; sigma &lt;- 5\n\n# unimportant predictor\nbeta_2 &lt;- 0\n\n# generate X samples\nx_1 &lt;- runif(n = n, min = 5, max = 15)\nx_2 &lt;- runif(n = n, min = 0, max = 5)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate Y observations using with both xs\n# -&gt; beta_2 = 0 cancels out x_2 terms\ny &lt;- beta_0 + beta_1 * x_1 + beta_2 * x_2 + epsilon\n\n# fit simplified (correct) model\nmod_correct &lt;- lm(y ~ x_1)\n\n\n\nResiduals vs fitted plot\n\nLooks like random noise, no systematic deviations when residuals for correct model are plotted against unimportant (unrelated) omitted predictor.\n\n\n\n\n# residual plot against unimportant predictor not included in model\n# -&gt; no systematic variation of residuals by additional predictor\nplot(x = x_2, y = resid(mod_correct), main = \"Residuals vs unimportant predictor\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\nlines(lowess(x = x_2, y = resid(mod_correct)), col = \"red\")\n\n\n\n\n\n\n\n\n\n\nDepartures\n\nMissing important predictor\n\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2 X_{2,i} + \\text{Normal}(0, \\sigma^2)\\)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 3; sigma &lt;- 5\n\n# important predictor\nbeta_2 &lt;- 5\n\n# generate X samples\nx_1 &lt;- runif(n = n, min = 5, max = 15)\nx_2 &lt;- runif(n = n, min = 0, max = 5)\n\n# generate normal error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\n\n# calculate Y observations using with both xs\ny &lt;- beta_0 + beta_1 * x_1 + beta_2 * x_2 + epsilon\n\n# fit incorrect model (missing X2)\nmod_missing_important_x &lt;- lm(y ~ x_1)\n\n\n\nResiduals vs fitted plot\n\nNow there is a pattern in the residuals for the omitted important predictor.\n\n\n\n\n# residual plot against important predictor not included in model\n# -&gt; now there is a clear pattern\nplot(x = x_2, y = resid(mod_missing_important_x), main = \"Missing predictor\")\nabline(h = 0, col = \"grey\", lty = \"dashed\")\nlines(lowess(x = x_2, y = resid(mod_missing_important_x)), col = \"red\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#effects-of-departures",
    "href": "notes-diagnostics-and-remedial-measures.html#effects-of-departures",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.4 Effects of departures",
    "text": "3.4 Effects of departures",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#overview-of-remedial-measures",
    "href": "notes-diagnostics-and-remedial-measures.html#overview-of-remedial-measures",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.5 Overview of remedial measures",
    "text": "3.5 Overview of remedial measures\n\n\nOverview\nMore details on transformations\n\n\n\n\n\nIf the normal error SLR model is not appropriate for a dataset, there are two basic choices, both of which have their advantages and disadvantages:\n\nAbandon this model and develop and use a more appropriate model.\n\n\nPro → May lead to a more complex model that could yield better insights.\nCon → May also lead to more complex procedures for estimating the parameters.\n\n\nUse some transformation on the data so that the normal error SLR model is appropriate for the transformed data.\n\n\nPro → Leads to relatively simple methods of estimation with smaller models (less parameters), which is desirable when the sample size is small.\nCon → Transformations may obscure the fundamental interconnections between the variables, though at other times they may illuminate them.\nNow we consider only transformations (will cover more complex models in later chapters).\n\n\n\n\n\n\nHere is a more in depth discussion of when to use which transformation:\nNonlinearity of the regression function\n\n\nWhen the regression function is not linear, a direct approach is to modify the SLR regression model by altering the nature of the regression function (these methods will be discussed much later). For example,\n\nQuadratic regression function (this is an example of a polynomial regression function) → \\(E(Y) = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\)\nExponential regression function (another example of a nonlinear regression function) → \\(E(Y) = \\beta_0 \\cdot \\beta_1^X\\)\n\n\nThe transformation approach uses a transformation to linearize (at least approximately linearize) a nonlinear regression function.\nWhen the nature of the regression function is unknown, exploratory analysis that does not require specifying a particular type of function is often useful (such as LOWESS regression).\n\nNonconstancy of the error variance\n\nWhen the error variance is not constant but varies in a systematic fashion, a direct approach is to modify the model to allow for this and use the method of weighted least squares to obtain the estimators of the parameters.\nTransformations can also be effective in stabilizing the variance, some will be shown shortly.\n\nNonindependence of error terms\n\nWhen the error terms are correlated, the easiest method is to switch to a time series model.\n\nNonnormality of error terms\n\nLack of normality and nonconstant error variances occur together. Fortunately, it is often the case that the same transformation that helps stabilize the variance is also helpful in approximately normalizing the error terms.\nThis is why we should address the nonconstant variance (apply the appropriate stabilizing transformation), then do residual analysis to see if there are still serious departures from normality present.\n\nOmission of important predictor variables\n\nWhen residual analysis indicates that an important predictor variable has been omitted from the model, the solution is to modify the model with multiple regression (two or more predictor variables).\n\nUnusual observations\n\nWhen unusual observations are present (could be outliers, high-leverage points, or influential points) using LSE or MLE may lead to serious distortions in the estimated regression function.\nWhen these unusual obesrvations are not data errors (and thus shouldn’t be thrown out), it may be desirable to use an estimation procedure that places less emphasis on them. One type of these models is discuss much later.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#transformations",
    "href": "notes-diagnostics-and-remedial-measures.html#transformations",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.6 Transformations",
    "text": "3.6 Transformations\n\nNow we will talk about the use of transformations on one or both of the original variables before carrying out the regression analysis.\nSimple transformations of either the response variable \\(Y\\) or the predictor variable \\(X\\), or of both, are often sufficient to make the SLR model appropriate for the transformed data.\n\n\n3.6.1 Transformations for nonlinear relation only\nOverview\n\nGoal → Linearize a nonlinear regression relation.\nUse when → Distribution of error terms is reasonably close to a normal distribution AND have approximately constant variance.\n\nStrategy → Transformations on \\(X\\) should be attempted.\n\nTransformations on \\(Y\\), such as \\(Y' = \\sqrt{Y}\\), may not be desirable here because they can materially change the shape of the distribution of the error terms from the normal distribution and may also lead to substantially differing error term variances.\ne.g.) \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\), where \\(\\epsilon \\sim \\text{Normal}\\,\\) \\(\\rightarrow\\) \\(\\sqrt{Y} = \\beta_0 + \\beta_1 X + \\text{new }\\epsilon\\)\n\n\n\nSpecific transformations\n\n\nBelow are some prototype nonlinear regression relations with some simple transformations on \\(X\\) that may be helpful for linearizing without affecting the distribution of \\(Y\\).\n\n\nSeveral transformations can be tried.\n\nScatter plots and residual plots based on each should be made and analyzed to decide which transformation is most effective.\n\n\n\n\n\n\n\n\nAt times, it may be helpful to introduce a constant into the transformation.\n\ne.g. If some of the \\(X\\) data are near zero and the reciprocal transformation is desired, we can shift the origin by using the transformation \\(X' = \\frac{1}{X + k}\\), where \\(k\\) is an appropriately chosen constant.\n\n\n\nDemo\n\n\nIncreasing at decreasing rate\nIncreasing at increasing rate\nDecreasing at decreasing rate\nEffect of constants and scalars\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nMain issue → See curvilinear pattern (in this case \\(Y\\) grows slower as \\(X\\) increases). So standard SLR model not appropriate (note that in reality, we have no idea what type model this came from).\nRoughly same variance in \\(Y\\) as \\(X\\) increases (imagining spread of observations relative to the smooth curve; the residual plot looks bad because of the lack of fit) and QQ plot looks decent enough → So only issue is the linearity, which means we want to transform \\(X\\) based on the specific pattern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt two transformations chosen based on the observed pattern: 1a) \\(X' = \\log_{10}(X)\\) 1b) \\(X' = \\ln(X)\\) and 2) \\(X' = \\sqrt{X}\\). Then view transformed scatterplots.\n\nAll greatly improve the linear aspect of the regression. Note that \\(\\log_{10}(X)\\) and \\(\\ln(X)\\) have the same linearizing effect, obviously the scales are different for the results.\nThe variability of the scatter at the different \\(X\\) levels is the same as before, since we did not make a transformation on \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at diagnostics for fitted models based on transformed \\(X\\)s.\n\nResidual plots are just fine now and normal QQ plots still look good.\n\n\n\n\n# fit models on transformed X variables\nmod_log10_x &lt;- lm(y ~ log10(x))\nmod_ln_x &lt;- lm(y ~ log(x))\nmod_sqrt_x &lt;- lm(y ~ sqrt(x))\n\n# view some diagnostic plots for models with transformations\nplot(mod_log10_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_ln_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_sqrt_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# view model summaries (just the R^2)\n# -&gt; ln and log10 models give different coefficients, but SAME MSE and R^2, see last tab 'effect of scalars and constants' for why\nsummary(mod_log10_x)\n\n\nCall:\nlm(formula = y ~ log10(x))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54439 -0.14655 -0.03141  0.17759  0.47865 \n\nCoefficients:\n            Estimate Std. Error t value           Pr(&gt;|t|)    \n(Intercept)   2.0733     0.1338   15.49 0.0000000000000029 ***\nlog10(x)      2.0745     0.1385   14.98 0.0000000000000067 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.254 on 28 degrees of freedom\nMultiple R-squared:  0.8891,    Adjusted R-squared:  0.8852 \nF-statistic: 224.5 on 1 and 28 DF,  p-value: 0.000000000000006704\n\nsummary(mod_ln_x)\n\n\nCall:\nlm(formula = y ~ log(x))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54439 -0.14655 -0.03141  0.17759  0.47865 \n\nCoefficients:\n            Estimate Std. Error t value           Pr(&gt;|t|)    \n(Intercept)  2.07334    0.13383   15.49 0.0000000000000029 ***\nlog(x)       0.90096    0.06013   14.98 0.0000000000000067 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.254 on 28 degrees of freedom\nMultiple R-squared:  0.8891,    Adjusted R-squared:  0.8852 \nF-statistic: 224.5 on 1 and 28 DF,  p-value: 0.000000000000006704\n\nsummary(mod_sqrt_x)$r.squared\n\n[1] 0.9156498\n\n\n\nCompare these the models \\(R^2\\) to the original untransformed: 0.88\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nSee curvilinear pattern (in this case \\(Y\\) grows faster as \\(X\\) increases) \\(\\Longrightarrow\\) SLR model not appropriate.\nConstant variance through smooth curve and okay QQ plot \\(\\Longrightarrow\\) Try \\(X\\) transformation to straighten out \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt two transformations chosen based on the observed pattern: 1) \\(X' = X^2\\) and 2) \\(X' = \\mathrm{e}^X\\).\n\nBoth seem to improve the linearity, although not perfectly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at diagnostics for fitted models based on transformed \\(X\\)s.\n\nStill a bit of curvature in the residual plot, but normal QQ plots still look good.\n\n\n\n\n# fit models on transformed X variables\nmod_squared_x &lt;- lm(y ~ I(x^2))\nmod_exp_x &lt;- lm(y ~ exp(x))\n\n# view some diagnostic plots for models with transformations\nplot(mod_squared_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_exp_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# view model summaries (just the R^2)\nsummary(mod_squared_x)$r.squared\n\n[1] 0.8566729\n\nsummary(mod_exp_x)$r.squared\n\n[1] 0.8580996\n\n\n\nCompare these the models \\(R^2\\) to the original untransformed: 0.707\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nSee curvilinear pattern (in this case \\(Y\\) decreases slower as \\(X\\) increases) \\(\\Longrightarrow\\) SLR again model not appropriate.\nStill constant variance and approximately normal residuals, \\(\\Longrightarrow\\) Just transform \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt two transformations chosen based on the observed pattern: 1a) \\(X' = 1 / X\\) 1b) \\(X' = 1 / (X + k)\\) and 2) \\(X' = \\mathrm{e}^{-X}\\).\n\n\\(1 / X\\) does not result in a linear relationship, so no need to continue with that. However all \\(X\\) values are in between 0 and 1, so we can try to introduce a constant to move data away from origin, say \\(X' = 1 / (1 + X)\\). This works better. \\(\\mathrm{e}^{-X}\\) seems to helps as well.\nNote that both transformations are decreasing, which means the association between \\(Y\\) and \\(X'\\) gets flipped and is now increasing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook at diagnostics for fitted models based on transformed \\(X\\)s.\n\nStill a bit of curvature in the residual plot, but normal QQ plots still look good.\n\n\n\n\n# fit models on transformed X variables\nmod_reciprocal_x_k &lt;- lm(y ~ I(1/(x+1)))\nmod_exp_neg_x &lt;- lm(y ~ exp(-x))\n\n# view some diagnostic plots for models with transformations\nplot(mod_reciprocal_x_k, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(mod_exp_neg_x, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# view model summaries (just the R^2)\nsummary(mod_reciprocal_x_k)$r.squared\n\n[1] 0.76538\n\nsummary(mod_exp_neg_x)$r.squared\n\n[1] 0.7236631\n\n\n\nCompare these the models \\(R^2\\) to the original untransformed: 0.617\n\n\n\nQuick demo of introducing constants to one or both sides of regression equation (so to the sample essentially) to see how impact results\nResults\n\n\nCoefficients\n\nAdding constants (all combos \\(X + k\\), \\(Y + k\\) and both) → Changes intercept, doesn’t affect slope.\nMultiplying by scalar → \\(k \\cdot X\\) doesn’t affect intercept, but does affect slope; \\(k \\cdot Y\\) does affect intercept and slope; Both \\(k \\cdot X\\) and \\(k \\cdot Y\\) have same intercept as only \\(k \\cdot Y\\) (because \\(k \\cdot X\\) doesn’t affect it), but the same slope as the original \\(X\\) and \\(Y\\) (in essence cancelling each others’ effect).\n\n\n\n\\(R^2\\) and \\(F\\)-stat\n\nAll models have the same.\n\n\n\n\\(MSE\\) and likelihood-related statistics (\\(loglik\\), \\(AIC\\), \\(BIC\\))\n\nOnly \\(k \\cdot Y\\) models have different \\(MSE\\) (larger because \\(Y\\) values are stretched with \\(\\lvert k \\rvert &gt; 1\\)) and likelihood-related statistics (optimizing a function of \\(Y\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n call \n    b0 \n    se(b0) \n    b1 \n    se(b1) \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n  \n\n\n lm = y ~ x \n    1.242 \n    1.239 \n    2.116 \n    0.156 \n    0.868 \n    0.864 \n    3.129 \n    184.658 \n    0 \n    1 \n    -75.751 \n    157.501 \n    161.705 \n  \n\n lm = y ~ I(x + k1) \n    -5.107 \n    1.667 \n    2.116 \n    0.156 \n    0.868 \n    0.864 \n    3.129 \n    184.658 \n    0 \n    1 \n    -75.751 \n    157.501 \n    161.705 \n  \n\n lm = I(y + k1) ~ x \n    4.242 \n    1.239 \n    2.116 \n    0.156 \n    0.868 \n    0.864 \n    3.129 \n    184.658 \n    0 \n    1 \n    -75.751 \n    157.501 \n    161.705 \n  \n\n lm = I(y + k1) ~ I(x + k1) \n    -2.107 \n    1.667 \n    2.116 \n    0.156 \n    0.868 \n    0.864 \n    3.129 \n    184.658 \n    0 \n    1 \n    -75.751 \n    157.501 \n    161.705 \n  \n\n lm = y ~ I(k2 * x) \n    1.242 \n    1.239 \n    1.058 \n    0.078 \n    0.868 \n    0.864 \n    3.129 \n    184.658 \n    0 \n    1 \n    -75.751 \n    157.501 \n    161.705 \n  \n\n lm = I(k2 * y) ~ x \n    2.485 \n    2.477 \n    4.233 \n    0.311 \n    0.868 \n    0.864 \n    6.257 \n    184.658 \n    0 \n    1 \n    -96.545 \n    199.090 \n    203.294 \n  \n\n lm = I(k2 * y) ~ I(k2 * x) \n    2.485 \n    2.477 \n    2.116 \n    0.156 \n    0.868 \n    0.864 \n    6.257 \n    184.658 \n    0 \n    1 \n    -96.545 \n    199.090 \n    203.294 \n  \n\n lm = y ~ log(x) \n    5.219 \n    1.485 \n    6.653 \n    0.755 \n    0.735 \n    0.725 \n    4.439 \n    77.632 \n    0 \n    1 \n    -86.247 \n    178.493 \n    182.697 \n  \n\n lm = y ~ log10(x) \n    5.219 \n    1.485 \n    15.319 \n    1.739 \n    0.735 \n    0.725 \n    4.439 \n    77.632 \n    0 \n    1 \n    -86.247 \n    178.493 \n    182.697 \n  \n\n\n\n\n\\(log_{10}(X)\\) and \\(\\ln(X)\\)\n\nThese equate to the models \\(y \\sim \\ln(X)\\) and \\(y \\sim k \\cdot log_{10}(X)\\), so we can switch between the two and these results show why these two transformations are essentially equivalent. Only the slopes of the resulting models differ, every other model summary is the same.\n\n\n\n\n\n\n3.6.2 Transformations for Nonnormality and Unequal Error Variances\nOverview\n\nUnequal error variances and nonnormality of the error terms frequently appear together.\n\nOften, these two departures show up as increasing skewness and increasing variability of the distributions of the error terms as the mean response \\(E(Y)\\) increases.\n\ne.g. Regression on yearly household expenditures on vacation versus household income. There will tend to be more variation and greater positive skewness (i.e. some very high yearly vacation expenditures) for high-income households than for low-income households, who tend to consistently spend much less for vacations.\n\n\nGoal → Fix nonnormality and nonconstant variance of regression relation.\n\nStrategy → We need a transformation on \\(Y\\) because the shapes and spreads of the distributions of \\(Y\\) need to be changed.\n\nA transformation like this may also help to linearize a curvilinear regression relation.\nOther times, a simultaneous transformation on \\(X\\) may be needed to obtain or maintain a linear regression relation.\n\n\n\nSpecific transformations\n\n\nBelow are some prototype regression relations with some simple transformations on \\(Y\\) that may be helpful for fixing non normality and unequal variances of the error distributions.\n\nSeveral alternative transformations on \\(Y\\) may be tried, as well as some simultaneous transformations on \\(X\\).\nAgain, scatter plots and residual plots should be made to determine the most effective transformation(s).\n\n\n\n\n\nSame as with the reciprocal transformation \\(X' = \\frac{1}{X + k}\\), we can consider use of constant values to validate the transformation function. e.g. If \\(Y\\) is negative, we can shift the origin for the log transformation \\(Y' = \\ln(Y + k)\\) so all \\(Y + k &gt; 0\\) for all \\(Y\\).\n\nDemo\n\nIncreasing linear trend, with increasing variance\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplot of relationship between \\(Y\\) and \\(X\\)\n\nMain issue → Because of the nonconstant variance, this leads us to think that there is issues with the error variance, perhaps also the structure of the model (but impossible to verify this totally). No serious departures from normality. Slight curvilinear pattern, but could be just because of the random non-constant variance\nBefore attempting transformations to linearize (working on \\(X\\)) or nonlinear regression, we should transform \\(Y\\) based on this pattern. This transformation may help to linearize and improve the normality as well (even though not of concern). If still not good, can try transformation on \\(X\\) in conjunction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempt transformations chosen based on the observed pattern: \\(Y' = \\frac{1}{Y}\\). Then view transformed scatterplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the non-constant variance appears to be fixed, but there is a curvi-linear relationship now. So we can try to transform \\(X\\) at the same time according to the prototype pattern.\n\n\n# view scatterplot, regression line and diagnostic plots for potential transformations\nplot(x = 1/x, y = 1/y, main = \"Transformation = 1/Y and 1/X\")\nabline(lm(1/y ~ I(1/x)), col = \"red\")\n\n\n\n\n\n\nplot(lm(1/y ~ I(1/x)), which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis introduces some other issues, so perhaps another method such as weighted least squares would be more appropriate.\n\n\n\n\n\n\n3.6.3 Box-Cox transformations\nMotivation:\n\nIt is often difficult to determine from diagnostic plots which transformation of \\(Y\\) is most appropriate for correcting skewness of the distributions of error terms, unequal error variances, and nonlinearity of the regression function.\nThe Box-Cox procedure automatically identifies a transformation from the family of power transformations on \\(Y\\).\n\nPower transformations have the form:\n\\[\nY' = Y^\\lambda,\n\\]\nwhere \\(\\lambda\\) is a parameter to be determined from the data.\nThis family encompasses the following simple transformations:\n\nThe normal error regression model with the response variable a member of the family of transformations becomes:\n\\[\nY^\\lambda = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\n\nThis includes an additional parameter \\(\\lambda\\) that needs to be estimated.\nThe Box-Cox procedure uses the method of maximum likelihood to estimate \\(\\lambda\\), as well as the other parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\).\nIn this way, the Box-Cox procedure identifies \\(\\hat{\\lambda}\\), the maximum likelihood estimate of \\(\\lambda\\) to use in the power transformation.\n\nDemo\n\n\nR functions\nManual\nResults\n\n\n\n\n# use same setup as linearly increasing trend with increasing variance\n\n## initialize items\n# -&gt; sample size, population parameters\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 0, max = 20)\n\n# generate error terms\n# -&gt; uniform errors that can increase with x\nepsilon &lt;- runif(n = n, min = 0, max = 2*x)\n\n# calculate observations Y\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# scatter plot with poorly fit regression line and smoothed curve added\nplot(x = x, y = y, main = \"Nonnormal errors with increasing variance \\n ~ Increasing to with linear trend\")\nabline(lm(y ~ x), col = \"red\")\n\n\n\n\n\n\n\nBy observation of the prototype pattern and the results, we tried the model \\(1/Y \\sim 1/X\\) to some success. Let’s see what the Box-Cox procedure recommends.\n\n# run boxcox procedure\n# -&gt; plot MLE of lambda\nMASS::boxcox(lm(y ~ x))\n\n# extract lambda\nbc &lt;- MASS::boxcox(lm(y ~ x))\n\n\n\n\n\n\nbc$x[which.max(bc$y)]\n\n[1] 0.5858586\n\n# run boxcox procedure\n# -&gt; plot of SSE\nALSM::boxcox.sse(x, y, l = seq(from = -1, to = 1, by = 0.1)) # to shorten output\n\n\n\n\n\n\n\n   lambda       SSE\n1    -1.0 46510.031\n2    -0.9 33711.038\n3    -0.8 24592.985\n4    -0.7 18071.065\n5    -0.6 13387.613\n6    -0.5 10011.828\n7    -0.4  7570.669\n8    -0.3  5801.114\n9    -0.2  4517.114\n10   -0.1  3586.645\n21    0.0  2915.742\n11    0.1  2437.361\n12    0.2  2103.575\n13    0.3  1880.113\n14    0.4  1742.509\n15    0.5  1673.396\n16    0.6  1660.589\n17    0.7  1695.732\n18    0.8  1773.350\n19    0.9  1890.173\n20    1.0  2044.669\n\n\n\n\nCan use standard regression procedures to obtain a \\(\\hat{lambda}\\) when software doesn’t automatically provide it. This procedure involves a numerical search in a range of potential \\(\\lambda\\) values such as \\(\\lambda = -2, -1.75, ..., 1.75, 2\\). For each \\(\\lambda\\) value:\n\n\n\\(Y_i^\\lambda\\) observations are first standardized so that the magnitude of the error sum of squares does not depend on the value of \\(\\lambda\\):\n\n\\[\nW_i = \\left\\{\n  \\begin{array}{ll}\n     K_1 (Y_i^\\lambda - 1) & \\lambda \\ne 0\\\\\n     K_2 \\ln(Y_i) & \\lambda = 0\\\\\n  \\end{array}\n\\right.\n\\]\n\nwhere \\(K_2 = \\big(\\prod_{i = 1}^{n}Y_i \\big)^{1/n}\\) and \\(K_1 = \\frac{1}{\\lambda K_2^{\\lambda -1}}\\) (note that \\(K_2\\) is the geometric mean of the \\(Y_i\\) observations).\n\n\nOnce the standardized observations \\(W_i\\) have been obtained for a given \\(\\lambda\\) value, they are regressed on the predictor variable \\(X\\) and the error sum of squares \\(SSE\\) is obtained.\n\n\nIt can be shown that the maximum likelihood estimate \\(\\hat{\\lambda}\\) is that value of \\(\\lambda\\) for which \\(SSE\\) is a minimum.\nIf desired, a finer search can be conducted in the neighborhood of the \\(\\lambda\\) value that minimizes \\(SSE\\). However, the Box-Cox procedure ordinarily is used only to provide a guidefor selecting a transformation, so overly precise results are not needed.\n\n\n# define set of lambdas\nlambdas &lt;- seq(from = -1, to = 1, by = 0.01)\n\n# standardize repsonses\nbox_cox &lt;- function(y, x, lambda = 1) {\n  \n  # define constants\n  K_2 = prod(y)^(1 / length(y))\n  K_1 = 1 / (lambda * K_2^(lambda - 1))\n  \n  # standardize y observations\n  W = if(lambda == 0){\n    K_2 * log(y)\n  }else{\n    K_1 * y^lambda - 1\n  }\n  \n  # regress W on X and get sse\n  sum(resid(lm(W ~ x))^2)\n}\n\n# calculate sse for each lambda value\nsse &lt;- lambdas %&gt;% map_dbl(\\(l) box_cox(y, x, lambda = l))\n\n# plt results and get minimum lambda\nplot(x = lambdas, y = sse, type = \"l\")\nabline(h = min(sse), col = \"grey\")\nabline(v = lambdas[which.min(sse)], col = \"grey\")\n\n\n\n\n\n\nlambdas[which.min(sse)]\n\n[1] 0.57\n\n\n\n\nBoth the R functions and the manual search suggest \\(\\hat{\\lambda} = 0.5\\) \\(\\longrightarrow\\) \\(Y' = \\sqrt{Y}\\). Lets fit the new model and view the diagnostic plots.\n\n# view scatterplot, regression line and diagnostic plots of transformation\nplot(x = x, y = sqrt(y), main = \"Transformation = sqrt(Y)\")\nabline(lm(sqrt(y) ~ x), col = \"red\")\n\n\n\n\n\n\nplot(lm(sqrt(y) ~ x), which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCould now try other remedial measures. But there seems to be issues beyond what a simple transformation can fix. Other methods should be used for this particular example.\n\n\n\nIn practice:\n\nWhen the Box-Cox procedure leads to a \\(\\lambda\\) value near 1, no transformation of \\(Y\\) may be needed.\n\nIt is reasonable to choose a nearby \\(\\lambda\\) value (e.g. can use \\(\\hat{\\lambda} = -0.5\\) instead of \\(\\hat{\\lambda} = -0.79\\)).\n\nThis can make it easier to understand without sacrificing much in terms of the effectiveness of the transformation.\nWhen choosing a more intuitive \\(\\hat{\\lambda}\\), we can look flatness of the likelihood function near the maximum or at the confidence interval for \\(\\lambda\\).\nThis works because the \\(SSE\\) is often fairly stable in the neighborhood of the estimate.\n\n\nAfter a transformation has been tentatively selected, residual plots and other analyses described earlier need to be employed to ascertain that the simple linear regression model is appropriate for the transformed data.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#using-the-transformed-model",
    "href": "notes-diagnostics-and-remedial-measures.html#using-the-transformed-model",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.7 Using the transformed model",
    "text": "3.7 Using the transformed model\n\n3.7.1 Overview\nWhen implementing a transformation, interpretations of the results have now changed.\nRegression coefficients:\n\n\nThe estimators \\(\\beta_0\\) and \\(\\beta_1\\) obtained by least squares have the least squares properties with respect to the transformed observations, not the original ones.\n\ni.e. Estimated coefficients need to be interpreted with respect to the transformed scales.\n\n\nThere is no straightforward way to ‘untransform’ the coefficients to values that can be interpreted on the original scales.\nWe cannot directly compare regression coefficients between models where the response variable transformation is different.\n\nEstimation and prediction:\n\n\nEven if we transform the response, we still want to express model estimates and predictions back in the original scale.\n\nThis is simply a matter of ‘back transforming’ by using the inverse function of the transformation.\nThis is very helpful for communicating results to the public.\n\n\n\n3.7.2 Interpreting confidence limits\nIn general, let \\(Y' = f(Y)\\) and let \\(f'\\) be the back-transformation (inverse) function.\n\nFor example, if \\(Y' = f(Y) = Y^2)\\) \\(\\longrightarrow\\) \\(f'(Y') = Y\\), so \\(f'(Y') = \\sqrt{Y^2} = Y\\).\n\nThen, back transform the mean and single response confidence interval \\((a, b)\\) as following: \\((f'(a), f'(b))\\). For example: \\((\\sqrt{a}, \\sqrt{b})\\).\nBack transforming the coefficients or the standard error is not as straightforward.\n\nDO NOT DO: \\(\\hat{Y} = f'(b_0) + f'(b_1)X = \\sqrt{b_0} + \\sqrt{b_1}X\\).\nHave to do: \\(\\hat{Y} = f'(\\hat{Y'}) = \\sqrt{b_0 + b_1X}\\).\n\nIf only \\(X\\) is transformed to \\(X'\\), then ne need to back transform the \\(Y's\\) estimation because \\(Y\\) hasn’t been transformed. For example: \\(\\hat{Y} = b_0 + b_1 X'\\).\nDemo\n\nOriginal data and model (with assumption violations)\n\n\n# plot data with model\nplasma &lt;- ALSM::Plasma \nmod_plasma &lt;- lm(y ~ x, data = plasma)\nplasma %$% plot(x, y)\nabline(mod_plasma)\n\n\n\n\n\n\n# diagnostic plots\nplot(mod_plasma, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference with bad model\n\n\n# plot data with model\nplasma &lt;- ALSM::Plasma \nmod_plasma &lt;- lm(y ~ x, data = plasma)\nplasma %$% plot(x, y)\nabline(mod_plasma)\n\n\n\n\n\n\n# diagnostic plots\nplot(mod_plasma, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# create dataset of both pointwise confidence and prediction limits\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconfs &lt;- predict(mod_plasma, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod_plasma, newdata = x_h, interval = \"pred\")\nplot(x = plasma$x, y = plasma$y, type = \"p\", main = \"Inference on bad model\")\nlines(x = x_h$x, y = confs[,\"fit\"],  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = confs[,\"lwr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = confs[,\"upr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = preds[,\"lwr\"],  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = preds[,\"upr\"],  type = \"l\", col = \"blue\")\n\n\n\n\n\n\n\n\nFix model\n\n\n# run boxcox and extract lambda\nMASS::boxcox(mod_plasma)\nbc &lt;- MASS::boxcox(mod_plasma)\n\n\n\n\n\n\nbc$x[which.max(bc$y)]\n\n[1] -0.5050505\n\n# fit transformed model -&gt; Y' = 1 / sqrt(Y)\nmod_plasma_prime &lt;- lm(1 / sqrt(y) ~ x, data = plasma)\n\n# plot model and diagnostic plots\nplasma %$% plot(x, 1 / sqrt(y), main = \"Transformed model\")\nabline(mod_plasma_prime)\n\n\n\n\n\n\nplot(mod_plasma_prime, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the working model is $ = $ 0.27 \\(+\\) 0.04 \\(X\\) (remember coefficients are in terms of new \\(Y'\\) scale).\nBands before back transformation\n\n\n# create dataset of both pointwise confidence and prediction limits of transformed model\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconfs &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"pred\")\nplot(x = plasma$x, y = 1 / sqrt(plasma$y), type = \"p\", main = \"Inference on transformed model (before back transformation)\")\nlines(x = x_h$x, y = confs[,\"fit\"],  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = confs[,\"lwr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = confs[,\"upr\"],  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = preds[,\"lwr\"],  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = preds[,\"upr\"],  type = \"l\", col = \"blue\")\n\n\n\n\n\n\n\n\nNow we can back transform \\(Y' = \\frac{1}{\\sqrt{Y}}\\) \\(\\Longrightarrow\\) \\(f'(Y') = (Y^{-1/2})^{-2} = (Y')^{-2} = Y\\).\n\nBands after back transformation\n\nThe predicted values should be \\(\\hat{Y} = (\\hat{Y'})^{-2}\\)\nThe confidence interval for the prediction, with for the mean or single response, should also be back transformed with \\((value)^{-2}\\).\n\n\n\n\n# create dataset of both pointwise confidence and prediction limits of back transformed model\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconfs &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"conf\")\npreds &lt;- predict(mod_plasma_prime, newdata = x_h, interval = \"pred\")\nplot(x = plasma$x, y = plasma$y, type = \"p\", main = \"Inference on transformed model AFTER back transformation\")\nlines(x = x_h$x, y = (confs[,\"fit\"])^-2,  type = \"l\", col = \"grey\")\nlines(x = x_h$x, y = (confs[,\"lwr\"])^-2,  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = (confs[,\"upr\"])^-2,  type = \"l\", col = \"green\")\nlines(x = x_h$x, y = (preds[,\"lwr\"])^-2,  type = \"l\", col = \"blue\")\nlines(x = x_h$x, y = (preds[,\"upr\"])^-2,  type = \"l\", col = \"blue\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#summary-of-remedial-measures",
    "href": "notes-diagnostics-and-remedial-measures.html#summary-of-remedial-measures",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.8 Summary of remedial measures",
    "text": "3.8 Summary of remedial measures\n\n\nFor nonlinear functional relationships with well behaved residuals:\n\nTry transforming \\(X\\)\nMay require a polynomial or piecewise fit\n\n\n\nFor non-constant or non-normal error variance, possible with a nonlinear functional form\n\nTry transforming \\(Y\\)\nThe Box-Cox procedure may be helpful\nIf the transformation on \\(Y\\) doesn’t fix the non-constant variance problem, weighted least squares can be used\n\n\nTransformations of \\(X\\) and \\(Y\\) can be used together.\n\nAny time you consider a transformation\n– Remember to recheck all the diagnostics.\n– Consider whether you gain enough to justify losing interpretability.\n– Reciprocal transformations make interpretation especially hard.\n– Consider back-transforming the results of the final model for presentation.\n\nFor very non-normal errors, especially those arising from discrete responses, generalized linear models are often a better option, but linear regression may be “good enough”.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-diagnostics-and-remedial-measures.html#exploration-of-shape-regression-function",
    "href": "notes-diagnostics-and-remedial-measures.html#exploration-of-shape-regression-function",
    "title": "3  Diagnostics and remedial measures",
    "section": "\n3.9 Exploration of shape regression function",
    "text": "3.9 Exploration of shape regression function\nMotivation\n\nScatter plots often indicate readily the nature of the regression function. But at other times however, the scatter plot is complex and it becomes difficult to see the nature of the regression relationship, if any, from the plot. In these cases, it is helpful to explore the nature of the regression relationship by fitting a smoothed curve without any constraints on the regression function.\nThese smoothed curves are also called nonparametric regression curves. They are useful not only for exploring regression relationships but also for confirming the nature of the regression function.\nThere are lots of methods for time series, such as moving averages (or the same thing but with medians to reduce the influence of outlying observations). But these methods are for when the \\(X\\) values (time points) are equally spaced out.\nThere are also many smoothing methods for regression data, with a main one being lowess method.\n\n\n3.9.1 Lowess method\nOverview\n\nThe lowess method (locally weighted regression scatter plot smoothing) obtains a smoothed curve by fitting successive linear regression functions in local neighborhoods and taking the fitted value at that particular \\(X\\) as the smoothed value.\n\nProcedure\n\nThe lowess method uses a number of refinements in obtaining the final smoothed values to improve the smoothing and to make the procedure robust to outlying observations.\n\n\nThe linear regression is weighted to give cases further from the middle \\(X\\) level in each neighborhood smaller weights.\nTo make the procedure robust to outlying observations, the linear regression fitting is repeated, with the weights revised so that cases that had large residuals in the first fitting receive smaller weights in the second fitting.\nTo improve the robustness of the procedure further, step 2 is repeated one or more times by revising the weights according to the size of the residuals in the latest fitting.\n\n\n\nTo implement the lowess procedure, one must choose (→ in practice):\n\nThe size of the successive neighborhoods to be used when fitting each linear regression → This is the primary choice.\nThe weight function that gives less weight to neighborhood cases with \\(X\\) values far from each center \\(X\\) level → Default weight functios suggested by the creator are adequate for many circumstances.\nAnother weight function that gives less weight to cases with large residuals → Default.\nThe number of iterations to make the procedure robust must be chosen → Two iterations appears to be sufficient.\n\n\n\nChoosing the neighborhood size\n\nThe larger the size, the smoother the function but the greater the danger that the smoothing will lose essential features of the regression relationship.\nIt may require some experimentation with different neighborhood sizes in order to find the size that best brings out the regression relationship.\n\n\n\nDemo\n\n# get dataset\nplasma &lt;- ALSM::Plasma\n\n# plot dataset with lowess smoother\n# arguments (with defaults)\n# -&gt; f = the smoother span (proportion of points in the plot which influence the smooth at each value) ~ neighborood size\nplot(plasma$x, plasma$y)\nlines(lowess(x = plasma$x, y = plasma$y, f = 2/3, iter = 3))\nlines(lowess(x = plasma$x, y = plasma$y, f = 1/4, iter = 3), col = \"blue\")\n\n\n\n\n\n\n\nNotes\n\nSmoothed curves, such as the lowess curve, do not provide an analytical expression for the functional form of the regression relationship. They only suggest the shape of the regression curve.\nThe lowess procedure is not restricted to fitting linear regression functions in each neighborhood. Higher-degree polynomials can also be utilized with this method.\nShowed previously that lowess is used in residual plots to show their pattern\n\n3.9.2 Use of smoothed curves to confirm the fitted regression function\nOverview\n\nSmoothed curves are useful not only in the exploratory a regression model is selected but they are also helpful in confirming the regression function chosen.\nThe procedure for confirmation is simple: The curve is plotted together with the confidence band for the fitted regression function. If the smoothed curve falls within the confidence band, we have supporting evidence of the appropriateness of the fitted regression function.\n\nDemo\n\n# fit untransformed model\nmod_plasma &lt;- lm(y ~ x, data = plasma)\n\n# calculate confidence band using R functions\n# -&gt; using textbook function (to confirm)\nx_h &lt;- data.frame(x = seq(from = min(plasma$x), to = max(plasma$x), by = 0.1))\nconf_band1 &lt;- ALSM::ci.reg(mod_plasma, newdata = x_h, type = \"w\")\n\n# -&gt; using predict() for in practice\nalpha &lt;- 0.05\nW &lt;- sqrt(2 * qf(1 - alpha, df1 = length(coef(mod_plasma)), df2 = df.residual(mod_plasma)))\nconf_band2 &lt;- predict(mod_plasma, newdata = x_h) %&gt;% \n  data.frame(fit = .) %&gt;% \n  mutate(x = x_h$x,\n         se = predict(mod_plasma, newdata = x_h, interval = \"pred\", se.fit = TRUE)$se.fit,\n         lwr = fit - W * se,\n         upr = fit + W * se)\n\n# plot smooth curve and actual confidence bands\nplot(x = plasma$x, y = plasma$y, main = \"Untransformed model\")\nlines(lowess(x = plasma$x, y = plasma$y, f = 2/3, iter = 3))\nconf_band2 %$% lines(x = x, y = lwr, col = \"green\")\nconf_band2 %$% lines(x = x, y = upr, col = \"green\")\nconf_band1 %$% lines(x = x, y = Lower.Band, col = \"blue\", lty = 2)\nconf_band1 %$% lines(x = x, y = Upper.Band, col = \"blue\", lty = 2)\n\n\n\n\n\n\n\n\n# fit transformed model\nmod_plasma_prime &lt;- lm(1 / sqrt(y) ~ x, data = plasma)\n\n# calculate confidence bands\nconf_bands &lt;- ALSM::ci.reg(mod_plasma_prime, newdata = x_h, type = \"w\")\n\n# plot smooth curve and actual confidence bands after back transforming\nplot(x = plasma$x, y = plasma$y, main = \"Transformed model\")\nlines(lowess(x = plasma$x, y = plasma$y, f = 2/3, iter = 3))\nconf_bands %$% lines(x = x, y = Lower.Band^(-2), col = \"blue\")\nconf_bands %$% lines(x = x, y = Upper.Band^(-2), col = \"blue\")",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diagnostics and remedial measures</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#matrix-algebra",
    "href": "notes-matrix-slr.html#matrix-algebra",
    "title": "4  Matrix approach to SLR",
    "section": "4.1 Matrix algebra",
    "text": "4.1 Matrix algebra",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#regression-matrices",
    "href": "notes-matrix-slr.html#regression-matrices",
    "title": "4  Matrix approach to SLR",
    "section": "4.2 Regression matrices",
    "text": "4.2 Regression matrices",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#sec-slr-matrix",
    "href": "notes-matrix-slr.html#sec-slr-matrix",
    "title": "4  Matrix approach to SLR",
    "section": "4.3 SLR in matrix form",
    "text": "4.3 SLR in matrix form\n(Assuming matrix knowledge (or at least can be found elsewhere))\n** Nice site showing r examples of most of the following: important-matrices-in-regression\n\n4.3.1 Model statement\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{10pt} \\text{where} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\\)\n\\[\\Longrightarrow \\hspace{20pt} \\underset{n \\times 1}{\\boldsymbol{Y}} = \\underset{n \\times 2}{\\boldsymbol{X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\beta}} + \\underset{n \\times 1}{\\boldsymbol{\\epsilon}} \\hspace{10pt} \\text{and} \\hspace{10pt} \\underset{n \\times 1}{\\boldsymbol{E(Y)}} = \\underset{n \\times 1}{\\boldsymbol{X\\beta}} \\tag{4.1}\\]\nbecause / where \\(\\hspace{10pt} \\underset{n \\times 1}{\\boldsymbol{E(\\epsilon)}} = \\underset{n \\times 1}{\\boldsymbol{0}} \\hspace{10pt} \\text{and} \\hspace{10pt} \\underset{n \\times n}{\\boldsymbol{\\sigma^2(\\epsilon)}} = \\underset{n \\times n}{\\boldsymbol{\\sigma^2 I}}\\)\n\n\n4.3.2 Least squares estimation of regression parameters\nThe normal equations \\[\n\\begin{align*}\n  \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum X_i &= \\sum Y_i \\\\\n  \\hat{\\beta}_0 \\sum X_i + \\hat{\\beta}_1 \\sum X_i^2 &= \\sum X_i Y_i\n\\end{align*}\n\\]\nin matrix terms are\n\\(\\underset{2 \\times 2}{\\boldsymbol{X'X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} = \\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\)\nwhere \\(\\hspace{10pt} \\underset{1 \\times 2}{\\boldsymbol{\\hat{\\beta'}}} = [\\hat{\\beta}_1, \\hat{\\beta}_2] \\hspace{10pt}\\) is the vector of least squares regression coefficients.\n\n\n4.3.3 Estimated regression coefficients\nSimply use some matrix algebra:\n\\((\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1} \\underset{2 \\times 2}{\\boldsymbol{X'X}} \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} = (\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1}\\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\)\n\\(\\Longrightarrow\\) \\(\\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} = (\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1}\\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\)\n\n\n4.3.4 Fitted values\nThe vector of the fitted values \\(\\hat{Y}_i\\) is \\(\\underset{1 \\times n}{\\boldsymbol{Y'}} = [\\hat{Y}_1, \\ldots, \\hat{Y}_n]\\).\nIn matrix notation, we have the following which can also be expressed using thehat matrix:\n\\[\n\\begin{align*}\n  \\underset{n \\times 1}{\\boldsymbol{Y}} &= \\underset{n \\times 2}{\\boldsymbol{X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}} \\\\\n   &= \\underset{n \\times 2}{\\boldsymbol{X}} \\, (\\underset{2 \\times 2}{\\boldsymbol{X'X}})^{-1}\\underset{2 \\times 1}{\\boldsymbol{X'Y}}\\\\\n   &= \\underset{n \\times n}{\\boldsymbol{H}} \\, \\underset{n \\times 1}{\\boldsymbol{Y}}\n\\end{align*}\n\\]\nwhere \\(\\hspace{10pt} \\underset{n \\times n}{\\boldsymbol{H}} = \\boldsymbol{X(X'X)^{-1}X'}\\).\nWe see from above that the fitted values \\(\\hat{Y}_i\\) can be expressed as linear combinations of the response variable observations \\(Y_i\\) , with the coefficients being elements of the matrix \\(\\boldsymbol{H}\\). And the hat matrix involves only the observations of the predictor variable \\(X\\).\nNotes about hat matrix:\n\nSymmetric (and thus square: \\(n \\times n\\) dimensions)\nIdempotent \\(\\Longrightarrow\\) \\(\\boldsymbol{H}\\boldsymbol{H} = \\boldsymbol{H}\\)\nPlays an important role in diagnostics for regression analysis when we determine the influence of outliers.\n\n\n\n4.3.5 Residuals\nLet the vector of the residuals \\(e_i = Y_i - \\hat{Y}_i\\) be\n\\(\\underset{1 \\times n}{\\boldsymbol{e'}} = [e_1, \\ldots, e_n]\\)\nIn matrix notation, we have:\n\\(\\underset{n \\times 1}{\\boldsymbol{e}} = \\underset{n \\times 1}{\\boldsymbol{Y}} - \\underset{n \\times 1}{\\boldsymbol{\\hat{Y}}} = \\underset{n \\times 1}{\\boldsymbol{Y}} - \\underset{n \\times 2}{\\boldsymbol{X}} \\, \\underset{2 \\times 1}{\\boldsymbol{\\hat{\\beta}}}\\)\nVariance covariance matrix of residuals\n\nThe residuals \\(e_i\\), like the fitted values \\(\\hat{Y}_i\\) , can be expressed as linear combinations of the response variable observations \\(Y_i\\)\n\n\\(\\boldsymbol{e} = \\boldsymbol{Y} - \\boldsymbol{\\hat{Y}} = \\boldsymbol{Y} - \\boldsymbol{HY} = (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{Y}\\)\nThus we have the important result:\n\\(\\underset{1 \\times n}{\\boldsymbol{e'}} = (\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}}) \\, \\underset{n \\times 1}{\\boldsymbol{Y}}\\)\nThe matrix \\(\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}}\\) is symmetric and idempotent like \\(\\boldsymbol{H}\\).\nThe variance-covariance of the vector of residuals involves this matrix:\n\\[\n\\begin{align*}\n  \\underset{n \\times n}{\\boldsymbol{\\sigma^2\\{e\\}}} &= \\boldsymbol{\\sigma^2\\{(\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{Y}\\}} \\\\\n   &= (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{\\sigma^2\\{Y\\}}(\\boldsymbol{I} - \\boldsymbol{H})' \\\\\n   &= \\sigma^2 (\\boldsymbol{I} - \\boldsymbol{H}) (\\boldsymbol{I} - \\boldsymbol{H}) \\hspace{20pt} \\text{because symmetry and }\\boldsymbol{\\sigma^2\\{Y\\}} = \\boldsymbol{\\sigma^2\\{\\epsilon\\}}= \\sigma^2 \\boldsymbol{I} \\\\\n   &= \\sigma^2 (\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}})\n\\end{align*}\n\\]\nand is estimated by\n\\(\\underset{n \\times n}{\\boldsymbol{s^2\\{e\\}}} = MSE (\\underset{n \\times n}{\\boldsymbol{I}} - \\underset{n \\times n}{\\boldsymbol{H}})\\)",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#sec-slr-anova-matrix",
    "href": "notes-matrix-slr.html#sec-slr-anova-matrix",
    "title": "4  Matrix approach to SLR",
    "section": "4.4 ANOVA results",
    "text": "4.4 ANOVA results\nTotal sum of squares: \\(SSTO = \\sum (Y_i - \\bar{Y})^2 = \\sum Y_i^2 - \\frac{(\\sum Y_i)^2}{n}\\).\n\nIn matrix form: \\(SSTO = \\boldsymbol{Y'}\\boldsymbol{Y} - (\\frac{1}{n})\\boldsymbol{Y'}\\boldsymbol{J}\\boldsymbol{Y}\\)\n\nSum of squared errors can be expressed similarly: \\(SSE = \\sum e_i^2 = \\sum (Y_i - \\hat{Y_i})^2\\)\n\nIn matrix form: \\(SSE = \\boldsymbol{e'}\\boldsymbol{e} = (\\boldsymbol{Y} - \\boldsymbol{X\\hat{\\beta}})'(\\boldsymbol{Y} - \\boldsymbol{X\\hat{\\beta}})\\).\nThis can be shown equivalent to (not worrying about derivation of this or next one):\n\n\\(SSE = \\boldsymbol{Y'Y} - \\boldsymbol{\\hat{\\beta}'X'Y}\\)\n\n\nSum of squares regression: \\(SSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\\)\n\nIn matrix form: \\(SSR = \\boldsymbol{\\hat{\\beta}'X'Y} - (\\frac{1}{n})\\boldsymbol{Y'}\\boldsymbol{J}\\boldsymbol{Y}\\)\n\n\n4.4.1 Sums of squares as quadradic forms\nQuadratic form:\n\\[\\underset{1 \\times 1}{\\boldsymbol{Y'AY}} = \\sum_{i = 1}^n \\sum_{j = 1}^na_{ij} Y_i Y_j\\]\nwhere \\(a_{ij} = a_{ji}\\) and \\(\\boldsymbol{A}\\) is a symmetric \\(n \\times n\\) called the matrix of the quadratic form.\nThe ANOVA sums of squares \\(SSTO\\), \\(SSE\\), and \\(SSR\\) are all quadratic forms, as can be seen by reexpressing \\(\\boldsymbol{\\hat{\\beta}'X'}\\):\n\\[\n\\begin{align*}\n\\boldsymbol{\\hat{\\beta}'X'} &= (\\boldsymbol{X\\hat{\\beta}})' \\\\\n  &= \\boldsymbol{\\hat{Y}}' \\\\\n  &= (\\boldsymbol{HY})' \\\\\n  &= \\boldsymbol{Y'H} \\hspace{20pt} \\text{because symmetry}\n\\end{align*}\n\\]\nThis result enables us to express the ANOVA sums of squares as follows (just pre and post multiply and simplify to get to the original forms):\n\\[\n\\begin{align*}\nSSTO &= \\boldsymbol{Y'} \\big[\\boldsymbol{I} - \\big(\\frac{1}{n}\\big) \\boldsymbol{J}\\big] \\boldsymbol{Y} \\\\\nSSE &= \\boldsymbol{Y'}(\\boldsymbol{I - H})\\boldsymbol{Y} \\\\\nSSR &= \\boldsymbol{Y'} \\big[\\boldsymbol{H} - \\big(\\frac{1}{n}\\big) \\boldsymbol{J}\\big] \\boldsymbol{Y}  \\\\\n\\end{align*}\n\\]",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "notes-matrix-slr.html#inferences-in-regression-analysis",
    "href": "notes-matrix-slr.html#inferences-in-regression-analysis",
    "title": "4  Matrix approach to SLR",
    "section": "4.5 Inferences in regression analysis",
    "text": "4.5 Inferences in regression analysis\n\n4.5.1 Regression coefficients\n\n(Can find proof in textbook)\n\n\n4.5.2 Mean response\n\n\n\n4.5.3 Prediction of new observation",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Matrix approach to SLR</span>"
    ]
  },
  {
    "objectID": "part2-mlr.html",
    "href": "part2-mlr.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "This section contains notes from Part 2: Multiple Linear Regression.",
    "crumbs": [
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#overview",
    "href": "notes-multiple-regression-1.html#overview",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.1 Overview",
    "text": "5.1 Overview\nMultiple regression analysis is one of the most widely used of all statistical methods. In this section, we first discuss a variety of multiple regression models.\nSince the matrix expressions for multiple regression are the same as for simple linear regression, can just refer to Chapter 4.\nThe need for multiple predictors\n\nOften times, a single predictor variable in the model provides an inadequate description since a number of key variables affect the response variable in important and distinctive ways.\nFurthermore, in situations like this, we frequently find that predictions of the response variable based on a model containing only a single predictor variable are too imprecise to be useful.\nMultiple predictors is useful in observational studies when there is no control over the variables as well as in experimental situations where the experimenter can control the \\(X\\) levels and investigate different combination simultaneously.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#multiple-regression-models",
    "href": "notes-multiple-regression-1.html#multiple-regression-models",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.2 Multiple regression models",
    "text": "5.2 Multiple regression models\n\n5.2.1 First order model with two predictors\nWhen there are two predictor variables \\(X_l\\) and \\(X_2\\), the regression model:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\epsilon_i\\]\nThis model is still linear in its predictors. It follows that if \\(E(\\epsilon_i) = 0\\)\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_1 + \\beta_1 X_2\\]\nNote that now predictions are a plane and any point on the response plane corresponds to the mean response \\(E(Y)\\) at the given combination of levels of \\(X_1\\) and \\(X_2\\).\n\nHere the vertical distance from \\(Y_i\\) to the response plane represents the error term \\(\\epsilon_i = Y_i - E(Y)\\).\nFrequently the regression function in multiple regression is called a regression surface or a response surface.\nMeaning of regression coefficients\n\n\nThe parameter \\(\\beta_0\\) is the intercept of the regression plane.\n\nIf the scope of the model includes \\(X_1 = 0\\) and \\(X_2 = 0\\), then \\(\\beta_0 = 10\\) for example represents the mean response \\(E(Y)\\) at \\(X_1 = 0\\) and \\(X_2 = 0\\).\n\n\n\nThe parameter \\(\\beta_1\\) indicates the change in the mean response \\(E(Y)\\) per unit increase in \\(X_1\\) when \\(X_2\\) is held constant. Same but reverse for \\(\\beta_2\\).\n\nFor example: If \\(E(Y) = 10 + 2X_1 + 5X_2\\) and the level of \\(X_2\\) is held at 2, then \\(E(Y) = 10 + 2X_1 + 5(2) = 20 + 2X_1\\).\nThis response function is a straight line with slope \\(\\beta_1 = 2\\). This is true for all other values of \\(X_2\\), only the value of the intercept changes.\n\n\n\nAdditive models vs interaction models\n\nWhen the effect of \\(X_1\\) on the mean response does not depend on the level of \\(X_2\\), and correspondingly the effect of \\(X_2\\) does not depend on the level of \\(X_1\\), the two predictor variables are said to have additive effects or not to interact.\nThus, the first-order regression model is designed for predictor variables whose effects on the mean response are additive or do not interact.\nThe parameters \\(\\beta_1\\) and \\(\\beta_2\\) are sometimes called partial regression coefficients because they reflect the partial effect of one predictor variable when the other predictor variable is included in the model and is held constant.\n\n5.2.2 First order model with more than two predictor variables\nWe consider now the case where there are \\(p - 1\\) predictor variables \\(X_1, \\ldots , X_{p-1}\\). The regression model:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1} + \\epsilon_i \\tag{5.1}\\]\nis called a first-order model with \\(p - 1\\) predictor variables. It can also be written as:\n\\[Y_i = \\beta_0 + \\sum_{k = 1}^{p - 1} \\beta_k X_{ik} + \\epsilon_i\\]\nIt follows that if \\(E(\\epsilon_i) = 0\\)\n\\[E(Y_i) =  \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1}\\]\nNotes about model:\n\nThis response function is a hyperplane, which is a plane in more than two dimensions. It is no longer possible to picture this plane.\nThe meaning of the parameters is the same as when there were two predictors, just extended to all other \\(X\\) levels held constant.\nThis first-order regression model is still designed for predictor variables whose effects on the mean response are additive and therefore do interact.\nWhen \\(p - 1 = 1\\), this model reduces to the SLR model we are used to.\n\n5.2.3 General linear regression model\nIn general, the variables \\(X_1, \\ldots, X_{p-1}\\) in a regression model do not need to represent different predictor variables, as we will see soon. Therefore, we define the general linear regression model, with normal error terms, simply in terms of \\(X\\) variables (same as before, just now more general of what the \\(X\\)s can represent):\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1} + \\epsilon_i \\tag{5.2}\\]\nwhere\n\n\\(\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}\\) are parameters\n\\(X_{i1}, \\ldots, X_{i,p-1}\\) are known constants\n\\(\\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0, \\sigma^2)\\)\n\\(i = 1, \\ldots, n\\)\n\nAgain since \\(E(\\epsilon_i) = 0\\),\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\cdots + \\beta_{p-1} X_{i,p-1}\\]\nThus, the general linear regression model with normal error terms implies that the observations \\(Y_i\\) are independent normal variables, with mean \\(E(Y)\\) as given by above and with constant variance \\(\\sigma^2\\).\nThis general linear model encompasses a vast variety of situations. We consider a few of these now.\n\\(p - 1\\) predictor variables\n\nWhen \\(X_1, \\ldots, X_{p-1}\\) represent \\(p - 1\\) different predictor variables, the general linear regression model above is simply Equation 5.1 seen earlier (first-order model in which there are no interaction effects between the predictor variables).\n\nQualitative predictors\n\nModel in Equation 5.2 encompasses not only quantitative predictor variables but also qualitative ones.\nWe use indicator variables that take on the values 0 and 1 to identify the classes of a qualitative variable. For example if length of hospital stay \\(Y\\) with \\(X_1\\) = age and \\(X_2\\) = gender of patient where:\n\n\\[\nX_2 =\n   \\left\\{\n   \\begin{array}{ll}\n     0 & \\text{female}\\\\\n     1 & \\text{male}\n  \\end{array}\n  \\right.\n\\]\n\nThe first-order regression model is (with variables defined above):\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\\] - The response function is\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2}\\]\n\nFor female patients, \\(X_2 = 0\\) and the response function becomes\n\n\\[E(Y_i) = \\beta_0 + \\beta_1 X_{i1}\\] - For male patients, \\(X_2 = 1\\) and the response function becomes\n\\[E(Y_i) = (\\beta_0 + \\beta_2) + \\beta_1 X_{i1}\\]\n\nThese two response functions represent parallel straight lines with different intercepts.\nIn general, we represent a qualitative variable with \\(c\\) classes by means of \\(c - 1\\) indicator variables.\n\nPolynomial regression\n\nPolynomial regression models are special cases of the general linear regression model. They contain squared and higher-order terms of the predictor variable(s), making the response function curvilinear.\nThe following is a polynomial regres’sion model with one predictor variable:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_i1 + \\beta_2 X_i^2+ \\epsilon_i\\]\n\nDespite the curvilinear nature of the response function for regression model above, it is a special casee of general linear regression model Equation 5.2. If we let \\(X_{i1} = X_i\\) and \\(X_{i2} = X_i^2\\), we can write the above model in the same format as usual:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\\]\nTransformed variables\n\nModels with transformed variables involve complex, curvilinear response functions, yet still are special cases of the general linear regression model. Consider the following model with transformed \\(Y\\) variable\n\n\\[\\ln(Y_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\epsilon_i\\] - Here, the response surface is complex, yet this model can still be treated as a general linear regression model if we let \\(Y_i' = \\ln(Y_i)\\). Then just write the regression model as:\n\\[Y_i' = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\epsilon_i\\]\n\nThis has the same form as Equation 5.2, the response variable just happens to be the natural logarithm of \\(Y\\).\nMany models can be transformed into the g’enerallinear regression model. For example, take this model:\n\n\\[Y_i = \\frac{1}{\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i}\\]\n\nIt can be transformed to the general linear regression model by letting \\(Y_i' = 1/Y_i\\). Then we have\n\n\\[Y_i' = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\\]\nInteraction effects\n\nWhen the effects of the predictor variables on the response variable are not additive, the effect of one predictor variable depends on the levels of the other predictor variables.\nOur general linear regression model encompasses regression models with nonadditive or interacting effects. An example of a nonadditive regression model with two predictor variables \\(X_1\\) and \\(X_2\\) is the following:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\epsilon_i\\] - Here the response function is complex because of the interaction term \\(\\beta_3 X_{i1} X_{i2}\\). Yet this regression model is still a special case of the general linear regression model. Let \\(X_3i = X_{i1} X_{i2}\\) and then write the model as:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\epsilon_i\\]\nCombination of cases\n\nA regression model may combine several of the elements we have just noted and still be treated as a general linear regression model.\n\n\n\nExamples of complex regression surfaces for two predictors:\n\n Meaning of linear in general linear regression model\n\nIt should be clear from the various examples that general linear regression model Equation 5.2 is not restricted to linear response surfaces.\nThe term linear model refers to the fact that the model is linear in the parameters; it does-not refer to the shape of the response surface.\nWe say that a regression model is linear in the parameters when it can be written in the form:\n\n\\[Y_i = c_{i0} \\beta_0 + c_{i1} \\beta_1 + c_{i2} \\beta_2 + \\cdots + c_{i,p-1} \\beta_{p-1} + \\epsilon_i \\tag{5.3}\\]\n\nHere the terms \\(c_{i0}, c_{i1}, \\ldots\\) are coefficients involving the predictor variables. For example, take the first-order model with two variables:\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_1 X_{i2} + \\epsilon_i\\]\n\nThis is linear in parameters with \\(c_{i0} = 1\\), \\(c_{i1} = X_{i1}\\) and \\(c_{i2} = X_{i2}\\).\nAn example of a nonlinear regression model is the following:\n\n\\[Y_i = \\beta_0 \\mathrm{e}^{\\beta_1 X_i} + \\epsilon_i\\]\n\nThis is a nonlinear regression model because it cannot be expressed in the form of Equation 5.3.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#sec-main-mlr-demo",
    "href": "notes-multiple-regression-1.html#sec-main-mlr-demo",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.3 General linear regression model in matrix terms",
    "text": "5.3 General linear regression model in matrix terms\nIt is a remarkable property of matrix algebra that the results for the general linear regression model Equation 5.2 in matrix notation appear exactly as those for the simple linear regression model Equation 4.1. Only the degrees of freedom and other constants related to the number of \\(X\\) variables and the dimensions of some matrices are different. Hence, We are able to present the results very concisely.\nWe need the following matrices:\n\nEverything is just extended to \\(p\\) parameters and \\(p - 1\\) predictor variables.\nIn matrix terms, the general linear regression model Equation 5.2 is:\n\\[\\underset{n \\times 1}{\\boldsymbol{Y}} = \\underset{n \\times p}{\\boldsymbol{X}} \\, \\underset{p \\times 1}{\\boldsymbol{\\beta}} + \\underset{n \\times 1}{\\boldsymbol{\\epsilon}} \\hspace{10pt} \\text{and} \\hspace{10pt} \\underset{n \\times 1}{\\boldsymbol{E(Y)}} = \\underset{n \\times 1}{\\boldsymbol{X\\beta}} \\tag{5.4}\\]\n\n5.3.1 Estimation of regression coefficients, fitted values and residuals\nThis is also the exact same, just extended (see Section 4.3).\n\\(\\underset{p \\times 1}{\\boldsymbol{\\hat{\\beta}}} = (\\underset{p \\times p}{\\boldsymbol{X'X}})^{-1}\\underset{p \\times 1}{\\boldsymbol{X'Y}}\\)\n\n\nDemo\n\n\nR functions\nManual (matrices)\n\n\n\n(True) Population model: \\(Y = \\beta_0 + \\beta_1 Z_1 + \\beta_2 Z_2 + \\beta_3 Z_3 + \\beta_4 Z_4 + \\beta_5 Z_5 + \\epsilon_i\\)\nwhere \\(\\hspace{10pt} Z_1 = X_1, Z_2 = X_2, Z_3 = X_3, Z_4 = X_1^2, Z_5 = X_1 X_2 \\hspace{10pt} \\text{and} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0, \\sigma^2)\\)\n\n# NOTE: generating a single dataset (and fitting single model) to use for all of the following examples\n# -&gt; so incorporating lots of aspects into this model (polynomial, interaction, omitted predictor, unnecessary predictor)\n\n# generate predictor data\n# -&gt; num_x number of base predictors (assumed known; generated from same dist)\n# -&gt; num_noise unimportant predictors (placed at end)\n# -&gt; hardcoded 2 higher order / interaction terms (X1^2 and X1*X2)\nn &lt;- 50\nnum_x &lt;- 3\nnum_noise &lt;- 1\nX &lt;- sapply(1:(num_x+num_noise), function(i) {rnorm(n = n, mean = 0, sd = 3)})\nZ &lt;- cbind(rep(1, n), X[, c(1:num_x)], X[, 1]^2, X[, 1] * X[ , 2], X[, -c(1:num_x)])\n\n# set betas according to which ones are noise (and intercept)\nbetas &lt;- c(1, rep(2, num_x+2), rep(0, num_noise))\n\n# generate mean response and add random error\nY &lt;- Z %*% betas + rnorm(n, mean = 0, sd = 5)\n\n(Fitted) Population model: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_4 + \\beta_5 X_1^2 + \\beta_6 X_2 X_3\\)\n\n# create dataframe of sample data using above\ndata_sample &lt;- cbind(Y, X) %&gt;% data.frame\ncolnames(data_sample) &lt;- c(\"Y\", paste0(\"X\",1:(length(data_sample)-1)))\n\n# fit mlr regression model\n# -&gt; omitting X3, adding in unnecessary predictor X4, and adding in correct higher order terms\nmod_mlr &lt;- lm(Y ~ X1 + X2 + X4 + I(X1^2) + I(X1*X2), data = data_sample, x = TRUE)\n\n# view model summary\n# now using broom functions\n# -&gt; see expected results\nbroom::tidy(mod_mlr)\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   1.71       1.56     1.10   2.79e- 1\n2 X1            1.60       0.457    3.50   1.09e- 3\n3 X2            1.83       0.375    4.89   1.38e- 5\n4 X4            0.0183     0.340    0.0538 9.57e- 1\n5 I(X1^2)       1.98       0.115   17.3    3.27e-21\n6 I(X1 * X2)    2.09       0.112   18.7    1.50e-22\n\n\n\n\nNow comparing lots of things\n\n# extract design matrix from model for simplicity\ndesign &lt;- mod_mlr$x\n\n# estimate coefficients\n# -&gt; betas_hat = (X'X)^(-1) X'Y\nbetas_hat &lt;- solve(t(design) %*% design) %*% t(design) %*% Y\n\ncompare(coef(mod_mlr), betas_hat)\n\n$comparison\n$comparison$result\n[1] FALSE\n\n$comparison$description\n[1] \"Attributes: &lt; names for current but not for target &gt;\"             \n[2] \"Attributes: &lt; Length mismatch: comparison on first 0 components &gt;\"\n[3] \"target is numeric, current is matrix\"                             \n\n$comparison$`element-wise`\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n$`coef(mod_mlr)`\n(Intercept)          X1          X2          X4     I(X1^2)  I(X1 * X2) \n 1.71366582  1.59820274  1.83372721  0.01830998  1.97847909  2.08745168 \n\n$betas_hat\n                  [,1]\n(Intercept) 1.71366582\nX1          1.59820274\nX2          1.83372721\nX4          0.01830998\nI(X1^2)     1.97847909\nI(X1 * X2)  2.08745168\n\n# fitted values\ny_hat &lt;- design %*% betas_hat\n\ncompare(fitted(mod_mlr), as.numeric(y_hat), return_items = FALSE)\n\n$comparison\n[1] TRUE\n\n\n\n\n\n\n5.3.2 ANOVA\nSum of squares are the exact same as in Section 4.4, just the degrees of freedom are adjusted for the mean squares.\n\n\\(SSTO\\) has the usual \\(n - 1\\) df.\n\\(SSE\\) had \\(n - p\\) df due to \\(p\\) parameters needed to be estimated in the regression model Equation 5.4.\n\\(SSR\\) has \\(p - 1\\) df, representing the number of \\(X\\) variables \\(X_1, \\ldots, X_{p-1}\\)\n\n\\[MSR = \\frac{SSR}{p - 1} \\hspace{20pt} MSE = \\frac{SSE}{n - p}\\]\n\n\nDemo\n\n\nR functions\nManual (matrices)\n\n\n\nReturning to the demo in Section 5.3.\n\n# calculate anova table\n(at &lt;- mod_mlr %&gt;% anova %&gt;% broom::tidy())\n\n# A tibble: 6 × 6\n  term          df    sumsq   meansq statistic   p.value\n  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 X1             1  1019.    1019.     14.7     4.03e- 4\n2 X2             1     1.83     1.83    0.0264  8.72e- 1\n3 X4             1     2.89     2.89    0.0416  8.39e- 1\n4 I(X1^2)        1 21569.   21569.    310.      1.49e-21\n5 I(X1 * X2)     1 24273.   24273.    349.      1.50e-22\n6 Residuals     44  3058.      69.5    NA      NA       \n\n# calculate sum of squares\nssto &lt;- at %&gt;% summarize(sum(sumsq))\nsse &lt;- at %&gt;% filter(term == \"Residuals\") %&gt;% pull(sumsq)\nssr &lt;- ssto - sse\nssr &lt;- at %&gt;% filter(term != \"Residuals\") %&gt;% summarize(sum(sumsq))\n\n\n\n\n# calculate sum of squares using quadratic forms\n# -&gt; SSTO = Y'[I-(1/n)J]Y \nSSTO &lt;- t(Y) %*% (diag(x = 1, nrow = length(Y)) - (1 / length(Y)) * matrix(data = rep(1, length(Y)^2), nrow = length(Y), ncol = length(Y))) %*% Y\n\n# -&gt; SSE = Y'(I-H)Y\nhat_matrix &lt;- design %*% solve(t(design) %*% design) %*% t(design)\nSSE &lt;- t(Y) %*% (Matrix::diag(x = 1, nrow = length(Y)) - hat_matrix) %*% Y\n\ncompare(as.numeric(sse), as.numeric(SSE))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(sse)`\n[1] 3058.017\n\n$`as.numeric(SSE)`\n[1] 3058.017\n\n# -&gt; SSR = Y'[H-(1/n)J]Y \nSSR &lt;- t(Y) %*% (hat_matrix - (1 / length(Y)) * matrix(data = rep(1, length(Y)^2), nrow = length(Y), ncol = length(Y))) %*% Y\n\ncompare(as.numeric(ssr), as.numeric(SSR))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(ssr)`\n[1] 46866.42\n\n$`as.numeric(SSR)`\n[1] 46866.42\n\n\n\n\n\n\n5.3.3 F-test for regression relation\n\n\n\nR functions\nManual (matrices)\n\n\n\nReturning to the demo in Section 5.3.\n\n# get f-stat model summary\n# -&gt; looking for statistic\nbroom::glance(mod_mlr)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.939         0.932  8.34      135. 1.60e-25     5  -174.  362.  375.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nContinuing with sum of squares calculated from previous demo in Section 5.3.2.\n\n# calculate sum of squares using quadratic forms\n# -&gt; F = MSR / MSE = (SSR / df_R) / (SSE / df_E)\nMSR &lt;- SSR / (length(betas_hat) - 1)\nMSE &lt;- SSE / (length(Y) - length(betas_hat))\nF_stat &lt;- MSR / MSE\n\n# extract mse from model summary\n# -&gt; df = dr_R and df.residual = df_E -&gt; used to get p-value of F-stat\nmse &lt;- broom::glance(mod_mlr)$sigma^2\ncompare(as.numeric(mse), as.numeric(MSE))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(mse)`\n[1] 69.50039\n\n$`as.numeric(MSE)`\n[1] 69.50039\n\ncompare(broom::glance(mod_mlr)$statistic, as.numeric(F_stat))\n\n$comparison\n[1] TRUE\n\n$`broom::glance(mod_mlr)$statistic`\n   value \n134.8666 \n\n$`as.numeric(F_stat)`\n[1] 134.8666\n\n\n\n\n\n\n5.3.4 Coefficient of multiple determination\nThe coefficient of multiple determination \\(R^2\\) is the same as in SLR:\n\\[R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\]\nIt measures the proportionate reduction of total variation in \\(Y\\) associated with the use of the set of X variables \\(X_1, \\ldots , X_{p_1}\\).\nAdding more \\(X\\) variables to the regression model can only increase \\(R^2\\) and never reduce it, because \\(SSE\\) can never become larger with more \\(X\\) variables and \\(SSTO\\) is always the same for a given set of responses.\nTherefore, we often use the adjusted coefficient of multiple determination \\(R^2_{adj}\\). It adjusts \\(R^2\\) by dividing each sum of squares by its respective df.\n\\[R^2_{adj} = 1 - \\Big(\\frac{SSE}{n - p}\\Big) \\Big/ \\Big(\\frac{SSTO}{n - 1}\\Big) = 1 - \\big(\\frac{n - 1}{n - p}\\big) \\frac{SSE}{SSTO}\\]\nThis adjusted coefficient of multiple determination may actually become smaller when another \\(X\\) variable is introduced into the model, because any decrease in \\(SSE\\) may be more than offset by the loss of a degree of freedom in the denominator \\(n - p\\).\nNotes\n\nIt can be shown that the coefficient of multiple determination \\(R^2\\)can be viewed as a coefficient of simple determination between the responses \\(Y_i\\) and the fitted values \\(\\hat{Y}_i\\).\n\nA large value of \\(R^2\\) does not necessarily imply that the fitted model is a useful one.\n\nFor instance, observations may have been taken at only a few levels of the predictor variables. Despite a high \\(R^2\\) in this case, the fitted model may not be useful if most predictions require extrapolations outside the region of observations.\nAgain, even though \\(R^2\\) is large, \\(MSE\\) may still be too large for inferences to be useful when high precision is required.\n\n\n\n\n\nR functions\nManual (matrices)\n\n\n\nReturning to the demo in Section 5.3.\n\n# view model statistics\n# -&gt; looking at r.squared and adj.r.squared\nbroom::glance(mod_mlr)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.939         0.932  8.34      135. 1.60e-25     5  -174.  362.  375.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nContinuing with sum of squares calculated from a previous demo in Section 5.3.2.\n\n# calculate coefficient of determination\n# -&gt; R^2 = SSR / SSTO = 1 - SSE / SSTO\nR2 &lt;- SSR / SSTO\nR2 &lt;- 1 - SSE / SSTO\n\ncompare(as.numeric(broom::glance(mod_mlr)$r.squared), as.numeric(R2))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(broom::glance(mod_mlr)$r.squared)`\n[1] 0.9387471\n\n$`as.numeric(R2)`\n[1] 0.9387471\n\n# calculate adjusted coefficient of determination\n# -&gt; R^2 = 1 - (n - 1) / (n - p) SSE / SSTO\nR2_adj &lt;- 1 - (length(Y) - 1) / (length(Y) - length(betas_hat)) * SSE / SSTO\n\ncompare(as.numeric(broom::glance(mod_mlr)$adj.r.squared), as.numeric(R2_adj))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(broom::glance(mod_mlr)$adj.r.squared)`\n[1] 0.9317865\n\n$`as.numeric(R2_adj)`\n[1] 0.9317865\n\n\n\n\n\n\n5.3.5 Coefficient of multiple correlation\nThe coefficient of multiple correlation \\(R\\) is the positive square root of \\(R^2\\):\n\\[R = \\sqrt{R^2}\\]\nWhen there is only one variable in the model, thec oefficient of multiple correlation \\(R\\) equals in absolute value the correlation coefficient \\(r\\) in SLR.\n\n5.3.6 Inferences about regression parameters\n\n\nDemo\n\n\nIndividual test on \\(\\beta_k\\)\nIndividual CI on \\(\\beta_k\\) – R functions\nIndividual CI on \\(\\beta_k\\) – Manual\n\n\n\n\nReturning to the demo in Section 5.3.\n\n\n# view coefficient summaries\n# -&gt; looking at std.error, statistic and p.value columns\n(summ1 &lt;- broom::tidy(mod_mlr))\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   1.71       1.56     1.10   2.79e- 1\n2 X1            1.60       0.457    3.50   1.09e- 3\n3 X2            1.83       0.375    4.89   1.38e- 5\n4 X4            0.0183     0.340    0.0538 9.57e- 1\n5 I(X1^2)       1.98       0.115   17.3    3.27e-21\n6 I(X1 * X2)    2.09       0.112   18.7    1.50e-22\n\n# just going to recreate the p-value from given info\n(summ2 &lt;- broom::glance(mod_mlr))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.939         0.932  8.34      135. 1.60e-25     5  -174.  362.  375.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# verify t-stat = estimate / std.error\ncompare(as.numeric(summ1[2,4]), as.numeric(summ1[2,2] / summ1[2,3]))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(summ1[2, 4])`\n[1] 3.4964\n\n$`as.numeric(summ1[2, 2]/summ1[2, 3])`\n[1] 3.4964\n\n# calculate p-value from theoretical (beta_k-hat - beta_k)/ SE(\\beta_k-hat) ~ t_(n-p) \np_value &lt;- pt(q = as.numeric(summ1[2,4]), df = summ2$df.residual, lower.tail = FALSE)*2\n\ncompare(as.numeric(summ1[2,5]), p_value)\n\n$comparison\n[1] TRUE\n\n$`as.numeric(summ1[2, 5])`\n[1] 0.001090026\n\n$p_value\n[1] 0.001090026\n\n\n\n\n\n# calculate confidence interval for single coefficient\n# -&gt; remember these are all individual inferences, not simultaneous confidence intervals\nconfint(mod_mlr)[2, ]\n\n    2.5 %    97.5 % \n0.6769793 2.5194262 \n\n\n\n\n\n# calculate lower and upper bounds\nalpha &lt;- 0.05 \nbound_lower &lt;- as.numeric(summ1[2, 2]) + qt(alpha / 2, df = summ2$df.residual) * as.numeric(summ1[2,3])\nbound_upper &lt;- as.numeric(summ1[2, 2]) + qt(1 - alpha / 2, df = summ2$df.residual) * as.numeric(summ1[2,3])\n\ncompare(as.numeric(confint(mod_mlr)[2, ]), c(bound_lower, bound_upper))\n\n$comparison\n[1] TRUE\n\n$`as.numeric(confint(mod_mlr)[2, ])`\n[1] 0.6769793 2.5194262\n\n$`c(bound_lower, bound_upper)`\n[1] 0.6769793 2.5194262\n\n\n\n\n\n\n\n\n\n5.3.7 Estimation of mean response and prediction of new observation",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  },
  {
    "objectID": "notes-multiple-regression-1.html#diagnostics-and-remedial-measures",
    "href": "notes-multiple-regression-1.html#diagnostics-and-remedial-measures",
    "title": "\n5  Multiple regression 1\n",
    "section": "\n5.4 Diagnostics and remedial measures",
    "text": "5.4 Diagnostics and remedial measures\nDiagnostics play an impol1ant role in the development and evaluation of multiple regression models. Most of the diagnostic procedures and remedial meausures for simple linear regression that we described in Chapter 3 carry over directly to multiple regression.\nScatterplot matrix\n\nBox plots, sequence plots, and dot plots for each of the predictor variables and for the response variable can provide helpful, preliminary univariate information about these variables.\nScatter plots of the \\(Y\\) against each \\(X\\) can aid in determining the nature and strength of the bivariate relationships and in identifying gaps in the data points as well as outlying data points. Scatter plots of \\(X\\) against each of the other \\(X\\)s provide similar information.\nA complement to the scatter plot matrix that may be useful at times is the correlation matrix (matrix format as \\([Y, X]\\)). Note this is a symmetric matrix with 1 on the main diagonal, so often can just print the upper or lower traingle matrix.\n\n\nDemo\nReturning to the demo in Section 5.3.\n\n# scatterplot matrix (of just the Xs, not the transformed variables as well)\npairs(data_sample)\n\n\n\n\n\n\n# correlation matrix\n(r &lt;- cor(data_sample) %&gt;% round(3))\n\n        Y     X1     X2     X3     X4\nY   1.000 -0.143  0.024  0.088  0.025\nX1 -0.143  1.000 -0.128 -0.048 -0.121\nX2  0.024 -0.128  1.000 -0.259  0.040\nX3  0.088 -0.048 -0.259  1.000 -0.061\nX4  0.025 -0.121  0.040 -0.061  1.000\n\n# correlation plot\ncorrplot::corrplot(r)\n\n\n\n\n\n\n# upper triangle correlation matrix\nr[lower.tri(r)] &lt;- NA\nr\n\n    Y     X1     X2     X3     X4\nY   1 -0.143  0.024  0.088  0.025\nX1 NA  1.000 -0.128 -0.048 -0.121\nX2 NA     NA  1.000 -0.259  0.040\nX3 NA     NA     NA  1.000 -0.061\nX4 NA     NA     NA     NA  1.000\n\n\nResidual plots\n\nSame as for SLR, a plot of the residuals against the fitted values is useful for assessing the appropriateness of the multiple regression function and the constancy of the variance of the error terms, as well as for providing information about outliers.\nBox plots and normal probability plots of the residuals are useful for examining whether the error terms are reasonably normally distributed.\nIn addition, residuals should be plotted against each of the predictor variables. Each of these plots can provide further information about the adequacy of the regression function with respect to that predictor variable (e.g., whether a curvature effect is required for that variable) and about possible variation in the magnitude of the error variance in relation to that predictor variable.\nResiduals should also be plotted against important predictor variables that were omitted from the model, to see if the omitted variables have substantial additional effects on the response variable that have not yet been recognized in the regression model.\nAlso, residuals should be plotted against interaction terms for potential interaction effects not included in the regression model, such as against \\(X_1 X_2\\), \\(X_1 X_3\\), and \\(X_2 X_3\\), to see whether some or all of these interaction terms are required in the model.\n\nA plot of the absolute residuals or the squared residuals against the fitted values is useful for examining the constancy of the variance of the error terms.\n\nIf nonconstancy is detected, a plot of the absolute residuals or the squared residuals against each of the predictor vaIiables may identify one or several of the predictor variables to which the magnitude of the error variability is related.\n\n\n\nDemo\n\n# residual plots\nplot(mod_mlr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemedial measures\n\nThe remedial measures described previously are also applicable to multiple regression.\n\nWhen a more complex model is required to recognize curvature or interaction effects, the multiple regression model can be expanded to include these effects.\n\nFor example, \\(X_2^2\\) might be added as a variable to take into account a curvature effect of \\(X_2\\), or \\(X_1 X_3\\) might be added as a variable to recognize an interaction effect between \\(X_1\\) and \\(X_3\\) on the response variable.\n\n\n\nAlternatively, transformations on the response and/or the predictor variables can be made as discussed before.\n\nTransformations on the response variable may be helpful when the distributions of the error terms are quite skewed and the variance of the error terms is not constant.\nTransformations of some of the predictor variables may be helpful when the effects of these variables are curvilinear.\nIn addition, transformations on \\(Y\\) and or the predictor variables may be helpful in eliminating or substantially reducing interaction effects.\n\n\nAs with simple linear regression, the usefulness of potential transformations needs to be examined by means of residual plots and other diagnostic tools to determine whether the multiple regression model for the transformed data is appropriate.\nCan still do the Box Cox transformation (Section 3.6.3) like usual.\n\nDemo\n\n# run boxcox procedure\n# -&gt; plot MLE of lambda\nMASS::boxcox(mod_mlr)\n\nError in boxcox.default(mod_mlr): response variable must be positive\n\n# add constant to response variable so can run box-cox\n# -&gt; from simulation for 'effects of constants and scalars', adding constant to Y only changes the intercept -&gt; still valid model\nmod_mlr_prime &lt;- lm(Y+50 ~ X1 + X2 + X4 + I(X1^2) + I(X1*X2), data = data_sample)\n\n# try again\nMASS::boxcox(mod_mlr_prime)\n\n\n\n\n\n\n# lambda = 1 -&gt; no transformation is needed, as expected\n\n\n5.4.1 Residual plots demo for various misfitting models\n::: panel-tabset #### Missing important predictor\nCreate residual plots against each \\(X\\) in data.\n\n# get residuals\ne &lt;- residuals(mod_mlr)\n\n# plot against each X\nnms_x &lt;- colnames(data_sample[, -1])\nmap2(data_sample[,-1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$X1\nNULL\n\n$X2\nNULL\n\n$X3\nNULL\n\n$X4\nNULL\n\n\nNote that there is no pattern for important predictors included in the model or for the unnecessary predictor. BUT for the important predictor \\(X_3\\) omitted from the model, there is clear systematic deviation!\nThis indicates that it should be in the model.\n\n\nMissing polynomial predictor\nMissing interaction predictor\n\n\n\nCreate residual plots.\n\n# fit new model without polyomial predictor\nmod_no_poly &lt;- update(mod_mlr, . ~ . - I(X1^2))\nbroom::tidy(mod_no_poly)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)   17.8       3.46      5.13  0.00000601  \n2 X1            -2.08      1.11     -1.87  0.0679      \n3 X2             2.30      1.03      2.23  0.0309      \n4 X4            -0.559     0.933    -0.598 0.553       \n5 I(X1 * X2)     2.12      0.308     6.90  0.0000000144\n\n# see residuals vs fitted\nplot(mod_no_poly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get residuals\ne &lt;- residuals(mod_no_poly)\n\n# plot against each X\nnms_x &lt;- colnames(data_sample[, -1])\nmap2(data_sample[,-1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$X1\nNULL\n\n$X2\nNULL\n\n$X3\nNULL\n\n$X4\nNULL\n\n\nNote that there is curvature in the residuals vs fitted. Then when viewing vs each predictor, it is clear that it comes from \\(X_1\\)!\nThis indicates that a polynomial term for \\(X_1\\) should be in the model.\nAlso note that missing the polynomial term hides the effect of missing \\(X_3\\) \\(\\Longrightarrow\\) Need to model the terms in there correctly first before looking at more variables.\n\n\nCreate residual plots.\n\n# fit new model without polyomial predictor\nmod_no_interaction &lt;- update(mod_mlr, . ~ . - I(X1*X2))\nbroom::tidy(mod_no_interaction)\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)   -0.968     4.60     -0.210 0.834      \n2 X1             2.25      1.35      1.67  0.103      \n3 X2            -0.377     1.05     -0.359 0.721      \n4 X4             0.646     1.00      0.646 0.522      \n5 I(X1^2)        2.02      0.339     5.96  0.000000359\n\n# see residuals vs fitted\nplot(mod_no_interaction)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get residuals\ne &lt;- residuals(mod_no_interaction)\n\n# plot against each X\nnms_x &lt;- colnames(data_sample[, -1])\nmap2(data_sample[,-1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$X1\nNULL\n\n$X2\nNULL\n\n$X3\nNULL\n\n$X4\nNULL\n\n\nNote that there is maybe a slight pattern with respect to \\(X_1\\) and \\(X_2\\), but definitely harder to detect.\nhardt o tell if should include an interaction or not, obviosuly it should be based on the known true model.\nAlso note that again this hides the effect of missing \\(X_3\\), just like the previous one.",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple regression 1</span>"
    ]
  }
]