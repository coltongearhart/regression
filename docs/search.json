[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression",
    "section": "",
    "text": "Overview\nThese course notes introduce regression models and accompanying assumptions. In addition, they cover the theory of least squares to understand the statistical inference procedures behind linear models. Extensive applications of modeling in R will be demonstrated, with a goal of being able to understand strengths and limitations of models. These will be applied to predict and perform inference, while developing sensitivity to important model assumptions.\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/regression.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "part1-slr.html",
    "href": "part1-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "This section contains notes from Part 1: Simple Linear Regression.",
    "crumbs": [
      "Simple linear regression"
    ]
  },
  {
    "objectID": "part2-mlr.html",
    "href": "part2-mlr.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "This section contains notes from Part 2: Multiple Linear Regression.",
    "crumbs": [
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "placeholder.html#placeholder",
    "href": "placeholder.html#placeholder",
    "title": "2  Placeholder",
    "section": "2.1 Placeholder",
    "text": "2.1 Placeholder\nContent",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Placeholder</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#regression-overview",
    "href": "notes-slr.html#regression-overview",
    "title": "1  Simple linear regression",
    "section": "\n1.1 Regression overview",
    "text": "1.1 Regression overview\nRegression overview\n\nGoal is to determine if and how one variable is related to a set of other variables.\n\nVariables\n\nResponse variable, denoted \\(Y\\), represents an outcome whose variation is being studied.\nExplanatory variable, denoted \\(X\\), represents the causes (i.e. potential reasons for variation).\n\n\n\nTwo types of relationships\n\nFunctional (deterministic) → There is an exact relation between two variables (have the form \\(y = ax+ b\\)).\nStatistical (probabilistic) → There is not an exact relation because there are other variables that affect the relationship (have the form \\(y = ax + b + \\epsilon\\)).\n\n\n\nRegression models and their uses\n\nStatistical models quantify the relationship between a response variable (i.e. a random variable) and explanatory variables, which are usually assumed to be deterministic (i.e. known exactly).\n\nElements of a statistical regression model\n\n\nIn general, observations do not fall directly on the curve of a relationship.\n\n\\(Y \\mid X\\) has a probability distribution.\n\\(E(Y \\mid X)\\) varies deterministically with \\(X\\).\n\n\n\n\nSo the statistical model is:\n\\[\n\\begin{align*}\n  Y &= E(Y \\mid X) + \\epsilon \\\\\n    &= f(X) + \\epsilon, \\hspace{20pt} \\text{where $\\epsilon$ has some distribution}\n\\end{align*}\n\\]\n\n\n\n\nTwo components of a statistical model:\n\n\\(f(X) = E(Y \\mid X)\\) → Defines relationship between \\(Y\\) and \\(X\\); explains the average behavior of the response.\n\\(\\epsilon\\) → An element of randomness (i.e. error). This contains the variation that \\(f(X)\\) cannot explain and/or that is of no interest.\n\n\nThis means \\(f(X) = E(Y \\mid X)\\) will be the same for all samples with the same \\(X\\) values. The only thing that changes is the random error \\(\\epsilon\\) and as a result \\(Y\\). Example \\(Y = 3 + 1X + \\epsilon\\):\n\n\nCode# create regression data frame to show different the deterministic and statistical relationships (assuming SLR normal error model for demonstration so can visualize)\n# generate\n# -&gt; X values (not from a random dist)\n# -&gt; f(X) = E(Y | X) = beta_0 + beta_1 X demonstration\n# -&gt; random error = epsilon ~ N(0,1)\n# -&gt; Y = f(X) + epsilon\nbeta_0 &lt;- 3\nbeta_1 &lt;- 1\ndata_ships &lt;- data.frame(X = seq(from = 10, to = 19, by = 1)) %&gt;% \n  mutate(f_X = beta_0 + beta_1 * X,\n         epsilon = rnorm(n = nrow(.), mean = 0, sd = 1),\n         Y = f_X + epsilon)\n\n# display data\ndata_ships %&gt;% display_nice(col.names = c(\"$X$\", \"$f(X) = E(Y \\\\mid X)$\", \"$\\\\epsilon \\\\sim N(0,1)$\", \"$Y = f(X) + \\\\epsilon$\"))\n\n\n\n $X$ \n    $f(X) = E(Y \\mid X)$ \n    $\\epsilon \\sim N(0,1)$ \n    $Y = f(X) + \\epsilon$ \n  \n\n\n 10 \n    13 \n    0.806 \n    13.806 \n  \n\n 11 \n    14 \n    1.418 \n    15.418 \n  \n\n 12 \n    15 \n    0.040 \n    15.040 \n  \n\n 13 \n    16 \n    0.284 \n    16.284 \n  \n\n 14 \n    17 \n    0.176 \n    17.176 \n  \n\n 15 \n    18 \n    -0.755 \n    17.245 \n  \n\n 16 \n    19 \n    -0.033 \n    18.967 \n  \n\n 17 \n    20 \n    -0.726 \n    19.274 \n  \n\n 18 \n    21 \n    1.280 \n    22.280 \n  \n\n 19 \n    22 \n    -0.377 \n    21.623 \n  \n\n\n\nCode# plot (X, Y)\n# -&gt; and add points and line for E(Y | X)\n# --&gt; population E(Y | X)) because using known betas (not estimating from sample data)\ndata_ships %$% \n  plot(x = X, y = Y, type = \"p\", main = \"Deterministic vs statistical relationship\")\ndata_ships %$% \n  points(x = X, y = f_X, col = \"red\", pch = 5)\ndata_ships %$% \n  lines(x = X, y = f_X, col = \"red\")\ntext(x = 18.5, y = 20, labels = \"f(X) = E(Y|X)\", col = \"red\")\n\n\n\n\n\n\n\nConstruction of statistical regression models\n\n\nSelection of predictor variables (how to decide which ones?).\n\nUse of outside information, historical knowledge, and/or experience.\nExploratory data analysis.\nVariable selection techniques → Find a subset of important variables (i.e. practical and easy to find).\n\n\n\nFunctional form of the regression relation (what is form of \\(f(X)\\)?).\n\n&lt; based on same info as (1) &gt;\nIf there is an abundance of data, maybe start with more complex models and then simplify.\n\n\n\nScope of model (when is the model useful?).\n\nWhen the model best predicts or describes the relationship between response and predictor variables.\n\n\n\nUses of statistical regression models\n\nDetermining whether an \\(X\\) “affects” \\(Y\\) or not.\nEstimation of impact of a given \\(X\\) on the \\(Y\\).\nEstimation of the mean of \\(Y\\) for a given \\(X\\) value.\nPrediction of a single value of \\(Y\\) for a given \\(X\\) value.\n\nTypical strategy for regression analysis\n\n\n1.1.1 Simple linear regression model (SLR)\nGoal of SLR\n\nInvestigate the relationship between \\(Y\\) and a single numeric independent variable \\(X\\), assuming that, in the population, the mean of \\(Y\\) is linearly related to the value of \\(X\\).\nPopulation relationship → \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).\nSample relationship → \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nData structure\n\nBoth \\(X\\) and \\(Y\\) on a random sample of \\(n\\) individuals are collected from the population of interest.\nThe resulting data has the form \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nHolds for all \\(n\\) values in the random sample of \\(n\\) pairs of values, \\((X_i, Y_i), \\, i = 1, \\ldots, n\\).\n\\(Y_i\\) → Dependent (or response) variable value. These are independent, but not identically distributed.\n\\(X_i\\) → Independent (or predictor) variable value. These are not random variables, rather known constants.\n\\(\\epsilon_i\\) → Random error term, assumed to have mean zero and variance \\(\\sigma^2\\). \\(\\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = \\mathrm{Corr}(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j : i \\ne j\\). Often, the \\(\\epsilon_i\\) are assumed to be independent and identically distributed \\((iid)\\), i.e. all have the same distribution with the same mean and variance.\n\\(\\beta_0\\) and \\(\\beta_1\\) → Fixed but unknown regression parameters that need to be estimated.\n\\(\\sigma^2\\) → Another parameter that needs estimated, but it is technically not a “regression” parameter since it does not determine the relationship between \\(Y\\) and \\(X\\) (i.e. it only deals with randomness).\nNote that \\(Y_i\\) and \\(\\epsilon_i\\) are random variables and therefore have distributions. Thus, discussing their mean and variances are appropriate.\nAlternate (equivalent) version of regression model\n\n\n\nResults\nDerivation\n\n\n\nModel statement\n\nCan use the deviation \\(X_i - \\bar{X}\\) as the predictor instead of \\(X_i\\). This leads to:\n\n\\[\nY_i = \\beta_0^* + \\beta_1 (X_i - \\bar{X}) + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\beta_0^* = \\beta_0 + \\beta_1 \\bar{X}\n\\]\n\nThis model can be useful in some derivations.\n\nLater results (jumping ahead)\n\n\nEstimated coefficients\n\nLSE for \\(\\beta_1\\) is the same as before.\nNow the LSE for \\(\\beta_0^*\\) can be found using \\(\\hat{\\beta}_0\\) :\n\n\n\n\\[\n\\hat{\\beta}_0^* = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X} = (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 \\bar{X} = \\bar{Y}\n\\]\n\nEstimated regression function\n\n\\[\n\\hat{Y} = \\bar{Y} - \\hat{\\beta}_1 (X - \\bar{X})\n\\]\n\n\nProperty of all regression functions\n\nUsing this alternative model, it is easy to see that all regression functions pass through the point \\((\\bar{X}, \\bar{Y})\\).\n\n\n\n\n\n\n\n\n\nSome implications of above\n\n\nResults\nDerivation\n\n\n\n\nMean of \\(Y_i\\) for given \\(X_i\\) → \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\)\nVariance of \\(Y_i\\) for given \\(X_i\\) → \\(V(Y_i) = \\sigma^2\\)\n\n\n\n\n\n\n\nInterpretation of regression parameters \\((\\beta_0, \\beta_1)\\)\n\n\n\\(\\beta_0\\)\n\n\\(Y\\)-intercept of the regression line and gives \\(Y\\)’s mean when \\(X = 0\\) → \\(E(Y \\mid X = 0) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\)\n\nOnly makes sense to interpret when \\(X=0\\) is within the scope of the model. So, ask two questions when deciding whether or not to interpret:\n\nDoes it make sense to interpret in context?\nDo we have data in the proximity of zero?\n\n\nIf answer “no” to either of these, then no need to interpret the intercept.\n\n\n\n\\(\\beta_1\\)\n\nSlope of the regression line and indicates the change in \\(Y\\)’s mean when \\(X\\) increases by one unit → \\(E(Y \\mid X = x^* + 1) - E(Y \\mid X = x^*) = [\\beta_0 + \\beta_1 (x^* + 1)] - [\\beta_0 + \\beta_1 x^*] = \\beta_1\\)\nDetermines whether a relationship exists between \\(Y\\) and \\(X\\).\nNote that regression does not substantiate or prove a cause-effect relationship. Rather it gives evidence that \\(Y\\) and \\(X\\) are related (but not that \\(X\\) “causes” the value of \\(Y\\)).\n\n\n\nModel scope (and the dangers of extrapolation)\n\nThe structural form of the relationship may not be the same as you move away from the observed predictor space. Therefore, using \\(\\hat{Y}\\) for estimation / prediction at \\(X\\) values outside of model scope would be extrapolation and results are not reliable.\nShould collect data from the entire region of predictor values of research interest.\n\n\nEstimators\n\n\nPurpose\n\nEstimators help us make inferences about population parameters.\nEach estimator corresponds to a population parameter (e.g. \\(\\hat{\\beta}_1 \\rightarrow \\beta_1\\)).\n\n\n\nEstimator / estimate distinction\n\nEstimators are based on formulas (e.g. \\(\\bar{X} = \\frac{1}{n} \\sum X_i\\)).\nEstimates are values computed from estimators.\n\n\n\nGood estimators\n\nBias of an estimator is the difference between the estimator’s expected value and the true value of the parameter being estimated:\n\nIf the bias equals zero, the estimator is said to be unbiased. \\[\\text{Bias} = E(\\hat{\\theta}) - \\theta\\]\n\ne.g. \\(\\bar{X}\\) and \\(\\mu\\) → \\(E(\\bar{X}) \\overset{\\surd} = \\mu\\).\nAnother way to think about it → \\(E(\\bar{X})\\) will be the center of the sampling distribution of \\(\\bar{X}\\) (which will be approximately normal because of the central limit theorem), and we want this to equal \\(\\mu\\).\n\n\n\nThe standard deviation of an estimator is referred to as the standard error of said estimator.\n\nIdeally, estimators will have relatively small standard deviations, in which case they are said to be efficient.\n\n\n\n\n\n1.1.2 Estimation of the regression function\nSetup\n\nFor each point we have an observed value \\(Y_i\\), a fitted value \\(\\hat{Y}_i\\) and a residual \\(\\hat{\\epsilon}_i\\).\nFitted regression function → \\(\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\), where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_0\\) are estimators of \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\nGoal\n\nGoal is to estimate the two “regression” parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nThere are several methods to do this.\n\n1.1.3 Method of least squares\nOverview\n\nThe method of least squares is one way to find “good” estimators of the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nFor each observation \\((X_i, Y_i)\\), this method considers the model error term, which is the deviation of \\(Y_i\\) from its expected value:\n\n\\[\n\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\n\\]\n\nThen we minimize the sum of some function of these errors:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\text{function of } \\epsilon_i \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - E(Y_i)\\big) \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - (\\beta_0 + \\beta_1 X_i)\\big) \\quad\\quad \\text{&lt; for SLR &gt;}\n\\end{align*}\n\\]\n\nFor least squares method specifically, we consider the sum of the \\(n\\) squared errors (deviations). Thus we have:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\epsilon_i^2 \\\\\n    &= \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\end{align*}\n\\]\n\n\nAccording to the method of least squares, the point estimators (functions of the random sample that estimate population quantities) of \\(\\beta_0\\) and \\(\\beta_1\\) are those values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), respectively, that minimize the criterion \\(Q\\) for the sample observations \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\nOnce data is collected, the estimates (realized values) for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are calculated based on the observed sample \\((x_1, y_1), \\ldots, (x_n, y_n)\\). (Note: no good notation for this, maybe \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\); similar idea for means: \\(\\mu\\), \\(\\bar{X}\\) and \\(\\bar{x}\\))\n\nLeast squares estimators\n\n\nThese can be found in two ways.\n\nNumerical methods (optimization) → Search procedures to systematically evaluate the least squares criterion \\(Q\\) for different estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) until the ones that minimize \\(Q\\) are found.\nAnalytical methods → Using calculus. The analytical approach is feasible when the regression model is not mathematically complex.\n\n\n\nThe general process for deriving the least squares estimators of any model is:\n\nWrite \\(Q\\) as a function of the model error term \\(\\epsilon_i\\) as shown above.\nTake derivative with respect to the desired parameter \\(\\beta_i\\).\nSet derivative equal to zero (notation: substituting population term \\(\\beta_i\\) for its estimator \\(\\hat{\\beta}_i\\)) and solve for \\(\\hat{\\beta}_i\\).\nCheck second derivative test to ensure global minimum (take derivative from result of step 2)\n\n\nUsing the analytical approach with the SLR model, we have the following results:\n\n\n\nResults\nDerivation\n\n\n\n\nThe estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are found by solving the simultaneous equations (called the normal equations):\n\n\\[\n\\begin{align*}\n  \\sum Y_i n &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum X_i \\\\\n  \\sum X_i Y_i &= \\hat{\\beta}_0 \\sum X_i + \\hat{\\beta}_1 \\sum X_i^2\n\\end{align*}\n\\]\n\nThis leads to\n\n\\[\n\\begin{align*}\n  \\text{Intercept} \\hspace{10pt} \\hat{\\beta}_0 \\hspace{10pt} &= \\hspace{10pt} \\frac{1}{n}\\sum Y_i + \\hat{\\beta}_1 \\frac{1}{n} \\sum X_i \\hspace{10pt} = \\hspace{10pt} \\bar{Y}- \\hat{\\beta}_1 \\bar{X} \\\\\n  \\text{Slope} \\hspace{10pt} \\hat{\\beta_1} \\hspace{10pt} &= \\hspace{10pt} \\frac{\\sum X_i Y_i -\\frac{1}{n} \\sum X_i Y_i}{\\sum X_i^2 - \\frac{1}{n}(\\sum X_i)^2} \\hspace{10pt} = \\hspace{10pt} \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \\hspace{10pt} = \\hspace{10pt} \\frac{S_{XY}}{S_{XX}}\n\\end{align*}\n\\]\n\nNote: We did not have to assume any distribution of the error term. These are the LSE estimators for any SLR model.\n\n\n\n\n\n\n\n\n\nDemo:\n\nFirst we can generate the sample data \\((x_1, y_1), \\ldots, (x_n, y_n)\\).\n\n\n\n\nCode# simulate sample regression dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = B0 + B1X + E\n# -&gt; Y | X ~ Uniform(B0 + B1*X - a, B0 + B1*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nmin &lt;- -2\nmax &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate error terms\n# -&gt; assumption for LSE is E(epsilon) = 0, so symmetric uniform about zero works\nepsilon &lt;- runif(n = n, min = min, max = max)\n\n# calculate y terms as function of x and error terms\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n\n\nCode# display observed sample data\ndata.frame(x, y) %&gt;% \n  head(10) %&gt;% \n  display_nice(col.names = c(\"$x$\", \"$y$\"))\n\n\n\n $x$ \n    $y$ \n  \n\n\n 9.765 \n    20.351 \n  \n\n 8.491 \n    16.669 \n  \n\n 12.340 \n    27.569 \n  \n\n 9.387 \n    18.608 \n  \n\n 5.947 \n    13.192 \n  \n\n 5.691 \n    12.419 \n  \n\n 10.377 \n    22.871 \n  \n\n 12.972 \n    26.589 \n  \n\n 11.687 \n    22.450 \n  \n\n 11.041 \n    22.329 \n  \n\n\n\nCode# plot sample data\nplot(x = x, y = y)\n\n\n\n\n\n\n\n\nThen we can fit the model and visualize it on the scatter plot.\n\n\nCode# add regression line to plot\nplot(x = x, y = y)\nabline(lm(formula = y ~ x), col = \"red\")\ntext(x = 14, y = 22, labels = bquote(hat(Y) * \" = \" * hat(beta)[0] + hat(beta)[1] * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nFinally we can get the estimates of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\n\n\nR Functions\nManual\n\n\n\n\n# fit SLR model\n# -&gt; lm() calls lm.fit() behind the scenes --&gt; this performs a QR decomposition to fit the model, which is a more stable way to solve for LSE\nmod_slr &lt;- lm(formula = y ~ x)\n\n# display estimated coefficients\nmod_slr$coefficients\n\n(Intercept)           x \n   1.382626    1.978814 \n\n\n\n# or can do:\ncoefficients(mod_slr)\ncoef(mod_slr) # alias for coefficients()\n\nThese are estimates of the population coefficients, so can compare above values to \\(\\beta_0\\) = 1 and \\(\\beta_1\\) = 2.\nProgramming note → Beautiful sites ((1) and (2)) explaining QR decomposition and how it relates to LSE; must go through and recreate at some point.\n\n\n\n# calculate estimates for B0 and B1 using LSE formulas\n# -&gt; b_1 = S_XY / S_XX\n# -&gt; b_0 = Y-bar - b_1 X-bar\ns_xy &lt;- sum((x - mean(x)) * (y - mean(y)))\ns_xx &lt;- sum((x - mean(x))^2)\nb_1 &lt;- s_xy / s_xx\nb_0 &lt;- mean(y) - b_1 * mean(x)\n\n\nCode# compare to results from lm()\ncompare(c(b_0, b_1), mod_slr$coefficients)\n\n$comparison\n(Intercept)           x \n       TRUE        TRUE \n\n$`c(b_0, b_1)`\n[1] 1.382626 1.978814\n\n$`mod_slr$coefficients`\n(Intercept)           x \n   1.382626    1.978814 \n\n\n\n\n\nLSE for other models\n\nCan also derive LSE estimators for other models using the same process.\nSometimes a transformation can make deriving estimators easier. This way we can get the model in a familiar form and derive like usual (simpler). Then we just have to make the appropriate substitutions at the end in order to get the estimators for the original models.\nThis strategy lessons the number of models we have to know how to derive (is just for deriving; no need to do it when fitting models because it’s an extra middle step that doesn’t change leads to equivalent results).\n\nExample 1\n\nRegression through the origin: \\(Y = \\beta X + \\epsilon\\).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X + a, B*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -5, max = 5)\n\n# Y | X ~ Uniform(min = BX + a, max = BX + b)\ny &lt;- runif(n = n, min = beta * x + min, max = beta * x + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 3, y = -5, labels = bquote(hat(Y) * \" = \" * hat(beta) * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model without intercept\nmod_noint &lt;- lm(formula = y ~ -1 + x)\n\n# display estimated coefficient\ncoefficients(mod_noint)\n\n       x \n1.895757 \n\n\n\n\n\n# calculate estimate B using LSE formula\nb &lt;- sum(x * y) / sum(x^2)\n\n\nCode# compare to results from lm()\ncompare(b, coefficients(mod_noint))\n\n$comparison\n   x \nTRUE \n\n$b\n[1] 1.895757\n\n$`coefficients(mod_noint)`\n       x \n1.895757 \n\n\n\n\n\n\n\nExample 2\n\nRegression through the origin with squared predictor variable: \\(Y = \\beta_1 X^2 + \\epsilon\\).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin with squared predictor dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X^2 + a, B*X^2 + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -1, max = 5)\n\n# Y | X ~ Uniform(min = BX^2 + a, max = BX^2 + b)\ny &lt;- runif(n = n, min = beta * x^2 + min, max = beta * x^2 + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 4, y = 15, labels = bquote(hat(Y) * \" = \" * hat(beta) * X^2) , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model with no intercept and squared X\n# -&gt; use asis function I() to treat `^` as an arithmetic operator and not a formula operator\n# -&gt; save design matrix for later demo\nmod_noint_squared &lt;- lm(formula = y ~ -1 + I(x^2), x = TRUE)\n\n# display estimated coefficient\ncoef(mod_noint_squared)\n\n  I(x^2) \n1.991871 \n\n\n\n\n\n# calculate estimates B using LSE formula\nb &lt;- sum(x^2 * y) / sum(x^4)\n\n\nCode# compare to results from lm()\ncompare(b, coef(mod_noint_squared))\n\n$comparison\nI(x^2) \n  TRUE \n\n$b\n[1] 1.991871\n\n$`coef(mod_noint_squared)`\n  I(x^2) \n1.991871 \n\n\n\n# demonstrating that transformed model way is equivalent\n# -&gt; square predictor variable\nx_star &lt;- x^2\nmod_transformed &lt;- lm(formula = y ~ -1 +  x_star, x = TRUE)\n\n# compare design matrices of two models\ncompare(head(mod_noint_squared$x), head(mod_transformed$x))\n\n$comparison\n  I(x^2)\n1   TRUE\n2   TRUE\n3   TRUE\n4   TRUE\n5   TRUE\n6   TRUE\n\n$`head(mod_noint_squared$x)`\n      I(x^2)\n1 19.2116270\n2  2.7604183\n3 16.1747149\n4 22.2916984\n5  0.1274457\n6  0.0110680\n\n$`head(mod_transformed$x)`\n      x_star\n1 19.2116270\n2  2.7604183\n3 16.1747149\n4 22.2916984\n5  0.1274457\n6  0.0110680\n\n# calculate estimates B using LSE formula\nb_star &lt;- sum(x_star * y) / sum(x_star^2)\n\n# compare betas from usual method and transformed model way\ncompare(b, b_star)\n\n$comparison\n[1] TRUE\n\n$b\n[1] 1.991871\n\n$b_star\n[1] 1.991871\n\n\n\n\n\nProperties of least squares estimators\n\n\nGauss-Markov theorem: If the conditions of the SLR model hold (i.e. \\(Y_i= \\beta_0 + \\beta_1 X_i + \\epsilon_i\\), where \\(\\epsilon_i\\) have mean zero, variance \\(\\sigma^2\\) and are uncorrelated), then the LSE \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the Best Linear Unbiased Estimators (BLUE) (note that \\(\\epsilon_i\\) do not have to be normal).\n\nLinear estimators → Through lots of algebra (shown later), \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta}_1\\) can be written as be written as a linear combination of the \\(Y_i\\): \\(\\sum k_i Y_i\\), where \\(a_i\\) are constant. Thus they are linear estimators.\nUnbiased estimators → Their expected values are \\(\\beta_0\\) and \\(\\beta_1\\), respectively. So neither estimator tends to overestimate or underestimate systematically.\nBest → More precise than all other linear unbiased estimators of \\(\\beta_0\\) and \\(\\beta_1\\) (smaller variance of the sampling distributions).\n\n\n\n1.1.4 Point estimation of the mean response\n\nFor the population relationship \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\), we have the regression function \\(E(Y) = \\beta_0 + \\beta_1 X\\) (because \\(E(\\epsilon) = 0\\)). This is estimated with \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nTerms and what they represent:\n\n\\(Y\\) is a value of the response variable; the observed value.\n\\(E(Y)\\) the mean response. So it is the center of the probability distribution of \\(Y\\) corresponding to the level \\(X\\) of the predictor variable (so technically a conditional expected value \\(E(Y \\mid X)\\).\n\\(\\hat{Y}\\) is a point estimator of the mean response when the level of the predictor variable is \\(X\\); the fitted value.\n\n\n\nApplication of Gauss-Markov theorem\n\n\n\\(\\hat{Y}\\) is the BLUE for \\(E(Y)\\) with \\(V(\\hat{Y}) = V(\\epsilon) = \\sigma^2\\).\n\n\n\n1.1.5 Residuals and model errors\n\nModel error term \\(\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\\) → Measures the difference between an observation and its expected value (unknown true regression line). It is unknown / unobservable.\nResidual \\(\\hat{\\epsilon}_i = e_i = Y_i - \\hat{Y}_i\\) → This is a known, observable estimate of the unobservable model error. Measures the deviation of the observed value from the fitted regression function.\n\n\n\nResiduals are very useful for studying whether the given regression model is appropriate for the data.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#estimation-of-error-terms-variance",
    "href": "notes-slr.html#estimation-of-error-terms-variance",
    "title": "1  Simple linear regression",
    "section": "\n1.2 Estimation of error terms variance",
    "text": "1.2 Estimation of error terms variance\nNeed to estimate the variance \\(\\sigma^2\\) of the error terms \\(\\epsilon_i\\) in a regression model to get an indication of the variability of the probability distributions of \\(Y\\). Also several inferences for models require an estimate of \\(\\sigma^2\\).\nMotivation\n\nFor a single population, the variance \\(\\sigma^2\\) is estimated by the sample variance \\(S^2\\), which is an unbiased estimator. We find \\(S^2\\) by taking the sum of the squared deviations of the observed value and the estimated mean (sum of squares) and then dividing it by the degrees of freedom \\(n - 1\\) (one df is lost when estimating \\(\\mu\\); mean square) .\n\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (Y_i - \\bar{Y})^2}{n - 1}\n\\]\nRegression model\n\nSame logic as above, except we use the residuals as the deviations because each \\(Y_i\\) comes from a different probability distribution with different (depends on the \\(X_i\\) level).\n\n\\[\n\\text{Error (residual) sum of squares} \\hspace{10pt} SSE = \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 = \\sum_{i = 1}^n e_i^2\n\\]\n\nThen divide by the \\(df = n - 2\\) to the mean square (two dfs are lost when because \\(\\beta_0\\) and \\(\\beta_1\\) need to be estimated when getting the estimated means \\(\\hat{Y_i}\\)).\n\n\\[\n\\text{Error (residual) mean square} \\hspace{10pt} S^2 = MSE = \\frac{SSE}{n - 2} = \\frac{\\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2}{n - 2} = \\frac{\\sum_{i = 1}^n e_i^2}{n - 2}\n\\]\n\nIt will be shown later that \\(MSE\\) is an unbiased estimator for \\(\\sigma^2\\) → \\(E(MSE) = \\sigma^2\\).\nAn estimator of the standard deviation is simply \\(S = \\sqrt{MSE}\\).\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# simulate dataset (using uniform errors)\n# initialize items, generate X values, errors and y values\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; min &lt;- -2; max &lt;- 2\nx &lt;- runif(n = n, min = 5, max = 15)\nepsilon &lt;- runif(n = n, min = min, max = max)\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# display model summary -&gt; looking for residual standard error = S = sqrt(MSE)\n# -&gt; save to object to extract it\n(summ &lt;- summary(mod))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6127 -0.7758 -0.1787  0.6272  2.2029 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  0.93619    0.71412   1.311               0.201    \nx            1.98386    0.06883  28.822 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.077 on 28 degrees of freedom\nMultiple R-squared:  0.9674,    Adjusted R-squared:  0.9662 \nF-statistic: 830.7 on 1 and 28 DF,  p-value: &lt; 0.00000000000000022\n\n# display S and MSE = S^2\nsumm$sigma\n\n[1] 1.076598\n\nsumm$sigma^2\n\n[1] 1.159063\n\n\nThese are estimates of population quantities for the error terms, and when simulating the data we assumed \\(\\epsilon_i \\sim \\text{Uniform}\\,(a, b) \\Longrightarrow \\sigma^2 = V(\\epsilon_i) = \\frac{(b - a)^2}{12}\\). For \\(a\\) = -2 and \\(b\\) = 2, \\(V(\\epsilon_i)\\) = 1.333.\n\n\n\n# SIDENOTE -&gt; showing multiple equivalent ways to do the same thing (just keep the last way)\n\n# get fitted values using functions\ny_hat &lt;- mod$fitted.values\ny_hat &lt;- fitted(mod)\ny_hat &lt;- fitted.values(mod) # alias for fitted()\ny_hat &lt;- predict(mod) # discussed more later\n\n# extract residuals using functions (or some manual calculations)\ne &lt;- residuals(mod)\ne &lt;- resid(mod) # alias for residuals()\ne &lt;- mod$residuals\ne &lt;- y - y_hat\n\n# calculate SSE and df\nsse &lt;- sum(e^2)\ndf &lt;- mod$df.residual # extract from model object\ndf &lt;- df.residual(mod)\ndf &lt;- summ$df[2] # extract from model summary -&gt; error df is the second item (even if have MLR model)\ndf &lt;- n - 2 # for SLR (with intercept)\n\n# calculate MSE\nmse &lt;- sse / df\n\n# calculate residual standard deviation\ns &lt;- sqrt(mse)\n\n# compare to results from summary(lm())\n# -&gt; s\ncompare(summ$sigma, s)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma`\n[1] 1.076598\n\n$s\n[1] 1.076598\n\n# -&gt; MSE\ncompare(summ$sigma^2, mse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2`\n[1] 1.159063\n\n$mse\n[1] 1.159063\n\n# -&gt; SSE = MSE * df\ncompare(summ$sigma^2 * summ$df[2], sse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2 * summ$df[2]`\n[1] 32.45377\n\n$sse\n[1] 32.45377",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#normal-error-regression-model",
    "href": "notes-slr.html#normal-error-regression-model",
    "title": "1  Simple linear regression",
    "section": "\n1.3 Normal error regression model",
    "text": "1.3 Normal error regression model\nLeast squares results\n\nNo matter what may be the form of the distribution of the error terms \\(\\epsilon_i\\) (and thus of the \\(Y_i\\)), the LSE provides unbiased point estimators of \\(\\beta_0\\) and \\(\\beta_1\\), that have minimum variance among all unbiased linear estimators (BLUEs).\n\nAssumptions on error term distribution\n\nThese are need \\(\\epsilon_i\\) is needed to set up interval estimates and make tests.\nThe standard assumption is that the error terms are normally distributed. This greatly simplifies the theory of regression analysis and is justifiable in many real-world situations where regression analysis is applied.\n\nNew regression model\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\n\\]\n\nThis is the same regression model as before, just with specified error distribution now (\\(iid\\) = independent and identically distributed).\nBecause this model assumes normal errors, the assumption of uncorrelatedness of the original (unspecified error) model now becomes an independence assumption (\\(\\perp \\!\\!\\! \\perp\\Longrightarrow \\mathrm{Corr}(\\epsilon_i,\\epsilon_j) = 0\\). So the outcome in anyone trial has no effect on the error term for any other trial (in terms of positive or negative, small or large).\nThis model means \\(Y_i \\overset{\\perp \\!\\!\\! \\perp}\\sim \\text{Normal}\\,\\) with \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\) and \\(V(Y_i) = \\sigma^2\\).\n\n\nJustification of the normality assumption.\n\nError terms frequently represent the effects of factors omitted from the model that affect the response to some extent and that vary at random without reference to the variable \\(X\\).\nThese random effects have a degree of mutual independence, the composite error term representing all these factors tends to normal as the number of factors becomes large (by the CLT).\nAlso, the estimation and testing procedures shown later are based on the \\(t\\) distribution and are usually only sensitive to large departures from normality. So, unless the departures from normality are serious, particularly with respect to skewness, the actual confidence coefficients and risks of errors will be close to the levels for exact normality.\n\n\n1.3.1 Estimation of paramters by method of maximum likelihood\nOverview\n\n\nThe method of maximum likelihood chooses as estimates values of the parameters that are most consistent with the sample data. The measure of consistency is the product of densities and is called the likelihood value \\(L(\\mu)\\).\n\nIf the value of \\(\\mu\\) is consistent with the sample data \\(\\Longrightarrow\\) densities relatively large \\(\\Longrightarrow\\) large likelihood value. If not, both will be small.\n\n\n\n\n\nJust like the LSE, there are two ways to find MLEs: a systematic numerical search and by use of an analytical solution.\n\nThe product of the densities viewed as a function of the unknown parameters is called the likelihood function.\n\nIf the likelihood function is relatively peaked in the neighborhood of the maximum, then the MLE estimate is precise because values of \\(\\mu\\) not near the MLE are much less consistent with the data.\nWhen it is relatively flat in a fairly wide region around the MLE, many values of the parameter are almost as consistent with the sample data as the MLE and therefore the MLE is relatively imprecise.\n\n\n\nSteps to MLEs\n\n\nFor multivariate parameter vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_{k})\\), write the likelihood function (i.e. joint density function) and the log-likelihood function\n\n\n\n\\[\nL(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\prod_{i = 1}^n f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\hspace{20pt} \\rightarrow \\hspace{20pt} \\ell(\\boldsymbol{\\theta}) = \\ln[L(\\boldsymbol{\\theta} \\mid \\mathbf{y})]\n\\]\n\nOptimize the log-likelihood function by taking the partial derivatives with respect to each parameters of interest \\(\\theta_1, \\ldots, \\theta_{k}\\).\n\n\nSet to zero and solve for each parameter of interest.\n\n\\[\n\\text{For } j = 1, \\ldots, k \\hspace{20pt} \\ell'(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\theta_j} \\ell(\\boldsymbol{\\theta}) = 0 \\hspace{20pt} \\rightarrow \\hspace{20pt} \\hat{\\boldsymbol{\\theta}} = (\\hat{\\theta}_1, \\ldots, \\hat{\\theta}_{k}) = \\text{potential MLE}\n\\]\n\nVerify that the global maximum of the log-likelihood function occurs at \\(\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}\\).\nFind the second derivative of the log-likelihood function, then plug in \\(\\hat{\\boldsymbol{\\theta}}\\) and see if less than zero.\n\nMLEs for normal error regression model\n\nGenerally\n\n\n\nResults\nDerivation\n\n\n\n\n\n\n\n\n\nParameter\nMLE\n\n\n\n\\(\\beta_0\\)\n\n\\(\\hat{\\beta}_0 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\beta_1\\)\n\n\\(\\hat{\\beta}_1 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\sigma^2\\)\n\\(\\displaystyle \\hat{\\sigma}^2 = \\frac{\\sum (Y_i - \\hat{Y_i})^2}{n}\\)\n\n\n\nNotes\n\n\nProperties of estimators\n\nSince MLEs for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the same as the LSE estimators, they have the same properties: BLUE (unbiased and minimum variance in class of unbiased linear estimators).\nAnd because they are MLEs, we can also say they are: consistent (\\(\\hat{\\beta}_i \\overset{p}\\rightarrow \\beta_i\\), converge in probability to their respective parameters); sufficient (captures all of the information about \\(\\beta_i\\) contained in the sample); minimum variance in class of unbiased estimators (linear or otherwise).\n\n\n\n\n\nMLE \\(\\hat{\\sigma}^2\\) and MSE\n\nRelationship \\(S^2 = MSE = \\frac{n}{n - 2} \\hat{\\sigma}^2 \\hspace{20pt} \\Longleftrightarrow \\hspace{20pt} \\hat{\\sigma}^2 = \\frac{n - 2}{n} MSE\\). However, for large \\(n\\), the difference is small.\nMLE \\(\\hat{\\sigma}^2\\) is biased → \\(E(\\hat{\\sigma}^2) = \\frac{n - 2}{n} \\sigma^2\\), which is an underestimation of \\(\\sigma^2\\). Because of this, MSE is generally used because it is unbiased. However, \\(\\hat{\\sigma}^2\\) is asymptotically unbiased (as \\(n \\rightarrow \\infty\\)) and has a smaller variance than MSE, which may be preferred in some scenarios (tradeoff between bias and precision).\n\n\nGenerally MLE give the same results as LSE.\n\n\n\n\n\n\n\nCan also derive MLEs for transformed models using the same strategy as with LSE.\n\nExample: Suppose we have \\(\\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\). We can transform \\(Y_i^* = \\beta_0^* + \\beta_1^* X_i + \\epsilon_i\\), derive like usual, then substitute at end to get untransformed estimators.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  }
]