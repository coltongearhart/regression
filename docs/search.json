[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression",
    "section": "",
    "text": "Overview\nThese course notes introduce regression models and accompanying assumptions. In addition, they cover the theory of least squares to understand the statistical inference procedures behind linear models. Extensive applications of modeling in R will be demonstrated, with a goal of being able to understand strengths and limitations of models. These will be applied to predict and perform inference, while developing sensitivity to important model assumptions.\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/regression.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "part1-slr.html",
    "href": "part1-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "This section contains notes from Part 1: Simple Linear Regression.",
    "crumbs": [
      "Simple linear regression"
    ]
  },
  {
    "objectID": "part2-mlr.html",
    "href": "part2-mlr.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "This section contains notes from Part 2: Multiple Linear Regression.",
    "crumbs": [
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "placeholder.html#placeholder",
    "href": "placeholder.html#placeholder",
    "title": "2  Placeholder",
    "section": "2.1 Placeholder",
    "text": "2.1 Placeholder\nContent",
    "crumbs": [
      "Multiple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Placeholder</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#regression-overview",
    "href": "notes-slr.html#regression-overview",
    "title": "1  Simple linear regression",
    "section": "\n1.1 Regression overview",
    "text": "1.1 Regression overview\nRegression overview\n\nGoal is to determine if and how one variable is related to a set of other variables.\n\nVariables\n\nResponse variable, denoted \\(Y\\), represents an outcome whose variation is being studied.\nExplanatory variable, denoted \\(X\\), represents the causes (i.e. potential reasons for variation).\n\n\n\nTwo types of relationships\n\nFunctional (deterministic) → There is an exact relation between two variables (have the form \\(y = ax+ b\\)).\nStatistical (probabilistic) → There is not an exact relation because there are other variables that affect the relationship (have the form \\(y = ax + b + \\epsilon\\)).\n\n\n\nRegression models and their uses\n\nStatistical models quantify the relationship between a response variable (i.e. a random variable) and explanatory variables, which are usually assumed to be deterministic (i.e. known exactly).\n\nElements of a statistical regression model\n\n\nIn general, observations do not fall directly on the curve of a relationship.\n\n\\(Y \\mid X\\) has a probability distribution.\n\\(E(Y \\mid X)\\) varies deterministically with \\(X\\).\n\n\n\n\nSo the statistical model is:\n\\[\n\\begin{align*}\n  Y &= E(Y \\mid X) + \\epsilon \\\\\n    &= f(X) + \\epsilon, \\hspace{20pt} \\text{where $\\epsilon$ has some distribution}\n\\end{align*}\n\\]\n\n\n\n\nTwo components of a statistical model:\n\n\\(f(X) = E(Y \\mid X)\\) → Defines relationship between \\(Y\\) and \\(X\\); explains the average behavior of the response.\n\\(\\epsilon\\) → An element of randomness (i.e. error). This contains the variation that \\(f(X)\\) cannot explain and/or that is of no interest.\n\n\nThis means \\(f(X) = E(Y \\mid X)\\) will be the same for all samples with the same \\(X\\) values. The only thing that changes is the random error \\(\\epsilon\\) and as a result \\(Y\\). Example \\(Y = 3 + 1X + \\epsilon\\):\n\n\nCode# create regression data frame to show different the deterministic and statistical relationships (assuming SLR normal error model for demonstration so can visualize)\n# generate\n# -&gt; X values (not from a random dist)\n# -&gt; f(X) = E(Y | X) = beta_0 + beta_1 X demonstration\n# -&gt; random error = epsilon ~ N(0,1)\n# -&gt; Y = f(X) + epsilon\nbeta_0 &lt;- 3\nbeta_1 &lt;- 1\ndata_ships &lt;- data.frame(X = seq(from = 10, to = 19, by = 1)) %&gt;% \n  mutate(f_X = beta_0 + beta_1 * X,\n         epsilon = rnorm(n = nrow(.), mean = 0, sd = 1),\n         Y = f_X + epsilon)\n\n# display data\ndata_ships %&gt;% display_nice(col.names = c(\"X\", \"f(X) = E(Y | X)\", \"$epsilon ~ N(0,1)\", \"Y = f(X) + epsilon\"))\n\n\n\n X \n    f(X) = E(Y | X) \n    $epsilon ~ N(0,1) \n    Y = f(X) + epsilon \n  \n\n\n 10 \n    13 \n    -1.050 \n    11.950 \n  \n\n 11 \n    14 \n    1.318 \n    15.318 \n  \n\n 12 \n    15 \n    -1.008 \n    13.992 \n  \n\n 13 \n    16 \n    0.034 \n    16.034 \n  \n\n 14 \n    17 \n    0.375 \n    17.375 \n  \n\n 15 \n    18 \n    -1.184 \n    16.816 \n  \n\n 16 \n    19 \n    -0.898 \n    18.102 \n  \n\n 17 \n    20 \n    1.079 \n    21.079 \n  \n\n 18 \n    21 \n    1.000 \n    22.000 \n  \n\n 19 \n    22 \n    -0.068 \n    21.932 \n  \n\n\n\nCode# plot (X, Y)\n# -&gt; and add points and line for E(Y | X)\n# --&gt; population E(Y | X)) because using known betas (not estimating from sample data)\ndata_ships %$% \n  plot(x = X, y = Y, type = \"p\", main = \"Deterministic vs statistical relationship\")\ndata_ships %$% \n  points(x = X, y = f_X, col = \"red\", pch = 5)\ndata_ships %$% \n  lines(x = X, y = f_X, col = \"red\")\ntext(x = 18.5, y = 20, labels = \"f(X) = E(Y|X)\", col = \"red\")\n\n\n\n\n\n\n\nConstruction of statistical regression models\n\n\nSelection of predictor variables (how to decide which ones?).\n\nUse of outside information, historical knowledge, and/or experience.\nExploratory data analysis.\nVariable selection techniques → Find a subset of important variables (i.e. practical and easy to find).\n\n\n\nFunctional form of the regression relation (what is form of \\(f(X)\\)?).\n\n&lt; based on same info as (1) &gt;\nIf there is an abundance of data, maybe start with more complex models and then simplify.\n\n\n\nScope of model (when is the model useful?).\n\nWhen the model best predicts or describes the relationship between response and predictor variables.\n\n\n\nUses of statistical regression models\n\nDetermining whether an \\(X\\) “affects” \\(Y\\) or not.\nEstimation of impact of a given \\(X\\) on the \\(Y\\).\nEstimation of the mean of \\(Y\\) for a given \\(X\\) value.\nPrediction of a single value of \\(Y\\) for a given \\(X\\) value.\n\nTypical strategy for regression analysis\n\n\n1.1.1 Simple linear regression model (SLR)\nGoal of SLR\n\nInvestigate the relationship between \\(Y\\) and a single numeric independent variable \\(X\\), assuming that, in the population, the mean of \\(Y\\) is linearly related to the value of \\(X\\).\nPopulation relationship → \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).\nSample relationship → \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nData structure\n\nBoth \\(X\\) and \\(Y\\) on a random sample of \\(n\\) individuals are collected from the population of interest.\nThe resulting data has the form \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\n\nModel statement → \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\nHolds for all \\(n\\) values in the random sample of \\(n\\) pairs of values, \\((X_i, Y_i), \\, i = 1, \\ldots, n\\).\n\\(Y_i\\) → Dependent (or response) variable value. These are independent, but not identically distributed.\n\\(X_i\\) → Independent (or predictor) variable value. These are not random variables, rather known constants.\n\\(\\epsilon_i\\) → Random error term, assumed to have mean zero and variance \\(\\sigma^2\\). \\(\\mathrm{Cov}(\\epsilon_i, \\epsilon_j) = \\mathrm{Corr}(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j : i \\ne j\\). Often, the \\(\\epsilon_i\\) are assumed to be independent and identically distributed \\((iid)\\), i.e. all have the same distribution with the same mean and variance.\n\\(\\beta_0\\) and \\(\\beta_1\\) → Fixed but unknown regression parameters that need to be estimated.\n\\(\\sigma^2\\) → Another parameter that needs estimated, but it is technically not a “regression” parameter since it does not determine the relationship between \\(Y\\) and \\(X\\) (i.e. it only deals with randomness).\nNote that \\(Y_i\\) and \\(\\epsilon_i\\) are random variables and therefore have distributions. Thus, discussing their mean and variances are appropriate.\nAlternate (equivalent) version of regression model\n\n\n\nResults\nDerivation\n\n\n\nModel statement\n\nCan use the deviation \\(X_i - \\bar{X}\\) as the predictor instead of \\(X_i\\). This leads to:\n\n\\[\nY_i = \\beta_0^* + \\beta_1 (X_i - \\bar{X}) + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\beta_0^* = \\beta_0 + \\beta_1 \\bar{X}\n\\]\n\nThis model can be useful in some derivations.\n\nLater results (jumping ahead)\n\n\nEstimated coefficients\n\nLSE for \\(\\beta_1\\) is the same as before.\nNow the LSE for \\(\\beta_0^*\\) can be found using \\(\\hat{\\beta}_0\\) :\n\n\n\n\\[\n\\hat{\\beta}_0^* = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X} = (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 \\bar{X} = \\bar{Y}\n\\]\n\nEstimated regression function\n\n\\[\n\\hat{Y} = \\bar{Y} - \\hat{\\beta}_1 (X - \\bar{X})\n\\]\n\n\nProperty of all regression functions\n\nUsing this alternative model, it is easy to see that all regression functions pass through the point \\((\\bar{X}, \\bar{Y})\\).\n\n\n\n\n\n\n\n\n\nSome implications of above\n\n\nResults\nDerivation\n\n\n\n\nMean of \\(Y_i\\) for given \\(X_i\\) → \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\)\nVariance of \\(Y_i\\) for given \\(X_i\\) → \\(V(Y_i) = \\sigma^2\\)\n\n\n\n\n\n\n\nInterpretation of regression parameters \\((\\beta_0, \\beta_1)\\)\n\n\n\\(\\beta_0\\)\n\n\\(Y\\)-intercept of the regression line and gives \\(Y\\)’s mean when \\(X = 0\\) → \\(E(Y \\mid X = 0) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\)\n\nOnly makes sense to interpret when \\(X=0\\) is within the scope of the model. So, ask two questions when deciding whether or not to interpret:\n\nDoes it make sense to interpret in context?\nDo we have data in the proximity of zero?\n\n\nIf answer “no” to either of these, then no need to interpret the intercept.\n\n\n\n\\(\\beta_1\\)\n\nSlope of the regression line and indicates the change in \\(Y\\)’s mean when \\(X\\) increases by one unit → \\(E(Y \\mid X = x^* + 1) - E(Y \\mid X = x^*) = [\\beta_0 + \\beta_1 (x^* + 1)] - [\\beta_0 + \\beta_1 x^*] = \\beta_1\\)\nDetermines whether a relationship exists between \\(Y\\) and \\(X\\).\nNote that regression does not substantiate or prove a cause-effect relationship. Rather it gives evidence that \\(Y\\) and \\(X\\) are related (but not that \\(X\\) “causes” the value of \\(Y\\)).\n\n\n\nModel scope (and the dangers of extrapolation)\n\nThe structural form of the relationship may not be the same as you move away from the observed predictor space. Therefore, using \\(\\hat{Y}\\) for estimation / prediction at \\(X\\) values outside of model scope would be extrapolation and results are not reliable.\nShould collect data from the entire region of predictor values of research interest.\n\n\nEstimators\n\n\nPurpose\n\nEstimators help us make inferences about population parameters.\nEach estimator corresponds to a population parameter (e.g. \\(\\hat{\\beta}_1 \\rightarrow \\beta_1\\)).\n\n\n\nEstimator / estimate distinction\n\nEstimators are based on formulas (e.g. \\(\\bar{X} = \\frac{1}{n} \\sum X_i\\)).\nEstimates are values computed from estimators.\n\n\n\nGood estimators\n\nBias of an estimator is the difference between the estimator’s expected value and the true value of the parameter being estimated:\n\nIf the bias equals zero, the estimator is said to be unbiased. \\[\\text{Bias} = E(\\hat{\\theta}) - \\theta\\]\n\ne.g. \\(\\bar{X}\\) and \\(\\mu\\) → \\(E(\\bar{X}) \\overset{\\surd} = \\mu\\).\nAnother way to think about it → \\(E(\\bar{X})\\) will be the center of the sampling distribution of \\(\\bar{X}\\) (which will be approximately normal because of the central limit theorem), and we want this to equal \\(\\mu\\).\n\n\n\nThe standard deviation of an estimator is referred to as the standard error of said estimator.\n\nIdeally, estimators will have relatively small standard deviations, in which case they are said to be efficient.\n\n\n\n\n\n1.1.2 Estimation of the regression function\nSetup\n\nFor each point we have an observed value \\(Y_i\\), a fitted value \\(\\hat{Y}_i\\) and a residual \\(\\hat{\\epsilon}_i\\).\nFitted regression function → \\(\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\), where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_0\\) are estimators of \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\nGoal\n\nGoal is to estimate the two “regression” parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nThere are several methods to do this.\n\n1.1.3 Method of least squares\nOverview\n\nThe method of least squares is one way to find “good” estimators of the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nFor each observation \\((X_i, Y_i)\\), this method considers the model error term, which is the deviation of \\(Y_i\\) from its expected value:\n\n\\[\n\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\n\\]\n\nThen we minimize the sum of some function of these errors:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\text{function of } \\epsilon_i \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - E(Y_i)\\big) \\\\\n    &= \\sum_{i = 1}^n \\text{function of } \\big(Y_i - (\\beta_0 + \\beta_1 X_i)\\big) \\quad\\quad \\text{&lt; for SLR &gt;}\n\\end{align*}\n\\]\n\nFor least squares method specifically, we consider the sum of the \\(n\\) squared errors (deviations). Thus we have:\n\n\\[\n\\begin{align*}\n  Q &= \\sum_{i = 1}^n \\epsilon_i^2 \\\\\n    &= \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\end{align*}\n\\]\n\n\nAccording to the method of least squares, the point estimators (functions of the random sample that estimate population quantities) of \\(\\beta_0\\) and \\(\\beta_1\\) are those values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), respectively, that minimize the criterion \\(Q\\) for the sample observations \\((X_1, Y_1), \\ldots, (X_n, Y_n)\\).\nOnce data is collected, the estimates (realized values) for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are calculated based on the observed sample \\((x_1, y_1), \\ldots, (x_n, y_n)\\). (Note: no good notation for this, maybe \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\); similar idea for means: \\(\\mu\\), \\(\\bar{X}\\) and \\(\\bar{x}\\))\n\nLeast squares estimators\n\n\nThese can be found in two ways.\n\nNumerical methods (optimization) → Search procedures to systematically evaluate the least squares criterion \\(Q\\) for different estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) until the ones that minimize \\(Q\\) are found.\nAnalytical methods → Using calculus. The analytical approach is feasible when the regression model is not mathematically complex.\n\n\n\nThe general process for deriving the least squares estimators of any model is:\n\nWrite \\(Q\\) as a function of the model error term \\(\\epsilon_i\\) as shown above.\nTake derivative with respect to the desired parameter \\(\\beta_i\\).\nSet derivative equal to zero (notation: substituting population term \\(\\beta_i\\) for its estimator \\(\\hat{\\beta}_i\\)) and solve for \\(\\hat{\\beta}_i\\).\nCheck second derivative test to ensure global minimum (take derivative from result of step 2)\n\n\nUsing the analytical approach with the SLR model, we have the following results:\n\n\n\nResults\nDerivation\n\n\n\n\nThe estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are found by solving the simultaneous equations (called the normal equations):\n\n\\[\n\\begin{align*}\n  \\sum Y_i n &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum X_i \\\\\n  \\sum X_i Y_i &= \\hat{\\beta}_0 \\sum X_i + \\hat{\\beta}_1 \\sum X_i^2\n\\end{align*}\n\\]\n\nThis leads to\n\n\\[\n\\begin{align*}\n  \\text{Intercept} \\hspace{10pt} \\hat{\\beta}_0 \\hspace{10pt} &= \\hspace{10pt} \\frac{1}{n}\\sum Y_i + \\hat{\\beta}_1 \\frac{1}{n} \\sum X_i \\hspace{10pt} = \\hspace{10pt} \\bar{Y}- \\hat{\\beta}_1 \\bar{X} \\\\\n  \\text{Slope} \\hspace{10pt} \\hat{\\beta_1} \\hspace{10pt} &= \\hspace{10pt} \\frac{\\sum X_i Y_i -\\frac{1}{n} \\sum X_i Y_i}{\\sum X_i^2 - \\frac{1}{n}(\\sum X_i)^2} \\hspace{10pt} = \\hspace{10pt} \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \\hspace{10pt} = \\hspace{10pt} \\frac{S_{XY}}{S_{XX}}\n\\end{align*}\n\\]\n\nNote: We did not have to assume any distribution of the error term. These are the LSE estimators for any SLR model.\n\n\n\n\n\n\n\n\n\nDemo:\n\nFirst we can generate the sample data \\((x_1, y_1), \\ldots, (x_n, y_n)\\).\n\n\n\n\nCode# simulate sample regression dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = B0 + B1X + E\n# -&gt; Y | X ~ Uniform(B0 + B1*X - a, B0 + B1*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta_0 &lt;- 1\nbeta_1 &lt;- 2\nmin &lt;- -2\nmax &lt;- 2\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate error terms\n# -&gt; assumption for LSE is E(epsilon) = 0, so symmetric uniform about zero works\nepsilon &lt;- runif(n = n, min = min, max = max)\n\n# calculate y terms as function of x and error terms\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n\n\nCode# display observed sample data\ndata.frame(x, y) %&gt;% \n  head(10) %&gt;% \n  display_nice(col.names = c(\"x\", \"y\"))\n\n\n\n x \n    y \n  \n\n\n 7.917 \n    18.127 \n  \n\n 9.795 \n    21.438 \n  \n\n 11.515 \n    24.736 \n  \n\n 6.321 \n    12.550 \n  \n\n 8.426 \n    18.699 \n  \n\n 13.068 \n    26.100 \n  \n\n 7.389 \n    13.980 \n  \n\n 5.484 \n    13.794 \n  \n\n 9.983 \n    20.721 \n  \n\n 9.129 \n    20.242 \n  \n\n\n\nCode# plot sample data\nplot(x = x, y = y)\n\n\n\n\n\n\n\n\nThen we can fit the model and visualize it on the scatter plot.\n\n\nCode# add regression line to plot\nplot(x = x, y = y)\nabline(lm(formula = y ~ x), col = \"red\")\ntext(x = 14, y = 22, labels = bquote(hat(Y) * \" = \" * hat(beta)[0] + hat(beta)[1] * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nFinally we can get the estimates of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\n\n\nR Functions\nManual\n\n\n\n\n# fit SLR model\n# -&gt; lm() calls lm.fit() behind the scenes --&gt; this performs a QR decomposition to fit the model, which is a more stable way to solve for LSE\nmod_slr &lt;- lm(formula = y ~ x)\n\n# display estimated coefficients\nmod_slr$coefficients\n\n(Intercept)           x \n   2.248209    1.841830 \n\n\n\n# or can do\ncoefficients(mod_slr)\ncoef(mod_slr) # alias for coefficients()\n\nThese are estimates of the population coefficients, so can compare above values to \\(\\beta_0\\) = 1 and \\(\\beta_1\\) = 2.\nProgramming note → Beautiful sites ((1) and (2)) explaining QR decomposition and how it relates to LSE; must go through and recreate at some point.\n\n\n\n# calculate estimates for B0 and B1 using LSE formulas\n# -&gt; b_1 = S_XY / S_XX\n# -&gt; b_0 = Y-bar - b_1 X-bar\ns_xy &lt;- sum((x - mean(x)) * (y - mean(y)))\ns_xx &lt;- sum((x - mean(x))^2)\nb_1 &lt;- s_xy / s_xx\nb_0 &lt;- mean(y) - b_1 * mean(x)\n\n\nCode# compare to results from lm()\ncompare(c(b_0, b_1), mod_slr$coefficients)\n\n$comparison\n(Intercept)           x \n       TRUE        TRUE \n\n$`c(b_0, b_1)`\n[1] 2.248209 1.841830\n\n$`mod_slr$coefficients`\n(Intercept)           x \n   2.248209    1.841830 \n\n\n\n\n\nLSE for other models\n\nCan also derive LSE estimators for other models using the same process.\n\nExample 1\n\nRegression through the origin: \\(Y = \\beta X + \\epsilon\\).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X + a, B*X + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -5, max = 5)\n\n# Y | X ~ Uniform(min = BX + a, max = BX + b)\ny &lt;- runif(n = n, min = beta * x + min, max = beta * x + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 3, y = -5, labels = bquote(hat(Y) * \" = \" * hat(beta) * \"X\") , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model without intercept\nmod_noint &lt;- lm(formula = y ~ -1 + x)\n\n# display estimated coefficient\ncoefficients(mod_noint)\n\n      x \n2.17725 \n\n\n\n\n\n# calculate estimate B using LSE formula\nb &lt;- sum(x * y) / sum(x^2)\n\n\nCode# compare to results from lm()\ncompare(b, coefficients(mod_noint))\n\n$comparison\n   x \nTRUE \n\n$b\n[1] 2.17725\n\n$`coefficients(mod_noint)`\n      x \n2.17725 \n\n\n\n\n\n\n\nExample 2\n\nRegression through the origin with squared predictor variable: \\(Y = \\beta_1 X^2 + \\epsilon\\).\nSometimes a transformation can make deriving estimators easier. This way we can get the model in a familiar form and derive like usual (simpler). Then we just have to make the appropriate substitutions at the end in order to get the estimators for the original models.\nThis strategy lessens the number of models we have to know how to derive (note this is just for deriving; no need to do it when fitting models because it’s an extra middle step that doesn’t change anything, leads to equivalent results).\n\n\n\n\n\nSetup and Derivation\nR Functions\nManual\n\n\n\n\nCode# simulate sample regression through the origin with squared predictor dataset from population model\n# -&gt; E ~ Uniform(a, b)\n# -&gt; Y = BX + E\n# -&gt; Y | X ~ Uniform(B*X^2 + a, B*X^2 + b)\n\n# initialize items\n# -&gt; sample size, population parameters and error distribution parameters\nn &lt;- 30\nbeta &lt;- 2\nmin &lt;- -3\nmax &lt;- 3\n\n# generate X values\nx &lt;- runif(n = n, min = -1, max = 5)\n\n# Y | X ~ Uniform(min = BX^2 + a, max = BX^2 + b)\ny &lt;- runif(n = n, min = beta * x^2 + min, max = beta * x^2 + max)\n\n# plot sample data\n# add regression line to plot and reference line for origin\nplot(x = x, y = y)\nabline(lm(formula = y ~ x - 1), col = \"red\")\nabline(h = 0, lty = 2, col = \"lightgrey\")\nabline(v = 0, lty = 2, col = \"lightgrey\")\ntext(x = 4, y = 15, labels = bquote(hat(Y) * \" = \" * hat(beta) * X^2) , col = \"red\")\n\n\n\n\n\n\n\n\nDerivation of estimator:\n\n\n\n\n\n# fit SLR model with no intercept and squared X\n# -&gt; use asis function I() to treat `^` as an arithmetic operator and not a formula operator\n# -&gt; save design matrix for later demo\nmod_noint_squared &lt;- lm(formula = y ~ -1 + I(x^2), x = TRUE)\n\n# display estimated coefficient\ncoef(mod_noint_squared)\n\n I(x^2) \n1.99715 \n\n\n\n\n\n# calculate estimates B using LSE formula\nb &lt;- sum(x^2 * y) / sum(x^4)\n\n\nCode# compare to results from lm()\ncompare(b, coef(mod_noint_squared))\n\n$comparison\nI(x^2) \n  TRUE \n\n$b\n[1] 1.99715\n\n$`coef(mod_noint_squared)`\n I(x^2) \n1.99715 \n\n\n\n# demonstrating that transformed model way is equivalent\n# -&gt; square predictor variable\nx_star &lt;- x^2\nmod_transformed &lt;- lm(formula = y ~ -1 +  x_star, x = TRUE)\n\n# compare design matrices of two models\ncompare(head(mod_noint_squared$x), head(mod_transformed$x))\n\n$comparison\n  I(x^2)\n1   TRUE\n2   TRUE\n3   TRUE\n4   TRUE\n5   TRUE\n6   TRUE\n\n$`head(mod_noint_squared$x)`\n      I(x^2)\n1 17.5787765\n2 16.4235865\n3  1.9257954\n4  0.6732229\n5  1.3335716\n6 18.0385428\n\n$`head(mod_transformed$x)`\n      x_star\n1 17.5787765\n2 16.4235865\n3  1.9257954\n4  0.6732229\n5  1.3335716\n6 18.0385428\n\n# calculate estimates B using LSE formula\nb_star &lt;- sum(x_star * y) / sum(x_star^2)\n\n# compare betas from usual method and transformed model way\ncompare(b, b_star)\n\n$comparison\n[1] TRUE\n\n$b\n[1] 1.99715\n\n$b_star\n[1] 1.99715\n\n\n\n\n\nProperties of least squares estimators\n\n\nGauss-Markov theorem: If the conditions of the SLR model hold (i.e. \\(Y_i= \\beta_0 + \\beta_1 X_i + \\epsilon_i\\), where \\(\\epsilon_i\\) have mean zero, variance \\(\\sigma^2\\) and are uncorrelated), then the LSE \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the Best Linear Unbiased Estimators (BLUE) (note that \\(\\epsilon_i\\) do not have to be normal).\n\nLinear estimators → Through lots of algebra (shown later), \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta}_1\\) can be written as be written as a linear combination of the \\(Y_i\\): \\(\\sum k_i Y_i\\), where \\(k_i\\) are constant. Thus they are linear estimators.\nUnbiased estimators → Their expected values are \\(\\beta_0\\) and \\(\\beta_1\\), respectively. So neither estimator tends to overestimate or underestimate systematically.\nBest → More precise than all other linear unbiased estimators of \\(\\beta_0\\) and \\(\\beta_1\\) (smaller variance of the sampling distributions).\n\n\n\n1.1.4 Point estimation of the mean response\n\nFor the population relationship \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\), we have the regression function \\(E(Y) = \\beta_0 + \\beta_1 X\\) (because \\(E(\\epsilon) = 0\\)). This is estimated with \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\).\n\nTerms and what they represent:\n\n\\(Y\\) is a value of the response variable; the observed value.\n\\(E(Y)\\) the mean response. So it is the center of the probability distribution of \\(Y\\) corresponding to the level \\(X\\) of the predictor variable (so technically a conditional expected value \\(E(Y \\mid X)\\).\n\\(\\hat{Y}\\) is a point estimator of the mean response when the level of the predictor variable is \\(X\\); the fitted value.\n\n\n\nApplication of Gauss-Markov theorem\n\n\n\\(\\hat{Y}\\) is the BLUE for \\(E(Y)\\) with \\(V(\\hat{Y}) = V(\\epsilon) = \\sigma^2\\).\n\n\n\n1.1.5 Residuals and model errors\n\nModel error term \\(\\epsilon_i = Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\\) → Measures the difference between an observation and its expected value (unknown true regression line). It is unknown / unobservable.\nResidual \\(\\hat{\\epsilon}_i = e_i = Y_i - \\hat{Y}_i\\) → This is a known, observable estimate of the unobservable model error. Measures the deviation of the observed value from the fitted regression function.\n\n\n\nResiduals are very useful for studying whether the given regression model is appropriate for the data.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#estimation-of-error-terms-variance",
    "href": "notes-slr.html#estimation-of-error-terms-variance",
    "title": "1  Simple linear regression",
    "section": "\n1.2 Estimation of error terms variance",
    "text": "1.2 Estimation of error terms variance\nNeed to estimate the variance \\(\\sigma^2\\) of the error terms \\(\\epsilon_i\\) in a regression model to get an indication of the variability of the probability distributions of \\(Y\\). Also several inferences for models require an estimate of \\(\\sigma^2\\).\nMotivation\n\nFor a single population, the variance \\(\\sigma^2\\) is estimated by the sample variance \\(S^2\\), which is an unbiased estimator. We find \\(S^2\\) by taking the sum of the squared deviations of the observed value and the estimated mean (sum of squares) and then dividing it by the degrees of freedom \\(n - 1\\) (one df is lost when estimating \\(\\mu\\); mean square) .\n\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (Y_i - \\bar{Y})^2}{n - 1}\n\\]\nRegression model\n\nSame logic as above, except we use the residuals as the deviations because each \\(Y_i\\) comes from a different probability distribution with different \\(X\\) (depends on the \\(X_i\\) level).\n\n\\[\n\\text{Error (residual) sum of squares} \\hspace{10pt} SSE = \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 = \\sum_{i = 1}^n e_i^2\n\\]\n\nThen divide by the \\(df = n - 2\\) to the mean square (two dfs are lost when because \\(\\beta_0\\) and \\(\\beta_1\\) need to be estimated when getting the estimated means \\(\\hat{Y_i}\\)).\n\n\\[\n\\text{Error (residual) mean square} \\hspace{10pt} S^2 = MSE = \\frac{SSE}{n - 2} = \\frac{\\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2}{n - 2} = \\frac{\\sum_{i = 1}^n e_i^2}{n - 2}\n\\]\n\nIt will be shown later that \\(MSE\\) is an unbiased estimator for \\(\\sigma^2\\) → \\(E(MSE) = \\sigma^2\\).\nAn estimator of the standard deviation is simply \\(S = \\sqrt{MSE}\\).\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# simulate dataset (using uniform errors)\n# initialize items, generate X values, errors and y values\nn &lt;- 30; beta_0 &lt;- 1; beta_1 &lt;- 2; min &lt;- -2; max &lt;- 2\nx &lt;- runif(n = n, min = 5, max = 15)\nepsilon &lt;- runif(n = n, min = min, max = max)\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# display model summary -&gt; looking for residual standard error = S = sqrt(MSE)\n# -&gt; save to object to extract it\n(summ &lt;- summary(mod))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6748 -1.1341  0.2154  1.0948  2.1662 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  2.82054    0.85005   3.318              0.00252 ** \nx            1.82294    0.08081  22.558 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.287 on 28 degrees of freedom\nMultiple R-squared:  0.9478,    Adjusted R-squared:  0.946 \nF-statistic: 508.9 on 1 and 28 DF,  p-value: &lt; 0.00000000000000022\n\n# display S and MSE = S^2\nsumm$sigma\n\n[1] 1.287457\n\nsumm$sigma^2\n\n[1] 1.657546\n\n\nThese are estimates of population quantities for the error terms, and when simulating the data we assumed \\(\\epsilon_i \\sim \\text{Uniform}\\,(a, b) \\Longrightarrow \\sigma^2 = V(\\epsilon_i) = \\frac{(b - a)^2}{12}\\). For \\(a\\) = -2 and \\(b\\) = 2, \\(V(\\epsilon_i)\\) = 1.333.\n\n\n\n# SIDENOTE -&gt; showing multiple equivalent ways to do the same thing (just keep the last way)\n\n# get fitted values using functions\ny_hat &lt;- mod$fitted.values\ny_hat &lt;- fitted(mod)\ny_hat &lt;- fitted.values(mod) # alias for fitted()\ny_hat &lt;- predict(mod) # discussed more later\n\n# extract residuals using functions (or some manual calculations)\ne &lt;- residuals(mod)\ne &lt;- resid(mod) # alias for residuals()\ne &lt;- mod$residuals\ne &lt;- y - y_hat\n\n# calculate SSE and df\nsse &lt;- sum(e^2)\ndf &lt;- mod$df.residual # extract from model object\ndf &lt;- df.residual(mod)\ndf &lt;- summ$df[2] # extract from model summary -&gt; error df is the second item (even if have MLR model)\ndf &lt;- n - 2 # for SLR (with intercept)\n\n# calculate MSE\nmse &lt;- sse / df\n\n# calculate residual standard deviation\ns &lt;- sqrt(mse)\n\n# compare to results from summary(lm())\n# -&gt; s\ncompare(summ$sigma, s)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma`\n[1] 1.287457\n\n$s\n[1] 1.287457\n\n# -&gt; MSE\ncompare(summ$sigma^2, mse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2`\n[1] 1.657546\n\n$mse\n[1] 1.657546\n\n# -&gt; SSE = MSE * df\ncompare(summ$sigma^2 * summ$df[2], sse)\n\n$comparison\n[1] TRUE\n\n$`summ$sigma^2 * summ$df[2]`\n[1] 46.41128\n\n$sse\n[1] 46.41128",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-slr.html#normal-error-regression-model",
    "href": "notes-slr.html#normal-error-regression-model",
    "title": "1  Simple linear regression",
    "section": "\n1.3 Normal error regression model",
    "text": "1.3 Normal error regression model\nLeast squares results\n\nNo matter what may be the form of the distribution of the error terms \\(\\epsilon_i\\) (and thus of the \\(Y_i\\)), the LSE provides unbiased point estimators of \\(\\beta_0\\) and \\(\\beta_1\\), that have minimum variance among all unbiased linear estimators (BLUEs).\n\nAssumptions on error term distribution\n\nThese assumptions on \\(\\epsilon_i\\) are needed to set up interval estimates and make tests.\nThe standard assumption is that the error terms are normally distributed. This greatly simplifies the theory of regression analysis and is justifiable in many real-world situations where regression analysis is applied.\n\nNew regression model\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{where} \\hspace{10pt} \\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\n\\]\n\nThis is the same regression model as before, just with specified error distribution now (\\(iid\\) = independent and identically distributed).\nBecause this model assumes normal errors, the assumption of uncorrelatedness of the original (unspecified error) model now becomes an independence assumption (\\(\\perp \\!\\!\\! \\perp\\Longrightarrow \\mathrm{Corr}(\\epsilon_i,\\epsilon_j) = 0\\). So the outcome in anyone trial has no effect on the error term for any other trial (in terms of positive or negative, small or large).\nThis model means \\(Y_i \\overset{\\perp \\!\\!\\! \\perp}\\sim \\text{Normal}\\,\\) with \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\) and \\(V(Y_i) = \\sigma^2\\).\n\n\nJustification of the normality assumption.\n\nError terms frequently represent the effects of factors omitted from the model that affect the response to some extent and that vary at random without reference to the variable \\(X\\).\nThese random effects have a degree of mutual independence, the composite error term representing all these factors tends to normal as the number of factors becomes large (by the CLT).\nAlso, the estimation and testing procedures shown later are based on the \\(t\\) distribution and are usually only sensitive to large departures from normality. So, unless the departures from normality are serious, particularly with respect to skewness, the actual confidence coefficients and risks of errors will be close to the levels for exact normality.\n\n\n1.3.1 Estimation of paramters by method of maximum likelihood\nOverview\n\n\nThe method of maximum likelihood chooses as estimates values of the parameters that are most consistent with the sample data. The measure of consistency is the product of densities and is called the likelihood value \\(L(\\mu)\\).\n\nIf the value of \\(\\mu\\) is consistent with the sample data \\(\\Longrightarrow\\) densities relatively large \\(\\Longrightarrow\\) large likelihood value. If not, both will be small.\n\n\n\n\n\nJust like the LSE, there are two ways to find MLEs: a systematic numerical search and by use of an analytical solution.\n\nThe product of the densities viewed as a function of the unknown parameters is called the likelihood function.\n\nIf the likelihood function is relatively peaked in the neighborhood of the maximum, then the MLE estimate is precise because values of \\(\\mu\\) not near the MLE are much less consistent with the data.\nWhen it is relatively flat in a fairly wide region around the MLE, many values of the parameter are almost as consistent with the sample data as the MLE and therefore the MLE is relatively imprecise.\n\n\n\nSteps to MLEs\n\n\nFor multivariate parameter vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_{k})\\), write the likelihood function (i.e. joint density function) and the log-likelihood function\n\n\n\n\\[\nL(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\prod_{i = 1}^n f(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\hspace{20pt} \\rightarrow \\hspace{20pt} \\ell(\\boldsymbol{\\theta}) = \\ln[L(\\boldsymbol{\\theta} \\mid \\mathbf{y})]\n\\]\n\nOptimize the log-likelihood function by taking the partial derivatives with respect to each parameters of interest \\(\\theta_1, \\ldots, \\theta_{k}\\).\n\n\nSet to zero and solve for each parameter of interest.\n\n\\[\n\\text{For } j = 1, \\ldots, k \\hspace{20pt} \\ell'(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\theta_j} \\ell(\\boldsymbol{\\theta}) = 0 \\hspace{20pt} \\rightarrow \\hspace{20pt} \\hat{\\boldsymbol{\\theta}} = (\\hat{\\theta}_1, \\ldots, \\hat{\\theta}_{k}) = \\text{potential MLE}\n\\]\n\nVerify that the global maximum of the log-likelihood function occurs at \\(\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}\\).\nFind the second derivative of the log-likelihood function, then plug in \\(\\hat{\\boldsymbol{\\theta}}\\) and see if less than zero.\n\nMLEs for normal error regression model\n\nGenerally\n\n\n\nResults\nDerivation\n\n\n\n\n\n\n\n\n\nParameter\nMLE\n\n\n\n\\(\\beta_0\\)\n\n\\(\\hat{\\beta}_0 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\beta_1\\)\n\n\\(\\hat{\\beta}_1 \\hspace{10pt}\\) Same as LSE\n\n\n\\(\\sigma^2\\)\n\\(\\displaystyle \\hat{\\sigma}^2 = \\frac{\\sum (Y_i - \\hat{Y_i})^2}{n}\\)\n\n\n\nNotes\n\n\nProperties of estimators\n\nSince MLEs for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the same as the LSE estimators, they have the same properties: BLUE (unbiased and minimum variance in class of unbiased linear estimators).\nAnd because they are MLEs, we can also say they are: consistent (\\(\\hat{\\beta}_i \\overset{p}\\rightarrow \\beta_i\\), converge in probability to their respective parameters); sufficient (captures all of the information about \\(\\beta_i\\) contained in the sample); minimum variance in class of unbiased estimators (linear or otherwise).\n\n\n\n\n\nMLE \\(\\hat{\\sigma}^2\\) and MSE\n\nRelationship \\(S^2 = MSE = \\frac{n}{n - 2} \\hat{\\sigma}^2 \\hspace{20pt} \\Longleftrightarrow \\hspace{20pt} \\hat{\\sigma}^2 = \\frac{n - 2}{n} MSE\\). However, for large \\(n\\), the difference is small.\nMLE \\(\\hat{\\sigma}^2\\) is biased → \\(E(\\hat{\\sigma}^2) = \\frac{n - 2}{n} \\sigma^2\\), which is an underestimation of \\(\\sigma^2\\). Because of this, MSE is generally used because it is unbiased. However, \\(\\hat{\\sigma}^2\\) is asymptotically unbiased (as \\(n \\rightarrow \\infty\\)) and has a smaller variance than MSE, which may be preferred in some scenarios (tradeoff between bias and precision).\n\n\nGenerally MLE gives the same results as LSE.\n\n\n\n\n\n\n\nCan also derive MLEs for transformed models using the same strategy as with LSE.\n\nExample: Suppose we have \\(\\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\). We can transform \\(Y_i^* = \\beta_0^* + \\beta_1^* X_i + \\epsilon_i\\), derive like usual, then substitute at end to get untransformed estimators.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#inferences-concerning-beta_1",
    "href": "notes-inference.html#inferences-concerning-beta_1",
    "title": "2  Inference",
    "section": "\n2.1 Inferences concerning \\(\\beta_1\\)\n",
    "text": "2.1 Inferences concerning \\(\\beta_1\\)\n\nOverview\n\n\nTest on slope \\(\\beta_1\\) and the implications\n\nWe often want to make inferences about \\(\\beta_1\\). A common test on \\(\\beta_1\\) has the form below.\nIf \\(\\beta_1 = 0\\) \\(\\Longrightarrow\\) Regression line in horizontal, which means there is no linear association between \\(Y\\) and \\(X\\), and even more no relation of any type because all probability distributions of \\(Y\\) are identical at all levels of \\(X\\): normal with \\(E(Y) = \\beta_0 + (0) X = \\beta_0\\) and variance \\(\\sigma^2\\).\n\n\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\n\n2.1.1 Sampling distribution of \\(\\hat{\\beta}_1\\)\n\n\n\nResults\nDerivation\n\n\n\nSampling distribution of \\(\\hat{\\beta}_1\\)\n\nRefers to distribution of \\(\\hat{\\beta}_1\\) from repeated sampling when the levels of the predictor variable \\(X\\) are held constant from sample to sample.\nRecall \\(\\displaystyle \\hat{\\beta}_1 = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\\); this is the point estimator.\nDistribution of \\(\\hat{\\beta}_1\\) is Normal with mean and variance:\n\n\\[\n\\begin{align*}\n   E(\\hat{\\beta}_1) &= \\beta_1 \\\\\n  V (\\hat{\\beta}_1) &= \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2}\n\\end{align*}\n\\]\n\nThen we can estimate the variance by replacing the parameter \\(\\sigma^2\\) with \\(MSE\\), the unbiased estimator of \\(\\sigma^2\\). This gives us \\(S^2_{\\hat{\\beta}_1}\\), which is an unbiased estimator for the variance of the sampling distribution of \\(\\hat{\\beta}_1\\). And we can take the positive square root to give us \\(S_{\\hat{\\beta}_1}\\), which is the point estimator of \\(\\sigma_{\\hat{\\beta}_1}\\).\n\n\\[\n  S^2_{\\hat{\\beta}_1} = \\frac{MSE}{\\sum(X_i - \\bar{X})^2} = \\frac{MSE}{S_{XX}} \\hspace{20pt} \\longrightarrow \\hspace{20pt} s_{\\hat{\\beta}_1} = \\sqrt{\\frac{MSE}{S_{XX}}} = \\frac{S}{\\sqrt{S_{XX}}}\n  \\]\n\n\nThus, \\(S^2_{\\hat{\\beta}_1}\\) is an unbiased estimator for the variance of the sampling distribution of \\(\\hat{\\beta}_1\\) and\n\nThis is also called the standard error (another way to think about it is the standard deviation of the sampling distribution) → \\(S_{\\hat{\\beta}_1} = SE(\\hat{\\beta}_1)\\)\n\n\n\n\n\n\nMain result for normal, mean and variance\n\n\n\\(\\hat{\\beta}_1\\) can be written as a linear combination of the observations \\(Y_i\\):\n\n\\[\n\\hat{\\beta_1}=\\sum k_i Y_i, \\hspace{20pt} \\text{where} \\hspace{10pt} k_i = \\frac{X_i - \\bar{X}}{\\sum(X_i - \\bar{X})^2}\n\\]\n\n\\(\\sum \\text{Normal} \\sim \\text{Normal}\\,\\), so we know \\(\\hat{\\beta}_1\\) must be Normal. Then properties of the coefficients \\(k_i\\) can be used to show the mean and variance.\nNote that \\(k_i\\)’s are fixed quantities because they are a function of only \\(X_i\\)’s (which are fixed quantities). Therefore, \\(\\hat{\\beta}_1\\) is a linear combination of \\(Y_i\\), where the coefficients are solely a function of the fixed \\(X_i\\) (this is why \\(\\hat{\\beta}_1\\) is a linear estimator).\n\nProperties of coefficients \\(k_i\\):\n\nProof of linear combination:\n\nProofs of properties of \\(k_i\\):\n\nProof of mean:\n\nProof of variance:\n\nCan also prove that \\(\\hat{\\beta}_1\\) has minimum variance among all unbiased linear estimators.\n\n\n\n\n\n2.1.2 Sampling distribution of \\((\\hat{\\beta}_1 - \\beta_1) / S_{\\hat{\\beta}_1}\\)\n\n\n\nResults\nDerivation\n\n\n\n\\[\n\\frac{\\hat{\\beta}_1 - \\beta_1}{S_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sqrt{MSE / S_{XX}}} \\sim \\text{t}\\,_{n - 2}\n\\]\n\n\nDeriving the distribution of the standardized slope → \\(\\displaystyle \\frac{\\hat{\\beta}_1 - E(\\hat{\\beta}_1)}{SE(\\hat{\\beta}_1)}\\)\n\nTechnically when standardizing with an estimated standard deviation it is referred to as studentized statistic.\n\n\n\n\n\n\n\n2.1.3 Confidence interval for \\(\\beta_1\\)\n\nForming interval\n\\[\n\\begin{align*}\n  & P(-t_{\\alpha/2, n-2} \\le \\frac{\\hat{\\beta}_1 - \\beta_1}{S_{\\hat{\\beta}_1}} \\le t_{\\alpha/2, n-2}) = 1 - \\alpha \\\\\n  & \\Longleftrightarrow \\hspace{20pt} P(\\hat{\\beta}_1 - t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1} \\le \\frac{ - \\beta_1}{S_{\\hat{\\beta}_1}} \\le \\hat{\\beta}_1 + t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1}) = 1 - \\alpha \\\\\n  & \\Longleftrightarrow \\hspace{20pt}  100(1 - \\alpha)\\% \\text{ CI } = \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{\\beta}_1} \\hspace{10pt} = \\hspace{10pt} \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{MSE / S_{XX}}\n\\end{align*}\n\\]\nInterpretation\n\nWith &lt; \\(100(1 - \\alpha)\\) &gt;% confidence, we estimate that the average &lt; \\(Y\\) context &gt; increases by between &lt; lower bound &gt; and &lt; upper bound &gt; for each additional unit increase in &lt; \\(X\\) context &gt;.\nRemember the scope of the regression model is restricted to some range of values of the predictor variable → May not be reasonable to use these slope estimates outside this range as the regression relation may not be linear then.\n\nDemo:\n\n\nR functions\nManual\n\n\n\n\n# simulate sample dataset for normal error regression model\n# -&gt; E ~ Normal(0, sigma^2)\n# -&gt; Y = B0 + B1*X + E\n# -&gt; Y | X ~ Normal(B0 + B1*X, sigma^2)\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 50\nbeta_0 &lt;- 2\nbeta_1 &lt;- 5\nsigma2 &lt;- 10\n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 10)\n\n# generate error terms\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sqrt(sigma2))\n      \n# calculate observations Y | X ~ Normal(B0 + B1*X, sigma^2)\ny &lt;- beta_0 + beta_1 * x + epsilon\n\n# plot sample data with regression line\nplot(x = x, y = y)\nabline(lm(formula = y ~ x ), col = \"red\")\n\n\n\n\n\n\n# display error terms\nx_plot &lt;- seq(from = -3 * sqrt(sigma2), to = 3 * sqrt(sigma2), by = 0.01)\nhist(epsilon, freq = FALSE, xlab = bquote(epsilon * \"~ Normal (\" * mu * \",\" * sigma^2 * \")\"), main = \"\")\nlines(x = x_plot,\n      y = dnorm(x = x_plot, mean = 0, sd = sqrt(sigma2)))\n\n\n\n\n\n\n\n\n# fit model\nmod_normal &lt;- lm(y ~ x)\n\n# display only coefficient summaries of slope\nsummary(mod_normal)$coefficients[2,] # Estimate is the middle of the interval\n\n                  Estimate                 Std. Error \n 5.22587398756125764975877  0.40211009692893356914922 \n                   t value                   Pr(&gt;|t|) \n12.99612724841586341995026  0.00000000000000002457706 \n\n# calculate confidence interval for slope\nalpha &lt;- 0.05\nconfint(mod_normal, parm = \"x\", level = 1 - alpha)\n\n     2.5 %   97.5 %\nx 4.417377 6.034371\n\n\n\n\n\n# using R functions to get the needed values (can reference how to calculate these manually above)\n\n# extract / calculate needed items\n# -&gt; point estimate = beta1-hat\n# -&gt; critical value = t_alpha/2, n-2\n# -&gt; SE(beta1-hat) = MSE / S_XX = S / sqrt(S_XX)\npe &lt;- as.numeric(coef(mod_normal)[\"x\"]) # SIDENOTE -&gt; as.numeric() just to remove the named number data type\nt_crit &lt;- abs(qt(p = alpha / 2, df = mod_normal$df.residual))\ns &lt;- summary(mod_normal)$sigma\ns_xx &lt;- sum((x - mean(x))^2)\nse_beta1_hat &lt;- s / sqrt(s_xx)\n\n# compare to Std. Error from summary of model coefficients\ncompare(summary(mod_normal)$coefficients[\"x\",\"Std. Error\"], se_beta1_hat)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"Std. Error\"]`\n[1] 0.4021101\n\n$se_beta1_hat\n[1] 0.4021101\n\n# calculate confidence interval for beta1\nci_limits &lt;- c(lower = pe - t_crit * se_beta1_hat, upper = pe + t_crit * se_beta1_hat)\n\n# compare to results from confint(lm())\ncompare(confint(mod_normal, parm = \"x\", level = 1 - alpha), ci_limits)\n\n$comparison\n  2.5 % 97.5 %\nx  TRUE   TRUE\n\n$`confint(mod_normal, parm = \"x\", level = 1 - alpha)`\n     2.5 %   97.5 %\nx 4.417377 6.034371\n\n$ci_limits\n   lower    upper \n4.417377 6.034371 \n\n\n\n\n\n\n2.1.4 Tests concerning \\(\\beta_1\\)\n\nOverview\n\nThe test shown below is called a test of utility of the model.\nIf reject → We conclude that \\(X\\) does contribute information for the prediction of \\(Y\\) when using the straight-line model.\n\nIf fail to reject → Then we conclude there is no linear relationship between \\(Y\\) and \\(X\\) (horizontal model). But keep in mind:\n\nAdditional data might indicate that \\(\\beta_1\\) differs from zero.\nA more complex relationship between \\(Y\\) and \\(X\\) may exist, which would require fitting a model other than the straight-line model.\n\n\nAll assumptions about the error terms (\\(\\epsilon_i \\overset{iid}\\sim \\text{Normal}\\,(0,\\sigma^2)\\)) should be satisfied.\n\nTwo-tailed test (most common)\n\nHypotheses\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\nTest statistic\n\n\\[\nTS = t^* = \\frac{\\hat{\\beta}_1 - 0}{S_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1}{\\sqrt{MSE / S_{XX}}}\n\\]\n\nRejection region and p-value\n\n\\[\n\\begin{align*}\n  RR &= \\{\\lvert t^* \\rvert &gt; t_{\\alpha/2, n - 2}\\} \\\\\n  p\\text{-value} &= 2 \\cdot P(t_{n-2} \\ge \\lvert t^* \\rvert)\n\\end{align*}\n\\]\n\n\nDecision\n\nReject \\(H_0\\) and conclude \\(H_A\\) if \\(\\hspace{10pt}\\) \\(TS \\in RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} \\le \\alpha\\)\nFail to reject \\(H_0\\) if \\(\\hspace{10pt}\\) \\(TS \\notin RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} &gt; \\alpha\\)\nCan also look at the \\(100(1 - \\alpha)\\%\\) CI for \\(\\beta_1\\) to see if contains 0.\n\n\n\nConclusion / Interpretation\n\nAt the \\(\\alpha\\) significance level, we &lt; have / do not have &gt; sufficient evidence of a significant linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;. &lt; if yes… &gt; This is a &lt; positive / negative &gt; linear relationship, indicating that as &lt; \\(X\\) context &gt; increases, &lt; \\(Y\\) context &gt; &lt; increases / decreases &gt;, on average.\n\n\n\nOther tests\n\n\nOne-tailed tests\n\n\\(H_A: \\beta_1 &lt; 0 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} RR = \\{t^*&lt; t_{\\alpha, n - 2}\\} \\hspace{10pt} \\text{and} \\hspace{10pt} p\\text{-value} = P(t_{n-2} \\le t^*)\\)\n\\(H_A: \\beta_1 &gt; 0 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} RR = \\{t^* &gt; t_{\\alpha, n - 2}\\} \\hspace{10pt} \\text{and} \\hspace{10pt} p\\text{-value} = P(t_{n-2} \\ge t^*)\\)\n\n\n\nTests against specified nonzero value of \\(\\beta_{1,0}\\)\n\n\\(TS = t^* = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{S_{\\hat{\\beta}_1}}\\)\nRR, \\(p\\)-value and decisions are the same, just based on new \\(t^*\\).\n\n\n\nDemo:\n\n\nR functions\nManual\nSimulation\n\n\n\n\n# using same model from the confidence interval for beta1 demo\n\n# display model summary, focusing on coefficient summaries for slope\n# -&gt; looking for t value (if doing traditional method test with RR) and Pr(&gt;|t|) = p-value\nsummary(mod_normal) # compare p-value to alpha\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5868 -2.4928 -0.4756  2.9264 11.0773 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   0.7398     3.1431   0.235               0.815    \nx             5.2259     0.4021  12.996 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.125 on 48 degrees of freedom\nMultiple R-squared:  0.7787,    Adjusted R-squared:  0.7741 \nF-statistic: 168.9 on 1 and 48 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n# calculate test statistic t* = beta1-hat / SE(beta1-hat)\nbeta1_hat &lt;- as.numeric(coef(mod_normal)[2])\nse_beta1_hat &lt;- summary(mod_normal)$coefficients[\"x\",\"Std. Error\"]\nt_star &lt;- beta1_hat / se_beta1_hat\n\n# compare to result from summary(lm())\ncompare(summary(mod_normal)$coefficients[\"x\",\"t value\"], t_star)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"t value\"]`\n[1] 12.99613\n\n$t_star\n[1] 12.99613\n\n# calculate p-value = 2 * P(t_(n-2) &gt;= |t*|)\np_value &lt;- 2 * pt(q = t_star, df = df.residual(mod_normal), lower.tail = FALSE)\n\n# compare to results from summary(lm())\ncompare(summary(mod_normal)$coefficients[\"x\",\"Pr(&gt;|t|)\"], p_value)\n\n$comparison\n[1] TRUE\n\n$`summary(mod_normal)$coefficients[\"x\", \"Pr(&gt;|t|)\"]`\n[1] 0.00000000000000002457706\n\n$p_value\n[1] 0.00000000000000002457706\n\n\n\n\nSimulation to determine how the magnitude of \\(\\beta_1\\) and \\(\\sigma\\) affect the resulting significance of the \\(X\\) variable in SLR when simulating data. For example, suppose \\(\\beta_1 = 1a\\) and \\(\\sigma = 3a\\), where \\(a = 1, 2, 3\\). Is there the same resulting significance for all values of \\(a\\)?\n\n# initialize simulation settings\n# -&gt; first the parameters of interest (that are variable)\n# -&gt; create all combos\n# -&gt; add in the constant settings\n# -&gt; arrange in a good order\n# -&gt; add simulation id column\n# -&gt; set row names equal to id (helps keep track of results later)\nparams &lt;- expand.grid(beta_1 = c(1,5,10),\n                      ratio = c(1:3,5)) %&gt;% \n  mutate(sigma = beta_1 * ratio) %&gt;% \n  mutate(n = 30,\n         beta_0 = 2) %&gt;%\n  arrange(beta_1, ratio) %&gt;% \n  bind_cols(data.frame(sim_id = 1:nrow(.)))\nrownames(params) &lt;- paste0(\"sim\", params$sim_id)\n\n# define function to run on each unique simulation setting\nsimulation &lt;- function(vec, vec_names, m = 1000) {\n  \n  # set names of parameter vector for clearer reference\n  names(vec) = vec_names\n  \n  # initialize results vector\n  results = rep(NA, m)\n  \n  # loop to simulate m models and extract summaries\n  for (i in 1:m) {\n    \n    # generate data\n    x = runif(n = vec[\"n\"], min = 5, max = 10)\n    y = rnorm(n = vec[\"n\"], mean = vec[\"beta_0\"] + vec[\"beta_1\"] * x, sd = vec[\"sigma\"])\n    \n    # fit model\n    mod = lm(y ~ x)\n    \n    # get summaries\n    # -&gt; just want t-stat\n    results[i] = summary(mod)$coefficients[\"x\", \"t value\"]\n    \n  }\n  \n  return(results)\n  \n}\n\n# run simulation for each parameter settings\n# -&gt; have to transpose so that now each column is a parameter settings and convert to dataframe so can pass to map(), which needs a list\nresults_raw &lt;- params %&gt;% \n  t %&gt;% \n  data.frame %&gt;% \n  map(\\(vec) simulation(vec, vec_names = names(params), m = 1000), .progress = T)\n\n# reformat results\n# -&gt; combine results into a dataframe\n# -&gt; convert to long so have one column for which simulation settings and one for the results\n# -&gt; attach the simulation settings to the results\n# --&gt; for the plot, want all parameters as factors -&gt; so before joining convert all\nresults &lt;- results_raw %&gt;% \n  bind_rows %&gt;% \n  pivot_longer(cols = 1:nrow(params),\n               names_to = \"sim_id\",\n               values_to = \"t\") %&gt;% \n  mutate(sim_id = as.factor(str_sub(sim_id, start = 4))) %&gt;% \n  left_join(mutate(params, across(everything(), as_factor)), by = \"sim_id\") # levels are slightly out of order....\n\n# create a plot facetted by beta value, with density curves for the sampling distributions of the t-stats for each value of the sigma / beta1 ratio\n# -&gt; add reference line for significance cutoff\n\n# set more informative labels for the facets\nlabels_beta &lt;- c(\"1\" = \"beta1 = 1\",\n                 \"5\" = \"beta1 = 5\",\n                 \"10\" = \"beta1 = 10\")\nggplot() + \n  geom_density(aes(x = t,\n                   group = ratio,\n                   color = ratio,\n                   fill = ratio),\n               data = results,\n               alpha = 0.2) + \n  geom_vline(aes(xintercept = t),\n             data = data.frame(t = abs(qt(0.025, df = params[1,\"n\"]-1))),\n             color = \"darkgrey\") + \n  facet_grid(beta_1 ~ .,\n             labeller = as_labeller(labels_beta)) + \n  guides(\n    fill = guide_legend(bquote(sigma *  \"/\" * beta[1] * \" ratio\")),\n    color = \"none\")\n\n\n\n\n\n\n\nBased on the sampling distributions, it appears that the relative magnitude is what is important. And if it is the same even for different values, then will get similar results.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#some-considerations-when-making-inferences",
    "href": "notes-inference.html#some-considerations-when-making-inferences",
    "title": "2  Inference",
    "section": "\n2.2 Some considerations when making inferences",
    "text": "2.2 Some considerations when making inferences\nEffects of departures from normality:\n\nIf the probability distributions of \\(Y\\) are not exactly normal but do not depart seriously \\(\\Longrightarrow\\) Sampling distribution of \\(\\hat{\\beta_1}\\) \\(\\approx\\) normal \\(\\Longrightarrow\\) Using the \\(t\\) distribution will provide \\(\\approx\\) \\((100 - \\alpha)\\%\\) CIs and \\(\\alpha\\)-level tests.\nThe estimator \\(\\hat{\\beta_1}\\) generally has the property of asymptotic normality (i.e. it’s distribution approaches normality under very general conditions as the sample size increases). So with a sufficiently large sample size, inference procedures are still valid even if distributions of \\(Y\\) have large departures from normality. Can switch from \\(t\\)-based procedures to \\(Z\\)-based with large \\(n\\).\n\nInterpretation of confidence coefficient and risks of errors\n\nOur regression model assumes that the \\(X_i\\) are known constants. So the confidence coefficient and risks of errors are interpreted with respect to taking repeated samples where the \\(X\\) observations are kept at the same levels as in the observed sample.\nFor example, confidence interval for \\(\\beta_1\\) with confidence coefficient 95% → If many independent samples are taken where the levels of \\(X\\) are the same as in the dataset, approximately 95% of the constructed confidence intervals would capture the true value of \\(\\beta_1\\).\n\nSpacing of the \\(X\\) levels\n\nFor a given \\(n\\) and \\(\\sigma^2\\), the variance of \\(\\hat{\\beta_1}\\) is affected by the spacing of the \\(X\\) levels in the observed data. As the spread in the \\(X\\) levels increases, \\(S_{xx}\\) increases and therefore \\(V(\\hat{\\beta_1})\\) decreases.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#interval-estimation-of-ey_h",
    "href": "notes-inference.html#interval-estimation-of-ey_h",
    "title": "2  Inference",
    "section": "\n2.3 Interval estimation of \\(E(Y_h)\\)\n",
    "text": "2.3 Interval estimation of \\(E(Y_h)\\)\n\nOverview\n\nA common objective is to estimate the mean of one or more probability distributions of \\(Y\\).\n\nSetup\n\nLet \\(X_h\\) = level of \\(X\\) that we wish to estimate the mean response for (may be a value which occurred in the sample, or some other value within the scope of the model).\nMean response when \\(X = X_h\\) is \\(E(Y_h)\\); this is what we are estimating.\n\n\n\n\n2.3.1 Sampling distribution of \\(\\hat{Y_h}\\)\n\n\n\nResults\nDerivation\n\n\n\nSampling distribution of \\(\\hat{Y_h}\\)\n\nAgain, this refers to the different of \\(\\hat{Y_h}\\) from repeated sampling when the levels of the predictor variable \\(X\\) are held constant from sample to sample.\nPoint estimator for \\(E(Y_h)\\) → \\(\\hat{Y_h} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_h\\)\nDistribution of \\(\\hat{Y_h}\\) is Normal with mean and variance:\n\n\\[\n  \\begin{align*}\n    E(\\hat{Y_h}) &= E(Y_h) \\\\\n    V(\\hat{Y_h}) &= \\sigma^2 \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]\n  \\end{align*}\n  \\]\n\nEstimate variance (and standard deviation) by substituting \\(MSE\\). This gives us:\n\n\\[\n  S^2_{\\hat{Y_h}} = MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg] \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{\\hat{Y_h}} = \\sqrt{S^2_{\\hat{Y_h}}} = S \\sqrt{\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{S_{XX}}}\n  \\] Notes\n\nThe variability of the sampling distribution of \\(Y_h\\) is affected by how far \\(X_h\\) is from \\(\\bar{X}\\) (from the numerator of second term in \\(V(\\hat{Y_h})\\)) \\(\\Longrightarrow\\) \\(V(\\hat{Y_h})\\) increases the further \\(X_h\\) is from \\(\\bar{X}\\).\n\nIntuitive / visual explanation of this affect\n\nIn the picture below, two regression lines are assumed to go through the same \\((\\bar{X}, \\bar{Y})\\) point to isolate the effect of variation in the estimated slope \\(\\hat{\\beta}_1\\) from sample to sample.\n\n\n\nWe see the difference between estimated responses is much smaller when \\(X\\) is near the mean \\(\\bar{X}\\). So the variation in slope from sample to sample has a much more pronounced effect for \\(X\\) levels far from the mean.\n\n\nWhen \\(X_h = 0\\) \\(\\Longrightarrow\\) \\(V(\\hat{Y_h})\\) reduces to \\(V(\\hat{\\beta}_0)\\) (likewise for the estimated variances). This is because when \\(X_h = 0\\), \\(\\hat{Y_h} = \\hat{\\beta}_0 + \\hat{\\beta}_1 (0) = \\hat{\\beta}_0\\).\n\n\n\n\n\n\n\n\n2.3.2 Sampling distribution of \\((\\hat{Y_h} - E(Y_h)) / S_{\\hat{Y_h}}\\)\n\n\\[\n\\frac{\\hat{Y_h} - E(Y_h)}{S_{\\hat{Y_h}}} = \\frac{\\hat{Y_h} - E(Y_h)}{\\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}} \\sim \\text{t}\\,_{n - 2}\n\\]\n\n2.3.3 Confidence interval for \\(E(Y_h)\\)\n\n\nGoal → Estimate the mean value of \\(Y\\) for a given value of \\(X\\).\nResults → Using the same general format / derivation of a \\(t\\) interval, we have\n\n\\[\n100(1 - \\alpha)\\% \\text{ CI } = \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{Y_h}} \\hspace{10pt} = \\hspace{10pt} \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot \\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}\n\\]\n\nInterpretation → With &lt; \\(1 - \\alpha\\) &gt;% confidence, we estimate that the true mean value of &lt; \\(Y\\) context &gt; for all individuals with an &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\n\nNotes\n\nInterpretations → Same interpretation rules about repeated sampling for constant \\(X\\) levels (because \\(X_i\\) are known constants in the regression model).\n\nConfidence interval width is the smallest when \\(X_h = \\bar{X}\\) (assuming everything else remains equal).\n\nDesign of experiments → Thus in an experiment to estimate the mean response at a particular level \\(X_h\\), the precision of the estimate will be best if (everything else remaining equal) the observations on \\(X\\) are spaced so that \\(\\bar{X} = X_h\\).\n\n\nTests → Can use the CI to perform a two-sided test as well.\n\nRobust → Confidence limits shown here for \\(\\hat{Y_h}\\) are not sensitive to moderate departures from the assumption that the error terms are normally distributed. If there is substantial departures from normality, still not sensitive if large \\(n\\); this robustness comes from robustness of CIs for \\(\\beta_0\\) and \\(\\beta_1\\).\n\nCan also think of these CIs as robust because they are only concerned with the center (location) of the distribution of \\(Y_h\\).\n\n\nMultiple intervals → ** CIs apply when a estimating a single mean response from the study. Will show later how to adjust when estimating several mean responses. **\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# simulate sample dataset for normal error regression model\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30\nbeta_0 &lt;- 2\nbeta_1 &lt;- 5\nsigma &lt;- 10 \n\n# generate X values\nx &lt;- runif(n = n, min = 5, max = 15)\n\n# generate response Y | X ~ Normal(B0 + B1*X, sigma^2)\n# -&gt; specify the conditional means and then incorporate the random error\n# -&gt; rnorm() can take a vector of means and iterates through them\n# --&gt; if there n &gt; length(means), then the means get recycled from the start\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# specify new X level (X range is 5 - 15)\n# -&gt; newdata frame should have columns with same name as data for the model\nx_h &lt;- data.frame(x = 12)\n\n# calculate just the point estimate of estimated E(Y_h)\n# -&gt; technically using the predict() function for class lm(), so just calling predict() is masking predict.lm() --&gt; other classes like predict.glm() have slightly different argument options\n# -&gt; by default predict() returns the fits (regression line) for all of the obs (X values) used in the model fit, can specify new data points with newdata = &lt; data_frame &gt;\npredict(mod, newdata = x_h)\n\n       1 \n60.87545 \n\n# calculate lower and upper bounds of interval estimate for E(Y_h)\n# -&gt; by default does 95% CI and returns the point estimate (fit) as well\npredict(mod, newdata = x_h, interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 60.87545 56.97016 64.78073\n\n# show items related to standard error of estimation se(Y_h-hat)\n# -&gt; residual.scale = S = sqrt(MSE)\npredict(mod, newdata = x_h, se.fit = TRUE)\n\n$fit\n       1 \n60.87545 \n\n$se.fit\n[1] 1.906499\n\n$df\n[1] 28\n\n$residual.scale\n[1] 8.681483\n\n# alternate way from ALSM package\n# -&gt; this is the package that goes with the textbook; provides some functions that are nowhere else, and some alternatives to common functions\n# --&gt; the implementation of this function is essentially what is done in the \"manual\" section\n# --&gt; type = \"m\" gives CI for mean observation; by default does CL of 95%, but specifies alpha\n# -&gt; SIDENOTE -&gt; formatting: original result is a dataframe which displays poorly, so convert to matrix\nALSM::ci.reg(mod, newdata = x_h, type = \"m\", alpha = 0.05) %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 60.87545   56.97016   64.78073\n\n\n\n\n\n# calculate point estimate using estimated coefficients\n# -&gt; hat(Y_h) = hat(beta_0) + hat(beta_1) X_h\nb &lt;- as.numeric(coef(mod))\npe &lt;- b[1] + b[2] * as.numeric(x_h)\n\n# calculate critical value\n# -&gt; use lower.tail = FALSE to get the positive version of t_alpha/2\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate se(Y_h-hat) = MSE (1/n (X_h - X-bar)^2 / S_XX) %&gt;% sqrt OR S * sqrt(1/n + S_XX)\nn &lt;- length(x)\nx_bar &lt;- mean(x)\ns_xx &lt;- sum((x - x_bar)^2)\nmse &lt;- (summary(mod)$sigma)^2\nmse &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$residual.scale\nse_yh_hat &lt;- sqrt(mse * (1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\nse_yh_hat &lt;- summary(mod)$sigma * sqrt((1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\n\n# calculate CI for E(Y_h)\nci_limits &lt;- c(lower = pe - t_crit * se_yh_hat, upper = pe + t_crit * se_yh_hat)\n\n# compare results to predict(lm(), type = \"confidence\")\n# -&gt; Y_h-hat and interval bounds\ncompare(predict(mod, newdata = x_h, interval = \"confidence\"), c(\"point estimate\" = pe, ci_limits))\n\n$comparison\n   fit  lwr  upr\n1 TRUE TRUE TRUE\n\n$`predict(mod, newdata = x_h, interval = \"confidence\")`\n       fit      lwr      upr\n1 60.87545 56.97016 64.78073\n\n$`c(`point estimate` = pe, ci_limits)`\npoint estimate          lower          upper \n      60.87545       56.97016       64.78073 \n\n# -&gt; se of estimation\n# --&gt; by default predict uses s = sqrt(mse) = residual standard error to get the se.fit (and the df as well = df.residual(mod)) ---&gt; assumes future obs have same error variance as originals used for fitting\n# --&gt; can specify a different variance to use for future obs with pred.var if desired\ncompare(predict(mod, newdata = x_h, se.fit = TRUE)$se.fit, se_yh_hat)\n\n$comparison\n[1] TRUE\n\n$`predict(mod, newdata = x_h, se.fit = TRUE)$se.fit`\n[1] 1.906499\n\n$se_yh_hat\n[1] 1.906499\n\n# can return fit, bounds, and standard error info from predict using type = \"terms\"\npredict(mod, newdata = x_h, type = \"terms\", interval = \"conf\")\n\n$fit\n        x\n1 9.87435\nattr(,\"constant\")\n[1] 51.0011\n\n$se.fit\n         x\n1 1.059465\n\n$lwr\n         x\n1 7.704134\nattr(,\"constant\")\n[1] 51.0011\n\n$upr\n         x\n1 12.04457\nattr(,\"constant\")\n[1] 51.0011\n\n$df\n[1] 28\n\n$residual.scale\n[1] 8.681483\n\n# -&gt; for the fit and bounds, for some reason this actually uses the alternate regression model -&gt; Y_h-hat = bar(y) + beta_1-hat (X_h - bar(x))\n# -&gt; results: constant = beta_1-hat (X_h - X-bar) and attr = Y-bar\n# -&gt; same process to get upper and lower too, add to Y-bar\n(x_star &lt;- predict(mod, newdata = x_h, type = \"terms\"))\n\n        x\n1 9.87435\nattr(,\"constant\")\n[1] 51.0011\n\ncompare(pe, mean(y) + b[2] * (as.numeric(x_h) - mean(x)))\n\n$comparison\n[1] TRUE\n\n$pe\n[1] 60.87545\n\n$`mean(y) + b[2] * (as.numeric(x_h) - mean(x))`\n[1] 60.87545\n\n\n\n\n\n# compare width of confidence intervals at two X levels\n\n# specify new X levels (X range is 5 - 15)\n# -&gt; optimal width will be at X = X-bar\nx_h_mean &lt;- data.frame(x = mean(x))\nx_h2 &lt;- data.frame(x = 8)\n\n# calculate width of intervals\npredict(mod, newdata = x_h_mean, interval = \"confidence\")[2:3] %&gt;% diff\n\n[1] 6.493511\n\npredict(mod, newdata = x_h2, interval = \"confidence\")[2:3] %&gt;% diff\n\n[1] 7.988876",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#prediction-of-new-observation",
    "href": "notes-inference.html#prediction-of-new-observation",
    "title": "2  Inference",
    "section": "\n2.4 Prediction of new observation",
    "text": "2.4 Prediction of new observation\nOverview\n\nGoal → Predict a new observation \\(Y\\) for a given \\(X\\) value. This new observation is viewed as the result of a new trial, independent of the trials the model is based on.\n\nSetup\n\nAgain, \\(X_h\\) is the \\(X\\) level for the new trial. Still assuming the underlying regression model is appropriate for the new observation.\nThe new observation on \\(Y\\) is \\(Y_{h(new)}\\); this is what we are estimating.\n\n\n\nDistinction between (1) estimation of the mean response \\(E(Y_h)\\) and (2) prediction of a new response \\(Y_{h(new)}\\)\n\nWe estimate the mean of the distribution of \\(Y\\).\nWe predict an individual outcome drawn from the distribution of \\(Y\\). Obviously, most outcomes deviate from the mean response; so this must be taken into account when predicting \\(Y_{h(new)}\\).\n\n\nPrediction has extra variability \\(\\Longrightarrow\\) Less precision.\n\n\n2.4.1 Prediction interval for \\(Y_{h(new)}\\) when parameters are known\nDemonstration of prediction intervals\n\nFor a simple example, assume the relevant parameters of the regression model are known:\n\n\\[\n\\beta_0 = 0.10, \\, \\beta_1 = 0.95, \\, \\sigma = 0.12 \\hspace{10pt} \\longrightarrow \\hspace{10pt} E(Y) = 0.10 + 0.95 X\n\\]\n\nIf we have a new observation with \\(X_h = 3.425\\) → \\(E(Y_h) = 0.10 + 0.95 (3.5) = 3.425\\) (so we know the center of the normal distribution of \\(Y_h\\)). Thus, using the empirical rule we have the following prediction interval:\n\n\\[\n99.7\\% \\text{ CI } = E(Y_h) \\pm 3 \\sigma \\hspace{10pt} = \\hspace{10pt} 3.425 \\pm 3 (0.12) \\hspace{10pt} \\Longrightarrow \\hspace{10pt} 3.065 \\le Y_{h(new)} \\le 3.785\n\\]\n\nBasic idea of prediction used here\n\nChoose a range in the distribution of \\(Y\\) where most of the observations will fall and then declare that the next observation will fall in this range.\nThe usefulness of the prediction interval depends on the width of the interval and the needs for precision by the user.\n\nGeneralizing for this simple scenario\n\nWhen the regression parameters of normal error regression model are known:\n\n\\[\n100(1 - \\alpha)\\% \\text{ PI for } Y_{h(new)} = E(Y_h) \\pm z_{\\alpha / 2} \\cdot \\sigma\n\\]\n\nCentering the limits around \\(E(Y_h)\\) results in the narrowest interval consistent with the specified probability of a correct prediction.\n\n2.4.2 Prediction interval for \\(Y_{h(new)}\\) when parameters are unknown\nOverview and demo\n\n\nWhen the regression parameters are unknown, they must be estimated.\n\nThe mean of the distribution of \\(Y\\) is estimated by \\(\\hat{Y_h}\\) as usual, and the variance of the distribution of \\(Y\\) is estimated with \\(MSE\\).\nHowever, we cannot simply use the previous PI with the parameters replaced by the corresponding point estimators. Here’s a demo of why:\nThe picture below shows two possible probability distributions of \\(Y\\), corresponding to the lower and upper limits of a CI for \\(E(Y_h)\\). In other words, the distribution of \\(Y\\) could be located as far left as the one shown, as far right as the other one shown, or anywhere in between.\n\n\n\n\n\n\nResults\n\nSince we do not know the mean \\(E(Y_h)\\) and only estimate it by a confidence interval, we cannot be certain of the location of the distribution of \\(Y\\).\nBecause of this uncertainty, PIs for \\(Y_{h(new)}\\) clearly must take into account two elements:\n\n\nVariation in possible location of the distribution of \\(Y\\).\nVariation within the probability distribution of \\(Y\\).\n\n\n\nPrediction interval for \\(Y_{h(new)}\\)\n\n\nSampling distribution\n\nNote that this studentized statistic uses the point estimator \\(\\hat{Y_h}\\) in the numerator rather than the true mean \\(E(Y_h)\\) because the true mean is unknown and cannot be used in making a prediction.\nFor \\(E(Y_h)\\) CIs shown earlier, we used \\([\\hat{Y_h} - E(Y_h)] / S_{\\hat{Y_h}}\\), which was okay because the only unknown was \\(E(Y_h)\\) and it is what we are estimating. But now there are two layers of uncertainty (variability). So the reference value in the numerator is also an estimate, not the true value.\n\n\n\n\\[\n\\frac{Y_{h(new)} - \\hat{Y_h}}{S_{pred}} \\sim \\text{t}\\,_{n-2}\n\\]\n\n\nPrediction interval\n\nInterpretation → With &lt; \\(1-\\alpha\\) &gt;% confidence, we predict that the true value of &lt; \\(Y\\) context &gt; for a single (or the next) &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\n\n\n\n\\[\n100(1 - \\alpha)\\% \\text{ PI for } Y_{h(new)} = \\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{pred}\n\\]\nStandard deviation of prediction \\(\\sigma^2_{pred}\\)\n\nThe numerator of the studentized statistic represents how far the new observation will deviate from the estimated mean (based on the original \\(n\\) cases in the study). This difference can be viewed as the prediction error, with \\(\\hat{Y_h}\\) serving as the best point estimate of the value of the new observation \\(Y_{h(new)}\\).\nWe can easily find the variance of this difference (because of independence of the new \\(Y_{h(new)}\\) and original \\(n\\) cases on which \\(\\hat{Y_h}\\) is based).\n\n\\[\n\\sigma^2_{pred} = V(Y_{h(new)} - \\hat{Y_h}) = V(Y_{h(new)}) + V(\\hat{Y_h}) = \\sigma^2 + \\sigma^2_{\\hat{Y_h}}.\n\\]\n\n\nThis has two components:\n\nVariance of the distribution of \\(Y\\) at \\(X = X_h\\) → \\(\\sigma^2\\)\nVariance of the sampling distribution of \\(\\hat{Y_h}\\) → \\(\\sigma^2_{\\hat{Y_h}}\\)\n\n\nThe unbiased estimator of \\(\\sigma^2_{pred}\\) is\n\n\\[\nS^2_{pred} = MSE + S^2_{\\hat{Y_h}} = MSE + MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg] = MSE \\bigg[1 + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum (X_i - \\bar{X})^2}\\bigg] \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{pred} = \\sqrt{S^2_{pred}}\n\\]\nNotes about prediction intervals\n\n\nAnalogous results / interpretations to estimation\n\nInterpretation → Again, the confidence coefficient refers to taking repeated samples based on the same set of \\(X\\) values, and calculating prediction limits for \\(Y_{h(new)}\\) for each sample.\nPrecision → PI width is the smallest when \\(X_h = \\bar{X}\\) (assuming everything else remains equal).\nOne interval → PIs apply for a single prediction based on the sample data.\n\n\nNot robust → PIs (unlike CIs for the mean response) are sensitive to departures from normality of the error terms distribution. Can think of this non-robustness a result of having to take into account the center of the distribution of \\(Y_h\\) (just like with CIs) AND also the tails (spread) of the distribution.\n\nPrecision → Even if PIs are too wide for useful predictions, they can still be informative for control / modelling purposes, specifically the estimated variance of prediction.\n\n\\(S^2_{pred} = MSE + S^2_{\\hat{Y_h}} \\hspace{10pt}\\) has two pieces: (1) \\(MSE\\) measures \\(X\\)-to-\\(X\\) variation within the probability distribution for \\(Y\\) (different response values for observations with same \\(X\\) level) and (2) \\(S^2_{\\hat{Y_h}}\\) measures sample-to-sample variation (mean response of samples with overall same \\(X\\) levels).\nSo if \\(MSE\\) is very large compared to \\(S^2_{\\hat{Y_h}}\\), e.g. \\(\\frac{MSE}{S^2_{pred}} \\ge 0.8 \\text{ or } 0.9\\), then the majority of the variation is from sample-to-sample. This could reflect other factors that aren’t being taken into account by the model. So perhaps a multiple linear regression model should be used, which could result in more useful predictions. NOT SURE HOW THIS WORKS\n\n\n\nEstimation vs Prediction\n\nLocation → For a particular \\(X_h\\), CIs and PIs have the same point estimate \\(\\hat{Y_h}\\), which is the estimate of the mean \\(E(Y_h)\\).\n\nPrecision\n\nThe difference between CIs and PIs then lies in the relative accuracy of the interval.\nCIs are narrower than PIs at the same \\(X_h\\).\nThe only way to obtain more accurate prediction for a new value of \\(Y\\) is to reduce the standard deviation of the regression model. This can be accomplished by using a curvilinear model, adding new independent variables, etc. or by collecting more data (width of both intervals decrease when the sample size increases).\n\n\nConceptual difference → CIs resemble PIs, except: A CI represents an inference on a parameter and is an interval that is intended to cover the value of the parameter; and a PI is a statement about the value to be taken by a random variable, the new observation \\(Y_{h(new)}\\).\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate just the point estimate of predicted Y_h(new)\n# -&gt; this is the same as the PE for a CI of E(Y_h) \npredict(mod, newdata = x_h)\n\n       1 \n60.87545 \n\n# calculate lower and upper bounds of prediction interval Y_h(new)\n# -&gt; by default does 95% CI and returns the point estimate (fit) as well\npredict(mod, newdata = x_h, interval = \"prediction\")\n\n       fit      lwr      upr\n1 60.87545 42.66847 79.08242\n\npredict(mod, newdata = x_h, interval = \"co\")\n\n       fit      lwr      upr\n1 60.87545 56.97016 64.78073\n\n# show items related to standard error of ESTIMATION se(Y_h-hat)\n# -&gt; ALWAYS returns this, even if specify interval = \"pred\"...\n# --&gt; suppose it implicitly adds the extra MSE term (from residual.scale) to get the prediction lwr and upr\npredict(mod, newdata = x_h, se.fit = TRUE)\n\n$fit\n       1 \n60.87545 \n\n$se.fit\n[1] 1.906499\n\n$df\n[1] 28\n\n$residual.scale\n[1] 8.681483\n\n# alternative way using ALSM\n# -&gt; type = \"n\" gives PI for single new observation\nALSM::ci.reg(mod, newdata = x_h, type = \"n\") %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 60.87545   42.66847   79.08242\n\n\n\n\n\n# calculate point estimate and critical value\n# -&gt; same as for confidence interval\nb &lt;- as.numeric(coef(mod))\npe &lt;- b[1] + b[2] * as.numeric(x_h)\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate standard error in prediction se(pred) = sqrt(MSE + var(Y-h-hat))\n# -&gt; save se(estimation = fit), shown earlier, then have to add in the extra MSE term\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nse_pred &lt;- sqrt(summary(mod)$sigma^2 + se_fit^2)\nse_pred &lt;- sqrt(summary(mod)$sigma^2 + se_fit^2)\n\n# OR calculate se(pred) using expanded formula se(pred) = MSE * (1 + 1/n (X_h - X-bar)^2 / S_XX) %&gt;% sqrt OR s * sqrt(...)\nn &lt;- length(x)\nx_bar &lt;- mean(x)\ns_xx &lt;- sum((x - x_bar)^2)\ns &lt;- (summary(mod)$sigma)\nse_pred &lt;- s * sqrt((1 + 1 / n + (as.numeric(x_h) - x_bar)^2 / s_xx))\n\n# calculate PI for Y_h(new)\npi_limits &lt;- c(lower = pe - t_crit * se_pred, upper = pe + t_crit * se_pred)\n\n# compare results to predict(lm(), type = \"prediction)\n# -&gt; Y_h-hat and interval bounds\ncompare(predict(mod, newdata = x_h, interval = \"prediction\"), c(\"point estimate\" = pe, pi_limits))\n\n$comparison\n   fit  lwr  upr\n1 TRUE TRUE TRUE\n\n$`predict(mod, newdata = x_h, interval = \"prediction\")`\n       fit      lwr      upr\n1 60.87545 42.66847 79.08242\n\n$`c(`point estimate` = pe, pi_limits)`\npoint estimate          lower          upper \n      60.87545       42.66847       79.08242 \n\n\n\n\n\n# compare width of prediction intervals at two X levels\n\n# specify new X levels (X range is 5 - 15)\n# -&gt; optimal width will again be at X = X-bar\nx_h_mean &lt;- data.frame(x = mean(x))\nx_h2 &lt;- data.frame(x = 8)\n\n# calculate width of intervals\npredict(mod, newdata = x_h_mean, interval = \"prediction\")[2:3] %&gt;% diff\n\n[1] 36.15434\n\npredict(mod, newdata = x_h2, interval = \"prediction\")[2:3] %&gt;% diff\n\n[1] 36.45261\n\n# compare widths of CI vs PI at the same X level\n\n# calculate width of intervals\npredict(mod, newdata = x_h, interval = \"conf\")[2:3] %&gt;% diff\n\n[1] 7.810573\n\npredict(mod, newdata = x_h, interval = \"pred\")[2:3] %&gt;% diff\n\n[1] 36.41395\n\n\n\n\n\n\n\n2.4.3 Prediction of mean of \\(m\\) observations for given \\(X_h\\)\n\nOverview\n\nGoal → Predict the mean of \\(m\\) new observations on \\(Y\\) for a given level of the predictor variable.\nSetup → \\(\\bar{Y}_{h(new)}\\) represents the mean of the new \\(Y\\) observations to be predicted.\n\nResults\n\n\n\\(100(1 - \\alpha)\\%\\) Prediction interval for \\(\\bar{Y}_{h(new)}\\) (assuming the new observations are independent):\n\n\\[\n\\hat{Y_h} \\pm t_{(1 - alpha /2, n - 2)} \\cdot S_{predmean}\n\\]\n\nStandard deviation in prediction of a mean \\(S_{predmean}\\)\n\n\n\\[\nS^2_{predmean} = \\frac{MSE}{m} + S^2_{\\hat{Y_h}}  = MSE \\bigg[\\frac{1}{m} + \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{S_{XX}}\\bigg]  \\hspace{20pt} \\longrightarrow \\hspace{20pt} S_{predmean} = \\sqrt{S^2_{predmean}}\n\\]\n\n\nThis has two components:\n\nVariance of the distribution mean of \\(m\\) observations from the probability distrubtion of \\(Y\\) at \\(X = X_h\\) → \\(\\sigma^2 / m\\)\nVariance of the sampling distribution of \\(\\hat{Y_h}\\) → \\(\\sigma^2_{\\hat{Y_h}}\\)\n\n\n\nNotes\n\nInterpretation → With &lt; \\(1 - \\alpha\\) &gt;% confidence, we predict that the true value of &lt; \\(Y\\) context &gt; for &lt; \\(m\\) &gt; &lt; \\(X\\) context &gt; of &lt; \\(X_h\\) &gt; to be between &lt; lower bound &gt; and &lt; upper bound &gt;.\nInterval still has the same center as when estimating \\(E(Y_h)\\) and predicting a single \\(Y_{h(new)}\\).\nThis prediction interval is narrower than when predicting for a single observation (because it involves the prediction of the mean for a group), but still wider than the confidence interval.\n\nWe can then obtain the prediction interval for the total of the \\(m\\) observations by multiplying each limit by \\(m\\).\n\ne.g) For \\(m = 4\\), if \\(5 \\le \\bar{Y}_{h(new)} \\le 15 \\hspace{10pt} \\Longrightarrow \\hspace{10pt} 5(4) = 20 \\le \\sum Y_{h(new)} \\le 15(4)= 60\\)\n\n\n\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate the predicted mean for m observations of Y_h(new)\n# -&gt; type = \"nm\" is gives PR for mean of m new observations at X_h\nALSM::ci.reg(mod, newdata = x_h, type = \"nm\") %&gt;% \n  as.matrix\n\n   x      Fit Lower.Band Upper.Band\n1 12 60.87545   42.66847   79.08242\n\n\n\n\n\n# set number of observations to predict at X_h\nm &lt;- 4\n\n# calculate just the point estimate of predicted Y-bar_h(new) and critical value\n# -&gt; PE is the same as previous CI and PI\npe &lt;- as.numeric(predict(mod, newdata = x_h))\nalpha &lt;- 0.05\nt_crit &lt;- qt(p = alpha / 2, df = df.residual(mod), lower.tail = FALSE)\n\n# calculate standard error in prediction se(pred) = sqrt(MSE / m + var(Y-h-hat))\n# -&gt; save se(fit), shown earlier, then have to add in the extra MSE / m term\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nse_pred &lt;- sqrt(summary(mod)$sigma^2 / m + se_fit^2)\n\n# calculate PI for Y-bar_h(new)\n(pi_limits &lt;- c(lower = pe - t_crit * se_pred, upper = pe + t_crit * se_pred))\n\n   lower    upper \n51.16402 70.58688 \n\n# compare to (widths) of previous types of intervals\n# -&gt; order from most to least precise: CI for E(Y_h), PI for Y-bar_h(new), PI for single Y_h-hat\npredict(mod, newdata = x_h, interval = \"conf\")\n\n       fit      lwr      upr\n1 60.87545 56.97016 64.78073\n\npredict(mod, newdata = x_h, interval = \"pred\")\n\n       fit      lwr      upr\n1 60.87545 42.66847 79.08242\n\n# interval for sum (total) of m predictions at X_h\n# -&gt; depends on context if this is meaningful\npi_limits * m\n\n   lower    upper \n204.6561 282.3475",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#confidence-band-for-regression-line",
    "href": "notes-inference.html#confidence-band-for-regression-line",
    "title": "2  Inference",
    "section": "\n2.5 Confidence band for regression line",
    "text": "2.5 Confidence band for regression line\nOverview\n\nThe goal is to obtain a confidence band for the entire regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\).\nThis band enables us to see the region in which the entire regression line lies and is particularly useful for determining the appropriateness of a fitted regression function.\n\nResults\n\n\nThis confidence band formula below the same form as the CI formula for \\(E(Y_h)\\), the mean response at \\(X_h\\) (\\(\\hat{Y_h} \\pm t_{\\alpha/2, n-2} \\cdot S_{\\hat{Y_h}}\\)), except it uses a different multiplier to adjust for multiple comparisons \\(\\Longrightarrow\\) Same point estimate and standard error.\n\nSome alternative procedures for developing confidence bands have been developed.\nThe one shown below is the Working-Hotelling confidence band (more will be said about this method later).\nThe simplicity of this method is that it is a direct extension of the confidence limits for a single mean response \\(E(Y_h)\\) shown earlier.\n\n\nThe \\(100(1 - \\alpha)\\%\\) confidence band for the regression line has boundaries at any level \\(X_h\\):\n\n\\[\n\\hat{Y_h} \\pm W \\cdot S_{\\hat{Y_h}} = \\hat{Y_h} \\pm W \\cdot \\sqrt{MSE \\bigg[\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}\\bigg]}\n\\]\n\n(Initial steps of deriving CI) → We are finding the two values such that \\(P(\\text{lower} \\le \\beta_0 + \\beta_1 X_h \\le \\text{upper}) = 1- \\alpha\\); And \\(\\hat{Y_h}\\) is the point estimator for \\(E(Y_h) = \\beta_0 + \\beta_1 X_h\\).\n\\(W\\) multiplier\n\n\\[\nW^2 = 2 \\cdot F_{(1-\\alpha; \\, 2, n-2)}\n\\]\n\nWe multiply by two for SLR becuase there are two estimated coefficients.\nThe \\(W\\)-multiplier is larger than the \\(t\\) multiplier because the confidence band must encompass the entire regression line, whereas the confidence limits for \\(E(Y_h)\\) at \\(X_h\\) apply only at the single level \\(X_h\\).\n\nNotes\n\n\nConfidence band confidence level\n\nInterpretation → Indicates the proportion of time that the estimating procedure will yield a band that covers the entire line, in a long series of samples in which the \\(X\\) observations are kept at the same level as in the actual study.\nTechnical application → The confidence band applies to the entire regression line over all real-numbered values of \\(X\\) from \\(-\\infty\\) to \\(\\infty\\).\nPractical application → In practice, the confidence band is ignored for that part of the regression line which is not of interest, so the confidence coefficient for this limited segment is somewhat higher than \\(1-\\alpha\\). Thus, \\(1-\\alpha\\) serves as a lower bound to the confidence coefficient.\n\n\nThings to look for → Can see if the slope is clearly positive or negative, can look at the levels of the regression line at different levels of \\(X\\) to gauge relative precision, etc.\n\nPrecision\n\nWith the somewhat wider limits for the entire regression line, we are able to draw conclusions about any and all mean responses for the entire regression line and not just about the mean response at a given \\(X\\) level.\nAlthough, generally confidence band lines at any value \\(X_h\\) often are not substantially wider than the confidence limits for the mean response at that single \\(X_h\\) level.\nJust like with the CI for \\(E(Y_h)\\), the boundary points of the confidence band for the regression line are wider apart the further \\(X_h\\) is from the mean \\(\\bar{X}\\) of the \\(X\\) observations.\n\n\nFormula → The lower and upper bounds together actually define a hyperbola.\n\nDemo\n\n\nR functions\nManual\nOther functions\n\n\n\n\n# using the same model as the confidence interval for E(Y_h) demo\n\n# calculate confidence band limits across scope of model\n# -&gt; initialize many x_h values covering min to max of original sample\nx_h &lt;- data.frame(x = seq(from = min(x), to = max(x), length = 20))\n\n# calculate confidence bands\n# -&gt; type = \"w\" uses the Working-Hotelling method with the W multiplier\nconf_band &lt;- ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05)\n\n# display results\nkable(conf_band[1:5,], format = \"html\", digits = 3) %&gt;% \n  kable_styling(full_width = FALSE,\n                position = \"left\")\n\n\n\nx\nFit\nLower.Band\nUpper.Band\n\n\n\n5.075\n25.452\n17.268\n33.637\n\n\n5.564\n27.953\n20.361\n35.545\n\n\n6.053\n30.454\n23.436\n37.472\n\n\n6.542\n32.955\n26.488\n39.423\n\n\n7.031\n35.456\n29.509\n41.403\n\n\n\n\n# compare width when estimating a single E(X_h) to the confidence bands\nc(x_h[1,], predict(mod, newdata = data.frame(x = x_h[1,]), interval = \"conf\")) %&gt;% \n  set_names(c(\"x\", \"fit\", \"lwr\", \"upr\"))\n\n        x       fit       lwr       upr \n 5.075098 25.452401 18.966137 31.938664 \n\n# plot fitted line and confidence bands on scatterplot\n# -&gt; also add reference line for X-bar showing where the most precision is\nconf_band %$% \n  matplot(x = x, y = cbind(Fit, Lower.Band, Upper.Band), type =  \"l\", lty = c(1,2,2), col = c(\"red\", \"blue\", \"blue\"), xlab = expression(X[h]), ylab = expression(hat(Y)[h]), main = bquote(.(100 * (1 - alpha)) * \"% Confidence band\"))\npoints(x, y)\nabline(v = mean(x), lty = 2, col = \"lightgrey\")\ntext(x = mean(x) + 0.5, y = min(y) + 1, labels = expression(bar(X)) , col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\n\n# calculate W multiplier = 2 F_crit(regression = of coefficients, residual)\n# degrees of freedom\n# -&gt; numerator -&gt; = df regression (# of coefficients) = 2 for SLR\n# -&gt; denominator -&gt; df residual (n - # of coefficients) = n - 2 for SLR\n# probability -&gt; F is right-tailed, so need to use 1 - alpha now\n# -&gt; no dividing by two or taking absolute value\nalpha &lt;- 0.05\n(W &lt;- sqrt(2 * qf(1 - alpha, df1 = length(coef(mod)), df2 = df.residual(mod))))\n\n[1] 2.584719\n\n# compare W to t at a specific X_h\nabs(qt(alpha / 2, df = df.residual(mod)))\n\n[1] 2.048407\n\n# calculate confidence bands\n# -&gt; use seq of x values from before\n# -&gt; get the fits and se(estimation) for each new x\n# -&gt; calculate lower and upper confidence band limits = Y_h-hat +- W * se(estimation)\nfit &lt;- as.numeric(predict(mod, newdata = x_h))\nse_fit &lt;- predict(mod, newdata = x_h, se.fit = TRUE)$se.fit\nlower &lt;- fit - W * se_fit\nupper &lt;- fit + W * se_fit\n\n# combine above info\ndata_conf_band &lt;- data.frame(x_h = x_h$x, fit, se_fit, lower, upper, width = upper - lower) \n\n# illustrate process of confidence bands over X range\ndata_conf_band %&gt;% \n  kable(col.names = c(\"X_h\", \"fit = Y_h-hat\", \"SE(fit) = S_{Y_h-hat}\", \"LB = Y_h-hat - W x S_{Y_h-hat}\", \"UB = $Y_h-hat + W x S_{Y_h-hat}$\", \"Width\"),\n        digits = 3) %&gt;% \n  kable_styling(full_width = FALSE,\n                position = \"left\")\n\n\n\nX_h\nfit = Y_h-hat\nSE(fit) = S_{Y_h-hat}\nLB = Y_h-hat - W x S_{Y_h-hat}\nUB = $Y_h-hat + W x S_{Y_h-hat}$\nWidth\n\n\n\n5.075\n25.452\n3.166\n17.268\n33.637\n16.369\n\n\n5.564\n27.953\n2.937\n20.361\n35.545\n15.184\n\n\n6.053\n30.454\n2.715\n23.436\n37.472\n14.036\n\n\n6.542\n32.955\n2.502\n26.488\n39.423\n12.935\n\n\n7.031\n35.456\n2.301\n29.509\n41.403\n11.894\n\n\n7.520\n37.957\n2.114\n32.492\n43.423\n10.931\n\n\n8.009\n40.458\n1.947\n35.425\n45.491\n10.066\n\n\n8.498\n42.959\n1.805\n38.295\n47.624\n9.329\n\n\n8.986\n45.460\n1.693\n41.085\n49.836\n8.751\n\n\n9.475\n47.961\n1.618\n43.778\n52.144\n8.365\n\n\n9.964\n50.462\n1.586\n46.362\n54.562\n8.199\n\n\n10.453\n52.963\n1.599\n48.830\n57.096\n8.266\n\n\n10.942\n55.464\n1.656\n51.184\n59.744\n8.559\n\n\n11.431\n57.965\n1.752\n53.436\n62.494\n9.058\n\n\n11.920\n60.466\n1.882\n55.600\n65.331\n9.731\n\n\n12.409\n62.967\n2.040\n57.695\n68.239\n10.544\n\n\n12.898\n65.468\n2.218\n59.734\n71.202\n11.468\n\n\n13.387\n67.969\n2.414\n61.730\n74.208\n12.478\n\n\n13.876\n70.470\n2.622\n63.692\n77.247\n13.555\n\n\n14.365\n72.971\n2.841\n65.629\n80.313\n14.684\n\n\n\n\n# compare results to ci.reg(type = \"w)\ncompare(head(ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05)), head(data_conf_band[,c(1,2,4,5)]))\n\n$comparison\n     x  Fit Lower.Band Upper.Band\n1 TRUE TRUE       TRUE       TRUE\n2 TRUE TRUE       TRUE       TRUE\n3 TRUE TRUE       TRUE       TRUE\n4 TRUE TRUE       TRUE       TRUE\n5 TRUE TRUE       TRUE       TRUE\n6 TRUE TRUE       TRUE       TRUE\n\n$`head(ALSM::ci.reg(mod, newdata = x_h, type = \"w\", alpha = 0.05))`\n         x      Fit Lower.Band Upper.Band\n1 5.075098 25.45240   17.26791   33.63689\n2 5.564014 27.95336   20.36136   35.54535\n3 6.052930 30.45432   23.43627   37.47237\n4 6.541846 32.95528   26.48768   39.42287\n5 7.030762 35.45624   29.50908   41.40339\n6 7.519678 37.95719   32.49188   43.42251\n\n$`head(data_conf_band[, c(1, 2, 4, 5)])`\n       x_h      fit    lower    upper\n1 5.075098 25.45240 17.26791 33.63689\n2 5.564014 27.95336 20.36136 35.54535\n3 6.052930 30.45432 23.43627 37.47237\n4 6.541846 32.95528 26.48768 39.42287\n5 7.030762 35.45624 29.50908 41.40339\n6 7.519678 37.95719 32.49188 43.42251\n\nhead(data_conf_band)\n\n       x_h      fit   se_fit    lower    upper    width\n1 5.075098 25.45240 3.166491 17.26791 33.63689 16.36898\n2 5.564014 27.95336 2.937262 20.36136 35.54535 15.18399\n3 6.052930 30.45432 2.715209 23.43627 37.47237 14.03610\n4 6.541846 32.95528 2.502244 26.48768 39.42287 12.93519\n5 7.030762 35.45624 2.300891 29.50908 41.40339 11.89431\n6 7.519678 37.95719 2.114471 32.49188 43.42251 10.93063\n\n\n\n\n\n# demo to verify what geom_smooth() gives\n\n# results\n# -&gt; lm regression lines line up as expected, BUT...\n# -&gt; the confidence bands using se = TRUE are actually the lower and upper **pointwise** confidence interval around the mean\n# -&gt; ** this means it is plotting ALL of the INDIVIDUAL CIs for E(Y_h), whereas Working-Hotelling confidence bands represent a confidence \"interval\" for the ENTIRE regression line\n\n# create dataframe of original observations the model was built on\ndata_original &lt;- data.frame(x, y)\n\n# set t multiplier for making single interval estimates of E(Y_h)\nt_crit &lt;- abs(qt(alpha / 2, df = df.residual(mod)))\n\n# create confidence band demo plotting dataset\n# -&gt; rename current lower and upper to have an indication of W multiplier\n# -&gt; calculate new lower and upper bounds based on t multiplier\n# -&gt; remove unneeded columns for plot\n# -&gt; reshaoe to long with to long with and columns for type of bound and value\n# -&gt; create indicator variable for type of multiplier used in calculating the lower and upper bounds (check what suffix is of bound) and then remove multiplier indication from bound column (just take off suffix)\ndata_conf_band_plot &lt;- data_conf_band %&gt;% \n  rename(lower_w = lower,\n         upper_w = upper) %&gt;% \n  mutate(lower_t = fit - t_crit * se_fit,\n         upper_t = fit + t_crit * se_fit) %&gt;% \n  select(-c(width, se_fit)) %&gt;% \n  pivot_longer(cols = starts_with(c(\"lower\", \"upper\")),\n               names_to = \"bound\",\n               values_to = \"value\") %&gt;% \n  mutate(multiplier = \n           case_when(\n             str_sub(bound, start = -1) == \"t\" ~ \"t\",\n             TRUE ~ \"W\"),\n         bound = str_sub(bound, end = -3))\n\n# create demo plot\n# layer:  geom_smooth()\n# -&gt; this adds regression line\n# -&gt; se + TRUE adds the shaded ribbon representing \"confidence bands\"\n# layer: geom_line() first one\n# -&gt; add manually calculated regression line, should line up exactly with above\n# layer: geom_line() second and third ones\n# -&gt; confidence bands for both multipliers\nggplot() + \n  geom_smooth(aes(x = x,\n                  y = y),\n              data = data_original,\n              method = \"lm\",\n              formula = y ~ x,\n              level = 1 - alpha,\n              se = TRUE,\n              color = \"yellow\",\n              linewidth = 1,\n              alpha = 0.5) + \n  geom_line(aes(x = x_h,\n                y = fit),\n            data = data_conf_band_plot,\n            color = \"red\",\n            linewidth = 0.5) + \n  geom_line(aes(x = x_h,\n                y = value,\n                color = multiplier),\n            data = filter(data_conf_band_plot, bound == \"lower\"),\n            linetype = 2) + \n  geom_line(aes(x = x_h,\n                y = value,\n                color = multiplier),\n            data = filter(data_conf_band_plot, bound == \"upper\"),\n            linetype = 2) + \n  scale_color_manual(name = \"Multiplier\", values = c(t = \"purple\", W = \"blue\")) + \n  labs(title = \"Demo of confidence bands\",\n       x = expression(X[h]),\n       y = expression(hat(Y)[h]))\n\n\n\n\n\n\n\nWhere the multipliers are \\(t_{\\alpha / 2, n - p}\\) and \\(W = 2 \\cdot F_{1 - \\alpha; p, n - p}\\).",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#analysis-of-variance-approach-to-regression",
    "href": "notes-inference.html#analysis-of-variance-approach-to-regression",
    "title": "2  Inference",
    "section": "\n2.6 Analysis of variance approach to regression",
    "text": "2.6 Analysis of variance approach to regression\n\nThis approach is very useful for multiple linear regression and other types of linear statistical models.\n\n\n2.6.1 Partitioning of total sum of squares\nOverview\n\nIn a regression setting, analysis of variance (ANOVA) allows us to capture the different sources of variability in the model.\nWe do this by partitioning the sums of squares and degrees of freedom associated with the the response variable \\(Y\\).\n\n\nTypes of sum of squares\n\nTotal sum of squares → Measured in terms of the deviations of the \\(Y_i\\) around their mean \\(\\bar{Y}\\).\n\n\\[\nSSTO = \\sum (Y_i - \\bar{Y})^2 = S_{YY}\n\\]\n\nMeasures the total variation of \\(Y\\), which tells us the uncertainty related to \\(Y\\) when the predictor variable \\(X\\) is not taken into account.\nIf all observations are the same, then \\(SSTO = 0\\). More variation in \\(Y_i\\), the larger \\(SSTO\\) is.\nSum of squares error → Measured in terms of the deviations of the \\(Y_i\\) around the fitted regression line \\(\\hat{Y_i}\\) (i.e. the residuals \\(e_i\\)).\n\n\\[\nSSE = \\sum (Y_i - \\hat{Y_i})^2\n\\]\n\nMeasures the remaining variation / uncertainty in the \\(Y_i\\)’s after we utilize the predictor variable \\(X\\) (i.e. “unexplained” variation).\nIf all observations fall on the fitted line, \\(SSE = 0\\). More variation in \\(Y\\) around the fitted line, the larger \\(SSE\\) is.\nSum of squares regression → Measured in terms of the deviations of the fitted \\(\\hat{Y_i}\\) around their mean \\(\\bar{Y}\\).\n\n\\[\nSSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\n\\]\n\nMeasures the variation in the \\(Y_i\\)’s that is associated with the regression line.\nIn other words, it measures the variation in the \\(Y_i\\)’s that is accounted for by the relationship between \\(Y\\) and \\(X\\) (i.e. “explained” variation). We are essentially upgrading our original prediction for \\(Y\\) from \\(\\bar{Y}\\) to now \\(\\hat{Y}\\); so this \\(\\approx\\) (conceptually) measures how much better the predictions become.\nSo, the larger \\(SSR\\) is in relation to \\(SSTO\\), the greater is the effect of the regression relation in accounting for the total variation in the \\(Y_i\\) observations.\n\n\n\nResults\nDerivation\n\n\n\n\nPartitioning individual deviations\n\n\n\n\nThe two components are:\n\nThe deviation of the fitted value \\(\\hat{Y_i}\\) around the mean \\(\\bar{Y}\\).\nThe deviation of the observation \\(Y_i\\) around the fitted regression line.\n\n\nThis relationship holds for the sum of the squared deviations as well:\n\n\\[\n\\begin{align*}\n  \\sum (Y_i - \\bar{Y})^2 &= \\sum (\\hat{Y_i} - \\bar{Y})^2 + \\sum (Y_i - \\hat{Y_i})^2\\\\\n  SSTO &= SSR + SSE\n\\end{align*}\n\\]\n\n\n\nPartitioning total sum of squares\n\n\n\nThe formulas for \\(SSTO\\), \\(SSE\\) and \\(SSR\\) above are best for computations. But an alternate form of \\(SSR\\) is useful for deriving analytical results is shown below:\n\n\\[\n\\begin{align*}\n  SSR &= \\sum [\\hat{Y_i} - \\bar{Y}]^2 \\\\\n      &= \\sum [(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i) - \\bar{Y}]^2 \\\\\n      &= \\sum [(\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 X_i - \\bar{Y}]^2 \\\\\n      &= \\hat{\\beta}_1^2 \\sum [X_i - \\bar{X}]^2\n\\end{align*}\n\\]\n\n\n\n\n2.6.2 Breakdown of degrees of freedom\nDegrees of freedom for each sum of square\n\n\n\\(SSTO\\) → Has \\(n-1\\) degrees of freedom associated with it.\n\nOne degree of freedom is lost because the deviations \\(Y_i - \\bar{Y}\\) are subject to one constraint: they must sum to zero (\\(\\sum (Y_i - \\bar{Y}) = 0\\)).\nEquivalently, one degree of freedom is lost because the sample mean \\(\\bar{Y}\\) is used to estimate the population mean.\n\n\n\n\\(SSE\\) → Has \\(n-2\\) degrees of freedom associated with it.\n\nTwo degrees of freedom are lost because the two parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated in obtaining the fitted values \\(\\hat{Y_i}\\).\n\n\n\n\\(SSR\\) → Has 1 degrees of freedom associated with it.\n\nAlthough there are \\(n\\) deviations \\(\\hat{Y_i} - \\bar{Y}\\), all fitted values are calculated from the same estimated regression line. So the regression line just has two degrees of freedom (corresponding to the slope and intercept, for SLR \\(p = 2\\))…\nBUT then, one degree of freedom is lost because again we have to estimate the mean in order to calculate \\(\\sum (\\hat{Y_i} - \\bar{Y})^2\\), thus \\(p - 1\\) \\(\\Longleftrightarrow\\) the deviations \\(\\hat{Y_i} - \\bar{Y}\\) are subject to one constraint: they must sum to zero.\n\n\n(Note: The constraints on the deviations come from the properties of the fitted LSE line.)\n\nProperty of degrees of freedom\n\nDfs are additive\n\n\\[\n\\begin{align*}\n  n - 1 &= 1 + (n-2) \\\\\n  df_{TO} &= df_{R} +df_{E}\n\\end{align*}\n\\]\n\n2.6.3 Mean squares\n\nGeneral definition → A sum of squares divided by its associated degrees of freedom is called a mean square (\\(MS\\)).\n\nTwo mean squares\n\nMean square regression:\n\n\\[MSR = \\frac{SSR}{df_R} = \\frac{SSR}{1}\\]\n\nMean square error:\n\n\\[MSE = \\frac{SSR}{df_E} = \\frac{SSE}{n-2}\\]\n\nNote → Mean squares are not additive \\(\\Longrightarrow\\) \\(MSR + MSE \\ne MSTO\\)\n\n2.6.4 ANOVA table\n\nThe breakdowns of the total sum of squares and associated degrees of freedom are displayed in the form of an analysis of variance table.\nBelow shows an extra column for expected mean squares, which will be needed for inference (usual ANOVA tables have everything except this column).\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\n\\(df\\)\n\\(SS\\)\n\\(MS\\)\n\\(E(MS)\\)\n\\(F\\)\n\n\\(p\\)-value\n\n\n\nRegression\n\\(df_R = 1\\)\n\n\\(SSR = \\sum (\\hat{Y_i} - \\bar{Y})^2\\) 1\n\\(MSR = \\frac{SSR}{1}\\)\n\\(\\sigma^2 + \\beta_1 \\sum (X_i - \\bar{X})^2\\)\n\\(\\frac{MSR}{MSE}\\)\n\\(P(F_{(df_R, df_E)} &gt; F)\\)\n\n\nError\n\\(df_E = n - 2\\)\n\\(SSE = \\sum (Y_i - \\hat{Y_i})^2\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\\(\\sigma^2\\)\n\n\n\n\nTotal\n\\(df_{TO} = n -1\\)\n\\(SSTO = \\sum (Y_i - \\bar{Y})^2\\)\n\n\n\n\n\n\n\n\nCould convert \\(SSTO = S_{YY}\\) to a mean square as well by dividing by \\(df_{TO} = n - 1\\) → This gives us \\(\\frac{1}{n - 1}{\\sum (Y_i - \\bar{Y})^2}\\) = Sample variance of \\(Y_i\\) (totally unrelated to the regression line; just the variance of a set of numbers, which is technically a mean square).\nThis is different than the regression \\(S^2 = MSE\\) which is an estimate of the error variance which is found by taking the deviations relative to the fitted line (not \\(bar{Y}\\)).\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 3; sigma &lt;- 15 \n\n# generate data\nx &lt;- runif(n = n, min = 5, max = 15)\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# now introducing another function to fit models and the different results / outputs from each\n\n# fit equivalent models\n# -&gt; lm() fits linear models\n# --&gt; also one-way anova (ancova) models, but aov() works better with anova analyses\nmod_lm &lt;- lm(y ~ x)\nmod_aov &lt;- aov(y ~ x)\n\n# the main difference between lm() and aov() is mainly in the form of the output\n\n# calling (printing) the model object\n# -&gt; for lm object gives estimated coefficients\nmod_lm\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -3.457        3.308  \n\n# -&gt; for aov object it gives breakdown of SS and df for each variable and sigma estimate\nmod_aov\n\nCall:\n   aov(formula = y ~ x)\n\nTerms:\n                       x Residuals\nSum of Squares  2797.647  3092.719\nDeg. of Freedom        1        28\n\nResidual standard error: 10.50972\nEstimated effects may be unbalanced\n\n# summary() function\n# -&gt; for lm it gives regression-style output, i.e. regression coefficients with standard errors and t-tests\nsummary(mod_lm)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.456  -7.245  -1.550   6.543  23.120 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  -3.4569     6.8870  -0.502      0.62    \nx             3.3079     0.6573   5.033 0.0000254 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.51 on 28 degrees of freedom\nMultiple R-squared:  0.475, Adjusted R-squared:  0.4562 \nF-statistic: 25.33 on 1 and 28 DF,  p-value: 0.00002536\n\n# -&gt; for aov it gives anova table,which is the same information but represented as sums of squares estimates with F ratios\nsummary(mod_aov)\n\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx            1   2798  2797.6   25.33 0.0000254 ***\nResiduals   28   3093   110.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# can switch between the two summaries by calling a summary method\n# -&gt; get anova table from lm object\nsummary.aov(mod_lm)\n\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx            1   2798  2797.6   25.33 0.0000254 ***\nResiduals   28   3093   110.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# -&gt; get regression-style output from aov object\nsummary.lm(mod_aov)\n\n\nCall:\naov(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.456  -7.245  -1.550   6.543  23.120 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  -3.4569     6.8870  -0.502      0.62    \nx             3.3079     0.6573   5.033 0.0000254 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.51 on 28 degrees of freedom\nMultiple R-squared:  0.475, Adjusted R-squared:  0.4562 \nF-statistic: 25.33 on 1 and 28 DF,  p-value: 0.00002536\n\n# alternative (more straight-forward) way to get anova table \n# -&gt; use anova() on lm object &lt;==&gt; anova.lm() -&gt; this is one of the two main uses of this function\n# -&gt; looking for Df, Sum Sq, and Mean Sq\n# --&gt; gives sequential SS (will look into more later)\nanova(mod_lm) %&gt;% as.matrix # could do anova(mod_aov), but makes for sense just to do summary() if already have aov object\n\n          Df   Sum Sq   Mean Sq  F value        Pr(&gt;F)\nx          1 2797.647 2797.6474 25.32856 0.00002535802\nResiduals 28 3092.719  110.4543       NA            NA\n\n# save usual lm model as another object to simplify notation\nmod &lt;- mod_lm\n\n\n\n\n# recreating each value in the anova table output\n\n# degrees of freedom \n# -&gt; regression: each individual coefficient gets 1 df, then lose 1; so for SLR df = 2 - 1 = 1\n# -&gt; error: df = n - # of predictors (p) as usual\ndf_e &lt;- mod$df.residual\ndf_r &lt;- length(coef(mod)) - 1\n\n# sums of squares\n# -&gt; SSR = explained error (improved prediction) -&gt; Y-hat - Y-bar\n# -&gt; SSE = still unexplained error (residuals) -&gt; Y - Y-hat\nssr &lt;- sum((fitted(mod) - mean(y))^2)\nsse &lt;- sum((y - fitted(mod))^2)\n\n# mean squares = respective SS / df\nmsr &lt;- ssr / df_r\nmse &lt;- sse / df_e\n\n# combine (organize) into anova table layout for comparison\n# -&gt; fill matrix by column\nat &lt;- matrix(data = c(c(df_r, df_e), c(ssr, sse), c(msr, mse)), nrow = 2, byrow = FALSE)\ncolnames(at) &lt;- c(\"df\", \"SS\", \"MS\")\nrownames(at) &lt;- c(\"x\", \"error\")\n\n# compare results to relevant pieces of anova(lm())\ncompare(anova(mod_lm)[,1:3], at)\n\n$comparison\n            Df Sum Sq Mean Sq\nx         TRUE   TRUE    TRUE\nResiduals TRUE   TRUE    TRUE\n\n$`anova(mod_lm)[, 1:3]`\n          Df Sum Sq Mean Sq\nx          1 2797.7 2797.65\nResiduals 28 3092.7  110.45\n\n$at\n      df       SS        MS\nx      1 2797.647 2797.6474\nerror 28 3092.719  110.4543\n\n\n\n\n\n# demonstrate additive df and SS\n\n# df total = n - 1 (have to estimate pop mean, so lose 1)\ndf_to &lt;- length(y) - 1\n\n# SSTO = total deviation (S_YY) -&gt; Y - Y-bar\nssto &lt;- sum((y - mean(y))^2)\n\n# compare pieces from anova to calculated totals\ncompare(c(sum(anova(mod)[, \"Df\"]), sum(anova(mod)[, \"Sum Sq\"])), c(df_to, ssto))\n\n$comparison\n[1] TRUE TRUE\n\n$`c(sum(anova(mod)[, \"Df\"]), sum(anova(mod)[, \"Sum Sq\"]))`\n[1]   29.000 5890.367\n\n$`c(df_to, ssto)`\n[1]   29.000 5890.367\n\n\n\n\n\n\n2.6.5 Expected mean squares\nGoal → In order to make inferences based on the analysis of variance approach, we need to know the expected value of each of the mean squares.\n\n\nResults\nDerivation\n\n\n\n\nThe expected value of a mean square is the mean of its sampling distribution and tells us what is being estimated by the mean square.\nIt can be shown that:\n\n\\[\n\\begin{align*}\n  E(MSE) &= \\sigma^2 \\\\\n  E(MSR) &= \\sigma^2 + \\beta_1^2 \\sum (X_i - \\bar{X})^2\n\\end{align*}\n\\]\n\nNote that the first result goes with earlier statement that \\(MSE\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nImportant implications\n\nThe mean of the sampling distribution of \\(MSE\\) is \\(\\sigma^2\\) whether or not \\(Y\\) and \\(X\\) are linearly related (i.e. whether or not \\(\\beta_1 = 0\\)).\nThe mean of the sampling distribution of \\(MSR\\) is also \\(\\sigma^2\\) when \\(\\beta_1 = 0\\).\n\n\nThus, when \\(\\beta_1 = 0\\), the sampling distributions of \\(MSE\\) and \\(MSR\\) are located identically and \\(MSE\\) and \\(MSR\\) will tend to be relatively close to each other.\nBut, when \\(\\beta_1 \\ne 0\\), the mean of the sampling distribution of \\(MSR\\) will be greater than \\(\\sigma^2\\) (because \\(\\beta_1^2 \\sum (X_i - \\bar{X})^2\\) then must be positive) and therefore located to the right of that of \\(MSE\\). So, \\(MSR\\) will tend to be larger than \\(MSE\\).\nThese results suggest that a comparison of \\(MSR\\) and \\(MSE\\) is useful for testing whether or not \\(\\beta_1 = 0\\).",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#f-test-of-beta_1-0-vs-beta_1-ne-0",
    "href": "notes-inference.html#f-test-of-beta_1-0-vs-beta_1-ne-0",
    "title": "2  Inference",
    "section": "\n2.7 \\(F\\) test of \\(\\beta_1 = 0\\) vs \\(\\beta_1 \\ne 0\\)\n",
    "text": "2.7 \\(F\\) test of \\(\\beta_1 = 0\\) vs \\(\\beta_1 \\ne 0\\)\n\nOverview\n\nThe analysis of variance approach lets us perform very useful test for regression models (and other linear statistical models).\n\n\n\nResults\nDerivation\n\n\n\nTest on slope\n\nHypotheses → For the SLR, ANOVA gives us a test for:\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\\\\n  H_A &: \\beta_1 \\ne 0\n\\end{align*}\n\\]\n\nTest statistic → For ANOVA, the test statistic \\(F^*\\) compares \\(MSR\\) and \\(MSE\\)\n\n\n\\[\nTS = F^* = \\frac{MSR}{MSE}\n\\]\n\n(see derivation) Under \\(H_0: \\beta_1 = 0\\) → \\(F^* \\sim \\text{F}\\,_{(1, n-2)}\\)\n\nRejection region and p-value\n\n\n\\(F^*\\) values near 1 support \\(H_0\\) and large \\(F^*\\) values support \\(H_A\\) (if the model is useful, we expect \\(MSR\\) to be large compared to \\(MSE\\)) \\(\\Longrightarrow\\) Upper-tailed test.\n\n\n\n\\[\n\\begin{align*}\n  RR &= \\{F^* &gt; F_{(1 - \\alpha;\\, 1, n - 2)}\\} \\\\\n  p\\text{-value} &= P(F_{(1, n-2)} \\ge F^*)\n\\end{align*}\n\\]\n\nNote → \\(F_{(1-\\alpha;\\,1,n-2)}\\) is the \\(100 (1-\\alpha)\\) percentile of the appropriate \\(F\\) distribution (different notation meaning than \\(t_{\\alpha / 2}\\) because not a symmetric distribution now).\n\nDecision → Same rules as usual, now just with \\(F\\)-distribution\n\nReject \\(H_0\\) and conclude \\(H_A\\) if \\(\\hspace{10pt}\\) \\(TS \\in RR \\hspace{10pt} \\Longleftrightarrow \\hspace{10pt} p\\text{-value} \\le \\alpha\\); Fail to reject \\(H_0\\) if previous not true.\n\n\n\nConclusion / Interpretation\n\nAt the \\(\\alpha\\) significance level, we &lt; have / do not have &gt; sufficient evidence of a significant linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;.\n\n\n\nEquivalence of \\(F\\) test and \\(t\\) test\n\n\nThe \\(F\\)-test is algebraically equivalent to the two-tailed \\(t\\) test → \\(F^* = (t^*)^2\\)\n\nSame relationship for the critical values when defining the rejection region / p-value → \\(F_{(1-\\alpha; \\, 1, n-2)} = t_{\\alpha/2, n-2}^2\\).\n\n\nWill get same conclusion either way, but the \\(t\\) test is more flexible because it can be used for one-sided alternatives involving \\(\\beta_1\\), while the \\(F\\) test cannot.\n\n\n\nSampling distribution of \\(F^*\\)\n\nGoal → In order to be able to construct a statistical decision rule and examine its properties, we need to know the sampling distribution of \\(F^*\\).\n\nDerivation → Start by considering the sampling distribution of \\(F^*\\) under \\(H_0: \\beta_1 = 0\\). We will use the following theorem:\n\n\nCochran’s theorem → Let \\(Y_1, \\ldots, Y_{n}\\) represent a random sample from the same normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Suppose \\(SSTO = \\sum (Y_i - \\bar{Y})^2\\) is partitioned into \\(k\\) sums of squares \\(SS_r\\), each with degrees of freedom \\(df_r\\). If \\(\\displaystyle \\sum_{r=1}^k df_{r} = n - 1\\), then each of the \\(\\frac{SS_r}{\\sigma^2}\\) terms are independent \\(\\chi^2\\) random variables with \\(df_r\\) degrees of freedom.\n\n\n\n\n\n\nUnder \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\\(F^* \\sim \\text{Non-central F}\\,_{(1, n - 2)}\\) with non-centrality parameter \\(\\lambda\\).\nStill \\(SSR \\perp \\!\\!\\! \\perp SSE\\) and \\(SSE / \\sigma^2 \\sim \\chi^2_{n-2}\\). But the condition that both \\(SSR / \\sigma^2\\) and \\(SSE / \\sigma^2\\) are \\(\\chi^2\\) random variables requires \\(\\beta_1 = 0\\).\n\n\n\nEquivalence of \\(F\\) test and \\(t\\) test\n\n\n\n\nDemo\n\n\nR functions\nManual\n\n\n\n\n# continuing previous anova table demo\n\n# show anova table\n# -&gt; looking for F value` and Pr(&gt;F)\nanova(mod) %&gt;% as.matrix\n\n          Df   Sum Sq   Mean Sq  F value        Pr(&gt;F)\nx          1 2797.647 2797.6474 25.32856 0.00002535802\nResiduals 28 3092.719  110.4543       NA            NA\n\n# equivalence of F test and two sided t-test\ncompare(anova(mod)[\"x\",\"F value\"], summary(mod)$coefficients[\"x\",\"t value\"]^2)\n\n$comparison\n[1] TRUE\n\n$`anova(mod)[\"x\", \"F value\"]`\n[1] 25.32856\n\n$`summary(mod)$coefficients[\"x\", \"t value\"]^2`\n[1] 25.32856\n\n\n\n\n\n# continuing to recreate each value in the anova table output\n\n# calculate F and p-value\n# -&gt; TS F* = MSR / MSE\n# -&gt; p-value = P(F(df reg, df error) &gt; F*)\nF_star &lt;- msr / mse\np_value &lt;- pf(q = F_star, df1 = df_r, df2 = df_e, lower.tail = FALSE)\n\n# compare results to relevant pieces of anova(lm())\ncompare(anova(mod_lm)[1,4:5], c(F_star, p_value))\n\n$comparison\n  F value Pr(&gt;F)\nx    TRUE   TRUE\n\n$`anova(mod_lm)[1, 4:5]`\n  F value     Pr(&gt;F)    \nx  25.329 0.00002536 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`c(F_star, p_value)`\n[1] 25.32855920502  0.00002535802",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#general-linear-test-approach",
    "href": "notes-inference.html#general-linear-test-approach",
    "title": "2  Inference",
    "section": "\n2.8 General linear test approach",
    "text": "2.8 General linear test approach\nOverview\n\n\nThe ANOVA \\(F\\) test above is an example of a General Linear Test (GLT) (also called a global or an omnibus test) for a statistical model, which is an approach that can be used for highly complex tests of linear statistical models, as well as for simple tests.\n\nFor SLR, the global test (the significance of a model test), the ANVOA \\(F\\) test, and the \\(t\\) test for the linear impact are all equivalent.\n\n\n\nIt has three basic steps, which are described in more detail below:\n\nFit the full model and obtain the error sum of squares \\(SSE(F)\\).\nFit the reduced model under Ho and obtain the error sum of squares \\(SSE(R)\\).\nUse test statistic / p-value to make decision.\n\n\n\nFull model\n\n\nStart with the model considered to be appropriate for the data (or the model with all available predictors); this is called the full / unrestricted model.\n\nFor SLR, the full model is just the normal error regression model:\n\n\n\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\hspace{20pt} \\text{Full model}\n\\]\n\nWe need to fit the full model and get the error sum of squares, denoted \\(SSE(F)\\) (deviations of \\(Y_i\\) and its estimated expected value \\(\\hat{Y_i}\\), which is the fitted regression line). For the full model, we have:\n\n\\[\nSSE(F) = \\sum (Y_i - \\hat{Y_i})^2 = \\sum [Y_i - (\\hat{\\beta}_0  + \\hat{\\beta}_1 X_i)]^2 = SSE\n\\]\n\n\nWith a SLR full model, the error sum of squares is the usual \\(SSE\\).\n\nReduced model\n\nNext we consider \\(H_0\\). For SLR, we have:\n\n\\[\n\\begin{align*}\n  H_0 &: \\beta_1 = 0 \\hspace{20pt} \\text{Reduced model is appropriate} \\\\\n  H_A &: \\beta_1 \\ne 0 \\hspace{20pt} \\text{Full model is appropriate}\n\\end{align*}\n\\]\n\nThe model under \\(H_0\\) is called the reduced / restricted model. When \\(\\beta_1 = 0\\), the full model reduces to:\n\n\\[\nY_i = \\beta_0 + 0 \\cdot X_i + \\epsilon_i = \\beta_0 + \\epsilon_i \\hspace{20pt} \\text{Reduced model}\n\\]\n\n\nThen we fit the reduced model and again get the error sum of squares, now denoted \\(SSE(R)\\).\n\n\nFor this particular (a SLR) reduced model, it can easily be shown that the LSE and MLE estimator of \\(\\beta_0\\) is \\(\\bar{Y}\\). Thus the estimated expected value of each observation is \\(\\hat{\\beta}_0 = \\bar{Y}\\) and we can get the error sum of squares with:\n\n\n\n\\[\nSSE(R) = \\sum (Y_i - \\hat{Y_i})^2 = \\sum (Y_i - \\hat{\\beta}_0)^2 = \\sum (Y_i - \\bar{Y})^2 = SSTO\n\\]\n\nThus, for any intercept-only reducted model \\(\\Longrightarrow\\) \\(SSE(R) = SSTO\\).\n\nTest statistic and decision\n\n\nLogic → Now we compare the two sum of squares from the full and reduced model using the fact that the reduced \\(SSE\\) is always greater than or equal to the full \\(SSE\\).\n\n\n\\(SSE(R) \\ge SSE(F)\\) → More parameters in the model ALWAYS leads to a better fit (i.e. less unexplained variability = more explained variability) \\(\\Longrightarrow\\) Smaller are the deviations around the fitted regression function.\n\n\n\nComparison scenarios\n\n\n\\(SSE(F)\\) are close \\(SSE(R)\\)\n\n\n\nUsing the full model does not account for much more unexplained variability than does the reduced model \\(\\Longrightarrow\\) Added parameters in the full model do not really help to reduce the unexplained variation \\(\\Longrightarrow\\) Reduced model is adequate and \\(H_0\\) holds.\n\n\n\\(SSE(R) &lt;&lt; SSE(F)\\)\n\n\nThe additional parameters in the full model do help to substantially reduce the unexplained variability in \\(Y_i\\), which means \\(H_A\\) holds.\n\n\nTest statistic is a function of the difference in two \\(SSE\\)s, relative to the full \\(SSE\\):\n\n\\[\nTS = F^* = \\frac{SSE(R) - SSE(F)}{df_R - df_F} \\Big/ \\frac{SSE(F)}{df_F} = \\frac{SSE(R) - SSE(F)}{df_R - df_F}  \\Big/ MSE(F)\n\\]\n\nUnder \\(H_0: \\beta_1 = 0 \\hspace{10pt} \\text{Reduced model}\\) → \\(F^* \\sim \\text{F}\\,_{(df_R - df_F, df_F)}\\)\n\nRejection region and p-value\n\nAgain, we reject for large values of \\(F^*\\) (large difference \\(SSE(R) - SSE(F)\\) supports \\(H_A\\)).\n\n\n\n\\[\n\\begin{align*}\n  RR &= \\{F^* &gt; F_{(1 - \\alpha;\\, df_R - df_F, \\, df_F)}\\} \\\\\n  p\\text{-value} &= P(F_{(df_R - df_F, \\, df_F} \\ge F^*)\n\\end{align*}\n\\]\n\nFor SLR (testing whether or not \\(\\beta_1 = 0\\)), we have:\n\n\\[\n\\begin{align*}\n  SSE(R) &= SSTO \\hspace{20pt} SSE(F) = SSE \\\\\n  f_R &= n - 1 \\hspace{45pt} df_F = n - 2\n\\end{align*}\n\\]\n\nSo the test statistic becomes\n\n\\[\nF^* = \\frac{SSTO - SSE}{(n - 1) - (n - 2)} \\Big/ \\frac{SSE}{n - 2} = \\frac{SSR}{1} \\Big/ \\frac{SSE}{n - 2} = \\frac{MSR}{MSE}\n\\]\n\nThis is equivalent to the ANOVA \\(F\\) test shown earlier.\n\n\n\nR functions\nManual\n\n\n\n\n# using the same data as anova table demos\n\n# fit full model\n# -&gt; for SLR, full model is E(Y) = B0 + B1 X\nmod_full &lt;- lm(y ~ x)\n\n# fit reduced model\n# -&gt; for SLR, full model is E(Y) = B0\n# -&gt; to fit an intercept-only model, specify 1 on the RHS\nmod_reduced &lt;- lm(y ~ 1)\n\n# perform general linear test of reduced vs full model\n# -&gt; testing H0: beta_1 = 0 (reduced model) vs HA: beta_1 != 0 (full model)\n# function call\n# -&gt; anova() on multiple lm objects &lt;==&gt; anova.lmlist() -&gt; this is the second main uses of this function\n# -&gt; typically will supply models smallest to largest (this makes for a natural interpretation of the results); but it works regardless\n# -&gt; models MUST BE nested AND fit on the same dataset in order for results to make statistical sense\n# results\n# -&gt; sequentially gives the change in dfs and SS from mod 1 to mod 2 (then from mod 2 to mod 3, and so on...)\n# -&gt; so it is df_1 - df_2 and SS_1 - SS_2 ==&gt; if nested from smallest to largest model, results will all be positive and interpreted as \"additional reductions\"\n# -&gt; by default, performs F test comparing models in the order specified (again sequentially)\nanova(mod_reduced, mod_full, test = \"F\") %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq        F        Pr(&gt;F)\n1     29 5890.367 NA        NA       NA            NA\n2     28 3092.719  1  2797.647 25.32856 0.00002535802\n\n# demo with more than two models\nmod_squared &lt;- lm(y ~ x + I(x^2))\nanova(mod_reduced, mod_full, mod_squared) %&gt;% as.matrix\n\n  Res.Df      RSS Df  Sum of Sq          F        Pr(&gt;F)\n1     29 5890.367 NA         NA         NA            NA\n2     28 3092.719  1 2797.64739 24.6187028 0.00003375526\n3     27 3068.256  1   24.46354  0.2152739 0.64638394821\n\n# demo showing how the sequential comparison works\n# -&gt; above compared mod 1 to mod 2 and then mod 2 to mod 3\n# -&gt; now comparing mod 1 directly to mod 3\nanova(mod_reduced, mod_squared) %&gt;% as.matrix\n\n  Res.Df      RSS Df Sum of Sq        F       Pr(&gt;F)\n1     29 5890.367 NA        NA       NA           NA\n2     27 3068.256  2  2822.111 12.41699 0.0001500081\n\n\n\n\n\n# general linear test of reduced vs full model\n\n# using the same data as anova table demos\n\n# for full model -&gt;  get the corresponding SSE and df \n# -&gt; for SLR, full model is E(Y) = B0 + B1 X\nsse_f &lt;- resid(mod_full)^2 %&gt;% sum\ndf_f &lt;- df.residual(mod_full)\n\n# for reduced model -&gt; get the corresponding SSE and df\nsse_r &lt;- resid(mod_reduced)^2 %&gt;% sum\ndf_r &lt;- df.residual(mod_reduced)\n\n# calculate F stat and p-value\n# -&gt; using the shortcut for second term: F* = [(SSE(R) - SSE(F)) / (df_R - df_F)] / MSE(F)\nmse_f &lt;- summary(mod_full)$sigma^2\nF_star &lt;- ((sse_r - sse_f) / (df_r - df_f)) / mse_f\np_value &lt;- pf(q = F_star, df1 = df_r - df_f, df2 = df_f, lower.tail = FALSE)\n\n# combine (organize) into anova table layout for comparison\nat &lt;- matrix(data = c(c(df_r, df_f), c(sse_r, sse_f), c(NA, df_r - df_f), c(NA, sse_r - sse_f), c(NA, F_star), c(NA, p_value)), nrow = 2, byrow = FALSE)\ncolnames(at) &lt;- c(\"df_E Original\", \"SSE Original\", \"Change df\", \"Change SSE\", \"F*\", \"p-value\")\nrownames(at) &lt;- c(\"Mod 1 - Reduced\", \"Mod 2 - Full\")\n\n# compare to results from anova(reduced mod, full model)\ncompare(anova(mod_reduced, mod_full), at)\n\n$comparison\n  Res.Df  RSS   Df Sum of Sq    F Pr(&gt;F)\n1   TRUE TRUE   NA        NA   NA     NA\n2   TRUE TRUE TRUE      TRUE TRUE   TRUE\n\n$`anova(mod_reduced, mod_full)`\nAnalysis of Variance Table\n\nModel 1: y ~ 1\nModel 2: y ~ x\n  Res.Df    RSS Df Sum of Sq      F     Pr(&gt;F)    \n1     29 5890.4                                   \n2     28 3092.7  1    2797.7 25.329 0.00002536 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$at\n                df_E Original SSE Original Change df Change SSE       F*\nMod 1 - Reduced            29     5890.367        NA         NA       NA\nMod 2 - Full               28     3092.719         1   2797.647 25.32856\n                      p-value\nMod 1 - Reduced            NA\nMod 2 - Full    0.00002535802\n\n# for SLR ==&gt; equivalent to F test on beta 1\ncompare(anova(mod_reduced, mod_full)[2, \"F\"], anova(mod_full)[\"x\", \"F value\"])\n\n$comparison\n[1] TRUE\n\n$`anova(mod_reduced, mod_full)[2, \"F\"]`\n[1] 25.32856\n\n$`anova(mod_full)[\"x\", \"F value\"]`\n[1] 25.32856",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#descriptive-measures-of-linear-association-between-x-and-y",
    "href": "notes-inference.html#descriptive-measures-of-linear-association-between-x-and-y",
    "title": "2  Inference",
    "section": "\n2.9 Descriptive measures of linear association between \\(X\\) and \\(Y\\)\n",
    "text": "2.9 Descriptive measures of linear association between \\(X\\) and \\(Y\\)\n\nOverview\n\nThere is no one single measure to completely describe the usefulness of a regression model for a particular application.\nIf the goal is estimation of parameters and means and predicting new observations, usefulness of estimates or predictions depends upon the width of the interval and the user’s needs for precision. This can vary from one application to another.\nRather than making inferences, goals could be to describe the degree of linear association between \\(Y\\) and \\(X\\). Again the usefulness of which measure and its value depend on the application.\n\n\n2.9.1 Coefficient of determination\nOverview\n\nA very common measure because of its simplicity is the coefficient of determination \\(R^2\\), which is a measure of the effect of \\(X\\) in reducing the uncertainty in predicting \\(Y\\). This reduction in sum of squares (\\(SSTO - SSE = SSR\\)) gets expressed as a proportion:\n\n\\[\nR^2 = \\frac{SSR}{SSTO} = 1-\\frac{SSE}{SSTO}, \\hspace{20pt} \\text{range:} \\hspace{10pt} 0 \\le R^2 \\le 1\n\\] Interpretation\n\n\n&lt; \\(R^2 *100\\) &gt;% of the variation in &lt; \\(Y\\) context &gt; can be explained by the linear relationship between &lt; \\(Y\\) context &gt; and &lt; \\(X\\) context &gt;.\n\nSo, the larger \\(R^2\\) is, the more the total variation of \\(Y\\) is reduced by introducing the predictor variable \\(X\\) \\(\\Longleftrightarrow\\) greater degree of linear association between \\(Y\\) and \\(X\\).\n\n\nPractically, this indicates the quality of the fit by measuring the proportion of variability explained by the fitted model.\n\nLimiting values\n\nWhen the fitted regression line is horizontal (\\(\\hat{\\beta}_1 = 0\\) and \\(\\hat{Y_i} = \\bar{Y}\\)) \\(\\Longrightarrow\\) \\(SSE = SSTO\\) and \\(R^2 = 0\\).\nWhen there is a perfect fit (all of the points lie on the fitted regression line) \\(\\Longrightarrow\\) \\(SSE = 0\\) and \\(R^2 = 1\\).\nIn practice, unlikely to be exactly equal to either of these. Also note that context of data (scientific field of the application) has a big impact on general values of \\(R^2\\) and consequently what is interpreted as “strong”.\nIf \\(R^2\\) is small, we can consider adding other independent variables that can explain a significant portion of the remaining unexplained variability in the model.\n\n\n\nLimitations of \\(R^2\\)\n\n\nUsefulness in prediction → A high coefficient of determination does not necessarily indicate that useful (precise) predictions can be made.\n\nThis is because \\(R^2\\) measures only a relative reduction from \\(SSTO\\) and provides no information about absolute precision for estimating a mean response or predicting a new observation.\n\n\n\nQuality of fit → A high coefficient of determination does not necessarily indicate that the estimated regression line is a good fit and similarly an \\(R^2\\) near zero does not necessarily indicate that \\(Y\\) and \\(X\\) are not related.\n\nThis is because \\(R^2\\) measures the degree of linear association between \\(Y\\) and \\(X\\), whereas the actual regression relation may be curvilinear. So make sure to look at the scatterplot.\n\n\n\n\nInflating \\(R^2\\)\n\n\n\\(X\\) level spacing → The value taken by \\(R^2\\) in a given sample tends to be affected by the spacing of the \\(X\\) observations: Wider spacing in \\(X_i\\) \\(\\Longrightarrow\\) higher \\(R^2\\). Here’s why:\n\nWider spacing (larger spread) in \\(X_i\\) in the sample when \\(\\hat{\\beta}_1 \\ne 0\\) \\(\\Longrightarrow\\) larger spread of the observed \\(Y_i\\) around \\(\\bar{Y}\\) \\(\\Longrightarrow\\) Larger \\(SSTO\\).\nAnd since \\(SSE\\) is unaffected (\\(V(Y_i) = \\sigma^2\\) for all \\(X_i\\)), \\(SSR\\) has to increase. Then we can see from either representation \\(R_2 = SSR/ SSTO = 1 - SSE / SSTO\\), that \\(R^2\\) will increase.\n\nOverfitting → \\(R^2\\) can be artificially inflated by including additional model terms (adding extra predictors).\n\nThis is because \\(SSR\\) always increases with more predictors, even if they are completely unrelated to the response variable.\nLater \\(R^2_{adj}\\) will be discussed which corrects for the inclusion of extra predictors.\n\n\n\n\n\nDemo\n\n\nR functions\nManual\nProperties\n\n\n\n\nCode# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 5; sigma &lt;- 10\n\n# generate data\nx &lt;- runif(n = n, min = 5, max = 15)\ny &lt;- rnorm(n = n, mean = beta_0 + beta_1 * x, sd = sigma)\n\n\n\n# fit model\nmod &lt;- lm(y ~ x)\n\n# display summary\n# -&gt; looking for Multiple R-squared\nsummary(mod)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.481  -7.421   0.067   9.760  24.381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7649     9.6903   1.214 0.234854    \nx             3.9357     0.9168   4.293 0.000191 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.4 on 28 degrees of freedom\nMultiple R-squared:  0.3969,    Adjusted R-squared:  0.3754 \nF-statistic: 18.43 on 1 and 28 DF,  p-value: 0.0001909\n\n\n\n# plot x and y with regression line\n# -&gt; add annotation for R^2 value\nplot(x = x, y = y)\nabline(mod, col = \"red\")\ntext(x = 14, y = 30, labels = bquote(R^2 * \" = \" * .(round(summary(mod)$r.squared,3))), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n# get anova table for model\nat &lt;- anova(mod)\nas.matrix(at)\n\n          Df   Sum Sq   Mean Sq  F value       Pr(&gt;F)\nx          1 3306.803 3306.8030 18.42694 0.0001908607\nResiduals 28 5024.734  179.4548       NA           NA\n\n# calculate R^2 from sums of squares from anova table\n# R^2 = SSR / SSTO = SSR / (SSR + SSE) = 1 - SSE / SSTO\nssr &lt;- at[\"x\", \"Sum Sq\"]\nsse &lt;- at[\"Residuals\", \"Sum Sq\"]\nr_squared &lt;- ssr / (ssr + sse)\nr_squared &lt;- 1 - sse / (ssr + sse)\n\n# compare results from summary(lm())\ncompare(summary(mod)$r.squared, r_squared)\n\n$comparison\n[1] TRUE\n\n$`summary(mod)$r.squared`\n[1] 0.3969019\n\n$r_squared\n[1] 0.3969019\n\n\n\n\n\n# demo to show how more spacing of X levels increases R^2 value\n\n# initialize items\n# -&gt; sample size, population parameters and error variance\nn &lt;- 30; beta_0 &lt;- 2; beta_1 &lt;- 3; sigma &lt;- 5 \n\n# generate two datasets\n# -&gt; generate common error terms, so the only difference is the spacing of the X levels\n# -&gt; generate two X vectors, one with larger spread of X values\n# -&gt; calculate Y\n# -&gt; add indicator for spread and then combine into one dataframe\nepsilon &lt;- rnorm(n = n, mean = 0, sd = sigma)\ndata_samples &lt;- list(runif(n = n, min = 8, max = 12),\n                     runif(n = n, min = 2, max = 16)) %&gt;% \n  map(\\(x) data.frame(x = x,\n                      y = beta_0 + beta_1 * x + epsilon)) %&gt;% \n  map2(c(\"smaller\", \"larger\"), \\(df, spread) mutate(df, spread = spread)) %&gt;% \n  bind_rows\n\n# confirm same error terms\n# -&gt; calculate epsilon as Y - E(Y) and then add observation number column to sort by\ndata_samples %&gt;% \n  mutate(epsilon = y - (beta_0 + beta_1 * x)) %&gt;% \n  mutate(observation = 1:n, .by = spread) %&gt;% \n  arrange(observation) %&gt;% \n  head(n = 6) %&gt;% \n  display_nice\n\n\n\n x \n    y \n    spread \n    epsilon \n    observation \n  \n\n\n 8.809 \n    25.438 \n    smaller \n    -2.988 \n    1 \n  \n\n 9.987 \n    28.973 \n    larger \n    -2.988 \n    1 \n  \n\n 10.771 \n    40.676 \n    smaller \n    6.363 \n    2 \n  \n\n 5.397 \n    24.555 \n    larger \n    6.363 \n    2 \n  \n\n 8.808 \n    30.112 \n    smaller \n    1.687 \n    3 \n  \n\n 14.214 \n    46.330 \n    larger \n    1.687 \n    3 \n  \n\n\n\n# calculate R^2 for each spread dataset\ndata_samples %&gt;% \n  split(.$spread) %&gt;% \n  map_dbl(\\(df) summary(lm(y ~ x, data = df))$r.squared) %&gt;% \n  round(3)\n\n larger smaller \n  0.843   0.348 \n\n# create plot of both sets of points and regression line\n# -&gt; add reference line for Y-bar for each dataset (to show how SSTO is calculated)\nggplot(aes(x = x,\n           y = y,\n           color = spread),\n       data = data_samples) + \n  geom_point() + \n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE,\n              fullrange = TRUE) + \n  geom_hline(aes(yintercept = y_bar,\n                 color = spread),\n             data = summarize(data_samples, y_bar = mean(y), .by = spread),\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n2.9.2 Coefficient of correlation\n\nA measure of linear association between \\(Y\\) and \\(X\\) when both \\(Y\\) and \\(X\\) are random is the coefficient of correlation. This is the signed square root of \\(R^2\\):\n\n\\[\nr = \\pm \\sqrt{R^2}\n\\]\n\nA plus or minus sign is attached to this measure according to whether the slope of the fitted regression line is positive or negative. Thus, \\(-1 \\le r \\le 1\\).\nONLY in SLR can the coefficient of determination \\(R^2\\) be computed as the square of the correlation coefficient \\(r^2\\).\n\n\n# continuing previous example\n\n# calculate correlation\nr &lt;- cor(x, y)\n\n# compare to squared-correlation to R^2\ncompare(summary(mod)$r.squared, r^2)\n\n$comparison\n[1] TRUE\n\n$`summary(mod)$r.squared`\n[1] 0.3969019\n\n$`r^2`\n[1] 0.3969019\n\n\n\nNote → Regression models do not contain a parameter to be estimated by \\(R^2\\) or \\(r\\). These are simply descriptive measures of the degree of linear association between \\(Y\\) and \\(X\\) in the sample observations that may, or may not, be useful in any instance.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "notes-inference.html#considerations-in-applying-regression-analysis",
    "href": "notes-inference.html#considerations-in-applying-regression-analysis",
    "title": "2  Inference",
    "section": "\n2.10 Considerations in applying regression analysis",
    "text": "2.10 Considerations in applying regression analysis\nReminders when implementing regression models\n\nFrequently, regression analysis is used to make inferences for the future (e.g. a school board wants to predict future enrollments by using a regression model containing several demographic variables as predictor variables). In these situations, the validity of the regression application depends on whether basic conditions in the future will be similar to those at the time the regression analysis is based on.\nIn predicting new observations on \\(Y\\), the predictor variable \\(X\\) itself often has to be predicted. Therefore, predictions are dependent upon the correctness of the population projection (i.e. they are conditional predictions).\nBe careful of extrapolation. We cannot be sure that the regression function that fits the past data is appropriate over a wider range of the predictor variable.\nA statistical test that concludes \\(\\beta_1 \\ne 0\\) does not establish a cause-and-effect relation between the predictor and response variables. With nonexperimental data, both the \\(X\\) and \\(Y\\) variables may be simultaneously influenced by other variables not in the regression model. On the other hand, the existence of a regression relation in controlled experiments is often good evidence of a cause-and-effect relation.\nWe frequently wish to estimate several mean responses or predict several new observations for different levels of the predictor variable; this causes some special problems to arise. The confidence coefficients for the limits for estimating a mean response and for the prediction limits for a new observation only for a single level of \\(X\\) for a given sample.\nWhen observations on the predictor variable \\(X\\) are subject to measurement errors, the resulting parameter estimates are generally no longer unbiased.",
    "crumbs": [
      "Simple linear regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference</span>"
    ]
  }
]