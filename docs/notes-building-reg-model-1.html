<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>8&nbsp; Building the regression model 1 – Model selection and validation – Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./notes-reg-models-quan-and-qual.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part2-mlr.html">Multiple linear regression</a></li><li class="breadcrumb-item"><a href="./notes-building-reg-model-1.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Building the regression model 1 – Model selection and validation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part2-mlr.html">Multiple linear regression</a></li><li class="breadcrumb-item"><a href="./notes-building-reg-model-1.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Building the regression model 1 – Model selection and validation</span></a></li></ol></nav>
      <div class="quarto-title-block"><div><h1 class="title">
<span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Building the regression model 1 – Model selection and validation</span>
</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Regression</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part1-slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simple linear regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-diagnostics-and-remedial-measures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Diagnostics and remedial measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-matrix-slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Matrix approach to SLR</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part2-mlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiple linear regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-multiple-regression-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple regression 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-multiple-regression-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multiple regression 2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-reg-models-quan-and-qual.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Regression models for quantitative and qualitative predictors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-building-reg-model-1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Building the regression model 1 – Model selection and validation</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#overview-of-the-model-building-process" id="toc-overview-of-the-model-building-process" class="nav-link active" data-scroll-target="#overview-of-the-model-building-process"><span class="header-section-number">8.1</span> Overview of the model building process</a>
  <ul class="collapse">
<li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection"><span class="header-section-number">8.1.1</span> (1) Data collection</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation"><span class="header-section-number">8.1.2</span> (1) Data preparation</a></li>
  <li><a href="#preliminary-model-investigation" id="toc-preliminary-model-investigation" class="nav-link" data-scroll-target="#preliminary-model-investigation"><span class="header-section-number">8.1.3</span> (2) Preliminary model investigation</a></li>
  <li><a href="#reduction-of-explanatory-variables" id="toc-reduction-of-explanatory-variables" class="nav-link" data-scroll-target="#reduction-of-explanatory-variables"><span class="header-section-number">8.1.4</span> (2) Reduction of explanatory variables</a></li>
  <li><a href="#model-refinement-and-selection" id="toc-model-refinement-and-selection" class="nav-link" data-scroll-target="#model-refinement-and-selection"><span class="header-section-number">8.1.5</span> (3) Model refinement and selection</a></li>
  <li><a href="#model-validation" id="toc-model-validation" class="nav-link" data-scroll-target="#model-validation"><span class="header-section-number">8.1.6</span> (4) Model validation</a></li>
  <li><a href="#overfitting-simulation" id="toc-overfitting-simulation" class="nav-link" data-scroll-target="#overfitting-simulation"><span class="header-section-number">8.1.7</span> Overfitting simulation</a></li>
  <li><a href="#demo" id="toc-demo" class="nav-link" data-scroll-target="#demo"><span class="header-section-number">8.1.8</span> Demo</a></li>
  </ul>
</li>
  <li>
<a href="#criteria-for-model-selection" id="toc-criteria-for-model-selection" class="nav-link" data-scroll-target="#criteria-for-model-selection"><span class="header-section-number">8.2</span> Criteria for model selection</a>
  <ul class="collapse">
<li><a href="#r2_p-or-sse_p-criterion" id="toc-r2_p-or-sse_p-criterion" class="nav-link" data-scroll-target="#r2_p-or-sse_p-criterion"><span class="header-section-number">8.2.1</span> <span class="math inline">\(R^2_p\)</span> or <span class="math inline">\(SSE_p\)</span> criterion</a></li>
  <li><a href="#r2_ap-or-mse_p-criterion" id="toc-r2_ap-or-mse_p-criterion" class="nav-link" data-scroll-target="#r2_ap-or-mse_p-criterion"><span class="header-section-number">8.2.2</span> <span class="math inline">\(R^2_{a,p}\)</span> or <span class="math inline">\(MSE_p\)</span> criterion</a></li>
  <li><a href="#mallows-c_p-criterion" id="toc-mallows-c_p-criterion" class="nav-link" data-scroll-target="#mallows-c_p-criterion"><span class="header-section-number">8.2.3</span> Mallow’s <span class="math inline">\(C_p\)</span> criterion</a></li>
  <li><a href="#aic_p-and-bic_p-criterion" id="toc-aic_p-and-bic_p-criterion" class="nav-link" data-scroll-target="#aic_p-and-bic_p-criterion"><span class="header-section-number">8.2.4</span> <span class="math inline">\(AIC_p\)</span> and <span class="math inline">\(BIC_p\)</span> criterion</a></li>
  <li><a href="#press_p-criterion" id="toc-press_p-criterion" class="nav-link" data-scroll-target="#press_p-criterion"><span class="header-section-number">8.2.5</span> <span class="math inline">\(PRESS_p\)</span> criterion</a></li>
  </ul>
</li>
  <li>
<a href="#automatic-search-procedures-for-model-selection" id="toc-automatic-search-procedures-for-model-selection" class="nav-link" data-scroll-target="#automatic-search-procedures-for-model-selection"><span class="header-section-number">8.3</span> Automatic search procedures for model selection</a>
  <ul class="collapse">
<li><a href="#best-subsets-algorithms" id="toc-best-subsets-algorithms" class="nav-link" data-scroll-target="#best-subsets-algorithms"><span class="header-section-number">8.3.1</span> “Best” subsets algorithms</a></li>
  <li><a href="#stepwise-regression-methods" id="toc-stepwise-regression-methods" class="nav-link" data-scroll-target="#stepwise-regression-methods"><span class="header-section-number">8.3.2</span> Stepwise regression methods</a></li>
  <li><a href="#other-stepwise-procedures" id="toc-other-stepwise-procedures" class="nav-link" data-scroll-target="#other-stepwise-procedures"><span class="header-section-number">8.3.3</span> Other stepwise procedures</a></li>
  </ul>
</li>
  <li>
<a href="#model-validation-1" id="toc-model-validation-1" class="nav-link" data-scroll-target="#model-validation-1"><span class="header-section-number">8.4</span> Model validation</a>
  <ul class="collapse">
<li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"><span class="header-section-number">8.4.1</span> Overview</a></li>
  <li><a href="#collection-of-new-data-to-check-model" id="toc-collection-of-new-data-to-check-model" class="nav-link" data-scroll-target="#collection-of-new-data-to-check-model"><span class="header-section-number">8.4.2</span> Collection of new data to check model</a></li>
  <li><a href="#comparison-with-theory-empirical-evidence-or-simulation-results" id="toc-comparison-with-theory-empirical-evidence-or-simulation-results" class="nav-link" data-scroll-target="#comparison-with-theory-empirical-evidence-or-simulation-results"><span class="header-section-number">8.4.3</span> Comparison with theory, empirical evidence, or simulation results</a></li>
  <li><a href="#data-splitting" id="toc-data-splitting" class="nav-link" data-scroll-target="#data-splitting"><span class="header-section-number">8.4.4</span> Data splitting</a></li>
  <li><a href="#demo-data-splitting" id="toc-demo-data-splitting" class="nav-link" data-scroll-target="#demo-data-splitting"><span class="header-section-number">8.4.5</span> Demo – Data splitting</a></li>
  <li><a href="#demo-get-best-model-from-train" id="toc-demo-get-best-model-from-train" class="nav-link" data-scroll-target="#demo-get-best-model-from-train"><span class="header-section-number">8.4.6</span> Demo – Get best model from train</a></li>
  <li><a href="#demo-validate-model" id="toc-demo-validate-model" class="nav-link" data-scroll-target="#demo-validate-model"><span class="header-section-number">8.4.7</span> Demo – Validate model</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><!-- % define LaTeX macros (/shortcuts) --><!-- % define vector (without parentheses, so when writing out in like a definition) of the form X_1, ..., X_n, where X and n are variable. NOTE: to call use $\vecn{X}{n}$ --><!-- % shortcut for ~ 'Named dist ' in normal font with space before parameters would go --><!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go --><!-- % define independence symbol (it basically makes two orthogonal symbols very close to each other. The number of \! controls the space between each of the orthogonal symbols) --><!-- % shortcut for Cov(X,Y) with formatting for Cov --><!-- % shortcut for Corr(X,Y) with formatting for Corr --><!-- % shortcut for non-italic e in math mode --><!-- % shortcut for matrix notation --><!-- % shortcut for null hypothesis formatted nicely --><!-- % shortcut for alternative hypothesis formatted nicely --><section id="overview-of-the-model-building-process" class="level2" data-number="8.1"><h2 data-number="8.1" class="anchored" data-anchor-id="overview-of-the-model-building-process">
<span class="header-section-number">8.1</span> Overview of the model building process</h2>
<p>The figure below presents a strategy for the building of a regression model. This strategy involves three or, sometimes, four phases:</p>
<ol type="1">
<li><p>Data collection and preparation</p></li>
<li><p>Reduction of explanatory or predictor variables (for exploratory observational studies)</p></li>
<li><p>Model refinement and selection</p></li>
<li><p>Model validation</p></li>
</ol>
<p><img src="files/images/regression-flow-chart-advanced.png" class="img-fluid" style="width:80.0%"></p>
<p>Here we will overview each phase, then dive deeper later.</p>
<section id="data-collection" class="level3" data-number="8.1.1"><h3 data-number="8.1.1" class="anchored" data-anchor-id="data-collection">
<span class="header-section-number">8.1.1</span> (1) Data collection</h3>
<ul>
<li>The data collection requirements for building a regression model vary with the nature of the study. It is useful to distinguish four types of studies.</li>
</ul>
<p>Controlled experiments</p>
<ul>
<li>
<p>In a controlled experiment, the experimenter controls the levels of the explanatory variables and assigns treatment, consisting of a combination of levels of the explanatory to each experimental unit and observes the response.</p>
<ul>
<li>In controlled experiments, the explanatory variables are often called <em>factors</em> or <em>control variables</em>.</li>
</ul>
</li>
<li>
<p>The data collection requirements for controlled experiments are straightforward, though not necessarily simple. Observations for each experimental unit are needed on the response variable and on the level of each of the control variables used for that experimental unit.</p>
<ul>
<li>There may be difficult measurement and scaling problems for the response variable that are unique to the area of application.</li>
</ul>
</li>
</ul>
<p>Controlled experiments with covariates</p>
<ul>
<li><p>Statistical design of experiments uses supplemental information, such as characteristics of the experimental units, in designing the experiment so as to reduce the variance of the experimental error terms in the regression model.</p></li>
<li>
<p>Sometimes, however, it is not possible to incorporate this supplemental information into the design of the experiment. Instead, it may be possible for the experimenter to incorporate this information into the regression model and thereby reduce the error variance by including <em>uncontrolled variables</em> or <em>covariates</em> in the model.</p>
<ul>
<li>Use of covariates in the regression model make the analysis of the effects of the explanatory variables on the accuracy response more precise.</li>
</ul>
</li>
</ul>
<p>Confirmatory observational studies</p>
<ul>
<li><p>These studies, based on observational, not experimental, data, are intended to test (i.e., to confirm or not to confirm) hypotheses derived from previous studies or from hunches.</p></li>
<li>
<p>For these studies, data are collected for explanatory variables that previous studies have shown to affect the response variable, as well as for the new variable or variables involved in the hypothesis.</p>
<ul>
<li><p>In this context, the explanatory variable(s) involved in the hypothesis are sometimes called the <em>primary variables</em>, and the explanatory variables that are included to reflect existing knowledge are called the <em>control variables</em> (<em>known risk factors</em> in epidemiology).</p></li>
<li><p>The control variables here are not controlled as in an experimental study, but they are used to account for known influences on the response variable.</p></li>
</ul>
</li>
<li>
<p>Data collection for confirmatory observational studies involves obtaining observations on the response variable, the control variables, and the primary explanatory variable(s).</p>
<ul>
<li>Here, as in controlled experiments, there may be important and complex problems of measurement.</li>
</ul>
</li>
</ul>
<p>Explanatory observational studies</p>
<ul>
<li><p>In the social, behavioral, and health sciences, management, and other fields, it is often not possible to conduct controlled experiments. Furthermore, adequate knowledge for conducting confirmatory observational studies may be lacking.</p></li>
<li><p>As a result, many studies in these fields are exploratory observational studies where investigators search for explanatory variables that might be related to the response variable.</p></li>
<li><p>To complicate matters further, any available theoretical models may involve explanatory variables that are not directly measurable, such as a family’s future earnings ov the next 10 years.</p></li>
<li><p>Under these conditions, investigators are often forced to prospect for explanatory variables that could conceivably be related to the response variable under study. Obviously, such a set of potentially useful explanatory variables can be large.</p></li>
<li>
<p>After a lengthy list of potentially useful explanatory variables has been compiled, some of these variables can be quickly screened out. An explanatory variable (1) may not be fundamental to the problem, (2) may be subject to large measurement errors, and/or (3) may effectively duplicate another explanatory variable in the list.</p>
<ul>
<li>Explanatory variables that cannot be measured may either be deleted or replaced by proxy variables that are highly correlated with them.</li>
</ul>
</li>
<li>
<p>The number of cases to be collected for an exploratory observational regression study depends on the size of the pool of potentially useful explanatory variables available at this stage.</p>
<ul>
<li><p>More cases are required when the pool is large than when it is small.</p></li>
<li><p>A general rule of thumb states that there should be at least 6 to 10 cases for every variable in the pool.</p></li>
</ul>
</li>
<li><p>The actual data collection for the pool of potentially useful explanatory variables and for the response variable again may involve important issues of measurement, just as for the other types of studies.</p></li>
</ul></section><section id="data-preparation" class="level3" data-number="8.1.2"><h3 data-number="8.1.2" class="anchored" data-anchor-id="data-preparation">
<span class="header-section-number">8.1.2</span> (1) Data preparation</h3>
<ul>
<li><p>Once the data have been collected, edit checks should be performed and plots prepared to identify gross data errors as well as extreme outliers.</p></li>
<li><p>Difficulties with data errors are especially prevalent in large data sets and should be corrected or resolved before the model building begins.</p></li>
<li><p>Whenever possible, the investigator should carefully monitor and control the data collection process to reduce the likelihood of data errors.</p></li>
</ul></section><section id="preliminary-model-investigation" class="level3" data-number="8.1.3"><h3 data-number="8.1.3" class="anchored" data-anchor-id="preliminary-model-investigation">
<span class="header-section-number">8.1.3</span> (2) Preliminary model investigation</h3>
<ul>
<li><p>Once the data have been properly edited, the formal modeling process can begin.</p></li>
<li><p>A variety of diagnostics should be employed to identify (1) the functional forms in which the explanatory variables should enter the regression model and (2) important interactions that should be included in the model.</p></li>
<li><p>Scatter plots and residual plots are useful for determining relationships and their strengths.</p></li>
<li><p>Selected explanatory variables can be fitted in regression functions to explore relationships, possible strong interactions and the need for transformations.</p></li>
<li>
<p>Whenever possible, of course, one should also rely on the investigator’s prior knowledge and expertise to suggest appropriate transformations and interactions to investigate. This is particularly important when the number of potentially useful explanatory variables is large.</p>
<ul>
<li>In this case, it may be very difficult to investigate all possible pairwise interactions, and prior knowledge should be used to identify the important ones.</li>
</ul>
</li>
</ul></section><section id="reduction-of-explanatory-variables" class="level3" data-number="8.1.4"><h3 data-number="8.1.4" class="anchored" data-anchor-id="reduction-of-explanatory-variables">
<span class="header-section-number">8.1.4</span> (2) Reduction of explanatory variables</h3>
<ul>
<li>Again this process looks different for the different types of studies.</li>
</ul>
<p>Controlled experiments</p>
<ul>
<li><p>The reduction of explanatory variables in the model-building phase is usually not an important issue for controlled experiments.</p></li>
<li><p>The experimenter has chosen the explanatory variables for investigation, and a regression model is to be developed that will enable the investigator to study the effects’of these variables on the response variable.</p></li>
<li><p>After the model has been developed, including the use of appropriate functional forms for the variables and the inclusion of important interaction terms, the inferential procedures considered in previous chapters will be used to determine whether the explanatory variables have effects on the response variable and, if so, the nature and magnitude of the effects.</p></li>
<li>
<p>Controlled experiments can usually avoid many of the problems in exploratory observational studies discussed below.</p>
<ul>
<li>For example, the effects of latent predictor variables are minimized by using randomization.</li>
</ul>
<p>In addition, adequate ranges of the explanatory variables can be selected and correlations among the explanatory variables can be eliminated by appropriate choices of their levels.</p>
</li>
</ul>
<p>Controlled experiments with covariates</p>
<ul>
<li><p>In studies of controlled experiments with covariates, some reduction of the covariates may take place investigators often cannot be sure in advance that the selected covariates will be helpful in reducing the error variance.</p></li>
<li><p>The number of covariates considered in controlled experiments is usually small, so no special problems are encountered in determining whether some or all of the covariates should be dropped from the regression model.</p></li>
</ul>
<p>Confirmatory observational studies</p>
<ul>
<li><p>Generally, no reduction of explanatory variables should take place in confirmatory observational studies.</p></li>
<li><p>The control variables were chosen on the basis of prior knowledge and should be retained for comparison with earlier studies even if some of the control variables tum out not to lead to any error variance reduction in the study at hand.</p></li>
<li><p>The primary variables are the ones whose influence on the response variable is to be examined and therefore need to be present in the model.</p></li>
</ul>
<p>Explanatory observational studies</p>
<ul>
<li><p>In exploratory observational studies, the number of explanatory variables that remain after the initial screening typically is still large.</p></li>
<li>
<p>Further, many of these variables frequently will be highly intercorrelated. Hence, the investigator usually will wish to reduce the number of explanatory variables to be used in the final model. There are several reasons for this.</p>
<ul>
<li><p>A regression model with numerous explanatory variables may be difficult to maintain.</p></li>
<li><p>Further, regression models with a limited number of explanatory variables are easier to work with and understand.</p></li>
<li><p>Finally, the presence of many highly intercorrelated explanatory variables may substantially increase the sampling variation of the regression coefficients, detract from the model’s descriptive abilities (falsely inflated <span class="math inline">\(R^2\)</span>), and not improve, or even worsen, the model’s predictive ability.</p></li>
<li><p>An actual worsening of the model’s predictive ability can occur when explanatory variables are kept in the regression model that are not related to the response variable, given the other explanatory variables in the model (overfitting). In that case, the variances of the fitted values <span class="math inline">\(\sigma^2\{\hat{Y}_i\}\)</span> tend to become larger with the inclusion of the useless additional explanatory variables.<!-- ??? --></p></li>
<li><p>(Note that the multicollinearity effects were all confirmed with the simulation in <a href="notes-multiple-regression-2.html#sec-simulation-multicollinearity" class="quarto-xref"><span>Section 6.6.6</span></a>)</p></li>
</ul>
</li>
<li>
<p>Once the investigator has tentatively decided upon the functional form of the regression relations (whether given variables are to appear in linear form, quadratic form, etc.) and whether any interaction terms are to be included, the next step in many exploratory observational studies is to identify a few “good” subsets of <span class="math inline">\(X\)</span> variables for further intensive study.</p>
<ul>
<li>These subsets should include not only the potential explanatory variables first-order form but also any needed quadratic and other curvature terms and any necessary interaction terms.</li>
</ul>
</li>
<li>
<p>The identification of “good” subsets of potentially useful explanatory variables to be included in the final regression model and the determination of appropriate functional and interaction relations for these variables usually constitute some of the most difficult problems in regression analysis.</p>
<ul>
<li><p>Since the uses of regression models vary, no one subset of explanatory vruiables may always be “best”.</p></li>
<li><p>For instance, a descriptive use of a regression model typically will emphasize precise estimation of the regression coefficients, whereas a predictive use will focus on the prediction errors.</p></li>
<li><p>Often, different subsets of the pool of potential explanatory variables will best serve these varying purposes.</p></li>
<li><p>Even for a given purpose, it is often found that several subsets are about equally “good” according to a given criterion, and the choice among these “good” subsets needs to be made on the basis of additional considerations.</p></li>
</ul>
</li>
<li>
<p>The choice of a few appropriate subsets of explanatory variables for final consideration in exploratory observational studies needs to be done with great care. Elimination of key explanatory variables can seriously damage the explanatory power of the model and lead to biased estimates of regression coefficients, mean responses, and predictions of new observations, as well as biased estimates of the error variance.</p>
<ul>
<li><p>The bias in these estimates is related to the fact that with observational data, the error terms in an underfitted regression model may reflect nonrandom effects of the explanatory variables not incorporated in the regression model. Important omitted explanatory variables are sometimes called <em>latent explanatory variables</em>.</p></li>
<li><p>On the other hand, if too many explanatory variables are included in the subset, then this overfitted model will often result in variances of estimated parameters that are larger than those for simpler models.</p></li>
<li><p>Another danger with observational data is that important explanatory variables may be observed only over narrow ranges. As a result, such important explanatory variables may be omitted just because they occur in the sample within a narrow range of values and therefore turn out to be statistically nonsignificant.</p></li>
</ul>
</li>
<li>
<p>A number of algorithms have been developed to help with variable selection.</p>
<ul>
<li><p>But the process of developing a useful regression model must be pragmatic and needs to utilize large doses of subjective judgment.</p></li>
<li><p>Explanatory variables that are considered essential should be included in the regression model before any automation is sought.</p></li>
<li><p>Further, algorithms that identify only a single subset of explanatory variables as “best” need to be supplemented so that additional subsets are also considered before the final regression mopel is decided upon.</p></li>
</ul>
</li>
</ul>
<p>WRONG APPROACH</p>
<ul>
<li>
<p>Fit full model and drop all unsignificant coefficients (according to <span class="math inline">\(t\)</span>-test).</p>
<ul>
<li><p>Reason for bad: This procedure can lead to the dropping of important intercorrelated explanatory variables (easier to fail to reject with high degree of multicollinearity).</p></li>
<li><p>So a good procedure needs to be able to take into account multicollinearity (i.e.&nbsp;not drop all of them).</p></li>
</ul>
</li>
</ul></section><section id="model-refinement-and-selection" class="level3" data-number="8.1.5"><h3 data-number="8.1.5" class="anchored" data-anchor-id="model-refinement-and-selection">
<span class="header-section-number">8.1.5</span> (3) Model refinement and selection</h3>
<ul>
<li><p>At this stage in the model-building process, the tentative regression model, or the several “good” regression models in the case of exploratory observational studies, need to checked in detail for curvature and interaction effects (this is the second check of higher order terms, one was done preliminarily in (2) Reduction of explanatory variables).</p></li>
<li><p>Residual plots are helpful in deciding whether one model is to be preferred over another. In addition, the diagnostic checks to be described in the next chapter are useful for identifying influential outlying observations, multicollinearity, etc.</p></li>
<li><p>The selection of the ultimate regression model often depends greatly upon these diagnostic results. For example, one fitted model may be very much influenced by a single case, whereas another is not. Again, one fitted model may show correlations among the error terms, whereas another does not.</p></li>
<li>
<p>When repeat observations are available, formal tests for lack of fit can be made. In any case, a variety of residual plots and analyses can be employed to identify any lack of fit, outliers, and influential observations.</p>
<ul>
<li>For instance, residual plots against cross-product and/or power terms not included in the regression model can be useful in identifying ways in which the model fit can be improved further.</li>
</ul>
</li>
<li>
<p>When an automatic selection procedure is utilized for an exploratory observational study and only a single model is identified as “best,” other models should also be explored.</p>
<ul>
<li><p>One procedure is to use the number of explanatory variables in the model identified as “best” as an estimate of the number of explanatory variables needed in the regression model.</p></li>
<li><p>Then the investigator explores and identifies other candidate models with approximately the same number of explanatory variables identified by the automatic procedure.</p></li>
</ul>
</li>
<li><p>Eventually, after thorough checking and various remedial actions, such as transformations, the investigator narrows the number of competing models to one or just a few.</p></li>
<li><p>At this point, it is good statistical practice to assess the validity of the remaining candidates through model validation studies. These methods can be used to help decide upon a final regression model, and to determine how well the model will perform in practice.</p></li>
</ul></section><section id="model-validation" class="level3" data-number="8.1.6"><h3 data-number="8.1.6" class="anchored" data-anchor-id="model-validation">
<span class="header-section-number">8.1.6</span> (4) Model validation</h3>
<ul>
<li><p>Model validity refers to the stability and reasonableness of the regression coefficients, the plausibility and usability of the regression function, and the ability to generalize inferences drawn from the regression analysis.</p></li>
<li><p>Validation is a useful and necessary pan of the model-building process. Several methods of assessing model validity will be described at the end of this chapter.</p></li>
</ul></section><section id="overfitting-simulation" class="level3" data-number="8.1.7"><h3 data-number="8.1.7" class="anchored" data-anchor-id="overfitting-simulation">
<span class="header-section-number">8.1.7</span> Overfitting simulation</h3>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># overfitting simulation</span></span>
<span></span>
<span><span class="co"># simulation for this???</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="demo" class="level3" data-number="8.1.8"><h3 data-number="8.1.8" class="anchored" data-anchor-id="demo">
<span class="header-section-number">8.1.8</span> Demo</h3>
<ul>
<li>Modeling process: Start with first order model with all predictors and assess diagnostics for intial problems</li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_surgery</span> <span class="op">&lt;-</span> <span class="fu">ALSM</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ALSM/man/SurgicalUnit.html">SurgicalUnit</a></span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">lny</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># start with first order model with all predictors and assess diagnostics</span></span>
<span><span class="va">mod_start</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">data_surgery</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mod_start</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-3-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>See curvature and non constant variance and some slight issues with normality.</p></li>
<li><p>Try linearizing transformation and fix the non constant variance with <span class="math inline">\(Y' = \ln(Y)\)</span>.</p></li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit model with transformed response</span></span>
<span><span class="va">mod_start_prime</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">data_surgery</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mod_start_prime</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-4-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>Transformation fixed most issues, now can continue to investigate the appropriateness of first-order additive terms.</li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># correlation matrix and scatterplot matrix to assess strength of linear relationships (on the lookout for curvature)</span></span>
<span><span class="co"># -&gt; extra steps to get new response variable in there</span></span>
<span><span class="va">corr</span> <span class="op">&lt;-</span> <span class="va">data_surgery</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>lny <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">y</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">cor</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu">corrplot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/corrplot/man/corrplot.html">corrplot</a></span><span class="op">(</span><span class="va">corr</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">data_surgery</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>lny <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">x6</span>, <span class="va">x7</span>, <span class="va">x8</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">pairs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>No apparent curvature for <span class="math inline">\(X_k\)</span> with the response <span class="math inline">\(\ln(Y)\)</span>.</p></li>
<li><p>Strong linear associations with the response (<span class="math inline">\(X_3, X_4\)</span> are the highest).</p></li>
<li><p>May be some multicollinearity (<span class="math inline">\(X_4\)</span> is correlated with <span class="math inline">\(X_1, X_2, X_3\)</span>).</p></li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># check for significant interaction effects by plotting residuals against all possible interactions</span></span>
<span></span>
<span><span class="co"># fit fully crossed interaction model to extract design matrix</span></span>
<span><span class="va">mod_start_prime_crossed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span><span class="op">^</span><span class="fl">2</span>, <span class="va">data_surgery</span>, x <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># get residuals of original model</span></span>
<span><span class="va">e</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">mod_start_prime</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># extract interaction terms</span></span>
<span><span class="co"># -&gt; interaction symbol : gets recoded as .</span></span>
<span><span class="va">X_int</span> <span class="op">&lt;-</span> <span class="va">mod_start_prime_crossed</span><span class="op">$</span><span class="va">x</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">data.frame</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">contains</span><span class="op">(</span><span class="st">"."</span><span class="op">)</span>, <span class="op">-</span><span class="fu">contains</span><span class="op">(</span><span class="st">"Intercept"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># plot residuals against all interaction terms</span></span>
<span><span class="va">nms_x_int</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">X_int</span><span class="op">)</span></span>
<span><span class="fu">map2</span><span class="op">(</span><span class="va">X_int</span>, <span class="va">nms_x_int</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">nm</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">e</span>, main <span class="op">=</span> <span class="va">nm</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lowess.html">lowess</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">e</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">"grey"</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># no obvious visual signs of significant interactions -&gt; maybe x3 and x4</span></span>
<span><span class="co"># -&gt; following textbook and ignoring these</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="criteria-for-model-selection" class="level2" data-number="8.2"><h2 data-number="8.2" class="anchored" data-anchor-id="criteria-for-model-selection">
<span class="header-section-number">8.2</span> Criteria for model selection</h2>
<ul>
<li>
<p>From any set of <span class="math inline">\(p - 1\)</span> predictors, <span class="math inline">\(2^{p-1}\)</span> alternative models can be constructed (this comes from the fact that each predictor can either be included or exlcuded from the model).</p>
<ul>
<li><p>For example if <span class="math inline">\(p = 3\)</span>, then there are two predictors + intercept.</p></li>
<li><p>Can have intercept only model (<span class="math inline">\(Y_i = \beta_0 + \epsilon_i\)</span>), a one variable model (<span class="math inline">\(Y_i = \beta_0 + \beta_1 X_1 + \epsilon_i\)</span> or <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_2 + \epsilon_i\)</span>) and the only two predictor model (<span class="math inline">\(Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon_i\)</span>) for a total of <span class="math inline">\(2^{3-1} = 4\)</span> models.</p></li>
</ul>
</li>
<li><p>The number of possible models increases quickly, and will be impossible to examine every model in depth. Model selection procedures, also known as subset selection or variables selection procedures, have been developed to identify a small group of regression models that are “good” according to a specified criterion.</p></li>
<li><p>A detailed examination can then be made of a limited number of the more promising or “candidate” models, leading to the selection of the final regression model to be employed. This limited number might consist of three to six “good” subsets according to the criteria specified, so the investigator can then carefully study these regression models for choosing the final model.</p></li>
</ul>
<section id="r2_p-or-sse_p-criterion" class="level3" data-number="8.2.1"><h3 data-number="8.2.1" class="anchored" data-anchor-id="r2_p-or-sse_p-criterion">
<span class="header-section-number">8.2.1</span> <span class="math inline">\(R^2_p\)</span> or <span class="math inline">\(SSE_p\)</span> criterion</h3>
<ul>
<li>Want to identify subset in which the coefficient of multiple determination <span class="math inline">\(R^2\)</span> is high, or equivalently <span class="math inline">\(SSE\)</span> is low (both of which are indexed by how many parameters are in the model = one less than the number of the predictors because of the intercept).</li>
</ul>
<p><span class="math display">\[R^2_p = 1 - \frac{SSE_p}{SSTO}\]</span></p>
<ul>
<li><p>Since the denominator is constant for all models (just used <span class="math inline">\(\bar{Y}\)</span>), <span class="math inline">\(R^2_p\)</span> and <span class="math inline">\(SSE_p\)</span> vary inversely.</p></li>
<li>
<p>Note that the <span class="math inline">\(R^2_p\)</span> criterion is not intended to identify the subsets that maximize this criterion.</p>
<ul>
<li><p>We know that <span class="math inline">\(R^2_p\)</span> can never decrease as additional <span class="math inline">\(X\)</span> variables are included in the model. Hence, <span class="math inline">\(R^2_p\)</span> will be a maximum when all <span class="math inline">\(P - 1\)</span> (<span class="math inline">\(P\)</span> = total number of available predictors) potential <span class="math inline">\(X\)</span> variables are included in the regression model.</p></li>
<li><p>Dimininshing return: The intent in using the <span class="math inline">\(R^2_p\)</span> criterion is to find the point where adding more <span class="math inline">\(X\)</span> variables is not worthwhile because it leads to a very small increase in <span class="math inline">\(R^2_p\)</span>.</p></li>
</ul>
</li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># pick random model to calculate each of the following statistics for (so can compare to book as well)</span></span>
<span><span class="va">mod_example</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span>, <span class="va">data_surgery</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># p</span></span>
<span><span class="co"># -&gt; including intercept</span></span>
<span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">coef</span> <span class="op">%&gt;%</span> <span class="va">length</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4</code></pre>
</div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># extract R^2_p and SSE_p</span></span>
<span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">r.squared</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7572331</code></pre>
</div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">anova</span> <span class="op">%&gt;%</span> <span class="va">tidy</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">term</span> <span class="op">==</span> <span class="st">"Residuals"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sumsq</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.10851</code></pre>
</div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># match book!</span></span>
<span></span>
<span><span class="co"># more ways</span></span>
<span><span class="fu">augment</span><span class="op">(</span><span class="va">mod_example</span>, data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>lny <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_surgery</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/rsq_trad.html">rsq_trad</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">.</span>, truth <span class="op">=</span> <span class="st">"lny"</span>, estimate <span class="op">=</span> <span class="st">".fitted"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 rsq_trad standard       0.757</code></pre>
</div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/rsq_trad.html">rsq_trad_vec</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_surgery</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>                        estimate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_example</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7572331</code></pre>
</div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">compare</span><span class="op">(</span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">r.squared</span><span class="op">)</span>,</span>
<span>        <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/rsq_trad.html">rsq_trad_vec</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_surgery</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>                        estimate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_example</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`mod_example %&gt;% glance %&gt;% pull(r.squared)`
[1] 0.7572331

$`yardstick::rsq_trad_vec(truth = log(data_surgery$y), estimate = predict(mod_example))`
[1] 0.7572331</code></pre>
</div>
</div>
</section><section id="r2_ap-or-mse_p-criterion" class="level3" data-number="8.2.2"><h3 data-number="8.2.2" class="anchored" data-anchor-id="r2_ap-or-mse_p-criterion">
<span class="header-section-number">8.2.2</span> <span class="math inline">\(R^2_{a,p}\)</span> or <span class="math inline">\(MSE_p\)</span> criterion</h3>
<ul>
<li>Can use <span class="math inline">\(R^2_{adj} = R^2_{a,p} \text{ (in this context)}\)</span> to take into account the number of parameters in the model through the degrees of freedom. We know that it can decrease as the number of parameters increases if the decrease in <span class="math inline">\(MSE\)</span> isn’t enough to offset the loss of degrees of freedom.</li>
</ul>
<p><span class="math display">\[R^2_{a,p} = 1 - \Big(\frac{n - 1}{n - p}\Big) \frac{SSE_p}{SSTO} = 1 - \frac{MSE_p}{SSTO / (n - 1)}\]</span></p>
<ul>
<li>This only increases if <span class="math inline">\(MSE_p\)</span> decreases since <span class="math inline">\(SSTO / (n - 1)\)</span> is fixed for the given <span class="math inline">\(Y\)</span> observations. Thus, <span class="math inline">\(R^2_{a,p}\)</span> and <span class="math inline">\(MSE_p\)</span> provide equivalent information.</li>
</ul>
<p>Demo</p>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># extract R^2_a.p and MSE_p</span></span>
<span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">adj.r.squared</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7426671</code></pre>
</div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">raise_to_power</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.06217021</code></pre>
</div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># match book!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="mallows-c_p-criterion" class="level3" data-number="8.2.3"><h3 data-number="8.2.3" class="anchored" data-anchor-id="mallows-c_p-criterion">
<span class="header-section-number">8.2.3</span> Mallow’s <span class="math inline">\(C_p\)</span> criterion</h3>
<p>Overview</p>
<ul>
<li>This criterion is concerned with the <em>total mean squared error</em> of the <span class="math inline">\(n\)</span> fitted values for each subset regression model.</li>
</ul>
<p><span class="math display">\[C_p = \frac{SSE_p}{MSE(X_1, \ldots, X_{P-1})} + (2p - n)\]</span></p>
<p>Specifics</p>
<embed src="files/docs/mallows-cp.pdf" type="application/pdf" width="100%" height="700px"><p>Interpreting Mallow’s <span class="math inline">\(C_p\)</span></p>
<ul>
<li>Good models: When there is no bias in the regression model with <span class="math inline">\(p - 1\)</span> <span class="math inline">\(X\)</span> variables, then</li>
</ul>
<p><span class="math display">\[E(C_p) \approx p \hspace{20pt} \text{when }E(\hat{Y}_i) \approx \mu_i\]</span></p>
<ul>
<li><p>If there are several good models, take the more parsimonious model.</p></li>
<li><p>Acceptable values: <span class="math inline">\(C_p &lt;\approx p\)</span></p></li>
</ul>
<p>Interpreting Mallow’s <span class="math inline">\(C_p\)</span> plot</p>
<ul>
<li>
<p>When the <span class="math inline">\(C_p\)</span> values for all possible regression models are plotted against <span class="math inline">\(p\)</span>, those models with little bias will tend to fall near the line <span class="math inline">\(C_p = p\)</span>.</p>
<ul>
<li><p>Models with substantial bias will tend to fall considerably above this line.</p></li>
<li><p><span class="math inline">\(C_p\)</span> values below the line <span class="math inline">\(C_p = p\)</span> are interpreted as showing no bias, being below the line due to sampling error.</p></li>
</ul>
</li>
</ul>
<p>Notes</p>
<ul>
<li>Effective use of the <span class="math inline">\(C_p\)</span> criterion requires careful development of the pool of <span class="math inline">\(P-1\)</span> potential <span class="math inline">\(X\)</span> variables, with the predictor variables expressed in appropriate form (linear, quadratic, transformed), and important interactions included, so that <span class="math inline">\(MSE(X_1,\ldots,X_{p-1})\)</span> provides an unbiased estimate of the error variance <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>Demo</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" aria-current="page">R functions</a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Manual</a></li>
</ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate mallow's Cp</span></span>
<span><span class="co"># -&gt; model is the candidate model and fullmodel has P - 1 predictors</span></span>
<span><span class="va">mod_example_full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span> <span class="op">+</span> <span class="va">x4</span>, <span class="va">data_surgery</span><span class="op">)</span></span>
<span><span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_mallows_cp.html">ols_mallows_cp</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">mod_example</span>, fullmodel <span class="op">=</span> <span class="va">mod_example_full</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.387945</code></pre>
</div>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># matches book!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Mallows Cp = SSE_candidate / MSE_full + (2p - n)</span></span>
<span><span class="co"># -&gt; calculate needed values</span></span>
<span><span class="va">SSE_p</span> <span class="op">&lt;-</span> <span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">anova</span> <span class="op">%&gt;%</span> <span class="va">tidy</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">term</span> <span class="op">==</span> <span class="st">"Residuals"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sumsq</span><span class="op">)</span></span>
<span><span class="va">MSE_full</span> <span class="op">&lt;-</span> <span class="va">mod_example_full</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">raise_to_power</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">coef</span> <span class="op">%&gt;%</span> <span class="va">length</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">mod_example_full</span><span class="op">)</span></span>
<span><span class="va">mallows_cp</span> <span class="op">&lt;-</span> <span class="va">SSE_p</span> <span class="op">/</span> <span class="va">MSE_full</span> <span class="op">+</span> <span class="fl">2</span><span class="op">*</span><span class="va">p</span> <span class="op">-</span> <span class="va">n</span></span>
<span></span>
<span><span class="fu">compare</span><span class="op">(</span><span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_mallows_cp.html">ols_mallows_cp</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">mod_example</span>, fullmodel <span class="op">=</span> <span class="va">mod_example_full</span><span class="op">)</span>,</span>
<span>        <span class="va">mallows_cp</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`olsrr::ols_mallows_cp(model = mod_example, fullmodel = mod_example_full)`
[1] 3.387945

$mallows_cp
[1] 3.387945</code></pre>
</div>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># very close to book, going with roundoff error</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section><section id="aic_p-and-bic_p-criterion" class="level3" data-number="8.2.4"><h3 data-number="8.2.4" class="anchored" data-anchor-id="aic_p-and-bic_p-criterion">
<span class="header-section-number">8.2.4</span> <span class="math inline">\(AIC_p\)</span> and <span class="math inline">\(BIC_p\)</span> criterion</h3>
<ul>
<li><p>These two criterion, in addition to <span class="math inline">\(R^2_{a,p}\)</span> and <span class="math inline">\(C_p\)</span>, penalize models having large number of predictors.</p></li>
<li><p><span class="math inline">\(AIC\)</span> = Akaike’s information criteria and <span class="math inline">\(BIC\)</span> = Bayesian information criteria.</p></li>
<li><p>Goal: Find models with small values of each:</p></li>
</ul>
<p><span class="math display">\[
\begin{align*}
  AIC &amp;= n \ln(SSE_p) - n \ln(n) + 2p\\
  BIC &amp;= n \ln(SSE_p) - n \ln(n) + p\ln(n)
\end{align*}
\]</span></p>
<ul>
<li><p>Notice the first terms <span class="math inline">\(n \ln(SSE_p)\)</span> decrease as <span class="math inline">\(p\)</span> increases, the second term <span class="math inline">\(n \ln(n)\)</span> is fixed (for a given sample size of <span class="math inline">\(n\)</span>), and the last term (the penalty term) increases as <span class="math inline">\(p\)</span> increases.</p></li>
<li><p>Models with small <span class="math inline">\(SSE_p\)</span> do well with this criteria as long as the penalties aren’ too large. And for <span class="math inline">\(n \ge 8\)</span>, the penalty for <span class="math inline">\(BIC_p\)</span> is larger <span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(BIC_p\)</span> favors more parsimonious models.</p></li>
</ul>
<p>Demo</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">R functions</a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Manual</a></li>
</ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># different ways to calculate these statistics</span></span>
<span><span class="co"># -&gt; (taking into account something vs not taking into account something)</span></span>
<span><span class="co"># -&gt; https://stats.stackexchange.com/questions/43733/what-is-the-difference-between-aic-and-extractaic-in-r</span></span>
<span></span>
<span><span class="co"># one way</span></span>
<span></span>
<span><span class="co"># calculate AIC_p and BIC_p</span></span>
<span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">AIC</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 9.08398</code></pre>
</div>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">AIC</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 9.08398</code></pre>
</div>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">BIC</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 19.0289</code></pre>
</div>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">BIC</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 19.0289</code></pre>
</div>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># does not match matches book... </span></span>
<span></span>
<span><span class="co"># another way</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/extractAIC.html">extractAIC</a></span><span class="op">(</span><span class="va">mod_example</span>, k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="co"># AIC used default k = 2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]    4.0000 -146.1614</code></pre>
</div>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/extractAIC.html">extractAIC</a></span><span class="op">(</span>k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="co"># BIC -&gt; just have a different k value = ln(n)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]    4.0000 -138.2054</code></pre>
</div>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># match book!</span></span>
<span></span>
<span><span class="co"># be careful comparing, need to compare like to like</span></span>
<span><span class="co"># -&gt; from now on always going to use the not book way because it has better compatibility across lm() and glm()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate needed items</span></span>
<span><span class="va">SSE_p</span> <span class="op">&lt;-</span> <span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">anova</span> <span class="op">%&gt;%</span> <span class="va">tidy</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">term</span> <span class="op">==</span> <span class="st">"Residuals"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sumsq</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="va">mod_example</span> <span class="op">%&gt;%</span> <span class="va">coef</span> <span class="op">%&gt;%</span> <span class="va">length</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">mod_example_full</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># AIC = n ln(SSE_p) - n ln(n) + 2p</span></span>
<span><span class="va">AIC_p</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">SSE_p</span><span class="op">)</span> <span class="op">-</span> <span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">+</span> <span class="fl">2</span><span class="op">*</span><span class="va">p</span></span>
<span></span>
<span><span class="co"># BIC = n ln(n) SSE_p - n ln(n) + p ln(n)</span></span>
<span><span class="va">BIC_p</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">SSE_p</span><span class="op">)</span><span class="op">-</span> <span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">+</span> <span class="va">p</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/extractAIC.html">extractAIC</a></span><span class="op">(</span><span class="va">mod_example</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">AIC_p</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`extractAIC(mod_example)[2]`
[1] -146.1614

$AIC_p
[1] -146.1614</code></pre>
</div>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/extractAIC.html">extractAIC</a></span><span class="op">(</span><span class="va">mod_example</span>, k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">mod_example</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">BIC_p</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`extractAIC(mod_example, k = log(nobs(mod_example)))[2]`
[1] -138.2054

$BIC_p
[1] -138.2054</code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="press_p-criterion" class="level3" data-number="8.2.5"><h3 data-number="8.2.5" class="anchored" data-anchor-id="press_p-criterion">
<span class="header-section-number">8.2.5</span> <span class="math inline">\(PRESS_p\)</span> criterion</h3>
<ul>
<li>The <span class="math inline">\(PRESS_p\)</span> (prediction sum of squares) criterion is a measure of how well the use of the fitted values for a subset model (in terms of the training data) can predict the observed responses <span class="math inline">\(Y_i\)</span>. It is similar to <span class="math inline">\(SSE = \sum (Y_i - \hat{Y}_i)^2\)</span> except each fitted value <span class="math inline">\(\hat{Y}_i\)</span> is obtained by:</li>
</ul>
<ol type="1">
<li><p>Deleting the <span class="math inline">\(i\)</span>th case from the dataset.</p></li>
<li><p>Estimating the regression function for the subset model from the remaining <span class="math inline">\(n - 1\)</span> cases.</p></li>
<li><p>Then using the fitted regression function to obtain the predicted value <span class="math inline">\(\hat{Y}_{i(i)}\)</span> for the <span class="math inline">\(i\)</span>th case (note <span class="math inline">\(\hat{Y}_{i(i)}\)</span> = predicted value for the <span class="math inline">\(i\)</span>th case when the <span class="math inline">\((i)\)</span>th observation was omitted when the regression function was fit.</p></li>
</ol>
<ul>
<li>The prediction error for the <span class="math inline">\(i\)</span>th case is then</li>
</ul>
<p><span class="math display">\[Y_i - \hat{Y}_{i(i)}\]</span></p>
<ul>
<li>The <span class="math inline">\(PRESS_p\)</span> criterion is the sum of the squared prediction errors over all <span class="math inline">\(n\)</span> cases:</li>
</ul>
<p><span class="math display">\[\sum_{i = 1}^n (Y_i - \hat{Y}_{i(i)})^2\]</span></p>
<ul>
<li><p>Good models: Models with small <span class="math inline">\(PRESS_p\)</span> fit well in the sense of having small prediction errors and are considered candidate models.</p></li>
<li><p><span class="math inline">\(PRESS_p\)</span> values can be calculated without requiring <span class="math inline">\(n\)</span> separate regression runs, each time deleting one of the <span class="math inline">\(n\)</span> cases using formulas in the next chapter.</p></li>
<li><p>Note that <span class="math inline">\(PRESS_p\)</span> values can also be used for model validation.</p></li>
</ul></section></section><section id="automatic-search-procedures-for-model-selection" class="level2" data-number="8.3"><h2 data-number="8.3" class="anchored" data-anchor-id="automatic-search-procedures-for-model-selection">
<span class="header-section-number">8.3</span> Automatic search procedures for model selection</h2>
<ul>
<li><p>Two common approaches for automating variable selection are “best” subsets regression and stepwise regression.</p></li>
<li><p>Will not cover all options and variations of methods that are available.</p></li>
<li><p>It is essential that the specific features of the package employed be fully understood so that intelligent use of the package can be made.</p></li>
<li><p>For example, some variations allow variables to be considered in groups (such as all indicators for a categorical predictor) or to force variables into the model (if have a prioir beliefs).</p></li>
<li>
<p>All to say, there is no unique way of searching for “good” subsets of <span class="math inline">\(X\)</span> variables, and subjective elements must play an important role in the search process.</p>
<ul>
<li><p>Judgment needs to play an important role in model building for exploratory studies. Some explanatory variables may be known to be more fundamental than others and therefore should be retained in the regression model if the primary purpose is to develop a good explanatory model.</p></li>
<li><p>For example, keep all indicator variables for a categorical predictor, keep lower order terms if a higher order term is included, etc.</p></li>
</ul>
</li>
<li>
<p>An important issue in exploratory model building that we have not will be considered is the bias in estimated regression coefficients and in estimated mean responses.</p>
<ul>
<li>This is where model validation comes into play (don’t get lost in finding the best fit)</li>
</ul>
</li>
</ul>
<p>Demo</p>
<div class="cell">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate PRESS</span></span>
<span><span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_press.html">ols_press</a></span><span class="op">(</span><span class="va">mod_example</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.914315</code></pre>
</div>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># matches book!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="best-subsets-algorithms" class="level3" data-number="8.3.1"><h3 data-number="8.3.1" class="anchored" data-anchor-id="best-subsets-algorithms">
<span class="header-section-number">8.3.1</span> “Best” subsets algorithms</h3>
<ul>
<li><p>These algorithms provide the best subsets according to the specified criterion, as well as identifying several “good” subsets for each possible number of <span class="math inline">\(X\)</span> variables in the model.</p></li>
<li><p>When the pool of potential <span class="math inline">\(X\)</span> variables is very large, say greater than 30 or 40, even the “best” subset algorithms may require excessive computer time. Under these conditions, one of the stepwise regression may need to be used.</p></li>
<li><p>Use several different criterion when evaluating the “best” subsets. This is one way to get multiple candidate models.</p></li>
<li><p>Once the investigator has identified a few “good” subsets for intensive examination, a final choice of the model variables must be made with the help of residual analyses, industry knowledge, and finally then confirmed through model validation.</p></li>
<li>
<p>Strengths</p>
<ul>
<li><p>Exhaustive search of subsets.</p></li>
<li><p>Results in several good candidate models.</p></li>
</ul>
</li>
<li>
<p>Weaknesses</p>
<ul>
<li>Computationally infeasible when there is a large number of predictors.</li>
</ul>
</li>
</ul>
<p>Demo</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true"><code>leaps::regsubsets</code></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false"><code>olsrr::ols_step_best_subset</code></a></li>
</ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell">
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># define function to format results for plotting</span></span>
<span><span class="va">format_results</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">best_subs</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># get summary statistics</span></span>
<span>  <span class="va">summ</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">best_subs</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># combine to single dataframe and rename columns</span></span>
<span>  <span class="va">results</span> <span class="op">=</span> <span class="va">summ</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span> <span class="op">%&gt;%</span> <span class="fu">reduce</span><span class="op">(</span><span class="va">bind_cols</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">summ</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># append number of predictor variables</span></span>
<span>  <span class="va">results</span> <span class="op">=</span> <span class="va">results</span> <span class="op">%&gt;%</span> <span class="fu">mutate</span><span class="op">(</span>p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">best_subs</span><span class="op">$</span><span class="va">np</span>, <span class="va">best_subs</span><span class="op">$</span><span class="va">nbest</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">summ</span><span class="op">$</span><span class="va">outmat</span><span class="op">)</span><span class="op">]</span>, .before <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># define function to plot results</span></span>
<span><span class="va">plot_results</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">res</span>, <span class="va">var</span> <span class="op">=</span> <span class="st">"rsq"</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># plot point for each model at each p</span></span>
<span>  <span class="co"># -&gt; add line connecting optimal models at each p</span></span>
<span>  <span class="va">p</span> <span class="op">=</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">res</span>,</span>
<span>         <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">p</span>,</span>
<span>             y <span class="op">=</span> <span class="va">.data</span><span class="op">[[</span><span class="va">var</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu">geom_point</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"grey70"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu">geom_line</span><span class="op">(</span>data <span class="op">=</span> <span class="va">res</span> <span class="op">%&gt;%</span> <span class="fu">group_by</span><span class="op">(</span><span class="va">p</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># conditionally add Cp = p line</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">var</span>, <span class="st">"cp"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">p</span> <span class="op">=</span> <span class="va">p</span> <span class="op">+</span> </span>
<span>      <span class="fu">geom_abline</span><span class="op">(</span>intercept <span class="op">=</span> <span class="fl">0</span>,</span>
<span>                  slope <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># perform best subsets regression and save necessary output</span></span>
<span><span class="co"># -&gt; note there are options to force variables in and out</span></span>
<span><span class="co"># -&gt; nbest = nmax does ALL possible models</span></span>
<span><span class="va">best_subs</span> <span class="op">&lt;-</span> <span class="fu">leaps</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_start_prime</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data_surgery</span>, </span>
<span>                               nbest <span class="op">=</span> <span class="fl">1</span>, nvmax <span class="op">=</span> <span class="fl">8</span>, method <span class="op">=</span> <span class="st">"exhaustive"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># format and results</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu">format_results</span><span class="op">(</span><span class="va">best_subs</span><span class="op">)</span></span>
<span><span class="fu">map</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span>, \<span class="op">(</span><span class="va">var</span><span class="op">)</span> <span class="fu">plot_results</span><span class="op">(</span><span class="va">results</span>, <span class="va">var</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[2]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-15-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[3]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-15-3.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[4]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-15-4.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[5]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-15-5.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># this gives the single best model for number of predictor variables</span></span>
<span><span class="co"># -&gt; can use this to most easily get which variables are in the single best model_p</span></span>
<span><span class="va">best_subs</span> <span class="op">&lt;-</span> <span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_step_best_subset.html">ols_step_best_subset</a></span><span class="op">(</span><span class="va">mod_example_full</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># plot results also works, just plots only the best</span></span>
<span><span class="va">best_subs</span> <span class="op">%&gt;%</span> <span class="fu">rename</span><span class="op">(</span>p <span class="op">=</span> <span class="va">n</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="op">{</span><span class="fu">map</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">best_subs</span><span class="op">)</span><span class="op">[</span><span class="fl">4</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span>, \<span class="op">(</span><span class="va">var</span><span class="op">)</span> <span class="fu">plot_results</span><span class="op">(</span><span class="va">.</span>, <span class="va">var</span><span class="op">)</span><span class="op">)</span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[2]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-16-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[3]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-16-3.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[4]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-16-4.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[5]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-16-5.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[6]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-16-6.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[7]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-16-7.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="stepwise-regression-methods" class="level3" data-number="8.3.2"><h3 data-number="8.3.2" class="anchored" data-anchor-id="stepwise-regression-methods">
<span class="header-section-number">8.3.2</span> Stepwise regression methods</h3>
<p>Motivation</p>
<ul>
<li>In those occasional cases when the pool of potential <span class="math inline">\(X\)</span> variables contains 30 to 40 or even more variables, use of a “best” subsets algorithm may not be feasible. An automatic search procedure that develops the “best” subset of <span class="math inline">\(X\)</span> variables sequentially may then be helpful.</li>
</ul>
<p>Overview</p>
<ul>
<li><p>This search method develops a sequence of regression models, at each step adding or deleting an <span class="math inline">\(X\)</span> variable (iterative procedure).</p></li>
<li><p>Can use one of several different criterion for adding or deleting an <span class="math inline">\(X\)</span> variable, such as: reduction in error sum of squares, coefficient of partial correlation, <span class="math inline">\(t^*\)</span> statistic, or <span class="math inline">\(F^*\)</span> statistic.</p></li>
<li>
<p>Strengths</p>
<ul>
<li>Computational more efficient than evaluating ALL possible subsets.</li>
</ul>
</li>
<li>
<p>Weaknesses</p>
<ul>
<li><p>Experience has shown that each of the stepwise search procedures can sometimes err by identifying a suboptimal regression model as “best.”</p></li>
<li><p>End result is only a SINGLE “best” model. So it hides potentially other “good” models, whose “goodness” need to be evaluated using a variety of diagnostics.</p></li>
</ul>
</li>
<li>
<p>Strategy then</p>
<ul>
<li><p>We should use the subset identified by the automatic search procedure as a starting point for searching for other “good” subsets.</p></li>
<li><p>One possibility is to treat the number of <span class="math inline">\(X\)</span> variables in the regression model identified by the automatic search procedure as being about the right subset size and then use the “best” subsets procedure for subsets of this and nearby sizes.</p></li>
</ul>
</li>
</ul>
<p>Forward stepwise regression</p>
<ul>
<li><p>Step 0: Start with intercept-only model.</p></li>
<li>
<p>Step 1: Fit all one variable models and evaluate criteria. Find the best. For example, the largest <span class="math inline">\(\lvert t^* \rvert\)</span> or equivalently smallest <span class="math inline">\(p\)</span>-value.</p>
<ul>
<li>Since the degrees of freedom associated with <span class="math inline">\(MSE\)</span> vary depending on the number of <span class="math inline">\(X\)</span> variables in the model, and since repeated tests on the same data are undertaken, fixed <span class="math inline">\(t^*\)</span> limits for adding or deleting a variable have no precise probabilistic meaning. For this reason, software programs often favor the use of predetermined <span class="math inline">\(\alpha\)</span>-limits.</li>
</ul>
</li>
</ul>
<p><span class="math display">\[t^*_{k1} = \sqrt{\frac{\hat{\beta}_{k1}}{s\{\hat{\beta}_{k1}\}}} \Longrightarrow p\text{-value}\]</span></p>
<ul>
<li>Step 2: Start with the variable from the previous step and fit all 2 variable models. Find the best second variable and see if it meets the keep criteria.</li>
</ul>
<p><span class="math display">\[t^*_{k1} = \sqrt{\frac{MSR(X_{k2} \mid X_{k1})}{MSE(X_{k1}, X_{k2})}} \Longrightarrow p\text{-value}\]</span></p>
<ul>
<li><p>Step 3: Check to see if a variable should be deleted. Fit model with all predictors currently kept and see if one variable should be dropped (i.e.&nbsp;see if criteria is on wrong side of the keep criteria).</p></li>
<li><p>Step 4: Continue adding and checking to see if previous variable should be dropped until adding a variable doesn’t improve the model and dropping a variable doesn’t improve the model. Then algorithm is done.</p></li>
<li>
<p>Note that the stepwise regression algorithm allows an <span class="math inline">\(X\)</span> variable, brought into the model at an earlier stage, to be dropped subsequently if it is no longer helpful in conjunction with variables added at later stages.</p>
<ul>
<li>i.e.&nbsp;The order in which variables enter the regression model does not reflect their importance.</li>
</ul>
</li>
</ul>
<p>Choice of keep criteria in terms of <span class="math inline">\(\alpha\)</span></p>
<ul>
<li>
<p>The choice of <span class="math inline">\(\alpha\)</span> to-enter and <span class="math inline">\(\alpha\)</span>-to-remove values essentially represents a balancing of opposing tendencies.</p>
<ul>
<li><p>Simulation studies have shown that for scenarios with large pools of 1) uncorrelated predictors 2) that are not related to the response, larger <span class="math inline">\(\alpha\)</span>-to-enter values results in models that allow too many variables. Conversely, a small <span class="math inline">\(\alpha\)</span>-to-enter values results in models that are often underspecified, resulting in <span class="math inline">\(\sigma^2\)</span> being badly overestimated and the procedure being too conservative.</p></li>
<li><p>The max <span class="math inline">\(\alpha\)</span>-to-enter &lt; min <span class="math inline">\(\alpha\)</span>-to-remove. If not, will get an endless loop.</p></li>
</ul>
</li>
</ul>
<p>Demo</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true"><code>stats::step</code></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false"><code>olsrr</code></a></li>
</ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="cell">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># perform stepwise regression (forward)</span></span>
<span></span>
<span><span class="co"># specify starting and ending models</span></span>
<span><span class="va">mod_null</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>, <span class="va">data_surgery</span><span class="op">)</span> <span class="co"># starting with intercept model</span></span>
<span><span class="va">mod_full</span> <span class="op">&lt;-</span> <span class="va">mod_start_prime</span> <span class="co"># just need formula</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_full</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>log(y) ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8</code></pre>
</div>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># run procedure</span></span>
<span><span class="co"># -&gt; criterion is based on AIC from extractAIC()</span></span>
<span><span class="va">mod_step_aic</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mod_null</span>, scope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_full</span><span class="op">)</span>, direction <span class="op">=</span> <span class="st">"both"</span>, k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=-75.72
log(y) ~ 1

       Df Sum of Sq     RSS      AIC
+ x3    1    5.4708  7.3337 -103.811
+ x4    1    5.3967  7.4079 -103.268
+ x2    1    2.8303  9.9742  -87.205
+ x8    1    1.7808 11.0238  -81.802
+ x1    1    0.7770 12.0275  -77.096
+ x6    1    0.6889 12.1156  -76.703
&lt;none&gt;              12.8045  -75.716
+ x5    1    0.2694 12.5351  -74.864
+ x7    1    0.2067 12.5978  -74.595

Step:  AIC=-103.81
log(y) ~ x3

       Df Sum of Sq     RSS      AIC
+ x2    1    3.0209  4.3129 -130.479
+ x4    1    2.2018  5.1319 -121.089
+ x1    1    1.5512  5.7825 -114.644
+ x8    1    1.1386  6.1951 -110.922
&lt;none&gt;               7.3337 -103.811
+ x6    1    0.2582  7.0755 -103.747
+ x5    1    0.2390  7.0947 -103.600
+ x7    1    0.0659  7.2679 -102.298
- x3    1    5.4708 12.8045  -75.716

Step:  AIC=-130.48
log(y) ~ x3 + x2

       Df Sum of Sq    RSS      AIC
+ x8    1    1.4709 2.8420 -151.002
+ x1    1    1.2044 3.1085 -146.161
+ x4    1    0.6979 3.6150 -138.011
+ x7    1    0.2280 4.0849 -131.412
+ x5    1    0.1648 4.1481 -130.583
&lt;none&gt;              4.3129 -130.479
+ x6    1    0.0822 4.2306 -129.518
- x2    1    3.0209 7.3337 -103.811
- x3    1    5.6613 9.9742  -87.205

Step:  AIC=-151
log(y) ~ x3 + x2 + x8

       Df Sum of Sq    RSS      AIC
+ x1    1    0.6642 2.1778 -163.376
+ x4    1    0.4658 2.3761 -158.669
+ x6    1    0.1372 2.7048 -151.674
&lt;none&gt;              2.8420 -151.002
+ x5    1    0.0709 2.7711 -150.367
+ x7    1    0.0241 2.8179 -149.462
- x8    1    1.4709 4.3129 -130.479
- x2    1    3.3531 6.1951 -110.922
- x3    1    4.9403 7.7823  -98.605

Step:  AIC=-163.38
log(y) ~ x3 + x2 + x8 + x1

       Df Sum of Sq    RSS      AIC
+ x6    1    0.0966 2.0812 -163.826
&lt;none&gt;              2.1778 -163.376
+ x5    1    0.0760 2.1018 -163.293
+ x4    1    0.0415 2.1363 -162.415
+ x7    1    0.0224 2.1554 -161.935
- x1    1    0.6642 2.8420 -151.002
- x8    1    0.9307 3.1085 -146.161
- x2    1    2.9891 5.1670 -118.722
- x3    1    5.4459 7.6237  -97.717

Step:  AIC=-163.83
log(y) ~ x3 + x2 + x8 + x1 + x6

       Df Sum of Sq    RSS     AIC
+ x5    1    0.0769 2.0043 -163.86
&lt;none&gt;              2.0812 -163.83
- x6    1    0.0966 2.1778 -163.38
+ x7    1    0.0219 2.0593 -162.40
+ x4    1    0.0163 2.0649 -162.25
- x1    1    0.6236 2.7048 -151.67
- x8    1    0.9754 3.0567 -145.07
- x2    1    2.8287 4.9099 -119.48
- x3    1    5.0742 7.1554  -99.14

Step:  AIC=-163.86
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5

       Df Sum of Sq    RSS      AIC
&lt;none&gt;              2.0043 -163.858
- x5    1    0.0769 2.0812 -163.826
- x6    1    0.0975 2.1018 -163.293
+ x7    1    0.0326 1.9718 -162.743
+ x4    1    0.0022 2.0021 -161.919
- x1    1    0.6284 2.6327 -151.133
- x8    1    0.9011 2.9054 -145.810
- x2    1    2.7644 4.7688 -119.052
- x3    1    5.0752 7.0795  -97.716</code></pre>
</div>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># confirm from extractAIC()</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/extractAIC.html">extractAIC</a></span><span class="op">(</span><span class="va">mod_step_aic</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]    7.0000 -163.8583</code></pre>
</div>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># repeat for BIC criteria -&gt; k = log(nobs(mod))</span></span>
<span><span class="va">mod_step_bic</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mod_null</span>, scope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_full</span><span class="op">)</span>, direction <span class="op">=</span> <span class="st">"both"</span>, k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">mod_null</span><span class="op">)</span><span class="op">)</span>, trace <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="co"># criterion is based on BIC from extractAIC()</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/extractAIC.html">extractAIC</a></span><span class="op">(</span><span class="va">mod_step_bic</span>, k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/nobs.html">nobs</a></span><span class="op">(</span><span class="va">mod_step_bic</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]    5.000 -153.431</code></pre>
</div>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># compare models</span></span>
<span><span class="co"># -&gt; BIC is more conservative, which makes sense</span></span>
<span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_step_aic</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_step_bic</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
$comparison$result
[1] FALSE

$comparison$description
[1] "formulas differ in contents"

$comparison$`element-wise`
NULL


$`formula(mod_step_aic)`
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5

$`formula(mod_step_bic)`
log(y) ~ x3 + x2 + x8 + x1</code></pre>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="cell">
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># perform stepwise regression (forward)</span></span>
<span><span class="co"># -&gt; based on AIC</span></span>
<span><span class="va">mod_step_aic2</span> <span class="op">&lt;-</span> <span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_step_both_aic.html">ols_step_both_aic</a></span><span class="op">(</span><span class="va">mod_full</span>, progress <span class="op">=</span> <span class="cn">TRUE</span>, details <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Stepwise Selection Method 
-------------------------

Candidate Terms: 

1 . x1 
2 . x2 
3 . x3 
4 . x4 
5 . x5 
6 . x6 
7 . x7 
8 . x8 

 Step 0: AIC = 79.52928 
 log(y) ~ 1 


Variables Entered/Removed: 

                         Enter New Variables                       
-------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS      R-Sq     Adj. R-Sq 
-------------------------------------------------------------------
x3            1    51.434     5.471     7.334    0.427        0.416 
x4            1    51.977     5.397     7.408    0.421        0.410 
x2            1    68.040     2.830     9.974    0.221        0.206 
x8            1    73.443     1.781    11.024    0.139        0.123 
x1            1    78.149     0.777    12.028    0.061        0.043 
x6            1    78.543     0.689    12.116    0.054        0.036 
x5            1    80.381     0.269    12.535    0.021        0.002 
x7            1    80.651     0.207    12.598    0.016       -0.003 
-------------------------------------------------------------------

- x3 added 


 Step 1 : AIC = 51.43434 
 log(y) ~ x3 

                        Enter New Variables                       
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x2            1    24.767     8.492    4.313    0.663        0.650 
x4            1    34.156     7.673    5.132    0.599        0.583 
x1            1    40.602     7.022    5.783    0.548        0.531 
x8            1    44.323     6.609    6.195    0.516        0.497 
x6            1    51.499     5.729    7.075    0.447        0.426 
x5            1    51.645     5.710    7.095    0.446        0.424 
x7            1    52.947     5.537    7.268    0.432        0.410 
------------------------------------------------------------------

- x2 added 


 Step 2 : AIC = 24.76682 
 log(y) ~ x3 + x2 

                    Remove Existing Variables                     
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x2            1    51.434     5.471    7.334    0.427        0.416 
x3            1    68.040     2.830    9.974    0.221        0.206 
------------------------------------------------------------------

                        Enter New Variables                       
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x8            1     4.243     9.963    2.842    0.778        0.765 
x1            1     9.084     9.696    3.109    0.757        0.743 
x4            1    17.234     9.190    3.615    0.718        0.701 
x7            1    23.834     8.720    4.085    0.681        0.662 
x5            1    24.663     8.656    4.148    0.676        0.657 
x6            1    25.727     8.574    4.231    0.670        0.650 
------------------------------------------------------------------

- x8 added 


 Step 3 : AIC = 4.243223 
 log(y) ~ x3 + x2 + x8 

                    Remove Existing Variables                     
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x8            1    24.767     8.492    4.313    0.663        0.650 
x2            1    44.323     6.609    6.195    0.516        0.497 
x3            1    56.640     5.022    7.782    0.392        0.368 
------------------------------------------------------------------

                        Enter New Variables                       
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x1            1    -8.131    10.627    2.178    0.830        0.816 
x4            1    -3.424    10.428    2.376    0.814        0.799 
x6            1     3.572    10.100    2.705    0.789        0.772 
x5            1     4.879    10.033    2.771    0.784        0.766 
x7            1     5.783     9.987    2.818    0.780        0.762 
------------------------------------------------------------------

- x1 added 


 Step 4 : AIC = -8.130569 
 log(y) ~ x3 + x2 + x8 + x1 

                    Remove Existing Variables                     
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x1            1     4.243     9.963    2.842    0.778        0.765 
x8            1     9.084     9.696    3.109    0.757        0.743 
x2            1    36.524     7.638    5.167    0.596        0.572 
x3            1    57.529     5.181    7.624    0.405        0.369 
------------------------------------------------------------------

                        Enter New Variables                       
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x6            1    -8.580    10.723    2.081    0.837        0.821 
x5            1    -8.048    10.703    2.102    0.836        0.819 
x4            1    -7.170    10.668    2.136    0.833        0.816 
x7            1    -6.689    10.649    2.155    0.832        0.814 
------------------------------------------------------------------

- x6 added 


 Step 5 : AIC = -8.580332 
 log(y) ~ x3 + x2 + x8 + x1 + x6 

                    Remove Existing Variables                     
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x6            1    -8.131    10.627    2.178    0.830        0.816 
x1            1     3.572    10.100    2.705    0.789        0.772 
x8            1    10.176     9.748    3.057    0.761        0.742 
x2            1    35.768     7.895    4.910    0.617        0.585 
x3            1    56.105     5.649    7.155    0.441        0.396 
------------------------------------------------------------------

                        Enter New Variables                       
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x5            1    -8.613    10.800    2.004    0.843        0.823 
x7            1    -7.151    10.745    2.059    0.839        0.819 
x4            1    -7.005    10.740    2.065    0.839        0.818 
------------------------------------------------------------------

- x5 added 


 Step 6 : AIC = -8.612898 
 log(y) ~ x3 + x2 + x8 + x1 + x6 + x5 

                    Remove Existing Variables                     
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x5            1    -8.580    10.723    2.081    0.837        0.821 
x6            1    -8.048    10.703    2.102    0.836        0.819 
x1            1     4.113    10.172    2.633    0.794        0.773 
x8            1     9.435     9.899    2.905    0.773        0.749 
x2            1    36.193     8.036    4.769    0.628        0.589 
x3            1    57.529     5.725    7.080    0.447        0.390 
------------------------------------------------------------------

                        Enter New Variables                       
------------------------------------------------------------------
Variable     DF     AIC      Sum Sq     RSS     R-Sq     Adj. R-Sq 
------------------------------------------------------------------
x7            1    -7.497    10.833    1.972    0.846        0.823 
x4            1    -6.673    10.802    2.002    0.844        0.820 
------------------------------------------------------------------


No more variables to be added or removed.

Final Model Output 
------------------

                        Model Summary                         
-------------------------------------------------------------
R                       0.918       RMSE               0.207 
R-Squared               0.843       Coef. Var          3.211 
Adj. R-Squared          0.823       MSE                0.043 
Pred R-Squared          0.784       MAE                0.162 
-------------------------------------------------------------
 RMSE: Root Mean Square Error 
 MSE: Mean Square Error 
 MAE: Mean Absolute Error 

                               ANOVA                                
-------------------------------------------------------------------
               Sum of                                              
              Squares        DF    Mean Square      F         Sig. 
-------------------------------------------------------------------
Regression     10.800         6          1.800    42.209    0.0000 
Residual        2.004        47          0.043                     
Total          12.805        53                                    
-------------------------------------------------------------------

                                  Parameter Estimates                                   
---------------------------------------------------------------------------------------
      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper 
---------------------------------------------------------------------------------------
(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 
         x3     0.015         0.001        0.653    10.909    0.000     0.012    0.018 
         x2     0.014         0.002        0.473     8.051    0.000     0.010    0.017 
         x8     0.351         0.076        0.280     4.597    0.000     0.197    0.505 
         x1     0.072         0.019        0.233     3.839    0.000     0.034    0.109 
         x6     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 
         x5    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 
---------------------------------------------------------------------------------------</code></pre>
</div>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mod_step_aic2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># extract formula from ending model</span></span>
<span><span class="va">mod_step_aic2_formula</span> <span class="op">&lt;-</span> <span class="va">mod_step_aic2</span><span class="op">$</span><span class="va">predictors</span> <span class="op">%&gt;%</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"log(y) ~ "</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">.</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span><span class="op">)</span><span class="op">}</span> <span class="op">%&gt;%</span> <span class="va">as.formula</span></span>
<span></span>
<span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_step_aic</span><span class="op">)</span>, <span class="va">mod_step_aic2_formula</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`formula(mod_step_aic)`
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5

$mod_step_aic2_formula
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5
&lt;environment: 0x7fd67710dcb8&gt;</code></pre>
</div>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># confirm using AIC()</span></span>
<span><span class="co"># -&gt; this result is found in the printed output!</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mod_step_aic2_formula</span>, <span class="va">data_surgery</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -8.612898</code></pre>
</div>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># perform stepwise regression (forward)</span></span>
<span><span class="co"># -&gt; based on p-values, using default alpha-to-enter and alpha-to-remove</span></span>
<span><span class="va">mod_step_p</span> <span class="op">&lt;-</span> <span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_step_both_p.html">ols_step_both_p</a></span><span class="op">(</span><span class="va">mod_full</span>, pent <span class="op">=</span> <span class="fl">0.1</span>, prem <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span>
<span><span class="va">mod_step_p</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
                             Stepwise Selection Summary                               
-------------------------------------------------------------------------------------
                     Added/                   Adj.                                       
Step    Variable    Removed     R-Square    R-Square      C(p)        AIC       RMSE     
-------------------------------------------------------------------------------------
   1       x3       addition       0.427       0.416    117.4780    51.4343    0.3755    
   2       x2       addition       0.663       0.650     50.4920    24.7668    0.2908    
   3       x8       addition       0.778       0.765     18.9010     4.2432    0.2384    
   4       x1       addition       0.830       0.816      5.7340    -8.1306    0.2108    
-------------------------------------------------------------------------------------</code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="other-stepwise-procedures" class="level3" data-number="8.3.3"><h3 data-number="8.3.3" class="anchored" data-anchor-id="other-stepwise-procedures">
<span class="header-section-number">8.3.3</span> Other stepwise procedures</h3>
<p>Forward selection</p>
<ul>
<li>The forward selection search procedure is a simplified version of forward stepwise regression, omitting the test whether a variable once entered into the model should be dropped.</li>
</ul>
<p>Demo</p>
<div class="cell">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># perform forward selection regression</span></span>
<span><span class="co"># -&gt; based on AIC (could do BIC same way for step())</span></span>
<span><span class="va">mod_forward</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mod_null</span>, scope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_full</span><span class="op">)</span>, direction <span class="op">=</span> <span class="st">"forward"</span>, k <span class="op">=</span> <span class="fl">2</span>, trace <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mod_forward2</span> <span class="op">&lt;-</span> <span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_step_forward_aic.html">ols_step_forward_aic</a></span><span class="op">(</span><span class="va">mod_full</span><span class="op">)</span></span>
<span><span class="va">mod_forward2_formula</span> <span class="op">&lt;-</span> <span class="va">mod_forward2</span><span class="op">$</span><span class="va">predictors</span> <span class="op">%&gt;%</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"log(y) ~ "</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">.</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span><span class="op">)</span><span class="op">}</span> <span class="op">%&gt;%</span> <span class="va">as.formula</span></span>
<span></span>
<span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_forward</span><span class="op">)</span>, <span class="va">mod_forward2_formula</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`formula(mod_forward)`
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5

$mod_forward2_formula
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5
&lt;environment: 0x7fd6757d0ab8&gt;</code></pre>
</div>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_step_aic</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_forward</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`formula(mod_step_aic)`
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5

$`formula(mod_forward)`
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5</code></pre>
</div>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># olsrr based on p-values</span></span>
<span><span class="co"># -&gt; default enter seems high</span></span>
<span><span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_step_forward_p.html">ols_step_forward_p</a></span><span class="op">(</span><span class="va">mod_full</span>, pent <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
                            Selection Summary                             
-------------------------------------------------------------------------
        Variable                  Adj.                                       
Step    Entered     R-Square    R-Square      C(p)        AIC       RMSE     
-------------------------------------------------------------------------
   1    x3            0.4273      0.4162    117.4783    51.4343    0.3755    
   2    x2            0.6632      0.6500     50.4918    24.7668    0.2908    
   3    x8            0.7780      0.7647     18.9015     4.2432    0.2384    
   4    x1            0.8299      0.8160      5.7340    -8.1306    0.2108    
   5    x6            0.8375      0.8205      5.5282    -8.5803    0.2082    
   6    x5            0.8435      0.8235      5.7725    -8.6129    0.2065    
-------------------------------------------------------------------------</code></pre>
</div>
</div>
<p>Backward elimation</p>
<ul>
<li><p>The backward elimination search procedure is the opposite of forward selection. It begins with the model containing all potential <span class="math inline">\(X\)</span> variables and identifies the one with the largest <span class="math inline">\(p\)</span>-value and determines if it should be dropped. Then repeat with the remaining <span class="math inline">\(P-2\)</span> <span class="math inline">\(X\)</span> variables.</p></li>
<li><p>This process continues until no further <span class="math inline">\(X\)</span> variables can be dropped.</p></li>
<li><p>A stepwise modification can also be adapted that allows variables eliminated earlier to be added later: this modification is called the backward stepwise regression procedure.</p></li>
<li>
<p>For small and moderate numbers of variables in the pool of potential <span class="math inline">\(X\)</span> variables, some statisticians argue for backward stepwise search over forward stepwise search.</p>
<ul>
<li><p>A potential disadvantage of the forward stepwise approach is that the <span class="math inline">\(MSE\)</span> (and hence s{_k}$) will lend to be inflated during the initial steps, because important predictors have been omitted <span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(t^*\)</span> test statistics that are too small.</p></li>
<li><p>For the backward stepwise procedure, <span class="math inline">\(MSE\)</span> values tend to be more nearly unbiased because important predictors are retained at each step.</p></li>
<li><p>An argument in favor of the backward stepwise procedure can also be made in situations where it is useful as a first step to look at each <span class="math inline">\(X\)</span> variable in the regression function adjusted for all the other <span class="math inline">\(X\)</span> variables in the pool.</p></li>
</ul>
</li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># perform backward selection regression</span></span>
<span><span class="co"># -&gt; based on AIC (could do BIC same way for step())</span></span>
<span><span class="va">mod_backward</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mod_full</span>, scope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_full</span><span class="op">)</span>, direction <span class="op">=</span> <span class="st">"backward"</span>, k <span class="op">=</span> <span class="fl">2</span>, trace <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mod_backward2</span> <span class="op">&lt;-</span> <span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_step_backward_aic.html">ols_step_backward_aic</a></span><span class="op">(</span><span class="va">mod_full</span><span class="op">)</span></span>
<span><span class="va">mod_backward2_formula</span> <span class="op">&lt;-</span> <span class="va">mod_forward2</span><span class="op">$</span><span class="va">predictors</span> <span class="op">%&gt;%</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"log(y) ~ "</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">.</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span><span class="op">)</span><span class="op">}</span> <span class="op">%&gt;%</span> <span class="va">as.formula</span></span>
<span></span>
<span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_backward</span><span class="op">)</span>, <span class="va">mod_backward2_formula</span><span class="op">)</span> <span class="co"># same models, different order</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
$comparison$result
[1] FALSE

$comparison$description
[1] "formulas differ in contents"

$comparison$`element-wise`
NULL


$`formula(mod_backward)`
log(y) ~ x1 + x2 + x3 + x5 + x6 + x8

$mod_backward2_formula
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5
&lt;environment: 0x7fd6a36465b8&gt;</code></pre>
</div>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">compare</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_step_aic</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_backward</span><span class="op">)</span><span class="op">)</span> <span class="co"># same models, different order</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
$comparison$result
[1] FALSE

$comparison$description
[1] "formulas differ in contents"

$comparison$`element-wise`
NULL


$`formula(mod_step_aic)`
log(y) ~ x3 + x2 + x8 + x1 + x6 + x5

$`formula(mod_backward)`
log(y) ~ x1 + x2 + x3 + x5 + x6 + x8</code></pre>
</div>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># olsrr based on p-values</span></span>
<span><span class="fu">olsrr</span><span class="fu">::</span><span class="fu"><a href="https://olsrr.rsquaredacademy.com/reference/ols_step_backward_p.html">ols_step_backward_p</a></span><span class="op">(</span><span class="va">mod_full</span>, prem <span class="op">=</span> <span class="fl">0.3</span>, progress <span class="op">=</span> <span class="cn">TRUE</span>, details <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Backward Elimination Method 
---------------------------

Candidate Terms: 

1 . x1 
2 . x2 
3 . x3 
4 . x4 
5 . x5 
6 . x6 
7 . x7 
8 . x8 

We are eliminating variables based on p value...

- x4 

Backward Elimination: Step 1 

 Variable x4 Removed 

                        Model Summary                         
-------------------------------------------------------------
R                       0.920       RMSE               0.207 
R-Squared               0.846       Coef. Var          3.220 
Adj. R-Squared          0.823       MSE                0.043 
Pred R-Squared          0.781       MAE                0.162 
-------------------------------------------------------------
 RMSE: Root Mean Square Error 
 MSE: Mean Square Error 
 MAE: Mean Absolute Error 

                               ANOVA                                
-------------------------------------------------------------------
               Sum of                                              
              Squares        DF    Mean Square      F         Sig. 
-------------------------------------------------------------------
Regression     10.833         7          1.548    36.103    0.0000 
Residual        1.972        46          0.043                     
Total          12.805        53                                    
-------------------------------------------------------------------

                                  Parameter Estimates                                   
---------------------------------------------------------------------------------------
      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper 
---------------------------------------------------------------------------------------
(Intercept)     4.037         0.236                 17.096    0.000     3.562    4.513 
         x1     0.071         0.019        0.233     3.824    0.000     0.034    0.109 
         x2     0.014         0.002        0.468     7.900    0.000     0.010    0.017 
         x3     0.015         0.001        0.655    10.901    0.000     0.012    0.018 
         x5    -0.004         0.003       -0.084    -1.429    0.160    -0.009    0.002 
         x6     0.087         0.058        0.089     1.503    0.140    -0.029    0.203 
         x7     0.058         0.067        0.059     0.872    0.388    -0.076    0.192 
         x8     0.388         0.087        0.309     4.437    0.000     0.212    0.564 
---------------------------------------------------------------------------------------


- x7 

Backward Elimination: Step 2 

 Variable x7 Removed 

                        Model Summary                         
-------------------------------------------------------------
R                       0.918       RMSE               0.207 
R-Squared               0.843       Coef. Var          3.211 
Adj. R-Squared          0.823       MSE                0.043 
Pred R-Squared          0.784       MAE                0.162 
-------------------------------------------------------------
 RMSE: Root Mean Square Error 
 MSE: Mean Square Error 
 MAE: Mean Absolute Error 

                               ANOVA                                
-------------------------------------------------------------------
               Sum of                                              
              Squares        DF    Mean Square      F         Sig. 
-------------------------------------------------------------------
Regression     10.800         6          1.800    42.209    0.0000 
Residual        2.004        47          0.043                     
Total          12.805        53                                    
-------------------------------------------------------------------

                                  Parameter Estimates                                   
---------------------------------------------------------------------------------------
      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper 
---------------------------------------------------------------------------------------
(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 
         x1     0.072         0.019        0.233     3.839    0.000     0.034    0.109 
         x2     0.014         0.002        0.473     8.051    0.000     0.010    0.017 
         x3     0.015         0.001        0.653    10.909    0.000     0.012    0.018 
         x5    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 
         x6     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 
         x8     0.351         0.076        0.280     4.597    0.000     0.197    0.505 
---------------------------------------------------------------------------------------



No more variables satisfy the condition of p value = 0.3


Variables Removed: 

- x4 
- x7 


Final Model Output 
------------------

                        Model Summary                         
-------------------------------------------------------------
R                       0.918       RMSE               0.207 
R-Squared               0.843       Coef. Var          3.211 
Adj. R-Squared          0.823       MSE                0.043 
Pred R-Squared          0.784       MAE                0.162 
-------------------------------------------------------------
 RMSE: Root Mean Square Error 
 MSE: Mean Square Error 
 MAE: Mean Absolute Error 

                               ANOVA                                
-------------------------------------------------------------------
               Sum of                                              
              Squares        DF    Mean Square      F         Sig. 
-------------------------------------------------------------------
Regression     10.800         6          1.800    42.209    0.0000 
Residual        2.004        47          0.043                     
Total          12.805        53                                    
-------------------------------------------------------------------

                                  Parameter Estimates                                   
---------------------------------------------------------------------------------------
      model      Beta    Std. Error    Std. Beta      t        Sig      lower    upper 
---------------------------------------------------------------------------------------
(Intercept)     4.054         0.235                 17.272    0.000     3.582    4.527 
         x1     0.072         0.019        0.233     3.839    0.000     0.034    0.109 
         x2     0.014         0.002        0.473     8.051    0.000     0.010    0.017 
         x3     0.015         0.001        0.653    10.909    0.000     0.012    0.018 
         x5    -0.003         0.003       -0.078    -1.343    0.186    -0.009    0.002 
         x6     0.087         0.058        0.089     1.512    0.137    -0.029    0.203 
         x8     0.351         0.076        0.280     4.597    0.000     0.197    0.505 
---------------------------------------------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
                          Elimination Summary                           
-----------------------------------------------------------------------
        Variable                  Adj.                                     
Step    Removed     R-Square    R-Square     C(p)       AIC       RMSE     
-----------------------------------------------------------------------
   1    x4             0.846      0.8226    7.0288    -7.4974    0.2070    
   2    x7            0.8435      0.8235    5.7725    -8.6129    0.2065    
-----------------------------------------------------------------------</code></pre>
</div>
</div>
</section></section><section id="model-validation-1" class="level2" data-number="8.4"><h2 data-number="8.4" class="anchored" data-anchor-id="model-validation-1">
<span class="header-section-number">8.4</span> Model validation</h2>
<section id="overview" class="level3" data-number="8.4.1"><h3 data-number="8.4.1" class="anchored" data-anchor-id="overview">
<span class="header-section-number">8.4.1</span> Overview</h3>
<ul>
<li>The final step in the model-building process is the validation of the selected regression models. Model validation usually involves checking a candidate model against independent data. Three basic ways of validating a regression model are:</li>
</ul>
<ol type="1">
<li><p>Collection of new data to check the model and its predictive ability.</p></li>
<li><p>Comparison of results with theoretical expectations, earlier empirical results, and simulation results.</p></li>
<li><p>Use of a holdout sample to check the model and its predictive ability.</p></li>
</ol>
<p>Other types of studies</p>
<ul>
<li>
<p>When a regression model is used in a controlled experiment, a repetition of the experiment and its analysis serves to validate the findings in the initial study if similar results for the regression coefficients, predictive ability, and the like are obtained. Same for confirmatory observational studies, just use other data and repeat analyses.</p>
<ul>
<li><p>This is because there is no issue with variable selection.</p></li>
<li><p>Additionally, if the model is to be used for making predictions over the entire range of the <span class="math inline">\(X\)</span> observations, a possibility is to include data points that are uniformly distributed over the <span class="math inline">\(X\)</span> space.</p></li>
</ul>
</li>
<li>
<p>On the contrast, definitely need to validate for exploratory observational studies because they often start with large pools of <span class="math inline">\(X\)</span> variables which is narrowed down significantly.</p>
<ul>
<li>For these studies, validation of the regression model involves also the appropriateness of the variables selected, as well as the magnitudes of the regression coefficients, the predictive ability of the model, and the like.</li>
</ul>
</li>
</ul></section><section id="collection-of-new-data-to-check-model" class="level3" data-number="8.4.2"><h3 data-number="8.4.2" class="anchored" data-anchor-id="collection-of-new-data-to-check-model">
<span class="header-section-number">8.4.2</span> Collection of new data to check model</h3>
<ul>
<li>
<p>The best means of model validation is through the collection of new data. The purpose of collecting new data is to be able to examine whether the regression model developed from the earlier data is still applicable for the new data.</p>
<ul>
<li>If so, one has assurance about the applicability of the model to data beyond those on which the model is based.</li>
</ul>
</li>
</ul>
<p>Methods of checking validity</p>
<ul>
<li>There are a variety of methods of examining the validity of the regression model against the new data.</li>
</ul>
<ol type="1">
<li>
<p>One validation method is to reestimate the model form chosen earlier using the new data.</p>
<ul>
<li><p>The estimated regression coefficients and various characteristics of the fitted model are then compared for consistency to those of the regression model based on the em’lier data.</p></li>
<li><p>If the results are consistent, they provide strong support that the chosen regression model is applicable under broader circumstances than those related to the original data.</p></li>
</ul>
</li>
<li>
<p>A second validation method is designed to calibrate the predictive capability of the selected regression model.</p>
<ul>
<li><p>When a regression model is developed from given data, it is inevitable that the selected model is chosen, at least in large part, because it fits well the data at hand.</p></li>
<li><p>For a different set of random outcomes, one may likely have arrived at a different model in terms of the predictor valiables selected and/or their functional forms and interaction terms present in the model.</p></li>
<li><p>A result of this model development process is that the <span class="math inline">\(MSE\)</span> will tend to understate the inherent variability in making future predictions from the selected model.</p></li>
<li><p>A means of measuring the actual predictive capability of the selected regression model is to use this model to predict each case in the new data set and then to calculate the mean squared prediction error <span class="math inline">\(MSPR\)</span>:</p></li>
<li><p>(where <span class="math inline">\(Y_i\)</span> is the value of the response valiable in the <span class="math inline">\(i\)</span>th validation case; <span class="math inline">\(\hat{Y}_i\)</span> is the predicted value for the <span class="math inline">\(i\)</span>th validation case based on the model-building dataset; and <span class="math inline">\(n^*\)</span> is the number of cases in the validation set)</p></li>
</ul>
</li>
</ol>
<p><span class="math display">\[MSPR = \frac{1}{n^*}\sum_{i = 1}^{n^*} (Y_i - \hat{Y}_i)^2\]</span></p>
<ul>
<li>
<p>If the <span class="math inline">\(MSPR\)</span> is fairly close to <span class="math inline">\(MSE\)</span> based on the regression fit to the model-building data set, then the <span class="math inline">\(MSE\)</span> for the selected regression model is not seriously biased and gives an appropriate indication of the predictive ability of the model.</p>
<ul>
<li>If the <span class="math inline">\(MSPE\)</span> is much larger than <span class="math inline">\(MSE\)</span>, one should rely on the <span class="math inline">\(MSPE\)</span> as an indicator of how well the selected regression model will predict in the future.</li>
</ul>
</li>
</ul>
<p>Difficulties in replicating a study</p>
<ul>
<li><p>Difficulties often arise when new data are collected to validate a regression model, especially with observational studies.</p></li>
<li><p>Essentially, hard to get exactly identical conditions. In observational studies, things like settings and time frequently change.</p></li>
<li>
<p>Even though perfect replication is impossible, all validation studies are useful because No single study is fully useful until we know how much the results of the study can be generalized.</p>
<ul>
<li><p>If a replication study for which the conditions of the setting differ only slightly from those of the initial study substantially different regression results, then we learn that the results of the initial study cannot be readily generalized.</p></li>
<li><p>On the other hand, if the conditions differ substantially and the regression results are still similar, we find that the regression results can be generalized to apply under substantially varying conditions.</p></li>
<li><p>Still another possibility is that the regression results for the replication study differ substantially from those of the initial study, the differences being related to changes in the setting. This information may be useful for enriching the regression model by including new explanatory variables that make the model more widely applicable.</p></li>
</ul>
</li>
</ul></section><section id="comparison-with-theory-empirical-evidence-or-simulation-results" class="level3" data-number="8.4.3"><h3 data-number="8.4.3" class="anchored" data-anchor-id="comparison-with-theory-empirical-evidence-or-simulation-results">
<span class="header-section-number">8.4.3</span> Comparison with theory, empirical evidence, or simulation results</h3>
<ul>
<li><p>In some cases, theory, simulation rdmlts, or previous empirical results may be helpful in determining whether the selected model is reasonable.</p></li>
<li><p>Comparisons of regression coefficients and predictions with theoretical expectations, previous empirical results, or simulation results should be made.</p></li>
<li><p>Unfortunately, there is often little theory that can be used to validate regression models.</p></li>
</ul></section><section id="data-splitting" class="level3" data-number="8.4.4"><h3 data-number="8.4.4" class="anchored" data-anchor-id="data-splitting">
<span class="header-section-number">8.4.4</span> Data splitting</h3>
<p>Overview</p>
<ul>
<li><p>By far the preferred method to validate a regression model is through the collection of new data. Often, however, this is neither practical nor feasible.</p></li>
<li>
<p>An alternative is to use a training and testing set, which in effect an attempt to simulate replication of the study. The validation data set is used for validation in the same way as when new data are collected.</p>
<ul>
<li><p>The regression coefficients can be reestimated for the selected model and then compareed for consistency with the coefficients obtained from the model-building data set.</p></li>
<li><p>Also, predictions can be made for the testing data from the model built on the training data in order to calibrate the predictive ability of this regression model for the new data.</p></li>
<li><p>When the calibration data set is large enough, one can also study how the “good” models considered in the model selection phase fare with the new data.</p></li>
</ul>
</li>
</ul>
<p>How to split</p>
<ul>
<li><p><span class="math inline">\(n_{\text{train}} &gt;&gt; n_{\text{test}}\)</span>, can even go 50/50 if large enough dataset.</p></li>
<li><p>Splits of the data can be made at random or stratified to ensure certain spread of cases (roughly equal representation of <span class="math inline">\(X\)</span> levels) in the training and testing datasets.</p></li>
<li>
<p>Can also do <span class="math inline">\(k\)</span>-fold cross validation. The <span class="math inline">\(k\)</span> estimates of prediction error are then combined to produce a <span class="math inline">\(k\)</span>-fold cross-validation estimate.</p>
<ul>
<li><p>Note that when <span class="math inline">\(k = n\)</span>, this criteria is equal to <span class="math inline">\(PRESS_p\)</span> <span class="math inline">\(\Longrightarrow\)</span> <span class="math inline">\(k\)</span>-fold cross validation estimates are approximations to <span class="math inline">\(PRESS_p\)</span>.</p></li>
<li><p>Can also use a variation of <span class="math inline">\(PRESS_p\)</span> where <span class="math inline">\(m\)</span> observations are held out rather than 1, and the <span class="math inline">\(n-m\)</span> observations are used to fit the models.</p></li>
</ul>
</li>
<li>
<p>If a dataset for an exploratory observational study is very large, it can be divided into three parts.</p>
<ul>
<li><p>The first part is used for model training, the second part for cross-validation and model selection, and the third part for testing and calibrating the final model.</p></li>
<li><p>This approach avoids any bias resulting from estimating the regression parameters from the same data set used for developing the model.</p></li>
<li><p>But it still uses less data to fit the model, so less precise standard errors of estimated coefficients.</p></li>
</ul>
</li>
</ul>
<p>Drawbacks</p>
<ul>
<li><p>A possible drawback of data splitting is that the variances of the estimated regression coefficients developed from the model-building data set will usually be larger than those that would have been obtained from the fit to the entire data set.</p></li>
<li><p>If the model-building data set is reasonably large, however, these variances generally will not be that much larger than those for the entire data set.</p></li>
<li><p>In any case, once the model has been validated, it is customary practice to use the entire data set for estimating the final regression model.</p></li>
</ul>
<p>Results</p>
<ul>
<li>
<p>When regression models built on observational data do not predict well outside the range of the <span class="math inline">\(X\)</span> observations in the data set (extrapolation), the usual reason is the existence of multicollinearity among the <span class="math inline">\(X\)</span> variables.</p>
<ul>
<li>Ridge regression or other biased estimation techniques are possible solutions to this.</li>
</ul>
</li>
</ul></section><section id="demo-data-splitting" class="level3" data-number="8.4.5"><h3 data-number="8.4.5" class="anchored" data-anchor-id="demo-data-splitting">
<span class="header-section-number">8.4.5</span> Demo – Data splitting</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true"><code>tidymodels</code></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false"><code>caret</code></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-3" role="tab" aria-controls="tabset-5-3" aria-selected="false"><code>dplyr</code></a></li>
</ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="cell">
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># random split 2-way</span></span>
<span><span class="co"># -&gt; make train / test split</span></span>
<span><span class="va">data_split</span> <span class="op">&lt;-</span> <span class="va">data_surgery</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">initial_split</a></span><span class="op">(</span>prop <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">data_split</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">training</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="va">data_split</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">testing</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># stratified split</span></span>
<span><span class="co"># -&gt; https://www.tidymodels.org/start/case-study/</span></span>
<span><span class="co"># -&gt; based on response variable (ensures representative training and testing)</span></span>
<span><span class="co"># -&gt; if numeric, does based on binned quantiles</span></span>
<span><span class="co"># -&gt; if categorical, does it based on proportion of levels</span></span>
<span><span class="va">data_split_strat</span> <span class="op">&lt;-</span> <span class="va">data_surgery</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">initial_split</a></span><span class="op">(</span>prop <span class="op">=</span> <span class="fl">.7</span>, strata <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">data_train2</span> <span class="op">&lt;-</span> <span class="va">data_split_strat</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">training</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_test2</span> <span class="op">&lt;-</span> <span class="va">data_split_strat</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">testing</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># random split 3-way</span></span>
<span><span class="co"># -&gt; make train / validation / test split</span></span>
<span><span class="va">data_split_diamonds</span> <span class="op">&lt;-</span> <span class="va">diamonds</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_validation_split.html">initial_validation_split</a></span><span class="op">(</span>prop <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.6</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">data_train_diamonds</span> <span class="op">&lt;-</span> <span class="va">data_split_diamonds</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">training</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_validation_diamonds</span>  <span class="op">&lt;-</span> <span class="va">data_split_diamonds</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_validation_split.html">validation</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_test_diamonds</span>  <span class="op">&lt;-</span> <span class="va">data_split_diamonds</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">testing</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># k folds</span></span>
<span><span class="co"># -&gt; https://rsample.tidymodels.org/articles/Working_with_rsets.html</span></span>
<span><span class="co"># -&gt; access analysis and holdout data for the first fold</span></span>
<span><span class="co"># -&gt; setup to map() a function to perform on each fold split</span></span>
<span><span class="va">kfolds</span> <span class="op">&lt;-</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/vfold_cv.html">vfold_cv</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">data_surgery</span>, v <span class="op">=</span> <span class="fl">10</span>, repeats <span class="op">=</span> <span class="fl">1</span>, strata <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">kfolds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#  10-fold cross-validation using stratification 
# A tibble: 10 × 2
   splits         id    
   &lt;list&gt;         &lt;chr&gt; 
 1 &lt;split [48/6]&gt; Fold01
 2 &lt;split [48/6]&gt; Fold02
 3 &lt;split [48/6]&gt; Fold03
 4 &lt;split [48/6]&gt; Fold04
 5 &lt;split [48/6]&gt; Fold05
 6 &lt;split [48/6]&gt; Fold06
 7 &lt;split [48/6]&gt; Fold07
 8 &lt;split [50/4]&gt; Fold08
 9 &lt;split [50/4]&gt; Fold09
10 &lt;split [50/4]&gt; Fold10</code></pre>
</div>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">data_analysis1</span> <span class="op">&lt;-</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/as.data.frame.rsplit.html">analysis</a></span><span class="op">(</span><span class="va">kfolds</span><span class="op">$</span><span class="va">splits</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">data_holdout1</span> <span class="op">&lt;-</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/as.data.frame.rsplit.html">assessment</a></span><span class="op">(</span><span class="va">kfolds</span><span class="op">$</span><span class="va">splits</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># IMPORTANT NOTE: in practice would do the resampling method on the TRAINING data, then get the final model performance on the testing set</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<div class="cell">
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># random split</span></span>
<span></span>
<span><span class="co"># get indices in training data</span></span>
<span><span class="co"># then subset data to training and testing data</span></span>
<span><span class="va">in_train</span> <span class="op">&lt;-</span> <span class="fu">caret</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/createDataPartition.html">createDataPartition</a></span><span class="op">(</span><span class="va">data_surgery</span><span class="op">$</span><span class="va">y</span>, p <span class="op">=</span> <span class="fl">0.7</span><span class="op">)</span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">data_surgery</span><span class="op">[</span><span class="va">in_train</span><span class="op">$</span><span class="va">Resample1</span>, <span class="op">]</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="va">data_surgery</span><span class="op">[</span><span class="op">-</span><span class="op">(</span><span class="va">in_train</span><span class="op">$</span><span class="va">Resample1</span><span class="op">)</span>, <span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-5-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-3-tab">
<div class="cell">
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># random split</span></span>
<span></span>
<span><span class="co"># sample a proportion for training set and get all non matches for testing</span></span>
<span><span class="co"># -&gt; requires ID variable (saving new object to not overwrite original data)</span></span>
<span><span class="va">data_surgery_tmp</span> <span class="op">&lt;-</span> <span class="va">data_surgery</span> <span class="op">%&gt;%</span> <span class="fu">mutate</span><span class="op">(</span>id <span class="op">=</span> <span class="fu">row_number</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">data_surgery_tmp</span> <span class="op">%&gt;%</span> <span class="fu">slice_sample</span><span class="op">(</span>prop <span class="op">=</span> <span class="fl">0.7</span><span class="op">)</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="fu">anti_join</span><span class="op">(</span><span class="va">data_surgery_tmp</span>, <span class="va">data_train</span>, by <span class="op">=</span> <span class="fu">join_by</span><span class="op">(</span><span class="va">id</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section><section id="demo-get-best-model-from-train" class="level3" data-number="8.4.6"><h3 data-number="8.4.6" class="anchored" data-anchor-id="demo-get-best-model-from-train">
<span class="header-section-number">8.4.6</span> Demo – Get best model from train</h3>
<div class="cell">
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># random split 2-way</span></span>
<span><span class="co"># -&gt; make train / test split</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">data_split</span> <span class="op">&lt;-</span> <span class="va">data_surgery</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">initial_split</a></span><span class="op">(</span>prop <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">data_split</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">training</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="va">data_split</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">testing</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># get final best model</span></span>
<span></span>
<span><span class="co"># perform best subsets regression and save necessary output</span></span>
<span><span class="co"># -&gt; note there are options to force variables in and out</span></span>
<span><span class="co"># -&gt; nbest = nmax does ALL possible models</span></span>
<span><span class="va">best_subs</span> <span class="op">&lt;-</span> <span class="fu">leaps</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_start_prime</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data_train</span>, </span>
<span>                               nbest <span class="op">=</span> <span class="fl">8</span>, nvmax <span class="op">=</span> <span class="fl">8</span>, method <span class="op">=</span> <span class="st">"exhaustive"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># format and results</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu">format_results</span><span class="op">(</span><span class="va">best_subs</span><span class="op">)</span></span>
<span><span class="fu">map</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span>, \<span class="op">(</span><span class="va">var</span><span class="op">)</span> <span class="fu">plot_results</span><span class="op">(</span><span class="va">results</span>, <span class="va">var</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[2]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-25-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[3]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-25-3.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[4]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-25-4.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[[5]]</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="notes-building-reg-model-1_files/figure-html/unnamed-chunk-25-5.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># criteria -&gt; best model</span></span>
<span><span class="co"># rsq -&gt; 4</span></span>
<span><span class="co"># rss -&gt; 4</span></span>
<span><span class="co"># adjr2 -&gt; 3</span></span>
<span><span class="co"># cp -&gt; 4</span></span>
<span><span class="co"># bic -&gt; 3</span></span>
<span></span>
<span><span class="co"># going with best 4 parameter model (3 Xs)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">best_subs</span><span class="op">)</span><span class="op">$</span><span class="va">which</span> <span class="op">%&gt;%</span> <span class="va">data.frame</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/row.names.html">row.names</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"X3"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   X.Intercept.    x1   x2   x3    x4    x5    x6    x7   x8
X3         TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE</code></pre>
</div>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># for sake of simplicity, not evaluating multiple models and skipping some criteria like PRESSp</span></span>
<span></span>
<span><span class="co"># fit final model using best model</span></span>
<span><span class="co"># -&gt; keeping x7 so indicator variables x7 and x8 are together</span></span>
<span><span class="va">mod_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span> <span class="op">+</span> <span class="va">x7</span> <span class="op">+</span> <span class="va">x8</span>, <span class="va">data_train</span><span class="op">)</span></span>
<span><span class="va">mod_train</span> <span class="op">%&gt;%</span> <span class="va">tidy</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)   4.32     0.181      23.9   5.71e-22
2 x2            0.0156   0.00195     8.00  3.93e- 9
3 x3            0.0133   0.00157     8.46  1.16e- 9
4 x7            0.0466   0.0796      0.586 5.62e- 1
5 x8            0.344    0.0986      3.49  1.44e- 3</code></pre>
</div>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mod_train</span> <span class="op">%&gt;%</span> <span class="va">glance</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 12
  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1     0.821         0.799 0.207      36.7 1.55e-11     4   8.44 -4.88  4.79
# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
</div>
</div>
</section><section id="demo-validate-model" class="level3" data-number="8.4.7"><h3 data-number="8.4.7" class="anchored" data-anchor-id="demo-validate-model">
<span class="header-section-number">8.4.7</span> Demo – Validate model</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true">R functions</a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-2" role="tab" aria-controls="tabset-6-2" aria-selected="false">Manual</a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-3" role="tab" aria-controls="tabset-6-3" aria-selected="false"><span class="math inline">\(k\)</span>-fold cross validation <span class="math inline">\(RMSE\)</span> – Semi manual</a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-4" role="tab" aria-controls="tabset-6-4" aria-selected="false"><span class="math inline">\(k\)</span>-fold cross validation <span class="math inline">\(RMSE\)</span> – Full tidymodels</a></li>
</ul>
<div class="tab-content">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<div class="cell">
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># make predictions on testing data</span></span>
<span><span class="va">preds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_train</span>, newdata <span class="op">=</span> <span class="va">data_test</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># calculate rmse</span></span>
<span><span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/rmse.html">rmse_vec</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_test</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>, estimate <span class="op">=</span> <span class="va">preds</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3088084</code></pre>
</div>
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate MSPE</span></span>
<span><span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/rmse.html">rmse_vec</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_test</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>, estimate <span class="op">=</span> <span class="va">preds</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.09536261</code></pre>
</div>
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># compare regression coefficients fit on training data to testing data for consistency</span></span>
<span><span class="va">mod_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_train</span><span class="op">)</span>, <span class="va">data_test</span><span class="op">)</span></span>
<span><span class="fu">tidy</span><span class="op">(</span><span class="va">mod_test</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 5
  term        estimate std.error statistic    p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
1 (Intercept)  4.30      0.521       8.26  0.00000270
2 x2           0.00977   0.00595     1.64  0.126     
3 x3           0.0194    0.00446     4.35  0.000952  
4 x7          -0.0948    0.217      -0.437 0.670     
5 x8           0.555     0.278       2.00  0.0691    </code></pre>
</div>
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">tidy</span><span class="op">(</span><span class="va">mod_train</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)   4.32     0.181      23.9   5.71e-22
2 x2            0.0156   0.00195     8.00  3.93e- 9
3 x3            0.0133   0.00157     8.46  1.16e- 9
4 x7            0.0466   0.0796      0.586 5.62e- 1
5 x8            0.344    0.0986      3.49  1.44e- 3</code></pre>
</div>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># compare MSEp and MSE</span></span>
<span><span class="fu">compare</span><span class="op">(</span><span class="va">mod_test</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">raise_to_power</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>,</span>
<span>        <span class="va">mod_train</span> <span class="op">%&gt;%</span> <span class="va">glance</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">raise_to_power</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
$comparison$result
[1] FALSE

$comparison$description
[1] "Mean relative difference: 0.4767689"

$comparison$`element-wise`
[1] FALSE


$`mod_test %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2)`
[1] 0.08199894

$`mod_train %&gt;% glance %&gt;% pull(sigma) %&gt;% raise_to_power(2)`
[1] 0.0429044</code></pre>
</div>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_surgery</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2415945</code></pre>
</div>
</div>
<p>Conclusions</p>
<ul>
<li><p>Coefficients of the testing model are all within a standard error or two of the training model (only concern in x7 switches signs)</p></li>
<li><p><span class="math inline">\(MSE\)</span>s are close, especially considering scale of <span class="math inline">\(var(ln(y))\)</span> for reference. So it does perform well for predictive accuracy.</p></li>
</ul>
</div>
<div id="tabset-6-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-2-tab">
<div class="cell">
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate MSPE</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_test</span><span class="op">$</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">preds</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">data_test</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.09536261</code></pre>
</div>
<div class="sourceCode" id="cb129"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate RMSE = sqrt(MSPE)</span></span>
<span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data_test</span><span class="op">$</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">preds</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">data_test</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">sqrt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3088084</code></pre>
</div>
</div>
</div>
<div id="tabset-6-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-3-tab">
<div class="cell">
<div class="sourceCode" id="cb131"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># k folds</span></span>
<span><span class="va">kfolds</span> <span class="op">&lt;-</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/vfold_cv.html">vfold_cv</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">data_surgery</span>, v <span class="op">=</span> <span class="fl">10</span>, repeats <span class="op">=</span> <span class="fl">1</span>, strata <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># function to:</span></span>
<span><span class="co"># 1) fit model on analysis data</span></span>
<span><span class="co"># 2) make predictions on holdout data</span></span>
<span><span class="co"># ... will be the model formula</span></span>
<span><span class="va">holdout_results</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">splits</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># fit the model to the analysis data (90%: 9 of the 10 folds)</span></span>
<span>  <span class="va">mod_kfold</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">...</span>, data <span class="op">=</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/as.data.frame.rsplit.html">analysis</a></span><span class="op">(</span><span class="va">splits</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># save the holdout data (10%: last fold)</span></span>
<span>  <span class="va">data_holdout</span> <span class="op">=</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/as.data.frame.rsplit.html">assessment</a></span><span class="op">(</span><span class="va">splits</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># `augment` will save the predictions with the holdout data set</span></span>
<span>  <span class="va">preds</span> <span class="op">=</span> <span class="fu">augment</span><span class="op">(</span><span class="va">mod_kfold</span>, newdata <span class="op">=</span> <span class="va">data_holdout</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">preds</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># fit models for k-fold cross validation</span></span>
<span><span class="va">kfolds</span><span class="op">$</span><span class="va">results</span> <span class="op">&lt;-</span> <span class="va">kfolds</span><span class="op">$</span><span class="va">splits</span> <span class="op">%&gt;%</span> <span class="fu">map</span><span class="op">(</span><span class="va">holdout_results</span>, <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_train</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># calculate rmse for each fold</span></span>
<span><span class="va">kfolds</span><span class="op">$</span><span class="va">rmse</span> <span class="op">&lt;-</span> <span class="va">kfolds</span><span class="op">$</span><span class="va">results</span> <span class="op">%&gt;%</span> <span class="fu">map_dbl</span><span class="op">(</span>\<span class="op">(</span><span class="va">df</span><span class="op">)</span> <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/rmse.html">rmse_vec</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,estimate <span class="op">=</span> <span class="va">df</span><span class="op">$</span><span class="va">`.fitted`</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># calculate final cross validation rmse</span></span>
<span><span class="va">kfolds</span><span class="op">$</span><span class="va">rmse</span> <span class="op">%&gt;%</span> <span class="va">mean</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2440127</code></pre>
</div>
</div>
</div>
<div id="tabset-6-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-4-tab">
<p><a href="https://catalina.quarto.pub/introduction-to-tidymodels/">SLIDES</a></p>
<div class="cell">
<div class="sourceCode" id="cb133"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit a single linear regression model on entire data for practice</span></span>
<span><span class="co"># -&gt; https://www.tidymodels.org/start/models/</span></span>
<span></span>
<span><span class="co"># make the parsnip model </span></span>
<span><span class="va">spec_linreg</span> <span class="op">&lt;-</span> <span class="fu">parsnip</span><span class="fu">::</span><span class="fu"><a href="https://parsnip.tidymodels.org/reference/linear_reg.html">linear_reg</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># train the model</span></span>
<span><span class="va">fit_linreg</span> <span class="op">&lt;-</span> <span class="va">mod_linreg</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">parsnip</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">data_surgery</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(expr, envir, enclos): object 'mod_linreg' not found</code></pre>
</div>
<div class="sourceCode" id="cb135"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">tidy</span><span class="op">(</span><span class="va">fit_linreg</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in eval(expr, envir, enclos): object 'fit_linreg' not found</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb137"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit models for all resamples</span></span>
<span><span class="co"># -&gt; https://www.tidymodels.org/start/resampling/</span></span>
<span></span>
<span><span class="co"># use kfolds from above</span></span>
<span></span>
<span><span class="co"># define workflow</span></span>
<span><span class="va">wf_linreg</span> <span class="op">&lt;-</span> <span class="fu">workflows</span><span class="fu">::</span><span class="fu"><a href="https://workflows.tidymodels.org/reference/workflow.html">workflow</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">workflows</span><span class="fu">::</span><span class="fu"><a href="https://workflows.tidymodels.org/reference/add_model.html">add_model</a></span><span class="op">(</span><span class="va">spec_linreg</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">workflows</span><span class="fu">::</span><span class="fu"><a href="https://workflows.tidymodels.org/reference/add_formula.html">add_formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">formula</a></span><span class="op">(</span><span class="va">mod_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">wf_linreg</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>══ Workflow ════════════════════════════════════════════════════════════════════
Preprocessor: Formula
Model: linear_reg()

── Preprocessor ────────────────────────────────────────────────────────────────
log(y) ~ x2 + x3 + x7 + x8

── Model ───────────────────────────────────────────────────────────────────────
Linear Regression Model Specification (regression)

Computational engine: lm </code></pre>
</div>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit cross validation models </span></span>
<span><span class="va">rs_fit_linreg</span> <span class="op">&lt;-</span> <span class="va">wf_linreg</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">tune</span><span class="fu">::</span><span class="fu"><a href="https://tune.tidymodels.org/reference/fit_resamples.html">fit_resamples</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">.</span>,</span>
<span>                      resamples <span class="op">=</span> <span class="va">kfolds</span>,</span>
<span>                      metrics <span class="op">=</span> <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/metric_set.html">metric_set</a></span><span class="op">(</span><span class="fu">yardstick</span><span class="fu">::</span><span class="va"><a href="https://yardstick.tidymodels.org/reference/rmse.html">rmse</a></span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># get final cross validation measures</span></span>
<span><span class="fu">workflowsets</span><span class="fu">::</span><span class="fu"><a href="https://tune.tidymodels.org/reference/collect_predictions.html">collect_metrics</a></span><span class="op">(</span><span class="va">rs_fit_linreg</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 6
  .metric .estimator  mean     n std_err .config             
  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
1 rmse    standard   0.244    10  0.0325 Preprocessor1_Model1</code></pre>
</div>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">compare</span><span class="op">(</span><span class="va">kfolds</span><span class="op">$</span><span class="va">rmse</span> <span class="op">%&gt;%</span> <span class="va">mean</span>,</span>
<span>        <span class="fu">workflowsets</span><span class="fu">::</span><span class="fu"><a href="https://tune.tidymodels.org/reference/collect_predictions.html">collect_metrics</a></span><span class="op">(</span><span class="va">rs_fit_linreg</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">mean</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$comparison
[1] TRUE

$`kfolds$rmse %&gt;% mean`
[1] 0.2440127

$`workflowsets::collect_metrics(rs_fit_linreg) %&gt;% pull(mean)`
[1] 0.2440127</code></pre>
</div>
</div>
</div>
</div>
</div>


<!-- -->

</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./notes-reg-models-quan-and-qual.html" class="pagination-link" aria-label="Regression models for quantitative and qualitative predictors">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Regression models for quantitative and qualitative predictors</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb143" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Building the regression model 1 -- Model selection and validation</span></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load-prereqs</span></span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-10"><a href="#cb143-10" aria-hidden="true" tabindex="-1"></a><span class="co"># knitr options</span></span>
<span id="cb143-11"><a href="#cb143-11" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb143-12"><a href="#cb143-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-13"><a href="#cb143-13" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-14"><a href="#cb143-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-15"><a href="#cb143-15" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % define LaTeX macros (/shortcuts) --&gt;</span></span>
<span id="cb143-16"><a href="#cb143-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-17"><a href="#cb143-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % define vector (without parentheses, so when writing out in like a definition) of the form X_1, ..., X_n, where X and n are variable. </span><span class="al">NOTE</span><span class="co">: to call use $\vecn{X}{n}$ --&gt;</span></span>
<span id="cb143-18"><a href="#cb143-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-19"><a href="#cb143-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\vecn}<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{#1_1, \ldots, #1_{#2}}</span>
<span id="cb143-20"><a href="#cb143-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-21"><a href="#cb143-21" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for ~ 'Named dist ' in normal font with space before parameters would go --&gt;</span></span>
<span id="cb143-22"><a href="#cb143-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-23"><a href="#cb143-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\follow}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\sim \text{#1}\,}</span>
<span id="cb143-24"><a href="#cb143-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-25"><a href="#cb143-25" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go --&gt;</span></span>
<span id="cb143-26"><a href="#cb143-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-27"><a href="#cb143-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\followsp}<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{\overset{#1}\sim \text{#2}\,}</span>
<span id="cb143-28"><a href="#cb143-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-29"><a href="#cb143-29" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % define independence symbol (it basically makes two orthogonal symbols very close to each other. The number of \! controls the space between each of the orthogonal symbols) --&gt;</span></span>
<span id="cb143-30"><a href="#cb143-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-31"><a href="#cb143-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\ind}{\perp <span class="sc">\!\!\!</span> \perp}</span>
<span id="cb143-32"><a href="#cb143-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-33"><a href="#cb143-33" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for Cov(X,Y) with formatting for Cov --&gt;</span></span>
<span id="cb143-34"><a href="#cb143-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-35"><a href="#cb143-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathrm{Cov}(#1)}</span>
<span id="cb143-36"><a href="#cb143-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-37"><a href="#cb143-37" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for Corr(X,Y) with formatting for Corr --&gt;</span></span>
<span id="cb143-38"><a href="#cb143-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-39"><a href="#cb143-39" aria-hidden="true" tabindex="-1"></a>\newcommand{\corr}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathrm{Corr}(#1)}</span>
<span id="cb143-40"><a href="#cb143-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-41"><a href="#cb143-41" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for non-italic e in math mode --&gt;</span></span>
<span id="cb143-42"><a href="#cb143-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-43"><a href="#cb143-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathrm{e}}</span>
<span id="cb143-44"><a href="#cb143-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-45"><a href="#cb143-45" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for matrix notation --&gt;</span></span>
<span id="cb143-46"><a href="#cb143-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-47"><a href="#cb143-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\mat}<span class="co">[</span><span class="ot">3</span><span class="co">]</span>{\underset{#2 \times #3}{\boldsymbol{#1}}}</span>
<span id="cb143-48"><a href="#cb143-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-49"><a href="#cb143-49" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for null hypothesis formatted nicely --&gt;</span></span>
<span id="cb143-50"><a href="#cb143-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-51"><a href="#cb143-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\ho}{H_0}</span>
<span id="cb143-52"><a href="#cb143-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-53"><a href="#cb143-53" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for alternative hypothesis formatted nicely --&gt;</span></span>
<span id="cb143-54"><a href="#cb143-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-55"><a href="#cb143-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\ha}{H_A}</span>
<span id="cb143-56"><a href="#cb143-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-57"><a href="#cb143-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview of the model building process</span></span>
<span id="cb143-58"><a href="#cb143-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-59"><a href="#cb143-59" aria-hidden="true" tabindex="-1"></a>The figure below presents a strategy for the building of a regression model. This strategy involves three or, sometimes, four phases:</span>
<span id="cb143-60"><a href="#cb143-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-61"><a href="#cb143-61" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Data collection and preparation</span>
<span id="cb143-62"><a href="#cb143-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-63"><a href="#cb143-63" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Reduction of explanatory or predictor variables (for exploratory observational studies)</span>
<span id="cb143-64"><a href="#cb143-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-65"><a href="#cb143-65" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Model refinement and selection</span>
<span id="cb143-66"><a href="#cb143-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-67"><a href="#cb143-67" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Model validation</span>
<span id="cb143-68"><a href="#cb143-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-69"><a href="#cb143-69" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/regression-flow-chart-advanced.png)</span>{width="80%"}</span>
<span id="cb143-70"><a href="#cb143-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-71"><a href="#cb143-71" aria-hidden="true" tabindex="-1"></a>Here we will overview each phase, then dive deeper later.</span>
<span id="cb143-72"><a href="#cb143-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-73"><a href="#cb143-73" aria-hidden="true" tabindex="-1"></a><span class="fu">### (1) Data collection</span></span>
<span id="cb143-74"><a href="#cb143-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-75"><a href="#cb143-75" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The data collection requirements for building a regression model vary with the nature of the study. It is useful to distinguish four types of studies.</span>
<span id="cb143-76"><a href="#cb143-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-77"><a href="#cb143-77" aria-hidden="true" tabindex="-1"></a>Controlled experiments</span>
<span id="cb143-78"><a href="#cb143-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-79"><a href="#cb143-79" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In a controlled experiment, the experimenter controls the levels of the explanatory variables and assigns treatment, consisting of a combination of levels of the explanatory to each experimental unit and observes the response.</span>
<span id="cb143-80"><a href="#cb143-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-81"><a href="#cb143-81" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>In controlled experiments, the explanatory variables are often called *factors* or *control variables*.</span>
<span id="cb143-82"><a href="#cb143-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-83"><a href="#cb143-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The data collection requirements for controlled experiments are straightforward, though not necessarily simple. Observations for each experimental unit are needed on the response variable and on the level of each of the control variables used for that experimental unit.</span>
<span id="cb143-84"><a href="#cb143-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-85"><a href="#cb143-85" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>There may be difficult measurement and scaling problems for the response variable that are unique to the area of application.</span>
<span id="cb143-86"><a href="#cb143-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-87"><a href="#cb143-87" aria-hidden="true" tabindex="-1"></a>Controlled experiments with covariates</span>
<span id="cb143-88"><a href="#cb143-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-89"><a href="#cb143-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Statistical design of experiments uses supplemental information, such as characteristics of the experimental units, in designing the experiment so as to reduce the variance of the experimental error terms in the regression model.</span>
<span id="cb143-90"><a href="#cb143-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-91"><a href="#cb143-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sometimes, however, it is not possible to incorporate this supplemental information into the design of the experiment. Instead, it may be possible for the experimenter to incorporate this information into the regression model and thereby reduce the error variance by including *uncontrolled variables* or *covariates* in the model.</span>
<span id="cb143-92"><a href="#cb143-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-93"><a href="#cb143-93" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Use of covariates in the regression model make the analysis of the effects of the explanatory variables on the accuracy response more precise.</span>
<span id="cb143-94"><a href="#cb143-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-95"><a href="#cb143-95" aria-hidden="true" tabindex="-1"></a>Confirmatory observational studies</span>
<span id="cb143-96"><a href="#cb143-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-97"><a href="#cb143-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These studies, based on observational, not experimental, data, are intended to test (i.e., to confirm or not to confirm) hypotheses derived from previous studies or from hunches.</span>
<span id="cb143-98"><a href="#cb143-98" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-99"><a href="#cb143-99" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For these studies, data are collected for explanatory variables that previous studies have shown to affect the response variable, as well as for the new</span>
<span id="cb143-100"><a href="#cb143-100" aria-hidden="true" tabindex="-1"></a>variable or variables involved in the hypothesis.</span>
<span id="cb143-101"><a href="#cb143-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-102"><a href="#cb143-102" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>In this context, the explanatory variable(s) involved in the hypothesis are sometimes called the *primary variables*, and the explanatory variables that are included to reflect existing knowledge are called the *control variables* (*known risk factors* in epidemiology).</span>
<span id="cb143-103"><a href="#cb143-103" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-104"><a href="#cb143-104" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>The control variables here are not controlled as in an experimental study, but they are used to account for known influences on the response variable.</span>
<span id="cb143-105"><a href="#cb143-105" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-106"><a href="#cb143-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Data collection for confirmatory observational studies involves obtaining observations on the response variable, the control variables, and the primary explanatory variable(s).</span>
<span id="cb143-107"><a href="#cb143-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-108"><a href="#cb143-108" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Here, as in controlled experiments, there may be important and complex problems of measurement.</span>
<span id="cb143-109"><a href="#cb143-109" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-110"><a href="#cb143-110" aria-hidden="true" tabindex="-1"></a>Explanatory observational studies</span>
<span id="cb143-111"><a href="#cb143-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-112"><a href="#cb143-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the social, behavioral, and health sciences, management, and other fields, it is often not possible to conduct controlled experiments. Furthermore, adequate knowledge for conducting confirmatory observational studies may be lacking.</span>
<span id="cb143-113"><a href="#cb143-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-114"><a href="#cb143-114" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>As a result, many studies in these fields are exploratory observational studies where investigators search for explanatory variables that might be related to the response variable.</span>
<span id="cb143-115"><a href="#cb143-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-116"><a href="#cb143-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>To complicate matters further, any available theoretical models may involve explanatory variables that are not directly measurable, such as a family's future earnings ov the next 10 years.</span>
<span id="cb143-117"><a href="#cb143-117" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-118"><a href="#cb143-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Under these conditions, investigators are often forced to prospect for explanatory variables that could conceivably be related to the response variable under study. Obviously, such a set of potentially useful explanatory variables can be large.</span>
<span id="cb143-119"><a href="#cb143-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-120"><a href="#cb143-120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After a lengthy list of potentially useful explanatory variables has been compiled, some of these variables can be quickly screened out. An explanatory variable (1) may not be fundamental to the problem, (2) may be subject to large measurement errors, and/or (3) may effectively duplicate another explanatory variable in the list.</span>
<span id="cb143-121"><a href="#cb143-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-122"><a href="#cb143-122" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Explanatory variables that cannot be measured may either be deleted or replaced by proxy variables that are highly correlated with them.</span>
<span id="cb143-123"><a href="#cb143-123" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-124"><a href="#cb143-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The number of cases to be collected for an exploratory observational regression study depends on the size of the pool of potentially useful explanatory variables available at this stage.</span>
<span id="cb143-125"><a href="#cb143-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-126"><a href="#cb143-126" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>More cases are required when the pool is large than when it is small.</span>
<span id="cb143-127"><a href="#cb143-127" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-128"><a href="#cb143-128" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>A general rule of thumb states that there should be at least 6 to 10 cases for every variable in the pool.</span>
<span id="cb143-129"><a href="#cb143-129" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-130"><a href="#cb143-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The actual data collection for the pool of potentially useful explanatory variables and for the response variable again may involve important issues of measurement, just as for the other types of studies.</span>
<span id="cb143-131"><a href="#cb143-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-132"><a href="#cb143-132" aria-hidden="true" tabindex="-1"></a><span class="fu">### (1) Data preparation</span></span>
<span id="cb143-133"><a href="#cb143-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-134"><a href="#cb143-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Once the data have been collected, edit checks should be performed and plots prepared to identify gross data errors as well as extreme outliers.</span>
<span id="cb143-135"><a href="#cb143-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-136"><a href="#cb143-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Difficulties with data errors are especially prevalent in large data sets and should be corrected or resolved before the model building begins.</span>
<span id="cb143-137"><a href="#cb143-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-138"><a href="#cb143-138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Whenever possible, the investigator should carefully monitor and control the data collection process to reduce the likelihood of data errors.</span>
<span id="cb143-139"><a href="#cb143-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-140"><a href="#cb143-140" aria-hidden="true" tabindex="-1"></a><span class="fu">### (2) Preliminary model investigation</span></span>
<span id="cb143-141"><a href="#cb143-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-142"><a href="#cb143-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Once the data have been properly edited, the formal modeling process can begin.</span>
<span id="cb143-143"><a href="#cb143-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-144"><a href="#cb143-144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A variety of diagnostics should be employed to identify (1) the functional forms in which the explanatory variables should enter the regression model and (2) important interactions that should be included in the model.</span>
<span id="cb143-145"><a href="#cb143-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-146"><a href="#cb143-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scatter plots and residual plots are useful for determining relationships and their strengths.</span>
<span id="cb143-147"><a href="#cb143-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-148"><a href="#cb143-148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Selected explanatory variables can be fitted in regression functions to explore relationships, possible strong interactions and the need for transformations.</span>
<span id="cb143-149"><a href="#cb143-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-150"><a href="#cb143-150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Whenever possible, of course, one should also rely on the investigator's prior knowledge and expertise to suggest appropriate transformations and interactions to investigate. This is particularly important when the number of potentially useful explanatory variables is large.</span>
<span id="cb143-151"><a href="#cb143-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-152"><a href="#cb143-152" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>In this case, it may be very difficult to investigate all possible pairwise interactions, and prior knowledge should be used to identify the important ones.</span>
<span id="cb143-153"><a href="#cb143-153" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-154"><a href="#cb143-154" aria-hidden="true" tabindex="-1"></a><span class="fu">### (2) Reduction of explanatory variables</span></span>
<span id="cb143-155"><a href="#cb143-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-156"><a href="#cb143-156" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Again this process looks different for the different types of studies.</span>
<span id="cb143-157"><a href="#cb143-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-158"><a href="#cb143-158" aria-hidden="true" tabindex="-1"></a>Controlled experiments</span>
<span id="cb143-159"><a href="#cb143-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-160"><a href="#cb143-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The reduction of explanatory variables in the model-building phase is usually not an important issue for controlled experiments.</span>
<span id="cb143-161"><a href="#cb143-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-162"><a href="#cb143-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The experimenter has chosen the explanatory variables for investigation, and a regression model is to be developed that will enable the investigator to study the effects'of these variables on the response variable.</span>
<span id="cb143-163"><a href="#cb143-163" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-164"><a href="#cb143-164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After the model has been developed, including the use of appropriate functional forms for the variables and the inclusion of important interaction terms, the inferential procedures considered in previous chapters will be used to determine whether the explanatory variables have effects on the response variable and, if so, the nature and magnitude of the effects.</span>
<span id="cb143-165"><a href="#cb143-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-166"><a href="#cb143-166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Controlled experiments can usually avoid many of the problems in exploratory observational studies discussed below.</span>
<span id="cb143-167"><a href="#cb143-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-168"><a href="#cb143-168" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For example, the effects of latent predictor variables are minimized by using randomization.</span>
<span id="cb143-169"><a href="#cb143-169" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-170"><a href="#cb143-170" aria-hidden="true" tabindex="-1"></a>    In addition, adequate ranges of the explanatory variables can be selected and correlations among the explanatory variables can be eliminated by appropriate choices of their levels.</span>
<span id="cb143-171"><a href="#cb143-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-172"><a href="#cb143-172" aria-hidden="true" tabindex="-1"></a>Controlled experiments with covariates</span>
<span id="cb143-173"><a href="#cb143-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-174"><a href="#cb143-174" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In studies of controlled experiments with covariates, some reduction of the covariates may take place investigators often cannot be sure in advance that the selected covariates will be helpful in reducing the error variance.</span>
<span id="cb143-175"><a href="#cb143-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-176"><a href="#cb143-176" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The number of covariates considered in controlled experiments is usually small, so no special problems are encountered in determining whether some or all of the covariates should be dropped from the regression model.</span>
<span id="cb143-177"><a href="#cb143-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-178"><a href="#cb143-178" aria-hidden="true" tabindex="-1"></a>Confirmatory observational studies</span>
<span id="cb143-179"><a href="#cb143-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-180"><a href="#cb143-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generally, no reduction of explanatory variables should take place in confirmatory observational studies.</span>
<span id="cb143-181"><a href="#cb143-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-182"><a href="#cb143-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The control variables were chosen on the basis of prior knowledge and should be retained for comparison with earlier studies even if some of the control variables tum out not to lead to any error variance reduction in the study at hand.</span>
<span id="cb143-183"><a href="#cb143-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-184"><a href="#cb143-184" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The primary variables are the ones whose influence on the response variable is to be examined and therefore need to be present in the model.</span>
<span id="cb143-185"><a href="#cb143-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-186"><a href="#cb143-186" aria-hidden="true" tabindex="-1"></a>Explanatory observational studies</span>
<span id="cb143-187"><a href="#cb143-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-188"><a href="#cb143-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In exploratory observational studies, the number of explanatory variables that remain after the initial screening typically is still large.</span>
<span id="cb143-189"><a href="#cb143-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-190"><a href="#cb143-190" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>Further, many of these variables frequently will be highly intercorrelated. Hence, the investigator usually will wish to reduce the number of explanatory variables to be used in the final model. There are several reasons for this.</span>
<span id="cb143-191"><a href="#cb143-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-192"><a href="#cb143-192" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>A regression model with numerous explanatory variables may be difficult to maintain.</span>
<span id="cb143-193"><a href="#cb143-193" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-194"><a href="#cb143-194" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Further, regression models with a limited number of explanatory variables are easier to work with and understand.</span>
<span id="cb143-195"><a href="#cb143-195" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-196"><a href="#cb143-196" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Finally, the presence of many highly intercorrelated explanatory variables may substantially increase the sampling variation of the regression coefficients, detract from the model's descriptive abilities (falsely inflated $R^2$), and not improve, or even worsen, the model's predictive ability.</span>
<span id="cb143-197"><a href="#cb143-197" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-198"><a href="#cb143-198" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>An actual worsening of the model's predictive ability can occur when explanatory variables are kept in the regression model that are not related to the response variable, given the other explanatory variables in the model (overfitting). In that case, the variances of the fitted values $\sigma^2<span class="sc">\{</span>\hat{Y}_i<span class="sc">\}</span>$ tend to become larger with the inclusion of the useless additional explanatory variables.<span class="co">&lt;!-- ??? --&gt;</span></span>
<span id="cb143-199"><a href="#cb143-199" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-200"><a href="#cb143-200" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>(Note that the multicollinearity effects were all confirmed with the simulation in @sec-simulation-multicollinearity)</span>
<span id="cb143-201"><a href="#cb143-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-202"><a href="#cb143-202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Once the investigator has tentatively decided upon the functional form of the regression relations (whether given variables are to appear in linear form, quadratic form, etc.) and whether any interaction terms are to be included, the next step in many exploratory observational studies is to identify a few "good" subsets of $X$ variables for further intensive study.</span>
<span id="cb143-203"><a href="#cb143-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-204"><a href="#cb143-204" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>These subsets should include not only the potential explanatory variables first-order form but also any needed quadratic and other curvature terms and any necessary interaction terms.</span>
<span id="cb143-205"><a href="#cb143-205" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-206"><a href="#cb143-206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The identification of "good" subsets of potentially useful explanatory variables to be included in the final regression model and the determination of appropriate functional and interaction relations for these variables usually constitute some of the most difficult problems in regression analysis.</span>
<span id="cb143-207"><a href="#cb143-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-208"><a href="#cb143-208" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>Since the uses of regression models vary, no one subset of explanatory vruiables may always be "best".</span>
<span id="cb143-209"><a href="#cb143-209" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-210"><a href="#cb143-210" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For instance, a descriptive use of a regression model typically will emphasize precise estimation of the regression coefficients, whereas a predictive use will focus on the prediction errors.</span>
<span id="cb143-211"><a href="#cb143-211" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-212"><a href="#cb143-212" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Often, different subsets of the pool of potential explanatory variables will best serve these varying purposes.</span>
<span id="cb143-213"><a href="#cb143-213" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-214"><a href="#cb143-214" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Even for a given purpose, it is often found that several subsets are about equally "good" according to a given criterion, and the choice among these "good" subsets needs to be made on the basis of additional considerations.</span>
<span id="cb143-215"><a href="#cb143-215" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-216"><a href="#cb143-216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The choice of a few appropriate subsets of explanatory variables for final consideration in exploratory observational studies needs to be done with great care. Elimination of key explanatory variables can seriously damage the explanatory power of the model and lead to biased estimates of regression coefficients, mean responses, and predictions of new observations, as well as biased estimates of the error variance.</span>
<span id="cb143-217"><a href="#cb143-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-218"><a href="#cb143-218" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The bias in these estimates is related to the fact that with observational data, the error terms in an underfitted regression model may reflect nonrandom effects of the explanatory variables not incorporated in the regression model. Important omitted explanatory variables are sometimes called *latent explanatory variables*.</span>
<span id="cb143-219"><a href="#cb143-219" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-220"><a href="#cb143-220" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>On the other hand, if too many explanatory variables are included in the subset, then this overfitted model will often result in variances of estimated parameters that are larger than those for simpler models.</span>
<span id="cb143-221"><a href="#cb143-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-222"><a href="#cb143-222" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Another danger with observational data is that important explanatory variables may be observed only over narrow ranges. As a result, such important explanatory variables may be omitted just because they occur in the sample within a narrow range of values and therefore turn out to be statistically nonsignificant.</span>
<span id="cb143-223"><a href="#cb143-223" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-224"><a href="#cb143-224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A number of algorithms have been developed to help with variable selection.</span>
<span id="cb143-225"><a href="#cb143-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-226"><a href="#cb143-226" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>But the process of developing a useful regression model must be pragmatic and needs to utilize large doses of subjective judgment.</span>
<span id="cb143-227"><a href="#cb143-227" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-228"><a href="#cb143-228" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Explanatory variables that are considered essential should be included in the regression model before any automation is sought.</span>
<span id="cb143-229"><a href="#cb143-229" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-230"><a href="#cb143-230" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Further, algorithms that identify only a single subset of explanatory variables as "best" need to be supplemented so that additional subsets are also considered before the final regression mopel is decided upon.</span>
<span id="cb143-231"><a href="#cb143-231" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-232"><a href="#cb143-232" aria-hidden="true" tabindex="-1"></a>WRONG APPROACH</span>
<span id="cb143-233"><a href="#cb143-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-234"><a href="#cb143-234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fit full model and drop all unsignificant coefficients (according to $t$-test).</span>
<span id="cb143-235"><a href="#cb143-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-236"><a href="#cb143-236" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Reason for bad: This procedure can lead to the dropping of important intercorrelated explanatory variables (easier to fail to reject with high degree of multicollinearity).</span>
<span id="cb143-237"><a href="#cb143-237" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-238"><a href="#cb143-238" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>So a good procedure needs to be able to take into account multicollinearity (i.e. not drop all of them).</span>
<span id="cb143-239"><a href="#cb143-239" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-240"><a href="#cb143-240" aria-hidden="true" tabindex="-1"></a><span class="fu">### (3) Model refinement and selection</span></span>
<span id="cb143-241"><a href="#cb143-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-242"><a href="#cb143-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At this stage in the model-building process, the tentative regression model, or the several "good" regression models in the case of exploratory observational studies, need to checked in detail for curvature and interaction effects (this is the second check of higher order terms, one was done preliminarily in (2) Reduction of explanatory variables).</span>
<span id="cb143-243"><a href="#cb143-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-244"><a href="#cb143-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Residual plots are helpful in deciding whether one model is to be preferred over another. In addition, the diagnostic checks to be described in the next chapter are useful for identifying influential outlying observations, multicollinearity, etc.</span>
<span id="cb143-245"><a href="#cb143-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-246"><a href="#cb143-246" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The selection of the ultimate regression model often depends greatly upon these diagnostic results. For example, one fitted model may be very much influenced by a single case, whereas another is not. Again, one fitted model may show correlations among the error terms, whereas another does not.</span>
<span id="cb143-247"><a href="#cb143-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-248"><a href="#cb143-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When repeat observations are available, formal tests for lack of fit can be made. In any case, a variety of residual plots and analyses can be employed to identify any lack of fit, outliers, and influential observations.</span>
<span id="cb143-249"><a href="#cb143-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-250"><a href="#cb143-250" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For instance, residual plots against cross-product and/or power terms not included in the regression model can be useful in identifying ways in which the model fit can be improved further.</span>
<span id="cb143-251"><a href="#cb143-251" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-252"><a href="#cb143-252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When an automatic selection procedure is utilized for an exploratory observational study and only a single model is identified as "best," other models should also be explored.</span>
<span id="cb143-253"><a href="#cb143-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-254"><a href="#cb143-254" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>One procedure is to use the number of explanatory variables in the model identified as "best" as an estimate of the number of explanatory variables needed in the regression model.</span>
<span id="cb143-255"><a href="#cb143-255" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-256"><a href="#cb143-256" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Then the investigator explores and identifies other candidate models with approximately the same number of explanatory variables identified by the automatic procedure.</span>
<span id="cb143-257"><a href="#cb143-257" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-258"><a href="#cb143-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Eventually, after thorough checking and various remedial actions, such as transformations, the investigator narrows the number of competing models to one or just a few.</span>
<span id="cb143-259"><a href="#cb143-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-260"><a href="#cb143-260" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>At this point, it is good statistical practice to assess the validity of the remaining candidates through model validation studies. These methods can be used to help decide upon a final regression model, and to determine how well the model will perform in practice.</span>
<span id="cb143-261"><a href="#cb143-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-262"><a href="#cb143-262" aria-hidden="true" tabindex="-1"></a><span class="fu">### (4) Model validation</span></span>
<span id="cb143-263"><a href="#cb143-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-264"><a href="#cb143-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model validity refers to the stability and reasonableness of the regression coefficients, the plausibility and usability of the regression function, and the ability to generalize inferences drawn from the regression analysis.</span>
<span id="cb143-265"><a href="#cb143-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-266"><a href="#cb143-266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Validation is a useful and necessary pan of the model-building process. Several methods of assessing model validity will be described at the end of this chapter.</span>
<span id="cb143-267"><a href="#cb143-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-268"><a href="#cb143-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overfitting simulation</span></span>
<span id="cb143-269"><a href="#cb143-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-272"><a href="#cb143-272" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-273"><a href="#cb143-273" aria-hidden="true" tabindex="-1"></a><span class="co"># overfitting simulation</span></span>
<span id="cb143-274"><a href="#cb143-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-275"><a href="#cb143-275" aria-hidden="true" tabindex="-1"></a><span class="co"># simulation for this???</span></span>
<span id="cb143-276"><a href="#cb143-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-277"><a href="#cb143-277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-278"><a href="#cb143-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-279"><a href="#cb143-279" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo</span></span>
<span id="cb143-280"><a href="#cb143-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-281"><a href="#cb143-281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Modeling process: Start with first order model with all predictors and assess diagnostics for intial problems</span>
<span id="cb143-282"><a href="#cb143-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-285"><a href="#cb143-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-286"><a href="#cb143-286" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb143-287"><a href="#cb143-287" aria-hidden="true" tabindex="-1"></a>data_surgery <span class="ot">&lt;-</span> ALSM<span class="sc">::</span>SurgicalUnit <span class="sc">%&gt;%</span> </span>
<span id="cb143-288"><a href="#cb143-288" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>lny)</span>
<span id="cb143-289"><a href="#cb143-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-290"><a href="#cb143-290" aria-hidden="true" tabindex="-1"></a><span class="co"># start with first order model with all predictors and assess diagnostics</span></span>
<span id="cb143-291"><a href="#cb143-291" aria-hidden="true" tabindex="-1"></a>mod_start <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> ., data_surgery)</span>
<span id="cb143-292"><a href="#cb143-292" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod_start, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>)</span>
<span id="cb143-293"><a href="#cb143-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-294"><a href="#cb143-294" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-295"><a href="#cb143-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-296"><a href="#cb143-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>See curvature and non constant variance and some slight issues with normality.</span>
<span id="cb143-297"><a href="#cb143-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-298"><a href="#cb143-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Try linearizing transformation and fix the non constant variance with $Y' = \ln(Y)$.</span>
<span id="cb143-299"><a href="#cb143-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-302"><a href="#cb143-302" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-303"><a href="#cb143-303" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model with transformed response</span></span>
<span id="cb143-304"><a href="#cb143-304" aria-hidden="true" tabindex="-1"></a>mod_start_prime <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> ., data_surgery)</span>
<span id="cb143-305"><a href="#cb143-305" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod_start_prime, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>)</span>
<span id="cb143-306"><a href="#cb143-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-307"><a href="#cb143-307" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-308"><a href="#cb143-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-309"><a href="#cb143-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transformation fixed most issues, now can continue to investigate the appropriateness of first-order additive terms.</span>
<span id="cb143-310"><a href="#cb143-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-313"><a href="#cb143-313" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-314"><a href="#cb143-314" aria-hidden="true" tabindex="-1"></a><span class="co"># correlation matrix and scatterplot matrix to assess strength of linear relationships (on the lookout for curvature)</span></span>
<span id="cb143-315"><a href="#cb143-315" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; extra steps to get new response variable in there</span></span>
<span id="cb143-316"><a href="#cb143-316" aria-hidden="true" tabindex="-1"></a>corr <span class="ot">&lt;-</span> data_surgery <span class="sc">%&gt;%</span> </span>
<span id="cb143-317"><a href="#cb143-317" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">lny =</span> <span class="fu">log</span>(y)) <span class="sc">%&gt;%</span> </span>
<span id="cb143-318"><a href="#cb143-318" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>y) <span class="sc">%&gt;%</span> </span>
<span id="cb143-319"><a href="#cb143-319" aria-hidden="true" tabindex="-1"></a>  cor <span class="sc">%&gt;%</span></span>
<span id="cb143-320"><a href="#cb143-320" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="dv">3</span>)</span>
<span id="cb143-321"><a href="#cb143-321" aria-hidden="true" tabindex="-1"></a>corrplot<span class="sc">::</span><span class="fu">corrplot</span>(corr)</span>
<span id="cb143-322"><a href="#cb143-322" aria-hidden="true" tabindex="-1"></a>data_surgery <span class="sc">%&gt;%</span> </span>
<span id="cb143-323"><a href="#cb143-323" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">lny =</span> <span class="fu">log</span>(y)) <span class="sc">%&gt;%</span> </span>
<span id="cb143-324"><a href="#cb143-324" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(x6, x7, x8, y)) <span class="sc">%&gt;%</span> </span>
<span id="cb143-325"><a href="#cb143-325" aria-hidden="true" tabindex="-1"></a>  pairs</span>
<span id="cb143-326"><a href="#cb143-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-327"><a href="#cb143-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-328"><a href="#cb143-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-329"><a href="#cb143-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No apparent curvature for $X_k$ with the response $\ln(Y)$.</span>
<span id="cb143-330"><a href="#cb143-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-331"><a href="#cb143-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Strong linear associations with the response ($X_3, X_4$ are the highest).</span>
<span id="cb143-332"><a href="#cb143-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-333"><a href="#cb143-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>May be some multicollinearity ($X_4$ is correlated with $X_1, X_2, X_3$).</span>
<span id="cb143-334"><a href="#cb143-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-337"><a href="#cb143-337" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-338"><a href="#cb143-338" aria-hidden="true" tabindex="-1"></a><span class="co"># check for significant interaction effects by plotting residuals against all possible interactions</span></span>
<span id="cb143-339"><a href="#cb143-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-340"><a href="#cb143-340" aria-hidden="true" tabindex="-1"></a><span class="co"># fit fully crossed interaction model to extract design matrix</span></span>
<span id="cb143-341"><a href="#cb143-341" aria-hidden="true" tabindex="-1"></a>mod_start_prime_crossed <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> .<span class="sc">^</span><span class="dv">2</span>, data_surgery, <span class="at">x =</span> <span class="cn">TRUE</span>)</span>
<span id="cb143-342"><a href="#cb143-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-343"><a href="#cb143-343" aria-hidden="true" tabindex="-1"></a><span class="co"># get residuals of original model</span></span>
<span id="cb143-344"><a href="#cb143-344" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">residuals</span>(mod_start_prime)</span>
<span id="cb143-345"><a href="#cb143-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-346"><a href="#cb143-346" aria-hidden="true" tabindex="-1"></a><span class="co"># extract interaction terms</span></span>
<span id="cb143-347"><a href="#cb143-347" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; interaction symbol : gets recoded as .</span></span>
<span id="cb143-348"><a href="#cb143-348" aria-hidden="true" tabindex="-1"></a>X_int <span class="ot">&lt;-</span> mod_start_prime_crossed<span class="sc">$</span>x <span class="sc">%&gt;%</span> </span>
<span id="cb143-349"><a href="#cb143-349" aria-hidden="true" tabindex="-1"></a>  data.frame <span class="sc">%&gt;%</span> </span>
<span id="cb143-350"><a href="#cb143-350" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">contains</span>(<span class="st">"."</span>), <span class="sc">-</span><span class="fu">contains</span>(<span class="st">"Intercept"</span>))</span>
<span id="cb143-351"><a href="#cb143-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-352"><a href="#cb143-352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-353"><a href="#cb143-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-356"><a href="#cb143-356" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-357"><a href="#cb143-357" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb143-358"><a href="#cb143-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-359"><a href="#cb143-359" aria-hidden="true" tabindex="-1"></a><span class="co"># plot residuals against all interaction terms</span></span>
<span id="cb143-360"><a href="#cb143-360" aria-hidden="true" tabindex="-1"></a>nms_x_int <span class="ot">&lt;-</span> <span class="fu">colnames</span>(X_int)</span>
<span id="cb143-361"><a href="#cb143-361" aria-hidden="true" tabindex="-1"></a><span class="fu">map2</span>(X_int, nms_x_int, <span class="cf">function</span>(x, nm) {</span>
<span id="cb143-362"><a href="#cb143-362" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">x =</span> x, <span class="at">y =</span> e, <span class="at">main =</span> nm)</span>
<span id="cb143-363"><a href="#cb143-363" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">lowess</span>(<span class="at">x =</span> x, <span class="at">y =</span> e), <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb143-364"><a href="#cb143-364" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">"grey"</span>)</span>
<span id="cb143-365"><a href="#cb143-365" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb143-366"><a href="#cb143-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-367"><a href="#cb143-367" aria-hidden="true" tabindex="-1"></a><span class="co"># no obvious visual signs of significant interactions -&gt; maybe x3 and x4</span></span>
<span id="cb143-368"><a href="#cb143-368" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; following textbook and ignoring these</span></span>
<span id="cb143-369"><a href="#cb143-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-370"><a href="#cb143-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-371"><a href="#cb143-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-372"><a href="#cb143-372" aria-hidden="true" tabindex="-1"></a><span class="fu">## Criteria for model selection</span></span>
<span id="cb143-373"><a href="#cb143-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-374"><a href="#cb143-374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>From any set of $p - 1$ predictors, $2^{p-1}$ alternative models can be constructed (this comes from the fact that each predictor can either be included or exlcuded from the model).</span>
<span id="cb143-375"><a href="#cb143-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-376"><a href="#cb143-376" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For example if $p = 3$, then there are two predictors + intercept.</span>
<span id="cb143-377"><a href="#cb143-377" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-378"><a href="#cb143-378" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Can have intercept only model ($Y_i = \beta_0 + \epsilon_i$), a one variable model ($Y_i = \beta_0 + \beta_1 X_1 + \epsilon_i$ or $Y_i = \beta_0 + \beta_1 X_2 + \epsilon_i$) and the only two predictor model ($Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon_i$) for a total of $2^{3-1} = 4$ models.</span>
<span id="cb143-379"><a href="#cb143-379" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-380"><a href="#cb143-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The number of possible models increases quickly, and will be impossible to examine every model in depth. Model selection procedures, also known as subset selection or variables selection procedures, have been developed to identify a small group of regression models that are "good" according to a specified criterion.</span>
<span id="cb143-381"><a href="#cb143-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-382"><a href="#cb143-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A detailed examination can then be made of a limited number of the more promising or "candidate" models, leading to the selection of the final regression model to be employed. This limited number might consist of three to six "good" subsets according to the criteria specified, so the investigator can then carefully study these regression models for choosing the final model.</span>
<span id="cb143-383"><a href="#cb143-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-384"><a href="#cb143-384" aria-hidden="true" tabindex="-1"></a><span class="fu">### $R^2_p$ or $SSE_p$ criterion</span></span>
<span id="cb143-385"><a href="#cb143-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-386"><a href="#cb143-386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Want to identify subset in which the coefficient of multiple determination $R^2$ is high, or equivalently $SSE$ is low (both of which are indexed by how many parameters are in the model = one less than the number of the predictors because of the intercept).</span>
<span id="cb143-387"><a href="#cb143-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-388"><a href="#cb143-388" aria-hidden="true" tabindex="-1"></a>$$R^2_p = 1 - \frac{SSE_p}{SSTO}$$</span>
<span id="cb143-389"><a href="#cb143-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-390"><a href="#cb143-390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Since the denominator is constant for all models (just used $\bar{Y}$), $R^2_p$ and $SSE_p$ vary inversely.</span>
<span id="cb143-391"><a href="#cb143-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-392"><a href="#cb143-392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Note that the $R^2_p$ criterion is not intended to identify the subsets that maximize this criterion.</span>
<span id="cb143-393"><a href="#cb143-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-394"><a href="#cb143-394" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>We know that $R^2_p$ can never decrease as additional $X$ variables are included in the model. Hence, $R^2_p$ will be a maximum when all $P - 1$ ($P$ = total number of available predictors) potential $X$ variables are included in the regression model. </span>
<span id="cb143-395"><a href="#cb143-395" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-396"><a href="#cb143-396" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Dimininshing return: The intent in using the $R^2_p$ criterion is to find the point where adding more $X$ variables is not worthwhile because it leads to a very small increase in $R^2_p$.</span>
<span id="cb143-397"><a href="#cb143-397" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-400"><a href="#cb143-400" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-401"><a href="#cb143-401" aria-hidden="true" tabindex="-1"></a><span class="co"># pick random model to calculate each of the following statistics for (so can compare to book as well)</span></span>
<span id="cb143-402"><a href="#cb143-402" aria-hidden="true" tabindex="-1"></a>mod_example <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, data_surgery)</span>
<span id="cb143-403"><a href="#cb143-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-404"><a href="#cb143-404" aria-hidden="true" tabindex="-1"></a><span class="co"># p</span></span>
<span id="cb143-405"><a href="#cb143-405" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; including intercept</span></span>
<span id="cb143-406"><a href="#cb143-406" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> coef <span class="sc">%&gt;%</span> length</span>
<span id="cb143-407"><a href="#cb143-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-408"><a href="#cb143-408" aria-hidden="true" tabindex="-1"></a><span class="co"># extract R^2_p and SSE_p</span></span>
<span id="cb143-409"><a href="#cb143-409" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(r.squared)</span>
<span id="cb143-410"><a href="#cb143-410" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> anova <span class="sc">%&gt;%</span> tidy <span class="sc">%&gt;%</span> <span class="fu">filter</span>(term <span class="sc">==</span> <span class="st">"Residuals"</span>) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sumsq)</span>
<span id="cb143-411"><a href="#cb143-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-412"><a href="#cb143-412" aria-hidden="true" tabindex="-1"></a><span class="co"># match book!</span></span>
<span id="cb143-413"><a href="#cb143-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-414"><a href="#cb143-414" aria-hidden="true" tabindex="-1"></a><span class="co"># more ways</span></span>
<span id="cb143-415"><a href="#cb143-415" aria-hidden="true" tabindex="-1"></a><span class="fu">augment</span>(mod_example, <span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">lny =</span> <span class="fu">log</span>(data_surgery<span class="sc">$</span>y))) <span class="sc">%&gt;%</span> </span>
<span id="cb143-416"><a href="#cb143-416" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">rsq_trad</span>(<span class="at">data =</span> ., <span class="at">truth =</span> <span class="st">"lny"</span>, <span class="at">estimate =</span> <span class="st">".fitted"</span>)</span>
<span id="cb143-417"><a href="#cb143-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-418"><a href="#cb143-418" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq_trad_vec</span>(<span class="at">truth =</span> <span class="fu">log</span>(data_surgery<span class="sc">$</span>y),</span>
<span id="cb143-419"><a href="#cb143-419" aria-hidden="true" tabindex="-1"></a>                        <span class="at">estimate =</span> <span class="fu">predict</span>(mod_example))</span>
<span id="cb143-420"><a href="#cb143-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-421"><a href="#cb143-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-422"><a href="#cb143-422" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(mod_example <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(r.squared),</span>
<span id="cb143-423"><a href="#cb143-423" aria-hidden="true" tabindex="-1"></a>        yardstick<span class="sc">::</span><span class="fu">rsq_trad_vec</span>(<span class="at">truth =</span> <span class="fu">log</span>(data_surgery<span class="sc">$</span>y),</span>
<span id="cb143-424"><a href="#cb143-424" aria-hidden="true" tabindex="-1"></a>                        <span class="at">estimate =</span> <span class="fu">predict</span>(mod_example)))</span>
<span id="cb143-425"><a href="#cb143-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-426"><a href="#cb143-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-427"><a href="#cb143-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-428"><a href="#cb143-428" aria-hidden="true" tabindex="-1"></a><span class="fu">### $R^2_{a,p}$ or $MSE_p$ criterion</span></span>
<span id="cb143-429"><a href="#cb143-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-430"><a href="#cb143-430" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can use $R^2_{adj} = R^2_{a,p} \text{ (in this context)}$ to take into account the number of parameters in the model through the degrees of freedom. We know that it can decrease as the number of parameters increases if the decrease in $MSE$ isn't enough to offset the loss of degrees of freedom.</span>
<span id="cb143-431"><a href="#cb143-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-432"><a href="#cb143-432" aria-hidden="true" tabindex="-1"></a>$$R^2_{a,p} = 1 - \Big(\frac{n - 1}{n - p}\Big) \frac{SSE_p}{SSTO} = 1 - \frac{MSE_p}{SSTO / (n - 1)}$$</span>
<span id="cb143-433"><a href="#cb143-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-434"><a href="#cb143-434" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This only increases if $MSE_p$ decreases since $SSTO / (n - 1)$ is fixed for the given $Y$ observations. Thus, $R^2_{a,p}$ and $MSE_p$ provide equivalent information.</span>
<span id="cb143-435"><a href="#cb143-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-436"><a href="#cb143-436" aria-hidden="true" tabindex="-1"></a>Demo</span>
<span id="cb143-437"><a href="#cb143-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-440"><a href="#cb143-440" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-441"><a href="#cb143-441" aria-hidden="true" tabindex="-1"></a><span class="co"># extract R^2_a.p and MSE_p</span></span>
<span id="cb143-442"><a href="#cb143-442" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(adj.r.squared)</span>
<span id="cb143-443"><a href="#cb143-443" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sigma) <span class="sc">%&gt;%</span> <span class="fu">raise_to_power</span>(<span class="dv">2</span>)</span>
<span id="cb143-444"><a href="#cb143-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-445"><a href="#cb143-445" aria-hidden="true" tabindex="-1"></a><span class="co"># match book!</span></span>
<span id="cb143-446"><a href="#cb143-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-447"><a href="#cb143-447" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-448"><a href="#cb143-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-449"><a href="#cb143-449" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mallow's $C_p$ criterion</span></span>
<span id="cb143-450"><a href="#cb143-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-451"><a href="#cb143-451" aria-hidden="true" tabindex="-1"></a>Overview</span>
<span id="cb143-452"><a href="#cb143-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-453"><a href="#cb143-453" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This criterion is concerned with the *total mean squared error* of the $n$ fitted values for each subset regression model.</span>
<span id="cb143-454"><a href="#cb143-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-455"><a href="#cb143-455" aria-hidden="true" tabindex="-1"></a>$$C_p = \frac{SSE_p}{MSE(X_1, \ldots, X_{P-1})} + (2p - n)$$</span>
<span id="cb143-456"><a href="#cb143-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-457"><a href="#cb143-457" aria-hidden="true" tabindex="-1"></a>Specifics</span>
<span id="cb143-458"><a href="#cb143-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-459"><a href="#cb143-459" aria-hidden="true" tabindex="-1"></a>&lt;embed src="files/docs/mallows-cp.pdf" type="application/pdf" width="100%" height="700px"&gt;&lt;/embed&gt;</span>
<span id="cb143-460"><a href="#cb143-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-461"><a href="#cb143-461" aria-hidden="true" tabindex="-1"></a>Interpreting Mallow's $C_p$</span>
<span id="cb143-462"><a href="#cb143-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-463"><a href="#cb143-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good models: When there is no bias in the regression model with $p - 1$ $X$ variables, then</span>
<span id="cb143-464"><a href="#cb143-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-465"><a href="#cb143-465" aria-hidden="true" tabindex="-1"></a>$$E(C_p) \approx p \hspace{20pt} \text{when }E(\hat{Y}_i) \approx \mu_i$$</span>
<span id="cb143-466"><a href="#cb143-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-467"><a href="#cb143-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If there are several good models, take the more parsimonious model.</span>
<span id="cb143-468"><a href="#cb143-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-469"><a href="#cb143-469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Acceptable values: $C_p &lt;\approx p$</span>
<span id="cb143-470"><a href="#cb143-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-471"><a href="#cb143-471" aria-hidden="true" tabindex="-1"></a>Interpreting Mallow's $C_p$ plot</span>
<span id="cb143-472"><a href="#cb143-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-473"><a href="#cb143-473" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When the $C_p$ values for all possible regression models are plotted against $p$, those models with little bias will tend to fall near the line $C_p = p$.</span>
<span id="cb143-474"><a href="#cb143-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-475"><a href="#cb143-475" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Models with substantial bias will tend to fall considerably above this line.</span>
<span id="cb143-476"><a href="#cb143-476" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-477"><a href="#cb143-477" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>$C_p$ values below the line $C_p = p$ are interpreted as showing no bias, being below the line due to sampling error.</span>
<span id="cb143-478"><a href="#cb143-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-479"><a href="#cb143-479" aria-hidden="true" tabindex="-1"></a>Notes</span>
<span id="cb143-480"><a href="#cb143-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-481"><a href="#cb143-481" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Effective use of the $C_p$ criterion requires careful development of the pool of $P-1$ potential $X$ variables, with the predictor variables expressed in appropriate form (linear, quadratic, transformed), and important interactions included, so that $MSE(X_1,\ldots,X_{p-1})$ provides an unbiased estimate of the error variance $\sigma^2$.</span>
<span id="cb143-482"><a href="#cb143-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-483"><a href="#cb143-483" aria-hidden="true" tabindex="-1"></a>Demo</span>
<span id="cb143-484"><a href="#cb143-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-485"><a href="#cb143-485" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb143-486"><a href="#cb143-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-487"><a href="#cb143-487" aria-hidden="true" tabindex="-1"></a><span class="fu">#### R functions</span></span>
<span id="cb143-488"><a href="#cb143-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-491"><a href="#cb143-491" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-492"><a href="#cb143-492" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate mallow's Cp</span></span>
<span id="cb143-493"><a href="#cb143-493" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; model is the candidate model and fullmodel has P - 1 predictors</span></span>
<span id="cb143-494"><a href="#cb143-494" aria-hidden="true" tabindex="-1"></a>mod_example_full <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3 <span class="sc">+</span> x4, data_surgery)</span>
<span id="cb143-495"><a href="#cb143-495" aria-hidden="true" tabindex="-1"></a>olsrr<span class="sc">::</span><span class="fu">ols_mallows_cp</span>(<span class="at">model =</span> mod_example, <span class="at">fullmodel =</span> mod_example_full)</span>
<span id="cb143-496"><a href="#cb143-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-497"><a href="#cb143-497" aria-hidden="true" tabindex="-1"></a><span class="co"># matches book!</span></span>
<span id="cb143-498"><a href="#cb143-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-499"><a href="#cb143-499" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-500"><a href="#cb143-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-501"><a href="#cb143-501" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Manual</span></span>
<span id="cb143-502"><a href="#cb143-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-505"><a href="#cb143-505" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-506"><a href="#cb143-506" aria-hidden="true" tabindex="-1"></a><span class="co"># Mallows Cp = SSE_candidate / MSE_full + (2p - n)</span></span>
<span id="cb143-507"><a href="#cb143-507" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; calculate needed values</span></span>
<span id="cb143-508"><a href="#cb143-508" aria-hidden="true" tabindex="-1"></a>SSE_p <span class="ot">&lt;-</span> mod_example <span class="sc">%&gt;%</span> anova <span class="sc">%&gt;%</span> tidy <span class="sc">%&gt;%</span> <span class="fu">filter</span>(term <span class="sc">==</span> <span class="st">"Residuals"</span>) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sumsq)</span>
<span id="cb143-509"><a href="#cb143-509" aria-hidden="true" tabindex="-1"></a>MSE_full <span class="ot">&lt;-</span> mod_example_full <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sigma) <span class="sc">%&gt;%</span> <span class="fu">raise_to_power</span>(<span class="dv">2</span>)</span>
<span id="cb143-510"><a href="#cb143-510" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> mod_example <span class="sc">%&gt;%</span> coef <span class="sc">%&gt;%</span> length</span>
<span id="cb143-511"><a href="#cb143-511" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nobs</span>(mod_example_full)</span>
<span id="cb143-512"><a href="#cb143-512" aria-hidden="true" tabindex="-1"></a>mallows_cp <span class="ot">&lt;-</span> SSE_p <span class="sc">/</span> MSE_full <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p <span class="sc">-</span> n</span>
<span id="cb143-513"><a href="#cb143-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-514"><a href="#cb143-514" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(olsrr<span class="sc">::</span><span class="fu">ols_mallows_cp</span>(<span class="at">model =</span> mod_example, <span class="at">fullmodel =</span> mod_example_full),</span>
<span id="cb143-515"><a href="#cb143-515" aria-hidden="true" tabindex="-1"></a>        mallows_cp)</span>
<span id="cb143-516"><a href="#cb143-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-517"><a href="#cb143-517" aria-hidden="true" tabindex="-1"></a><span class="co"># very close to book, going with roundoff error</span></span>
<span id="cb143-518"><a href="#cb143-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-519"><a href="#cb143-519" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-520"><a href="#cb143-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-521"><a href="#cb143-521" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb143-522"><a href="#cb143-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-523"><a href="#cb143-523" aria-hidden="true" tabindex="-1"></a><span class="fu">### $AIC_p$ and $BIC_p$ criterion</span></span>
<span id="cb143-524"><a href="#cb143-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-525"><a href="#cb143-525" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These two criterion, in addition to $R^2_{a,p}$ and $C_p$, penalize models having large number of predictors.</span>
<span id="cb143-526"><a href="#cb143-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-527"><a href="#cb143-527" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$AIC$ = Akaike's information criteria and $BIC$ = Bayesian information criteria.</span>
<span id="cb143-528"><a href="#cb143-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-529"><a href="#cb143-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Goal: Find models with small values of each:</span>
<span id="cb143-530"><a href="#cb143-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-531"><a href="#cb143-531" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb143-532"><a href="#cb143-532" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb143-533"><a href="#cb143-533" aria-hidden="true" tabindex="-1"></a>  AIC &amp;= n \ln(SSE_p) - n \ln(n) + 2p<span class="sc">\\</span></span>
<span id="cb143-534"><a href="#cb143-534" aria-hidden="true" tabindex="-1"></a>  BIC &amp;= n \ln(SSE_p) - n \ln(n) + p\ln(n)</span>
<span id="cb143-535"><a href="#cb143-535" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb143-536"><a href="#cb143-536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb143-537"><a href="#cb143-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-538"><a href="#cb143-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Notice the first terms $n \ln(SSE_p)$ decrease as $p$ increases, the second term $n \ln(n)$ is fixed (for a given sample size of $n$), and the last term (the penalty term) increases as $p$ increases.</span>
<span id="cb143-539"><a href="#cb143-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-540"><a href="#cb143-540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models with small $SSE_p$ do well with this criteria as long as the penalties aren' too large. And for $n \ge 8$, the penalty for $BIC_p$ is larger $\Longrightarrow$ $BIC_p$ favors more parsimonious models.</span>
<span id="cb143-541"><a href="#cb143-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-542"><a href="#cb143-542" aria-hidden="true" tabindex="-1"></a>Demo</span>
<span id="cb143-543"><a href="#cb143-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-544"><a href="#cb143-544" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb143-545"><a href="#cb143-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-546"><a href="#cb143-546" aria-hidden="true" tabindex="-1"></a><span class="fu">#### R functions</span></span>
<span id="cb143-547"><a href="#cb143-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-550"><a href="#cb143-550" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-551"><a href="#cb143-551" aria-hidden="true" tabindex="-1"></a><span class="co"># different ways to calculate these statistics</span></span>
<span id="cb143-552"><a href="#cb143-552" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; (taking into account something vs not taking into account something)</span></span>
<span id="cb143-553"><a href="#cb143-553" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; https://stats.stackexchange.com/questions/43733/what-is-the-difference-between-aic-and-extractaic-in-r</span></span>
<span id="cb143-554"><a href="#cb143-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-555"><a href="#cb143-555" aria-hidden="true" tabindex="-1"></a><span class="co"># one way</span></span>
<span id="cb143-556"><a href="#cb143-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-557"><a href="#cb143-557" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate AIC_p and BIC_p</span></span>
<span id="cb143-558"><a href="#cb143-558" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(AIC)</span>
<span id="cb143-559"><a href="#cb143-559" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> AIC</span>
<span id="cb143-560"><a href="#cb143-560" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(BIC)</span>
<span id="cb143-561"><a href="#cb143-561" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> BIC</span>
<span id="cb143-562"><a href="#cb143-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-563"><a href="#cb143-563" aria-hidden="true" tabindex="-1"></a><span class="co"># does not match matches book... </span></span>
<span id="cb143-564"><a href="#cb143-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-565"><a href="#cb143-565" aria-hidden="true" tabindex="-1"></a><span class="co"># another way</span></span>
<span id="cb143-566"><a href="#cb143-566" aria-hidden="true" tabindex="-1"></a><span class="fu">extractAIC</span>(mod_example, <span class="at">k =</span> <span class="dv">2</span>) <span class="co"># AIC used default k = 2</span></span>
<span id="cb143-567"><a href="#cb143-567" aria-hidden="true" tabindex="-1"></a>mod_example <span class="sc">%&gt;%</span> <span class="fu">extractAIC</span>(<span class="at">k =</span> <span class="fu">log</span>(<span class="fu">nobs</span>(.))) <span class="co"># BIC -&gt; just have a different k value = ln(n)</span></span>
<span id="cb143-568"><a href="#cb143-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-569"><a href="#cb143-569" aria-hidden="true" tabindex="-1"></a><span class="co"># match book!</span></span>
<span id="cb143-570"><a href="#cb143-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-571"><a href="#cb143-571" aria-hidden="true" tabindex="-1"></a><span class="co"># be careful comparing, need to compare like to like</span></span>
<span id="cb143-572"><a href="#cb143-572" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; from now on always going to use the not book way because it has better compatibility across lm() and glm()</span></span>
<span id="cb143-573"><a href="#cb143-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-574"><a href="#cb143-574" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-575"><a href="#cb143-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-576"><a href="#cb143-576" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Manual</span></span>
<span id="cb143-577"><a href="#cb143-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-580"><a href="#cb143-580" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-581"><a href="#cb143-581" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate needed items</span></span>
<span id="cb143-582"><a href="#cb143-582" aria-hidden="true" tabindex="-1"></a>SSE_p <span class="ot">&lt;-</span> mod_example <span class="sc">%&gt;%</span> anova <span class="sc">%&gt;%</span> tidy <span class="sc">%&gt;%</span> <span class="fu">filter</span>(term <span class="sc">==</span> <span class="st">"Residuals"</span>) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sumsq)</span>
<span id="cb143-583"><a href="#cb143-583" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> mod_example <span class="sc">%&gt;%</span> coef <span class="sc">%&gt;%</span> length</span>
<span id="cb143-584"><a href="#cb143-584" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nobs</span>(mod_example_full)</span>
<span id="cb143-585"><a href="#cb143-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-586"><a href="#cb143-586" aria-hidden="true" tabindex="-1"></a><span class="co"># AIC = n ln(SSE_p) - n ln(n) + 2p</span></span>
<span id="cb143-587"><a href="#cb143-587" aria-hidden="true" tabindex="-1"></a>AIC_p <span class="ot">&lt;-</span> n <span class="sc">*</span> <span class="fu">log</span>(SSE_p) <span class="sc">-</span> n <span class="sc">*</span> <span class="fu">log</span>(n) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p</span>
<span id="cb143-588"><a href="#cb143-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-589"><a href="#cb143-589" aria-hidden="true" tabindex="-1"></a><span class="co"># BIC = n ln(n) SSE_p - n ln(n) + p ln(n)</span></span>
<span id="cb143-590"><a href="#cb143-590" aria-hidden="true" tabindex="-1"></a>BIC_p <span class="ot">&lt;-</span> n <span class="sc">*</span> <span class="fu">log</span>(SSE_p)<span class="sc">-</span> n <span class="sc">*</span> <span class="fu">log</span>(n) <span class="sc">+</span> p <span class="sc">*</span> <span class="fu">log</span>(n)</span>
<span id="cb143-591"><a href="#cb143-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-592"><a href="#cb143-592" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">extractAIC</span>(mod_example)[<span class="dv">2</span>], AIC_p)</span>
<span id="cb143-593"><a href="#cb143-593" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">extractAIC</span>(mod_example, <span class="at">k =</span> <span class="fu">log</span>(<span class="fu">nobs</span>(mod_example)))[<span class="dv">2</span>], BIC_p)</span>
<span id="cb143-594"><a href="#cb143-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-595"><a href="#cb143-595" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-596"><a href="#cb143-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-597"><a href="#cb143-597" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb143-598"><a href="#cb143-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-599"><a href="#cb143-599" aria-hidden="true" tabindex="-1"></a><span class="fu">### $PRESS_p$ criterion</span></span>
<span id="cb143-600"><a href="#cb143-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-601"><a href="#cb143-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The $PRESS_p$ (prediction sum of squares) criterion is a measure of how well the use of the fitted values for a subset model (in terms of the training data) can predict the observed responses $Y_i$. It is similar to $SSE = \sum (Y_i - \hat{Y}_i)^2$ except each fitted value $\hat{Y}_i$ is obtained by:</span>
<span id="cb143-602"><a href="#cb143-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-603"><a href="#cb143-603" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Deleting the $i$th case from the dataset.</span>
<span id="cb143-604"><a href="#cb143-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-605"><a href="#cb143-605" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Estimating the regression function for the subset model from the remaining $n - 1$ cases.</span>
<span id="cb143-606"><a href="#cb143-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-607"><a href="#cb143-607" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Then using the fitted regression function to obtain the predicted value $\hat{Y}_{i(i)}$ for the $i$th case (note $\hat{Y}_{i(i)}$ = predicted value for the $i$th case when the $(i)$th observation was omitted when the regression function was fit.</span>
<span id="cb143-608"><a href="#cb143-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-609"><a href="#cb143-609" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The prediction error for the $i$th case is then</span>
<span id="cb143-610"><a href="#cb143-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-611"><a href="#cb143-611" aria-hidden="true" tabindex="-1"></a>$$Y_i - \hat{Y}_{i(i)}$$</span>
<span id="cb143-612"><a href="#cb143-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-613"><a href="#cb143-613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The $PRESS_p$ criterion is the sum of the squared prediction errors over all $n$ cases:</span>
<span id="cb143-614"><a href="#cb143-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-615"><a href="#cb143-615" aria-hidden="true" tabindex="-1"></a>$$\sum_{i = 1}^n (Y_i - \hat{Y}_{i(i)})^2$$</span>
<span id="cb143-616"><a href="#cb143-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-617"><a href="#cb143-617" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good models: Models with small $PRESS_p$ fit well in the sense of having small prediction errors and are considered candidate models.</span>
<span id="cb143-618"><a href="#cb143-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-619"><a href="#cb143-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$PRESS_p$ values can be calculated without requiring $n$ separate regression runs, each time deleting one of the $n$ cases using formulas in the next chapter.</span>
<span id="cb143-620"><a href="#cb143-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-621"><a href="#cb143-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Note that $PRESS_p$ values can also be used for model validation.</span>
<span id="cb143-622"><a href="#cb143-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-623"><a href="#cb143-623" aria-hidden="true" tabindex="-1"></a><span class="fu">## Automatic search procedures for model selection</span></span>
<span id="cb143-624"><a href="#cb143-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-625"><a href="#cb143-625" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Two common approaches for automating variable selection are "best" subsets regression and stepwise regression.</span>
<span id="cb143-626"><a href="#cb143-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-627"><a href="#cb143-627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Will not cover all options and variations of methods that are available. </span>
<span id="cb143-628"><a href="#cb143-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-629"><a href="#cb143-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It is essential that the specific features of the package employed be fully understood so that intelligent use of the package can be made.</span>
<span id="cb143-630"><a href="#cb143-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-631"><a href="#cb143-631" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For example, some variations allow variables to be considered in groups (such as all indicators for a categorical predictor) or to force variables into the model (if have a prioir beliefs).</span>
<span id="cb143-632"><a href="#cb143-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-633"><a href="#cb143-633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>All to say, there is no unique way of searching for "good" subsets of $X$ variables, and subjective elements must play an important role in the search process.</span>
<span id="cb143-634"><a href="#cb143-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-635"><a href="#cb143-635" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Judgment needs to play an important role in model building for exploratory studies. Some explanatory variables may be known to be more fundamental than others and therefore should be retained in the regression model if the primary purpose is to develop a good explanatory model.</span>
<span id="cb143-636"><a href="#cb143-636" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-637"><a href="#cb143-637" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For example, keep all indicator variables for a categorical predictor, keep lower order terms if a higher order term is included, etc.</span>
<span id="cb143-638"><a href="#cb143-638" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-639"><a href="#cb143-639" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>An important issue in exploratory model building that we have not will be considered is the bias in estimated regression coefficients and in estimated mean responses.</span>
<span id="cb143-640"><a href="#cb143-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-641"><a href="#cb143-641" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>This is where model validation comes into play (don't get lost in finding the best fit)</span>
<span id="cb143-642"><a href="#cb143-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-643"><a href="#cb143-643" aria-hidden="true" tabindex="-1"></a>Demo</span>
<span id="cb143-644"><a href="#cb143-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-647"><a href="#cb143-647" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-648"><a href="#cb143-648" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate PRESS</span></span>
<span id="cb143-649"><a href="#cb143-649" aria-hidden="true" tabindex="-1"></a>olsrr<span class="sc">::</span><span class="fu">ols_press</span>(mod_example)</span>
<span id="cb143-650"><a href="#cb143-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-651"><a href="#cb143-651" aria-hidden="true" tabindex="-1"></a><span class="co"># matches book!</span></span>
<span id="cb143-652"><a href="#cb143-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-653"><a href="#cb143-653" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-654"><a href="#cb143-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-655"><a href="#cb143-655" aria-hidden="true" tabindex="-1"></a><span class="fu">### "Best" subsets algorithms</span></span>
<span id="cb143-656"><a href="#cb143-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-657"><a href="#cb143-657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These algorithms provide the best subsets according to the specified criterion, as well as identifying several "good" subsets for each possible number of $X$ variables in the model.</span>
<span id="cb143-658"><a href="#cb143-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-659"><a href="#cb143-659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When the pool of potential $X$ variables is very large, say greater than 30 or 40, even the "best" subset algorithms may require excessive computer time. Under these conditions, one of the stepwise regression may need to be used.</span>
<span id="cb143-660"><a href="#cb143-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-661"><a href="#cb143-661" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use several different criterion when evaluating the "best" subsets. This is one way to get multiple candidate models.</span>
<span id="cb143-662"><a href="#cb143-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-663"><a href="#cb143-663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Once the investigator has identified a few "good" subsets for intensive examination, a final choice of the model variables must be made with the help of residual analyses, industry knowledge, and finally then confirmed through model validation.</span>
<span id="cb143-664"><a href="#cb143-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-665"><a href="#cb143-665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Strengths</span>
<span id="cb143-666"><a href="#cb143-666" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-667"><a href="#cb143-667" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Exhaustive search of subsets.</span>
<span id="cb143-668"><a href="#cb143-668" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-669"><a href="#cb143-669" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Results in several good candidate models.</span>
<span id="cb143-670"><a href="#cb143-670" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-671"><a href="#cb143-671" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Weaknesses</span>
<span id="cb143-672"><a href="#cb143-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-673"><a href="#cb143-673" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Computationally infeasible when there is a large number of predictors.</span>
<span id="cb143-674"><a href="#cb143-674" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-675"><a href="#cb143-675" aria-hidden="true" tabindex="-1"></a>Demo</span>
<span id="cb143-676"><a href="#cb143-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-677"><a href="#cb143-677" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb143-678"><a href="#cb143-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-679"><a href="#cb143-679" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `leaps::regsubsets`</span></span>
<span id="cb143-680"><a href="#cb143-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-683"><a href="#cb143-683" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-684"><a href="#cb143-684" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to format results for plotting</span></span>
<span id="cb143-685"><a href="#cb143-685" aria-hidden="true" tabindex="-1"></a>format_results <span class="ot">&lt;-</span> <span class="cf">function</span>(best_subs) {</span>
<span id="cb143-686"><a href="#cb143-686" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-687"><a href="#cb143-687" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get summary statistics</span></span>
<span id="cb143-688"><a href="#cb143-688" aria-hidden="true" tabindex="-1"></a>  summ <span class="ot">=</span> <span class="fu">summary</span>(best_subs)</span>
<span id="cb143-689"><a href="#cb143-689" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-690"><a href="#cb143-690" aria-hidden="true" tabindex="-1"></a>  <span class="co"># combine to single dataframe and rename columns</span></span>
<span id="cb143-691"><a href="#cb143-691" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">=</span> summ[<span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>] <span class="sc">%&gt;%</span> <span class="fu">reduce</span>(bind_cols)</span>
<span id="cb143-692"><a href="#cb143-692" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(results) <span class="ot">=</span> <span class="fu">names</span>(summ)[<span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>]</span>
<span id="cb143-693"><a href="#cb143-693" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-694"><a href="#cb143-694" aria-hidden="true" tabindex="-1"></a>  <span class="co"># append number of predictor variables</span></span>
<span id="cb143-695"><a href="#cb143-695" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">=</span> results <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">p =</span> <span class="fu">sort</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>best_subs<span class="sc">$</span>np, best_subs<span class="sc">$</span>nbest))[<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(summ<span class="sc">$</span>outmat)], <span class="at">.before =</span> <span class="dv">1</span>)</span>
<span id="cb143-696"><a href="#cb143-696" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-697"><a href="#cb143-697" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(results)</span>
<span id="cb143-698"><a href="#cb143-698" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-699"><a href="#cb143-699" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb143-700"><a href="#cb143-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-701"><a href="#cb143-701" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to plot results</span></span>
<span id="cb143-702"><a href="#cb143-702" aria-hidden="true" tabindex="-1"></a>plot_results <span class="ot">&lt;-</span> <span class="cf">function</span>(res, <span class="at">var =</span> <span class="st">"rsq"</span>) {</span>
<span id="cb143-703"><a href="#cb143-703" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-704"><a href="#cb143-704" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot point for each model at each p</span></span>
<span id="cb143-705"><a href="#cb143-705" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -&gt; add line connecting optimal models at each p</span></span>
<span id="cb143-706"><a href="#cb143-706" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="at">data =</span> res,</span>
<span id="cb143-707"><a href="#cb143-707" aria-hidden="true" tabindex="-1"></a>         <span class="fu">aes</span>(<span class="at">x =</span> p,</span>
<span id="cb143-708"><a href="#cb143-708" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> .data[[var]])) <span class="sc">+</span> </span>
<span id="cb143-709"><a href="#cb143-709" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">"grey70"</span>) <span class="sc">+</span> </span>
<span id="cb143-710"><a href="#cb143-710" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">data =</span> res <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(p) <span class="sc">%&gt;%</span> <span class="fu">slice_head</span>(<span class="at">n =</span> <span class="dv">1</span>))</span>
<span id="cb143-711"><a href="#cb143-711" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-712"><a href="#cb143-712" aria-hidden="true" tabindex="-1"></a>  <span class="co"># conditionally add Cp = p line</span></span>
<span id="cb143-713"><a href="#cb143-713" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">identical</span>(var, <span class="st">"cp"</span>)) {</span>
<span id="cb143-714"><a href="#cb143-714" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">=</span> p <span class="sc">+</span> </span>
<span id="cb143-715"><a href="#cb143-715" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb143-716"><a href="#cb143-716" aria-hidden="true" tabindex="-1"></a>                  <span class="at">slope =</span> <span class="dv">1</span>)</span>
<span id="cb143-717"><a href="#cb143-717" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb143-718"><a href="#cb143-718" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-719"><a href="#cb143-719" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(p)</span>
<span id="cb143-720"><a href="#cb143-720" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-721"><a href="#cb143-721" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb143-722"><a href="#cb143-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-723"><a href="#cb143-723" aria-hidden="true" tabindex="-1"></a><span class="co"># perform best subsets regression and save necessary output</span></span>
<span id="cb143-724"><a href="#cb143-724" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; note there are options to force variables in and out</span></span>
<span id="cb143-725"><a href="#cb143-725" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; nbest = nmax does ALL possible models</span></span>
<span id="cb143-726"><a href="#cb143-726" aria-hidden="true" tabindex="-1"></a>best_subs <span class="ot">&lt;-</span> leaps<span class="sc">::</span><span class="fu">regsubsets</span>(<span class="fu">formula</span>(mod_start_prime), <span class="at">data =</span> data_surgery, </span>
<span id="cb143-727"><a href="#cb143-727" aria-hidden="true" tabindex="-1"></a>                               <span class="at">nbest =</span> <span class="dv">1</span>, <span class="at">nvmax =</span> <span class="dv">8</span>, <span class="at">method =</span> <span class="st">"exhaustive"</span>)</span>
<span id="cb143-728"><a href="#cb143-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-729"><a href="#cb143-729" aria-hidden="true" tabindex="-1"></a><span class="co"># format and results</span></span>
<span id="cb143-730"><a href="#cb143-730" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">format_results</span>(best_subs)</span>
<span id="cb143-731"><a href="#cb143-731" aria-hidden="true" tabindex="-1"></a><span class="fu">map</span>(<span class="fu">colnames</span>(results)[<span class="sc">-</span><span class="dv">1</span>], \(var) <span class="fu">plot_results</span>(results, var))</span>
<span id="cb143-732"><a href="#cb143-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-733"><a href="#cb143-733" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-734"><a href="#cb143-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-735"><a href="#cb143-735" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `olsrr::ols_step_best_subset`</span></span>
<span id="cb143-736"><a href="#cb143-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-739"><a href="#cb143-739" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-740"><a href="#cb143-740" aria-hidden="true" tabindex="-1"></a><span class="co"># this gives the single best model for number of predictor variables</span></span>
<span id="cb143-741"><a href="#cb143-741" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; can use this to most easily get which variables are in the single best model_p</span></span>
<span id="cb143-742"><a href="#cb143-742" aria-hidden="true" tabindex="-1"></a>best_subs <span class="ot">&lt;-</span> olsrr<span class="sc">::</span><span class="fu">ols_step_best_subset</span>(mod_example_full)</span>
<span id="cb143-743"><a href="#cb143-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-744"><a href="#cb143-744" aria-hidden="true" tabindex="-1"></a><span class="co"># plot results also works, just plots only the best</span></span>
<span id="cb143-745"><a href="#cb143-745" aria-hidden="true" tabindex="-1"></a>best_subs <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">p =</span> n) <span class="sc">%&gt;%</span> {<span class="fu">map</span>(<span class="fu">colnames</span>(best_subs)[<span class="dv">4</span><span class="sc">:</span><span class="dv">10</span>], \(var) <span class="fu">plot_results</span>(., var))}</span>
<span id="cb143-746"><a href="#cb143-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-747"><a href="#cb143-747" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-748"><a href="#cb143-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-749"><a href="#cb143-749" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb143-750"><a href="#cb143-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-751"><a href="#cb143-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stepwise regression methods</span></span>
<span id="cb143-752"><a href="#cb143-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-753"><a href="#cb143-753" aria-hidden="true" tabindex="-1"></a>Motivation</span>
<span id="cb143-754"><a href="#cb143-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-755"><a href="#cb143-755" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In those occasional cases when the pool of potential $X$ variables contains 30 to 40 or even more variables, use of a "best" subsets algorithm may not be feasible. An automatic search procedure that develops the "best" subset of $X$ variables sequentially may then be helpful.</span>
<span id="cb143-756"><a href="#cb143-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-757"><a href="#cb143-757" aria-hidden="true" tabindex="-1"></a>Overview</span>
<span id="cb143-758"><a href="#cb143-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-759"><a href="#cb143-759" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This search method develops a sequence of regression models, at each step adding or deleting an $X$ variable (iterative procedure).</span>
<span id="cb143-760"><a href="#cb143-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-761"><a href="#cb143-761" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can use one of several different criterion for adding or deleting an $X$ variable, such as: reduction in error sum of squares, coefficient of partial correlation, $t^*$ statistic, or $F^*$ statistic.</span>
<span id="cb143-762"><a href="#cb143-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-763"><a href="#cb143-763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Strengths</span>
<span id="cb143-764"><a href="#cb143-764" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-765"><a href="#cb143-765" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Computational more efficient than evaluating ALL possible subsets.</span>
<span id="cb143-766"><a href="#cb143-766" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-767"><a href="#cb143-767" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Weaknesses</span>
<span id="cb143-768"><a href="#cb143-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-769"><a href="#cb143-769" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>Experience has shown that each of the stepwise search procedures can sometimes err by identifying a suboptimal regression model as "best."</span>
<span id="cb143-770"><a href="#cb143-770" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-771"><a href="#cb143-771" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>End result is only a SINGLE "best" model. So it hides potentially other "good" models, whose "goodness" need to be evaluated using a variety of diagnostics.</span>
<span id="cb143-772"><a href="#cb143-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-773"><a href="#cb143-773" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Strategy then</span>
<span id="cb143-774"><a href="#cb143-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-775"><a href="#cb143-775" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>We should use the subset identified by the automatic search procedure as a starting point for searching for other "good" subsets.</span>
<span id="cb143-776"><a href="#cb143-776" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-777"><a href="#cb143-777" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>One possibility is to treat the number of $X$ variables in the regression model identified by the automatic search procedure as being about the right subset size and then use the "best" subsets procedure for subsets of this and nearby sizes.</span>
<span id="cb143-778"><a href="#cb143-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-779"><a href="#cb143-779" aria-hidden="true" tabindex="-1"></a>Forward stepwise regression</span>
<span id="cb143-780"><a href="#cb143-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-781"><a href="#cb143-781" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Step 0: Start with intercept-only model.</span>
<span id="cb143-782"><a href="#cb143-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-783"><a href="#cb143-783" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Step 1: Fit all one variable models and evaluate criteria. Find the best. For example, the largest $\lvert t^* \rvert$ or equivalently smallest $p$-value.</span>
<span id="cb143-784"><a href="#cb143-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-785"><a href="#cb143-785" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Since the degrees of freedom associated with $MSE$ vary depending on the number of $X$ variables in the model, and since repeated tests on the same data are undertaken, fixed $t^*$ limits for adding or deleting a variable have no precise probabilistic meaning. For this reason, software programs often favor the use of predetermined $\alpha$-limits.</span>
<span id="cb143-786"><a href="#cb143-786" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-787"><a href="#cb143-787" aria-hidden="true" tabindex="-1"></a>$$t^*_{k1} = \sqrt{\frac{\hat{\beta}_{k1}}{s\{\hat{\beta}_{k1}<span class="sc">\}</span>}} \Longrightarrow p\text{-value}$$</span>
<span id="cb143-788"><a href="#cb143-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-789"><a href="#cb143-789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Step 2: Start with the variable from the previous step and fit all 2 variable models. Find the best second variable and see if it meets the keep criteria.</span>
<span id="cb143-790"><a href="#cb143-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-791"><a href="#cb143-791" aria-hidden="true" tabindex="-1"></a>$$t^*_{k1} = \sqrt{\frac{MSR(X_{k2} \mid X_{k1})}{MSE(X_{k1}, X_{k2})}} \Longrightarrow p\text{-value}$$</span>
<span id="cb143-792"><a href="#cb143-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-793"><a href="#cb143-793" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Step 3: Check to see if a variable should be deleted. Fit model with all predictors currently kept and see if one variable should be dropped (i.e. see if criteria is on wrong side of the keep criteria).</span>
<span id="cb143-794"><a href="#cb143-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-795"><a href="#cb143-795" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Step 4: Continue adding and checking to see if previous variable should be dropped until adding a variable doesn't improve the model and dropping a variable doesn't improve the model. Then algorithm is done.</span>
<span id="cb143-796"><a href="#cb143-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-797"><a href="#cb143-797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Note that the stepwise regression algorithm allows an $X$ variable, brought into the model at an earlier stage, to be dropped subsequently if it is no longer helpful in conjunction with variables added at later stages.</span>
<span id="cb143-798"><a href="#cb143-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-799"><a href="#cb143-799" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>i.e. The order in which variables enter the regression model does not reflect their importance.</span>
<span id="cb143-800"><a href="#cb143-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-801"><a href="#cb143-801" aria-hidden="true" tabindex="-1"></a>Choice of keep criteria in terms of $\alpha$</span>
<span id="cb143-802"><a href="#cb143-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-803"><a href="#cb143-803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The choice of $\alpha$ to-enter and $\alpha$-to-remove values essentially represents a balancing of opposing tendencies.</span>
<span id="cb143-804"><a href="#cb143-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-805"><a href="#cb143-805" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Simulation studies have shown that for scenarios with large pools of 1) uncorrelated predictors 2) that are not related to the response, larger $\alpha$-to-enter values results in models that allow too many variables. Conversely, a small $\alpha$-to-enter values results in models that are often underspecified, resulting in $\sigma^2$ being badly overestimated and the procedure being too conservative.</span>
<span id="cb143-806"><a href="#cb143-806" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-807"><a href="#cb143-807" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The max $\alpha$-to-enter &lt; min $\alpha$-to-remove. If not, will get an endless loop.</span>
<span id="cb143-808"><a href="#cb143-808" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-809"><a href="#cb143-809" aria-hidden="true" tabindex="-1"></a>Demo</span>
<span id="cb143-810"><a href="#cb143-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-811"><a href="#cb143-811" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb143-812"><a href="#cb143-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-813"><a href="#cb143-813" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `stats::step`</span></span>
<span id="cb143-814"><a href="#cb143-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-817"><a href="#cb143-817" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-818"><a href="#cb143-818" aria-hidden="true" tabindex="-1"></a><span class="co"># perform stepwise regression (forward)</span></span>
<span id="cb143-819"><a href="#cb143-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-820"><a href="#cb143-820" aria-hidden="true" tabindex="-1"></a><span class="co"># specify starting and ending models</span></span>
<span id="cb143-821"><a href="#cb143-821" aria-hidden="true" tabindex="-1"></a>mod_null <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> <span class="dv">1</span>, data_surgery) <span class="co"># starting with intercept model</span></span>
<span id="cb143-822"><a href="#cb143-822" aria-hidden="true" tabindex="-1"></a>mod_full <span class="ot">&lt;-</span> mod_start_prime <span class="co"># just need formula</span></span>
<span id="cb143-823"><a href="#cb143-823" aria-hidden="true" tabindex="-1"></a><span class="fu">formula</span>(mod_full)</span>
<span id="cb143-824"><a href="#cb143-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-825"><a href="#cb143-825" aria-hidden="true" tabindex="-1"></a><span class="co"># run procedure</span></span>
<span id="cb143-826"><a href="#cb143-826" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; criterion is based on AIC from extractAIC()</span></span>
<span id="cb143-827"><a href="#cb143-827" aria-hidden="true" tabindex="-1"></a>mod_step_aic <span class="ot">&lt;-</span> <span class="fu">step</span>(<span class="at">object =</span> mod_null, <span class="at">scope =</span> <span class="fu">formula</span>(mod_full), <span class="at">direction =</span> <span class="st">"both"</span>, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb143-828"><a href="#cb143-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-829"><a href="#cb143-829" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm from extractAIC()</span></span>
<span id="cb143-830"><a href="#cb143-830" aria-hidden="true" tabindex="-1"></a><span class="fu">extractAIC</span>(mod_step_aic)</span>
<span id="cb143-831"><a href="#cb143-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-832"><a href="#cb143-832" aria-hidden="true" tabindex="-1"></a><span class="co"># repeat for BIC criteria -&gt; k = log(nobs(mod))</span></span>
<span id="cb143-833"><a href="#cb143-833" aria-hidden="true" tabindex="-1"></a>mod_step_bic <span class="ot">&lt;-</span> <span class="fu">step</span>(<span class="at">object =</span> mod_null, <span class="at">scope =</span> <span class="fu">formula</span>(mod_full), <span class="at">direction =</span> <span class="st">"both"</span>, <span class="at">k =</span> <span class="fu">log</span>(<span class="fu">nobs</span>(mod_null)), <span class="at">trace =</span> <span class="dv">0</span>) <span class="co"># criterion is based on BIC from extractAIC()</span></span>
<span id="cb143-834"><a href="#cb143-834" aria-hidden="true" tabindex="-1"></a><span class="fu">extractAIC</span>(mod_step_bic, <span class="at">k =</span> <span class="fu">log</span>(<span class="fu">nobs</span>(mod_step_bic)))</span>
<span id="cb143-835"><a href="#cb143-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-836"><a href="#cb143-836" aria-hidden="true" tabindex="-1"></a><span class="co"># compare models</span></span>
<span id="cb143-837"><a href="#cb143-837" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; BIC is more conservative, which makes sense</span></span>
<span id="cb143-838"><a href="#cb143-838" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">formula</span>(mod_step_aic), <span class="fu">formula</span>(mod_step_bic))</span>
<span id="cb143-839"><a href="#cb143-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-840"><a href="#cb143-840" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-841"><a href="#cb143-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-842"><a href="#cb143-842" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `olsrr`</span></span>
<span id="cb143-843"><a href="#cb143-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-846"><a href="#cb143-846" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-847"><a href="#cb143-847" aria-hidden="true" tabindex="-1"></a><span class="co"># perform stepwise regression (forward)</span></span>
<span id="cb143-848"><a href="#cb143-848" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; based on AIC</span></span>
<span id="cb143-849"><a href="#cb143-849" aria-hidden="true" tabindex="-1"></a>mod_step_aic2 <span class="ot">&lt;-</span> olsrr<span class="sc">::</span><span class="fu">ols_step_both_aic</span>(mod_full, <span class="at">progress =</span> <span class="cn">TRUE</span>, <span class="at">details =</span> <span class="cn">TRUE</span>)</span>
<span id="cb143-850"><a href="#cb143-850" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod_step_aic2)</span>
<span id="cb143-851"><a href="#cb143-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-852"><a href="#cb143-852" aria-hidden="true" tabindex="-1"></a><span class="co"># extract formula from ending model</span></span>
<span id="cb143-853"><a href="#cb143-853" aria-hidden="true" tabindex="-1"></a>mod_step_aic2_formula <span class="ot">&lt;-</span> mod_step_aic2<span class="sc">$</span>predictors <span class="sc">%&gt;%</span> {<span class="fu">paste0</span>(<span class="st">"log(y) ~ "</span>, <span class="fu">paste0</span>(., <span class="at">collapse =</span> <span class="st">" + "</span>))} <span class="sc">%&gt;%</span> as.formula</span>
<span id="cb143-854"><a href="#cb143-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-855"><a href="#cb143-855" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">formula</span>(mod_step_aic), mod_step_aic2_formula)</span>
<span id="cb143-856"><a href="#cb143-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-857"><a href="#cb143-857" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm using AIC()</span></span>
<span id="cb143-858"><a href="#cb143-858" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; this result is found in the printed output!</span></span>
<span id="cb143-859"><a href="#cb143-859" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(<span class="fu">lm</span>(mod_step_aic2_formula, data_surgery))</span>
<span id="cb143-860"><a href="#cb143-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-861"><a href="#cb143-861" aria-hidden="true" tabindex="-1"></a><span class="co"># perform stepwise regression (forward)</span></span>
<span id="cb143-862"><a href="#cb143-862" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; based on p-values, using default alpha-to-enter and alpha-to-remove</span></span>
<span id="cb143-863"><a href="#cb143-863" aria-hidden="true" tabindex="-1"></a>mod_step_p <span class="ot">&lt;-</span> olsrr<span class="sc">::</span><span class="fu">ols_step_both_p</span>(mod_full, <span class="at">pent =</span> <span class="fl">0.1</span>, <span class="at">prem =</span> <span class="fl">0.3</span>)</span>
<span id="cb143-864"><a href="#cb143-864" aria-hidden="true" tabindex="-1"></a>mod_step_p</span>
<span id="cb143-865"><a href="#cb143-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-866"><a href="#cb143-866" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-867"><a href="#cb143-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-868"><a href="#cb143-868" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb143-869"><a href="#cb143-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-870"><a href="#cb143-870" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other stepwise procedures</span></span>
<span id="cb143-871"><a href="#cb143-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-872"><a href="#cb143-872" aria-hidden="true" tabindex="-1"></a>Forward selection</span>
<span id="cb143-873"><a href="#cb143-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-874"><a href="#cb143-874" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The forward selection search procedure is a simplified version of forward stepwise regression, omitting the test whether a variable once entered into the model should be dropped.</span>
<span id="cb143-875"><a href="#cb143-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-876"><a href="#cb143-876" aria-hidden="true" tabindex="-1"></a>Demo</span>
<span id="cb143-877"><a href="#cb143-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-880"><a href="#cb143-880" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-881"><a href="#cb143-881" aria-hidden="true" tabindex="-1"></a><span class="co"># perform forward selection regression</span></span>
<span id="cb143-882"><a href="#cb143-882" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; based on AIC (could do BIC same way for step())</span></span>
<span id="cb143-883"><a href="#cb143-883" aria-hidden="true" tabindex="-1"></a>mod_forward <span class="ot">&lt;-</span> <span class="fu">step</span>(<span class="at">object =</span> mod_null, <span class="at">scope =</span> <span class="fu">formula</span>(mod_full), <span class="at">direction =</span> <span class="st">"forward"</span>, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">trace =</span> <span class="dv">0</span>)</span>
<span id="cb143-884"><a href="#cb143-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-885"><a href="#cb143-885" aria-hidden="true" tabindex="-1"></a>mod_forward2 <span class="ot">&lt;-</span> olsrr<span class="sc">::</span><span class="fu">ols_step_forward_aic</span>(mod_full)</span>
<span id="cb143-886"><a href="#cb143-886" aria-hidden="true" tabindex="-1"></a>mod_forward2_formula <span class="ot">&lt;-</span> mod_forward2<span class="sc">$</span>predictors <span class="sc">%&gt;%</span> {<span class="fu">paste0</span>(<span class="st">"log(y) ~ "</span>, <span class="fu">paste0</span>(., <span class="at">collapse =</span> <span class="st">" + "</span>))} <span class="sc">%&gt;%</span> as.formula</span>
<span id="cb143-887"><a href="#cb143-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-888"><a href="#cb143-888" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">formula</span>(mod_forward), mod_forward2_formula)</span>
<span id="cb143-889"><a href="#cb143-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-890"><a href="#cb143-890" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">formula</span>(mod_step_aic), <span class="fu">formula</span>(mod_forward))</span>
<span id="cb143-891"><a href="#cb143-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-892"><a href="#cb143-892" aria-hidden="true" tabindex="-1"></a><span class="co"># olsrr based on p-values</span></span>
<span id="cb143-893"><a href="#cb143-893" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; default enter seems high</span></span>
<span id="cb143-894"><a href="#cb143-894" aria-hidden="true" tabindex="-1"></a>olsrr<span class="sc">::</span><span class="fu">ols_step_forward_p</span>(mod_full, <span class="at">pent =</span> <span class="fl">0.3</span>)</span>
<span id="cb143-895"><a href="#cb143-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-896"><a href="#cb143-896" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-897"><a href="#cb143-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-898"><a href="#cb143-898" aria-hidden="true" tabindex="-1"></a>Backward elimation</span>
<span id="cb143-899"><a href="#cb143-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-900"><a href="#cb143-900" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The backward elimination search procedure is the opposite of forward selection. It begins with the model containing all potential $X$ variables and identifies the one with the largest $p$-value and determines if it should be dropped. Then repeat with the remaining $P-2$ $X$ variables.</span>
<span id="cb143-901"><a href="#cb143-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-902"><a href="#cb143-902" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This process continues until no further $X$ variables can be dropped.</span>
<span id="cb143-903"><a href="#cb143-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-904"><a href="#cb143-904" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A stepwise modification can also be adapted that allows variables eliminated earlier to be added later: this modification is called the backward stepwise regression procedure.</span>
<span id="cb143-905"><a href="#cb143-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-906"><a href="#cb143-906" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For small and moderate numbers of variables in the pool of potential $X$ variables, some statisticians argue for backward stepwise search over forward stepwise search.</span>
<span id="cb143-907"><a href="#cb143-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-908"><a href="#cb143-908" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>A potential disadvantage of the forward stepwise approach is that the $MSE$ (and hence s<span class="sc">\{</span>\hat{\beta}_k<span class="sc">\}</span>$) will lend to be inflated during the initial steps, because important predictors have been omitted $\Longrightarrow$ $t^*$ test statistics that are too small.</span>
<span id="cb143-909"><a href="#cb143-909" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-910"><a href="#cb143-910" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For the backward stepwise procedure, $MSE$ values tend to be more nearly unbiased because important predictors are retained at each step.</span>
<span id="cb143-911"><a href="#cb143-911" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-912"><a href="#cb143-912" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>An argument in favor of the backward stepwise procedure can also be made in situations where it is useful as a first step to look at each $X$ variable in the regression function adjusted for all the other $X$ variables in the pool.</span>
<span id="cb143-913"><a href="#cb143-913" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-916"><a href="#cb143-916" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-917"><a href="#cb143-917" aria-hidden="true" tabindex="-1"></a><span class="co"># perform backward selection regression</span></span>
<span id="cb143-918"><a href="#cb143-918" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; based on AIC (could do BIC same way for step())</span></span>
<span id="cb143-919"><a href="#cb143-919" aria-hidden="true" tabindex="-1"></a>mod_backward <span class="ot">&lt;-</span> <span class="fu">step</span>(<span class="at">object =</span> mod_full, <span class="at">scope =</span> <span class="fu">formula</span>(mod_full), <span class="at">direction =</span> <span class="st">"backward"</span>, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">trace =</span> <span class="dv">0</span>)</span>
<span id="cb143-920"><a href="#cb143-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-921"><a href="#cb143-921" aria-hidden="true" tabindex="-1"></a>mod_backward2 <span class="ot">&lt;-</span> olsrr<span class="sc">::</span><span class="fu">ols_step_backward_aic</span>(mod_full)</span>
<span id="cb143-922"><a href="#cb143-922" aria-hidden="true" tabindex="-1"></a>mod_backward2_formula <span class="ot">&lt;-</span> mod_forward2<span class="sc">$</span>predictors <span class="sc">%&gt;%</span> {<span class="fu">paste0</span>(<span class="st">"log(y) ~ "</span>, <span class="fu">paste0</span>(., <span class="at">collapse =</span> <span class="st">" + "</span>))} <span class="sc">%&gt;%</span> as.formula</span>
<span id="cb143-923"><a href="#cb143-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-924"><a href="#cb143-924" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">formula</span>(mod_backward), mod_backward2_formula) <span class="co"># same models, different order</span></span>
<span id="cb143-925"><a href="#cb143-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-926"><a href="#cb143-926" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(<span class="fu">formula</span>(mod_step_aic), <span class="fu">formula</span>(mod_backward)) <span class="co"># same models, different order</span></span>
<span id="cb143-927"><a href="#cb143-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-928"><a href="#cb143-928" aria-hidden="true" tabindex="-1"></a><span class="co"># olsrr based on p-values</span></span>
<span id="cb143-929"><a href="#cb143-929" aria-hidden="true" tabindex="-1"></a>olsrr<span class="sc">::</span><span class="fu">ols_step_backward_p</span>(mod_full, <span class="at">prem =</span> <span class="fl">0.3</span>, <span class="at">progress =</span> <span class="cn">TRUE</span>, <span class="at">details =</span> <span class="cn">TRUE</span>)</span>
<span id="cb143-930"><a href="#cb143-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-931"><a href="#cb143-931" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-932"><a href="#cb143-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-933"><a href="#cb143-933" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model validation</span></span>
<span id="cb143-934"><a href="#cb143-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-935"><a href="#cb143-935" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overview</span></span>
<span id="cb143-936"><a href="#cb143-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-937"><a href="#cb143-937" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The final step in the model-building process is the validation of the selected regression models. Model validation usually involves checking a candidate model against independent data. Three basic ways of validating a regression model are:</span>
<span id="cb143-938"><a href="#cb143-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-939"><a href="#cb143-939" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Collection of new data to check the model and its predictive ability.</span>
<span id="cb143-940"><a href="#cb143-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-941"><a href="#cb143-941" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Comparison of results with theoretical expectations, earlier empirical results, and simulation results.</span>
<span id="cb143-942"><a href="#cb143-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-943"><a href="#cb143-943" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Use of a holdout sample to check the model and its predictive ability.</span>
<span id="cb143-944"><a href="#cb143-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-945"><a href="#cb143-945" aria-hidden="true" tabindex="-1"></a>Other types of studies</span>
<span id="cb143-946"><a href="#cb143-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-947"><a href="#cb143-947" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When a regression model is used in a controlled experiment, a repetition of the experiment and its analysis serves to validate the findings in the initial study if similar results for the regression coefficients, predictive ability, and the like are obtained. Same for confirmatory observational studies, just use other data and repeat analyses.</span>
<span id="cb143-948"><a href="#cb143-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-949"><a href="#cb143-949" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>This is because there is no issue with variable selection.</span>
<span id="cb143-950"><a href="#cb143-950" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-951"><a href="#cb143-951" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Additionally, if the model is to be used for making predictions over the entire range of the $X$ observations, a possibility is to include data points that are uniformly distributed over the $X$ space.</span>
<span id="cb143-952"><a href="#cb143-952" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-953"><a href="#cb143-953" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>On the contrast, definitely need to validate for exploratory observational studies because they often start with large pools of $X$ variables which is narrowed down significantly.</span>
<span id="cb143-954"><a href="#cb143-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-955"><a href="#cb143-955" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>For these studies, validation of the regression model involves also the appropriateness of the variables selected, as well as the magnitudes of the regression coefficients, the predictive ability of the model, and the like.</span>
<span id="cb143-956"><a href="#cb143-956" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-957"><a href="#cb143-957" aria-hidden="true" tabindex="-1"></a><span class="fu">### Collection of new data to check model</span></span>
<span id="cb143-958"><a href="#cb143-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-959"><a href="#cb143-959" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The best means of model validation is through the collection of new data. The purpose of collecting new data is to be able to examine whether the regression model developed from the earlier data is still applicable for the new data.</span>
<span id="cb143-960"><a href="#cb143-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-961"><a href="#cb143-961" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If so, one has assurance about the applicability of the model to data beyond those on which the model is based.</span>
<span id="cb143-962"><a href="#cb143-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-963"><a href="#cb143-963" aria-hidden="true" tabindex="-1"></a>Methods of checking validity</span>
<span id="cb143-964"><a href="#cb143-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-965"><a href="#cb143-965" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There are a variety of methods of examining the validity of the regression model against the new data.</span>
<span id="cb143-966"><a href="#cb143-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-967"><a href="#cb143-967" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>One validation method is to reestimate the model form chosen earlier using the new data.</span>
<span id="cb143-968"><a href="#cb143-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-969"><a href="#cb143-969" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The estimated regression coefficients and various characteristics of the fitted model are then compared for consistency to those of the regression model based on the em'lier data.</span>
<span id="cb143-970"><a href="#cb143-970" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-971"><a href="#cb143-971" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If the results are consistent, they provide strong support that the chosen regression model is applicable under broader circumstances than those related to the original data.</span>
<span id="cb143-972"><a href="#cb143-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-973"><a href="#cb143-973" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A second validation method is designed to calibrate the predictive capability of the selected regression model.</span>
<span id="cb143-974"><a href="#cb143-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-975"><a href="#cb143-975" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>When a regression model is developed from given data, it is inevitable that the selected model is chosen, at least in large part, because it fits well the data at hand.</span>
<span id="cb143-976"><a href="#cb143-976" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-977"><a href="#cb143-977" aria-hidden="true" tabindex="-1"></a><span class="ss">    -  </span>For a different set of random outcomes, one may likely have arrived at a different model in terms of the predictor valiables selected and/or their functional forms and interaction terms present in the model.</span>
<span id="cb143-978"><a href="#cb143-978" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-979"><a href="#cb143-979" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>A result of this model development process is that the $MSE$ will tend to understate the inherent variability in making future predictions from the selected model.</span>
<span id="cb143-980"><a href="#cb143-980" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-981"><a href="#cb143-981" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>A means of measuring the actual predictive capability of the selected regression model is to use this model to predict each case in the new data set and then to calculate the mean squared prediction error $MSPR$:</span>
<span id="cb143-982"><a href="#cb143-982" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-983"><a href="#cb143-983" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>(where $Y_i$ is the value of the response valiable in the $i$th validation case; $\hat{Y}_i$ is the predicted value for the $i$th validation case based on the model-building dataset; and $n^*$ is the number of cases in the validation set)</span>
<span id="cb143-984"><a href="#cb143-984" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-985"><a href="#cb143-985" aria-hidden="true" tabindex="-1"></a>$$MSPR = \frac{1}{n^*}\sum_{i = 1}^{n^*} (Y_i - \hat{Y}_i)^2$$</span>
<span id="cb143-986"><a href="#cb143-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-987"><a href="#cb143-987" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If the $MSPR$ is fairly close to $MSE$ based on the regression fit to the model-building data set, then the $MSE$ for the selected regression model is not seriously biased and gives an appropriate indication of the predictive ability of the model.</span>
<span id="cb143-988"><a href="#cb143-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-989"><a href="#cb143-989" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If the $MSPE$ is much larger than $MSE$, one should rely on the $MSPE$ as an indicator of how well the selected regression model will predict in the future.</span>
<span id="cb143-990"><a href="#cb143-990" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-991"><a href="#cb143-991" aria-hidden="true" tabindex="-1"></a>Difficulties in replicating a study</span>
<span id="cb143-992"><a href="#cb143-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-993"><a href="#cb143-993" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Difficulties often arise when new data are collected to validate a regression model, especially with observational studies.</span>
<span id="cb143-994"><a href="#cb143-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-995"><a href="#cb143-995" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Essentially, hard to get exactly identical conditions. In observational studies, things like settings and time frequently change.</span>
<span id="cb143-996"><a href="#cb143-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-997"><a href="#cb143-997" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Even though perfect replication is impossible, all validation studies are useful because No single study is fully useful until we know how much the results of the study can be generalized.</span>
<span id="cb143-998"><a href="#cb143-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-999"><a href="#cb143-999" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If a replication study for which the conditions of the setting differ only slightly from those of the initial study substantially different regression results, then we learn that the results of the initial study cannot be readily generalized.</span>
<span id="cb143-1000"><a href="#cb143-1000" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1001"><a href="#cb143-1001" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>On the other hand, if the conditions differ substantially and the regression results are still similar, we find that the regression results can be generalized to apply under substantially varying conditions.</span>
<span id="cb143-1002"><a href="#cb143-1002" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1003"><a href="#cb143-1003" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Still another possibility is that the regression results for the replication study differ substantially from those of the initial study, the differences being related to changes in the setting. This information may be useful for enriching the regression model by including new explanatory variables that make the model more widely applicable.</span>
<span id="cb143-1004"><a href="#cb143-1004" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1005"><a href="#cb143-1005" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparison with theory, empirical evidence, or simulation results</span></span>
<span id="cb143-1006"><a href="#cb143-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1007"><a href="#cb143-1007" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In some cases, theory, simulation rdmlts, or previous empirical results may be helpful in determining whether the selected model is reasonable.</span>
<span id="cb143-1008"><a href="#cb143-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1009"><a href="#cb143-1009" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Comparisons of regression coefficients and predictions with theoretical expectations, previous empirical results, or simulation results should be made. </span>
<span id="cb143-1010"><a href="#cb143-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1011"><a href="#cb143-1011" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Unfortunately, there is often little theory that can be used to validate</span>
<span id="cb143-1012"><a href="#cb143-1012" aria-hidden="true" tabindex="-1"></a>regression models.</span>
<span id="cb143-1013"><a href="#cb143-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1014"><a href="#cb143-1014" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data splitting</span></span>
<span id="cb143-1015"><a href="#cb143-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1016"><a href="#cb143-1016" aria-hidden="true" tabindex="-1"></a>Overview</span>
<span id="cb143-1017"><a href="#cb143-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1018"><a href="#cb143-1018" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>By far the preferred method to validate a regression model is through the collection of new data. Often, however, this is neither practical nor feasible.</span>
<span id="cb143-1019"><a href="#cb143-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1020"><a href="#cb143-1020" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>An alternative is to use a training and testing set, which in effect an attempt to simulate replication of the study. The validation data set is used for validation in the same way as when new data are collected.</span>
<span id="cb143-1021"><a href="#cb143-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1022"><a href="#cb143-1022" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The regression coefficients can be reestimated for the selected model and then compareed for consistency with the coefficients obtained from the model-building data set.</span>
<span id="cb143-1023"><a href="#cb143-1023" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1024"><a href="#cb143-1024" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Also, predictions can be made for the testing data from the model built on the training data in order to calibrate the predictive ability of this regression model for the new data.</span>
<span id="cb143-1025"><a href="#cb143-1025" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1026"><a href="#cb143-1026" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>When the calibration data set is large enough, one can also study how the "good" models considered in the model selection phase fare with the new data.</span>
<span id="cb143-1027"><a href="#cb143-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1028"><a href="#cb143-1028" aria-hidden="true" tabindex="-1"></a>How to split</span>
<span id="cb143-1029"><a href="#cb143-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1030"><a href="#cb143-1030" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$n_{\text{train}} &gt;&gt; n_{\text{test}}$, can even go 50/50 if large enough dataset.</span>
<span id="cb143-1031"><a href="#cb143-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1032"><a href="#cb143-1032" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Splits of the data can be made at random or stratified to ensure certain spread of cases (roughly equal representation of $X$ levels) in the training and testing datasets.</span>
<span id="cb143-1033"><a href="#cb143-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1034"><a href="#cb143-1034" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can also do $k$-fold cross validation. The $k$ estimates of prediction error are then combined to produce a $k$-fold cross-validation estimate.</span>
<span id="cb143-1035"><a href="#cb143-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1036"><a href="#cb143-1036" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Note that when $k = n$, this criteria is equal to $PRESS_p$ $\Longrightarrow$ $k$-fold cross validation estimates are approximations to $PRESS_p$.</span>
<span id="cb143-1037"><a href="#cb143-1037" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1038"><a href="#cb143-1038" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Can also use a variation of $PRESS_p$ where $m$ observations are held out rather than 1, and the $n-m$ observations are used to fit the models.</span>
<span id="cb143-1039"><a href="#cb143-1039" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1040"><a href="#cb143-1040" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If a dataset for an exploratory observational study is very large, it can be divided into three parts.</span>
<span id="cb143-1041"><a href="#cb143-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1042"><a href="#cb143-1042" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>The first part is used for model training, the second part for cross-validation and model selection, and the third part for testing and calibrating the final model.</span>
<span id="cb143-1043"><a href="#cb143-1043" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1044"><a href="#cb143-1044" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>This approach avoids any bias resulting from estimating the regression parameters from the same data set used for developing the model.</span>
<span id="cb143-1045"><a href="#cb143-1045" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-1046"><a href="#cb143-1046" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>But it still uses less data to fit the model, so less precise standard errors of estimated coefficients.</span>
<span id="cb143-1047"><a href="#cb143-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1048"><a href="#cb143-1048" aria-hidden="true" tabindex="-1"></a>Drawbacks</span>
<span id="cb143-1049"><a href="#cb143-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1050"><a href="#cb143-1050" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A possible drawback of data splitting is that the variances of the estimated regression coefficients developed from the model-building data set will usually be larger than those that would have been obtained from the fit to the entire data set.</span>
<span id="cb143-1051"><a href="#cb143-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1052"><a href="#cb143-1052" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If the model-building data set is reasonably large, however, these variances generally will not be that much larger than those for the entire data set.</span>
<span id="cb143-1053"><a href="#cb143-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1054"><a href="#cb143-1054" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In any case, once the model has been validated, it is customary practice to use the entire data set for estimating the final regression model.</span>
<span id="cb143-1055"><a href="#cb143-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1056"><a href="#cb143-1056" aria-hidden="true" tabindex="-1"></a>Results</span>
<span id="cb143-1057"><a href="#cb143-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1058"><a href="#cb143-1058" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When regression models built on observational data do not predict well outside the range of the $X$ observations in the data set (extrapolation), the usual reason is the existence of multicollinearity among the $X$ variables.</span>
<span id="cb143-1059"><a href="#cb143-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1060"><a href="#cb143-1060" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Ridge regression or other biased estimation techniques are possible solutions to this.</span>
<span id="cb143-1061"><a href="#cb143-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1062"><a href="#cb143-1062" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo -- Data splitting</span></span>
<span id="cb143-1063"><a href="#cb143-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1064"><a href="#cb143-1064" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb143-1065"><a href="#cb143-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1066"><a href="#cb143-1066" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `tidymodels`</span></span>
<span id="cb143-1067"><a href="#cb143-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1070"><a href="#cb143-1070" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1071"><a href="#cb143-1071" aria-hidden="true" tabindex="-1"></a><span class="co"># random split 2-way</span></span>
<span id="cb143-1072"><a href="#cb143-1072" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; make train / test split</span></span>
<span id="cb143-1073"><a href="#cb143-1073" aria-hidden="true" tabindex="-1"></a>data_split <span class="ot">&lt;-</span> data_surgery <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(<span class="at">prop =</span> .<span class="dv">7</span>)</span>
<span id="cb143-1074"><a href="#cb143-1074" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> data_split <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">training</span>()</span>
<span id="cb143-1075"><a href="#cb143-1075" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> data_split <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">testing</span>()</span>
<span id="cb143-1076"><a href="#cb143-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1077"><a href="#cb143-1077" aria-hidden="true" tabindex="-1"></a><span class="co"># stratified split</span></span>
<span id="cb143-1078"><a href="#cb143-1078" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; https://www.tidymodels.org/start/case-study/</span></span>
<span id="cb143-1079"><a href="#cb143-1079" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; based on response variable (ensures representative training and testing)</span></span>
<span id="cb143-1080"><a href="#cb143-1080" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; if numeric, does based on binned quantiles</span></span>
<span id="cb143-1081"><a href="#cb143-1081" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; if categorical, does it based on proportion of levels</span></span>
<span id="cb143-1082"><a href="#cb143-1082" aria-hidden="true" tabindex="-1"></a>data_split_strat <span class="ot">&lt;-</span> data_surgery <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(<span class="at">prop =</span> .<span class="dv">7</span>, <span class="at">strata =</span> y)</span>
<span id="cb143-1083"><a href="#cb143-1083" aria-hidden="true" tabindex="-1"></a>data_train2 <span class="ot">&lt;-</span> data_split_strat <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">training</span>()</span>
<span id="cb143-1084"><a href="#cb143-1084" aria-hidden="true" tabindex="-1"></a>data_test2 <span class="ot">&lt;-</span> data_split_strat <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">testing</span>()</span>
<span id="cb143-1085"><a href="#cb143-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1086"><a href="#cb143-1086" aria-hidden="true" tabindex="-1"></a><span class="co"># random split 3-way</span></span>
<span id="cb143-1087"><a href="#cb143-1087" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; make train / validation / test split</span></span>
<span id="cb143-1088"><a href="#cb143-1088" aria-hidden="true" tabindex="-1"></a>data_split_diamonds <span class="ot">&lt;-</span> diamonds <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">initial_validation_split</span>(<span class="at">prop =</span> <span class="fu">c</span>(<span class="fl">0.6</span>, <span class="fl">0.2</span>))</span>
<span id="cb143-1089"><a href="#cb143-1089" aria-hidden="true" tabindex="-1"></a>data_train_diamonds <span class="ot">&lt;-</span> data_split_diamonds <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">training</span>()</span>
<span id="cb143-1090"><a href="#cb143-1090" aria-hidden="true" tabindex="-1"></a>data_validation_diamonds  <span class="ot">&lt;-</span> data_split_diamonds <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">validation</span>()</span>
<span id="cb143-1091"><a href="#cb143-1091" aria-hidden="true" tabindex="-1"></a>data_test_diamonds  <span class="ot">&lt;-</span> data_split_diamonds <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">testing</span>()</span>
<span id="cb143-1092"><a href="#cb143-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1093"><a href="#cb143-1093" aria-hidden="true" tabindex="-1"></a><span class="co"># k folds</span></span>
<span id="cb143-1094"><a href="#cb143-1094" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; https://rsample.tidymodels.org/articles/Working_with_rsets.html</span></span>
<span id="cb143-1095"><a href="#cb143-1095" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; access analysis and holdout data for the first fold</span></span>
<span id="cb143-1096"><a href="#cb143-1096" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; setup to map() a function to perform on each fold split</span></span>
<span id="cb143-1097"><a href="#cb143-1097" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(<span class="at">data =</span> data_surgery, <span class="at">v =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">1</span>, <span class="at">strata =</span> y)</span>
<span id="cb143-1098"><a href="#cb143-1098" aria-hidden="true" tabindex="-1"></a>kfolds</span>
<span id="cb143-1099"><a href="#cb143-1099" aria-hidden="true" tabindex="-1"></a>data_analysis1 <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">analysis</span>(kfolds<span class="sc">$</span>splits[[<span class="dv">1</span>]])</span>
<span id="cb143-1100"><a href="#cb143-1100" aria-hidden="true" tabindex="-1"></a>data_holdout1 <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">assessment</span>(kfolds<span class="sc">$</span>splits[[<span class="dv">1</span>]])</span>
<span id="cb143-1101"><a href="#cb143-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1102"><a href="#cb143-1102" aria-hidden="true" tabindex="-1"></a><span class="co"># IMPORTANT </span><span class="al">NOTE</span><span class="co">: in practice would do the resampling method on the TRAINING data, then get the final model performance on the testing set</span></span>
<span id="cb143-1103"><a href="#cb143-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1104"><a href="#cb143-1104" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1105"><a href="#cb143-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1106"><a href="#cb143-1106" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `caret`</span></span>
<span id="cb143-1107"><a href="#cb143-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1110"><a href="#cb143-1110" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1111"><a href="#cb143-1111" aria-hidden="true" tabindex="-1"></a><span class="co"># random split</span></span>
<span id="cb143-1112"><a href="#cb143-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1113"><a href="#cb143-1113" aria-hidden="true" tabindex="-1"></a><span class="co"># get indices in training data</span></span>
<span id="cb143-1114"><a href="#cb143-1114" aria-hidden="true" tabindex="-1"></a><span class="co"># then subset data to training and testing data</span></span>
<span id="cb143-1115"><a href="#cb143-1115" aria-hidden="true" tabindex="-1"></a>in_train <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">createDataPartition</span>(data_surgery<span class="sc">$</span>y, <span class="at">p =</span> <span class="fl">0.7</span>)</span>
<span id="cb143-1116"><a href="#cb143-1116" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> data_surgery[in_train<span class="sc">$</span>Resample1, ]</span>
<span id="cb143-1117"><a href="#cb143-1117" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> data_surgery[<span class="sc">-</span>(in_train<span class="sc">$</span>Resample1), ]</span>
<span id="cb143-1118"><a href="#cb143-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1119"><a href="#cb143-1119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1120"><a href="#cb143-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1121"><a href="#cb143-1121" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `dplyr`</span></span>
<span id="cb143-1122"><a href="#cb143-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1125"><a href="#cb143-1125" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1126"><a href="#cb143-1126" aria-hidden="true" tabindex="-1"></a><span class="co"># random split</span></span>
<span id="cb143-1127"><a href="#cb143-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1128"><a href="#cb143-1128" aria-hidden="true" tabindex="-1"></a><span class="co"># sample a proportion for training set and get all non matches for testing</span></span>
<span id="cb143-1129"><a href="#cb143-1129" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; requires ID variable (saving new object to not overwrite original data)</span></span>
<span id="cb143-1130"><a href="#cb143-1130" aria-hidden="true" tabindex="-1"></a>data_surgery_tmp <span class="ot">&lt;-</span> data_surgery <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">id =</span> <span class="fu">row_number</span>())</span>
<span id="cb143-1131"><a href="#cb143-1131" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> data_surgery_tmp <span class="sc">%&gt;%</span> <span class="fu">slice_sample</span>(<span class="at">prop =</span> <span class="fl">0.7</span>)</span>
<span id="cb143-1132"><a href="#cb143-1132" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> <span class="fu">anti_join</span>(data_surgery_tmp, data_train, <span class="at">by =</span> <span class="fu">join_by</span>(id))</span>
<span id="cb143-1133"><a href="#cb143-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1134"><a href="#cb143-1134" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1135"><a href="#cb143-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1136"><a href="#cb143-1136" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb143-1137"><a href="#cb143-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1138"><a href="#cb143-1138" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo -- Get best model from train</span></span>
<span id="cb143-1139"><a href="#cb143-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1142"><a href="#cb143-1142" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1143"><a href="#cb143-1143" aria-hidden="true" tabindex="-1"></a><span class="co"># random split 2-way</span></span>
<span id="cb143-1144"><a href="#cb143-1144" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; make train / test split</span></span>
<span id="cb143-1145"><a href="#cb143-1145" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb143-1146"><a href="#cb143-1146" aria-hidden="true" tabindex="-1"></a>data_split <span class="ot">&lt;-</span> data_surgery <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(<span class="at">prop =</span> .<span class="dv">7</span>)</span>
<span id="cb143-1147"><a href="#cb143-1147" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> data_split <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">training</span>()</span>
<span id="cb143-1148"><a href="#cb143-1148" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> data_split <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">testing</span>()</span>
<span id="cb143-1149"><a href="#cb143-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1150"><a href="#cb143-1150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1151"><a href="#cb143-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1154"><a href="#cb143-1154" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1155"><a href="#cb143-1155" aria-hidden="true" tabindex="-1"></a><span class="co"># get final best model</span></span>
<span id="cb143-1156"><a href="#cb143-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1157"><a href="#cb143-1157" aria-hidden="true" tabindex="-1"></a><span class="co"># perform best subsets regression and save necessary output</span></span>
<span id="cb143-1158"><a href="#cb143-1158" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; note there are options to force variables in and out</span></span>
<span id="cb143-1159"><a href="#cb143-1159" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; nbest = nmax does ALL possible models</span></span>
<span id="cb143-1160"><a href="#cb143-1160" aria-hidden="true" tabindex="-1"></a>best_subs <span class="ot">&lt;-</span> leaps<span class="sc">::</span><span class="fu">regsubsets</span>(<span class="fu">formula</span>(mod_start_prime), <span class="at">data =</span> data_train, </span>
<span id="cb143-1161"><a href="#cb143-1161" aria-hidden="true" tabindex="-1"></a>                               <span class="at">nbest =</span> <span class="dv">8</span>, <span class="at">nvmax =</span> <span class="dv">8</span>, <span class="at">method =</span> <span class="st">"exhaustive"</span>)</span>
<span id="cb143-1162"><a href="#cb143-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1163"><a href="#cb143-1163" aria-hidden="true" tabindex="-1"></a><span class="co"># format and results</span></span>
<span id="cb143-1164"><a href="#cb143-1164" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">format_results</span>(best_subs)</span>
<span id="cb143-1165"><a href="#cb143-1165" aria-hidden="true" tabindex="-1"></a><span class="fu">map</span>(<span class="fu">colnames</span>(results)[<span class="sc">-</span><span class="dv">1</span>], \(var) <span class="fu">plot_results</span>(results, var))</span>
<span id="cb143-1166"><a href="#cb143-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1167"><a href="#cb143-1167" aria-hidden="true" tabindex="-1"></a><span class="co"># criteria -&gt; best model</span></span>
<span id="cb143-1168"><a href="#cb143-1168" aria-hidden="true" tabindex="-1"></a><span class="co"># rsq -&gt; 4</span></span>
<span id="cb143-1169"><a href="#cb143-1169" aria-hidden="true" tabindex="-1"></a><span class="co"># rss -&gt; 4</span></span>
<span id="cb143-1170"><a href="#cb143-1170" aria-hidden="true" tabindex="-1"></a><span class="co"># adjr2 -&gt; 3</span></span>
<span id="cb143-1171"><a href="#cb143-1171" aria-hidden="true" tabindex="-1"></a><span class="co"># cp -&gt; 4</span></span>
<span id="cb143-1172"><a href="#cb143-1172" aria-hidden="true" tabindex="-1"></a><span class="co"># bic -&gt; 3</span></span>
<span id="cb143-1173"><a href="#cb143-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1174"><a href="#cb143-1174" aria-hidden="true" tabindex="-1"></a><span class="co"># going with best 4 parameter model (3 Xs)</span></span>
<span id="cb143-1175"><a href="#cb143-1175" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(best_subs)<span class="sc">$</span>which <span class="sc">%&gt;%</span> data.frame <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="fu">row.names</span>(.) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"X3"</span>))</span>
<span id="cb143-1176"><a href="#cb143-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1177"><a href="#cb143-1177" aria-hidden="true" tabindex="-1"></a><span class="co"># for sake of simplicity, not evaluating multiple models and skipping some criteria like PRESSp</span></span>
<span id="cb143-1178"><a href="#cb143-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1179"><a href="#cb143-1179" aria-hidden="true" tabindex="-1"></a><span class="co"># fit final model using best model</span></span>
<span id="cb143-1180"><a href="#cb143-1180" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; keeping x7 so indicator variables x7 and x8 are together</span></span>
<span id="cb143-1181"><a href="#cb143-1181" aria-hidden="true" tabindex="-1"></a>mod_train <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(y) <span class="sc">~</span> x2 <span class="sc">+</span> x3 <span class="sc">+</span> x7 <span class="sc">+</span> x8, data_train)</span>
<span id="cb143-1182"><a href="#cb143-1182" aria-hidden="true" tabindex="-1"></a>mod_train <span class="sc">%&gt;%</span> tidy</span>
<span id="cb143-1183"><a href="#cb143-1183" aria-hidden="true" tabindex="-1"></a>mod_train <span class="sc">%&gt;%</span> glance</span>
<span id="cb143-1184"><a href="#cb143-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1185"><a href="#cb143-1185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1186"><a href="#cb143-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1187"><a href="#cb143-1187" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo -- Validate model </span></span>
<span id="cb143-1188"><a href="#cb143-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1189"><a href="#cb143-1189" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb143-1190"><a href="#cb143-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1191"><a href="#cb143-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">#### R functions</span></span>
<span id="cb143-1192"><a href="#cb143-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1195"><a href="#cb143-1195" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1196"><a href="#cb143-1196" aria-hidden="true" tabindex="-1"></a><span class="co"># make predictions on testing data</span></span>
<span id="cb143-1197"><a href="#cb143-1197" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_train, <span class="at">newdata =</span> data_test)</span>
<span id="cb143-1198"><a href="#cb143-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1199"><a href="#cb143-1199" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate rmse</span></span>
<span id="cb143-1200"><a href="#cb143-1200" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rmse_vec</span>(<span class="at">truth =</span> <span class="fu">log</span>(data_test<span class="sc">$</span>y), <span class="at">estimate =</span> preds)</span>
<span id="cb143-1201"><a href="#cb143-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1202"><a href="#cb143-1202" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate MSPE</span></span>
<span id="cb143-1203"><a href="#cb143-1203" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rmse_vec</span>(<span class="at">truth =</span> <span class="fu">log</span>(data_test<span class="sc">$</span>y), <span class="at">estimate =</span> preds)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb143-1204"><a href="#cb143-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1205"><a href="#cb143-1205" aria-hidden="true" tabindex="-1"></a><span class="co"># compare regression coefficients fit on training data to testing data for consistency</span></span>
<span id="cb143-1206"><a href="#cb143-1206" aria-hidden="true" tabindex="-1"></a>mod_test <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">formula</span>(mod_train), data_test)</span>
<span id="cb143-1207"><a href="#cb143-1207" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(mod_test)</span>
<span id="cb143-1208"><a href="#cb143-1208" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(mod_train)</span>
<span id="cb143-1209"><a href="#cb143-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1210"><a href="#cb143-1210" aria-hidden="true" tabindex="-1"></a><span class="co"># compare MSEp and MSE</span></span>
<span id="cb143-1211"><a href="#cb143-1211" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(mod_test <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sigma) <span class="sc">%&gt;%</span> <span class="fu">raise_to_power</span>(<span class="dv">2</span>),</span>
<span id="cb143-1212"><a href="#cb143-1212" aria-hidden="true" tabindex="-1"></a>        mod_train <span class="sc">%&gt;%</span> glance <span class="sc">%&gt;%</span> <span class="fu">pull</span>(sigma) <span class="sc">%&gt;%</span> <span class="fu">raise_to_power</span>(<span class="dv">2</span>))</span>
<span id="cb143-1213"><a href="#cb143-1213" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(<span class="fu">log</span>(data_surgery<span class="sc">$</span>y))</span>
<span id="cb143-1214"><a href="#cb143-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1215"><a href="#cb143-1215" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1216"><a href="#cb143-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1217"><a href="#cb143-1217" aria-hidden="true" tabindex="-1"></a>Conclusions</span>
<span id="cb143-1218"><a href="#cb143-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1219"><a href="#cb143-1219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Coefficients of the testing model are all within a standard error or two of the training model (only concern in x7 switches signs)</span>
<span id="cb143-1220"><a href="#cb143-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1221"><a href="#cb143-1221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$MSE$s are close, especially considering scale of $var(ln(y))$ for reference. So it does perform well for predictive accuracy.</span>
<span id="cb143-1222"><a href="#cb143-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1223"><a href="#cb143-1223" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Manual</span></span>
<span id="cb143-1224"><a href="#cb143-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1227"><a href="#cb143-1227" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1228"><a href="#cb143-1228" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate MSPE</span></span>
<span id="cb143-1229"><a href="#cb143-1229" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((<span class="fu">log</span>(data_test<span class="sc">$</span>y) <span class="sc">-</span> preds)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">nrow</span>(data_test)</span>
<span id="cb143-1230"><a href="#cb143-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1231"><a href="#cb143-1231" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate RMSE = sqrt(MSPE)</span></span>
<span id="cb143-1232"><a href="#cb143-1232" aria-hidden="true" tabindex="-1"></a>(<span class="fu">sum</span>((<span class="fu">log</span>(data_test<span class="sc">$</span>y) <span class="sc">-</span> preds)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">nrow</span>(data_test)) <span class="sc">%&gt;%</span> sqrt</span>
<span id="cb143-1233"><a href="#cb143-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1234"><a href="#cb143-1234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1235"><a href="#cb143-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1236"><a href="#cb143-1236" aria-hidden="true" tabindex="-1"></a><span class="fu">#### $k$-fold cross validation $RMSE$ -- Semi manual</span></span>
<span id="cb143-1237"><a href="#cb143-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1240"><a href="#cb143-1240" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1241"><a href="#cb143-1241" aria-hidden="true" tabindex="-1"></a><span class="co"># k folds</span></span>
<span id="cb143-1242"><a href="#cb143-1242" aria-hidden="true" tabindex="-1"></a>kfolds <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(<span class="at">data =</span> data_surgery, <span class="at">v =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">1</span>, <span class="at">strata =</span> y)</span>
<span id="cb143-1243"><a href="#cb143-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1244"><a href="#cb143-1244" aria-hidden="true" tabindex="-1"></a><span class="co"># function to:</span></span>
<span id="cb143-1245"><a href="#cb143-1245" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) fit model on analysis data</span></span>
<span id="cb143-1246"><a href="#cb143-1246" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) make predictions on holdout data</span></span>
<span id="cb143-1247"><a href="#cb143-1247" aria-hidden="true" tabindex="-1"></a><span class="co"># ... will be the model formula</span></span>
<span id="cb143-1248"><a href="#cb143-1248" aria-hidden="true" tabindex="-1"></a>holdout_results <span class="ot">&lt;-</span> <span class="cf">function</span>(splits, ...) {</span>
<span id="cb143-1249"><a href="#cb143-1249" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-1250"><a href="#cb143-1250" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit the model to the analysis data (90%: 9 of the 10 folds)</span></span>
<span id="cb143-1251"><a href="#cb143-1251" aria-hidden="true" tabindex="-1"></a>  mod_kfold <span class="ot">=</span> <span class="fu">lm</span>(..., <span class="at">data =</span> rsample<span class="sc">::</span><span class="fu">analysis</span>(splits))</span>
<span id="cb143-1252"><a href="#cb143-1252" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-1253"><a href="#cb143-1253" aria-hidden="true" tabindex="-1"></a>  <span class="co"># save the holdout data (10%: last fold)</span></span>
<span id="cb143-1254"><a href="#cb143-1254" aria-hidden="true" tabindex="-1"></a>  data_holdout <span class="ot">=</span> rsample<span class="sc">::</span><span class="fu">assessment</span>(splits)</span>
<span id="cb143-1255"><a href="#cb143-1255" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-1256"><a href="#cb143-1256" aria-hidden="true" tabindex="-1"></a>  <span class="co"># `augment` will save the predictions with the holdout data set</span></span>
<span id="cb143-1257"><a href="#cb143-1257" aria-hidden="true" tabindex="-1"></a>  preds <span class="ot">=</span> <span class="fu">augment</span>(mod_kfold, <span class="at">newdata =</span> data_holdout)</span>
<span id="cb143-1258"><a href="#cb143-1258" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-1259"><a href="#cb143-1259" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(preds)</span>
<span id="cb143-1260"><a href="#cb143-1260" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-1261"><a href="#cb143-1261" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb143-1262"><a href="#cb143-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1263"><a href="#cb143-1263" aria-hidden="true" tabindex="-1"></a><span class="co"># fit models for k-fold cross validation</span></span>
<span id="cb143-1264"><a href="#cb143-1264" aria-hidden="true" tabindex="-1"></a>kfolds<span class="sc">$</span>results <span class="ot">&lt;-</span> kfolds<span class="sc">$</span>splits <span class="sc">%&gt;%</span> <span class="fu">map</span>(holdout_results, <span class="fu">formula</span>(mod_train))</span>
<span id="cb143-1265"><a href="#cb143-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1266"><a href="#cb143-1266" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate rmse for each fold</span></span>
<span id="cb143-1267"><a href="#cb143-1267" aria-hidden="true" tabindex="-1"></a>kfolds<span class="sc">$</span>rmse <span class="ot">&lt;-</span> kfolds<span class="sc">$</span>results <span class="sc">%&gt;%</span> <span class="fu">map_dbl</span>(\(df) yardstick<span class="sc">::</span><span class="fu">rmse_vec</span>(<span class="at">truth =</span> <span class="fu">log</span>(df<span class="sc">$</span>y),<span class="at">estimate =</span> df<span class="sc">$</span><span class="st">`</span><span class="at">.fitted</span><span class="st">`</span>))</span>
<span id="cb143-1268"><a href="#cb143-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1269"><a href="#cb143-1269" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate final cross validation rmse</span></span>
<span id="cb143-1270"><a href="#cb143-1270" aria-hidden="true" tabindex="-1"></a>kfolds<span class="sc">$</span>rmse <span class="sc">%&gt;%</span> mean</span>
<span id="cb143-1271"><a href="#cb143-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1272"><a href="#cb143-1272" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1273"><a href="#cb143-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1274"><a href="#cb143-1274" aria-hidden="true" tabindex="-1"></a><span class="fu">#### $k$-fold cross validation $RMSE$ -- Full tidymodels</span></span>
<span id="cb143-1275"><a href="#cb143-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1276"><a href="#cb143-1276" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">SLIDES</span><span class="co">](https://catalina.quarto.pub/introduction-to-tidymodels/)</span></span>
<span id="cb143-1277"><a href="#cb143-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1280"><a href="#cb143-1280" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1281"><a href="#cb143-1281" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a single linear regression model on entire data for practice</span></span>
<span id="cb143-1282"><a href="#cb143-1282" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; https://www.tidymodels.org/start/models/</span></span>
<span id="cb143-1283"><a href="#cb143-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1284"><a href="#cb143-1284" aria-hidden="true" tabindex="-1"></a><span class="co"># make the parsnip model </span></span>
<span id="cb143-1285"><a href="#cb143-1285" aria-hidden="true" tabindex="-1"></a>spec_linreg <span class="ot">&lt;-</span> parsnip<span class="sc">::</span><span class="fu">linear_reg</span>()</span>
<span id="cb143-1286"><a href="#cb143-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1287"><a href="#cb143-1287" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb143-1288"><a href="#cb143-1288" aria-hidden="true" tabindex="-1"></a>fit_linreg <span class="ot">&lt;-</span> mod_linreg <span class="sc">%&gt;%</span> </span>
<span id="cb143-1289"><a href="#cb143-1289" aria-hidden="true" tabindex="-1"></a>  parsnip<span class="sc">::</span><span class="fu">fit</span>(<span class="fu">log</span>(y) <span class="sc">~</span> ., <span class="at">data =</span> data_surgery)</span>
<span id="cb143-1290"><a href="#cb143-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1291"><a href="#cb143-1291" aria-hidden="true" tabindex="-1"></a><span class="fu">tidy</span>(fit_linreg)</span>
<span id="cb143-1292"><a href="#cb143-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1293"><a href="#cb143-1293" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1294"><a href="#cb143-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1297"><a href="#cb143-1297" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb143-1298"><a href="#cb143-1298" aria-hidden="true" tabindex="-1"></a><span class="co"># fit models for all resamples</span></span>
<span id="cb143-1299"><a href="#cb143-1299" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; https://www.tidymodels.org/start/resampling/</span></span>
<span id="cb143-1300"><a href="#cb143-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1301"><a href="#cb143-1301" aria-hidden="true" tabindex="-1"></a><span class="co"># use kfolds from above</span></span>
<span id="cb143-1302"><a href="#cb143-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1303"><a href="#cb143-1303" aria-hidden="true" tabindex="-1"></a><span class="co"># define workflow</span></span>
<span id="cb143-1304"><a href="#cb143-1304" aria-hidden="true" tabindex="-1"></a>wf_linreg <span class="ot">&lt;-</span> workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb143-1305"><a href="#cb143-1305" aria-hidden="true" tabindex="-1"></a>  workflows<span class="sc">::</span><span class="fu">add_model</span>(spec_linreg) <span class="sc">%&gt;%</span> </span>
<span id="cb143-1306"><a href="#cb143-1306" aria-hidden="true" tabindex="-1"></a>  workflows<span class="sc">::</span><span class="fu">add_formula</span>(<span class="fu">formula</span>(mod_train))</span>
<span id="cb143-1307"><a href="#cb143-1307" aria-hidden="true" tabindex="-1"></a>wf_linreg</span>
<span id="cb143-1308"><a href="#cb143-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1309"><a href="#cb143-1309" aria-hidden="true" tabindex="-1"></a><span class="co"># fit cross validation models </span></span>
<span id="cb143-1310"><a href="#cb143-1310" aria-hidden="true" tabindex="-1"></a>rs_fit_linreg <span class="ot">&lt;-</span> wf_linreg <span class="sc">%&gt;%</span> </span>
<span id="cb143-1311"><a href="#cb143-1311" aria-hidden="true" tabindex="-1"></a>  tune<span class="sc">::</span><span class="fu">fit_resamples</span>(<span class="at">object =</span> .,</span>
<span id="cb143-1312"><a href="#cb143-1312" aria-hidden="true" tabindex="-1"></a>                      <span class="at">resamples =</span> kfolds,</span>
<span id="cb143-1313"><a href="#cb143-1313" aria-hidden="true" tabindex="-1"></a>                      <span class="at">metrics =</span> yardstick<span class="sc">::</span><span class="fu">metric_set</span>(yardstick<span class="sc">::</span>rmse))</span>
<span id="cb143-1314"><a href="#cb143-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1315"><a href="#cb143-1315" aria-hidden="true" tabindex="-1"></a><span class="co"># get final cross validation measures</span></span>
<span id="cb143-1316"><a href="#cb143-1316" aria-hidden="true" tabindex="-1"></a>workflowsets<span class="sc">::</span><span class="fu">collect_metrics</span>(rs_fit_linreg)</span>
<span id="cb143-1317"><a href="#cb143-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1318"><a href="#cb143-1318" aria-hidden="true" tabindex="-1"></a><span class="fu">compare</span>(kfolds<span class="sc">$</span>rmse <span class="sc">%&gt;%</span> mean,</span>
<span id="cb143-1319"><a href="#cb143-1319" aria-hidden="true" tabindex="-1"></a>        workflowsets<span class="sc">::</span><span class="fu">collect_metrics</span>(rs_fit_linreg) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(mean))</span>
<span id="cb143-1320"><a href="#cb143-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1321"><a href="#cb143-1321" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb143-1322"><a href="#cb143-1322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-1323"><a href="#cb143-1323" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>