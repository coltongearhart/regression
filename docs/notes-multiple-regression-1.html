<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.517">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Regression - 5&nbsp; Multiple regression 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./part2-mlr.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part2-mlr.html">Multiple linear regression</a></li><li class="breadcrumb-item"><a href="./notes-multiple-regression-1.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple regression 1</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part2-mlr.html">Multiple linear regression</a></li><li class="breadcrumb-item"><a href="./notes-multiple-regression-1.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple regression 1</span></a></li></ol></nav>
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple regression 1</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Regression</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part1-slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simple linear regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Simple linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-diagnostics-and-remedial-measures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Diagnostics and remedial measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-matrix-slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Matrix approach to SLR</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part2-mlr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiple linear regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notes-multiple-regression-1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple regression 1</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">5.1</span> Overview</a></li>
  <li><a href="#multiple-regression-models" id="toc-multiple-regression-models" class="nav-link" data-scroll-target="#multiple-regression-models"><span class="header-section-number">5.2</span> Multiple regression models</a>
  <ul class="collapse">
  <li><a href="#first-order-model-with-two-predictors" id="toc-first-order-model-with-two-predictors" class="nav-link" data-scroll-target="#first-order-model-with-two-predictors"><span class="header-section-number">5.2.1</span> First order model with two predictors</a></li>
  <li><a href="#first-order-model-with-more-than-two-predictor-variables" id="toc-first-order-model-with-more-than-two-predictor-variables" class="nav-link" data-scroll-target="#first-order-model-with-more-than-two-predictor-variables"><span class="header-section-number">5.2.2</span> First order model with more than two predictor variables</a></li>
  <li><a href="#general-linear-regression-model" id="toc-general-linear-regression-model" class="nav-link" data-scroll-target="#general-linear-regression-model"><span class="header-section-number">5.2.3</span> General linear regression model</a></li>
  </ul></li>
  <li><a href="#general-linear-regression-model-in-matrix-terms" id="toc-general-linear-regression-model-in-matrix-terms" class="nav-link" data-scroll-target="#general-linear-regression-model-in-matrix-terms"><span class="header-section-number">5.3</span> General linear regression model in matrix terms</a>
  <ul class="collapse">
  <li><a href="#estimation-of-regression-coefficients-fitted-values-and-residuals" id="toc-estimation-of-regression-coefficients-fitted-values-and-residuals" class="nav-link" data-scroll-target="#estimation-of-regression-coefficients-fitted-values-and-residuals"><span class="header-section-number">5.3.1</span> Estimation of regression coefficients, fitted values and residuals</a></li>
  <li><a href="#anova" id="toc-anova" class="nav-link" data-scroll-target="#anova"><span class="header-section-number">5.3.2</span> ANOVA</a></li>
  <li><a href="#f-test-for-regression-relation" id="toc-f-test-for-regression-relation" class="nav-link" data-scroll-target="#f-test-for-regression-relation"><span class="header-section-number">5.3.3</span> F-test for regression relation</a></li>
  <li><a href="#coefficient-of-multiple-determination" id="toc-coefficient-of-multiple-determination" class="nav-link" data-scroll-target="#coefficient-of-multiple-determination"><span class="header-section-number">5.3.4</span> Coefficient of multiple determination</a></li>
  <li><a href="#coefficient-of-multiple-correlation" id="toc-coefficient-of-multiple-correlation" class="nav-link" data-scroll-target="#coefficient-of-multiple-correlation"><span class="header-section-number">5.3.5</span> Coefficient of multiple correlation</a></li>
  <li><a href="#inferences-about-regression-parameters" id="toc-inferences-about-regression-parameters" class="nav-link" data-scroll-target="#inferences-about-regression-parameters"><span class="header-section-number">5.3.6</span> Inferences about regression parameters</a></li>
  <li><a href="#estimation-of-mean-response-and-prediction-of-new-observation" id="toc-estimation-of-mean-response-and-prediction-of-new-observation" class="nav-link" data-scroll-target="#estimation-of-mean-response-and-prediction-of-new-observation"><span class="header-section-number">5.3.7</span> Estimation of mean response and prediction of new observation</a></li>
  </ul></li>
  <li><a href="#diagnostics-and-remedial-measures" id="toc-diagnostics-and-remedial-measures" class="nav-link" data-scroll-target="#diagnostics-and-remedial-measures"><span class="header-section-number">5.4</span> Diagnostics and remedial measures</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<!-- % define LaTeX macros (/shortcuts) -->
<!-- % define vector (without parentheses, so when writing out in like a definition) of the form X_1, ..., X_n, where X and n are variable. NOTE: to call use $\vecn{X}{n}$ -->
<!-- % shortcut for ~ 'Named dist ' in normal font with space before parameters would go -->
<!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go -->
<!-- % define independence symbol (it basically makes two orthogonal symbols very close to each other. The number of \! controls the space between each of the orthogonal symbols) -->
<!-- % shortcut for Cov(X,Y) with formatting for Cov -->
<!-- % shortcut for Corr(X,Y) with formatting for Corr -->
<!-- % shortcut for non-italic e in math mode -->
<!-- % shortcut for matrix notation -->
<section id="overview" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">5.1</span> Overview</h2>
<p>Multiple regression analysis is one of the most widely used of all statistical methods. In this section, we first discuss a variety of multiple regression models.</p>
<p>Since the matrix expressions for multiple regression are the same as for simple linear regression, can just refer to <a href="notes-matrix-slr.html" class="quarto-xref"><span>Chapter&nbsp;4</span></a>.</p>
<p>The need for multiple predictors</p>
<ul>
<li><p>Often times, a single predictor variable in the model provides an inadequate description since a number of key variables affect the response variable in important and distinctive ways.</p></li>
<li><p>Furthermore, in situations like this, we frequently find that predictions of the response variable based on a model containing only a single predictor variable are too imprecise to be useful.</p></li>
<li><p>Multiple predictors is useful in observational studies when there is no control over the variables as well as in experimental situations where the experimenter can control the <span class="math inline">\(X\)</span> levels and investigate different combination simultaneously.</p></li>
</ul>
</section>
<section id="multiple-regression-models" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="multiple-regression-models"><span class="header-section-number">5.2</span> Multiple regression models</h2>
<section id="first-order-model-with-two-predictors" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="first-order-model-with-two-predictors"><span class="header-section-number">5.2.1</span> First order model with two predictors</h3>
<p>When there are two predictor variables <span class="math inline">\(X_l\)</span> and <span class="math inline">\(X_2\)</span>, the regression model:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \epsilon_i\]</span></p>
<p>This model is still linear in its predictors. It follows that if <span class="math inline">\(E(\epsilon_i) = 0\)</span></p>
<p><span class="math display">\[E(Y_i) = \beta_0 + \beta_1 X_1 + \beta_1 X_2\]</span></p>
<p>Note that now predictions are a plane and any point on the response plane corresponds to the mean response <span class="math inline">\(E(Y)\)</span> at the given combination of levels of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<p><img src="files/images/response-plane.png" class="img-fluid" style="width:50.0%"></p>
<p>Here the vertical distance from <span class="math inline">\(Y_i\)</span> to the response plane represents the error term <span class="math inline">\(\epsilon_i = Y_i - E(Y)\)</span>.</p>
<p>Frequently the regression function in multiple regression is called a <em>regression surface</em> or a <em>response surface</em>.</p>
<p>Meaning of regression coefficients</p>
<ul>
<li><p>The parameter <span class="math inline">\(\beta_0\)</span> is the intercept of the regression plane.</p>
<ul>
<li>If the scope of the model includes <span class="math inline">\(X_1 = 0\)</span> and <span class="math inline">\(X_2 = 0\)</span>, then <span class="math inline">\(\beta_0 = 10\)</span> for example represents the mean response <span class="math inline">\(E(Y)\)</span> at <span class="math inline">\(X_1 = 0\)</span> and <span class="math inline">\(X_2 = 0\)</span>.</li>
</ul></li>
<li><p>The parameter <span class="math inline">\(\beta_1\)</span> indicates the change in the mean response <span class="math inline">\(E(Y)\)</span> per unit increase in <span class="math inline">\(X_1\)</span> when <span class="math inline">\(X_2\)</span> is held constant. Same but reverse for <span class="math inline">\(\beta_2\)</span>.</p>
<ul>
<li><p>For example: If <span class="math inline">\(E(Y) = 10 + 2X_1 + 5X_2\)</span> and the level of <span class="math inline">\(X_2\)</span> is held at 2, then <span class="math inline">\(E(Y) = 10 + 2X_1 + 5(2) = 20 + 2X_1\)</span>.</p></li>
<li><p>This response function is a straight line with slope <span class="math inline">\(\beta_1 = 2\)</span>. This is true for all other values of <span class="math inline">\(X_2\)</span>, only the value of the intercept changes.</p></li>
</ul></li>
</ul>
<p>Additive models vs interaction models</p>
<ul>
<li><p>When the effect of <span class="math inline">\(X_1\)</span> on the mean response does not depend on the level of <span class="math inline">\(X_2\)</span>, and correspondingly the effect of <span class="math inline">\(X_2\)</span> does not depend on the level of <span class="math inline">\(X_1\)</span>, the two predictor variables are said to have <em>additive effects</em> or not to <em>interact</em>.</p></li>
<li><p>Thus, the first-order regression model is designed for predictor variables whose effects on the mean response are additive or do not interact.</p></li>
<li><p>The parameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are sometimes called <em>partial regression coefficients</em> because they reflect the partial effect of one predictor variable when the other predictor variable is included in the model and is held constant.</p></li>
</ul>
</section>
<section id="first-order-model-with-more-than-two-predictor-variables" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="first-order-model-with-more-than-two-predictor-variables"><span class="header-section-number">5.2.2</span> First order model with more than two predictor variables</h3>
<p>We consider now the case where there are <span class="math inline">\(p - 1\)</span> predictor variables <span class="math inline">\(X_1, \ldots , X_{p-1}\)</span>. The regression model:</p>
<p><span id="eq-first-order-all-preds"><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i \tag{5.1}\]</span></span></p>
<p>is called a first-order model with <span class="math inline">\(p - 1\)</span> predictor variables. It can also be written as:</p>
<p><span class="math display">\[Y_i = \beta_0 + \sum_{k = 1}^{p - 1} \beta_k X_{ik} + \epsilon_i\]</span></p>
<p>It follows that if <span class="math inline">\(E(\epsilon_i) = 0\)</span></p>
<p><span class="math display">\[E(Y_i) =  \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1}\]</span></p>
<p>Notes about model:</p>
<ul>
<li><p>This response function is a <em>hyperplane</em>, which is a plane in more than two dimensions. It is no longer possible to picture this plane.</p></li>
<li><p>The meaning of the parameters is the same as when there were two predictors, just extended to all other <span class="math inline">\(X\)</span> levels held constant.</p></li>
<li><p>This first- order regression model is still designed for predictor variables whose effects on the mean response are additive and therefore do interact.</p></li>
<li><p>When <span class="math inline">\(p - 1 = 1\)</span>, this model reduces to the SLR model we are used to.</p></li>
</ul>
</section>
<section id="general-linear-regression-model" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="general-linear-regression-model"><span class="header-section-number">5.2.3</span> General linear regression model</h3>
<p>In general, the variables <span class="math inline">\(X_1, \ldots, X_{p-1}\)</span> in a regression model do not need to represent different predictor variables, as we see soon. Therefore, we define the general linear regression model, with normal error terms, simply in terms of <span class="math inline">\(X\)</span> variables (same as before, just now more general of what the <span class="math inline">\(X\)</span>s can represent):</p>
<p><span id="eq-gen-reg-mod"><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i \tag{5.2}\]</span></span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_{p-1}\)</span> are parameters</p></li>
<li><p><span class="math inline">\(X_{i1}, \ldots, X_{i,p-1}\)</span> are known constants</p></li>
<li><p><span class="math inline">\(\epsilon_i \overset{iid}\sim \text{Normal}\,(0, \sigma^2)\)</span></p></li>
<li><p><span class="math inline">\(i = 1, \ldots, n\)</span></p></li>
</ul>
<p>Again since <span class="math inline">\(E(\epsilon_i) = 0\)</span>,</p>
<p><span class="math display">\[E(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1}\]</span></p>
<p>Thus, the general linear regression model with normal error terms implies that the observations <span class="math inline">\(Y_i\)</span> are independent normal variables, with mean <span class="math inline">\(E(Y)\)</span> as given by above and with constant variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>This general linear model encompasses a vast variety of situations. We consider a few of these now.</p>
<p><span class="math inline">\(p - 1\)</span> predictor variables</p>
<ul>
<li>When <span class="math inline">\(X_1, \ldots, X_{p-1}\)</span> represent <span class="math inline">\(p - 1\)</span> different predictor variables, the general linear regression model above is simply <a href="#eq-first-order-all-preds" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> seen earlier (first-order model in which there are no interaction effects between the predictor variables).</li>
</ul>
<p>Qualitative predictors</p>
<ul>
<li><p>Model in <a href="#eq-gen-reg-mod" class="quarto-xref">Equation&nbsp;<span>5.2</span></a> encompasses not only quantitative predictor variables but also qualitative ones.</p></li>
<li><p>We use indicator variables that take on the values 0 and 1 to identify the classes of a qualitative variable. For example if length of hospital stay <span class="math inline">\(Y\)</span> with $X_1 = $ age and $X_2 = $ gender of patient where:</p></li>
</ul>
<p><span class="math display">\[
X_2 =
   \left\{
   \begin{array}{ll}
     0 &amp; \text{female}\\
     1 &amp; \text{male}
  \end{array}
  \right.
\]</span></p>
<ul>
<li>The first-order regression model is (with variables defined above):</li>
</ul>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i\]</span> - The response function is</p>
<p><span class="math display">\[E(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2}\]</span></p>
<ul>
<li>For female patients, <span class="math inline">\(X_2 = 0\)</span> and the response function becomes</li>
</ul>
<p><span class="math display">\[E(Y_i) = \beta_0 + \beta_1 X_{i1}\]</span> - For male patients, <span class="math inline">\(X_2 = 1\)</span> and the response function becomes</p>
<p><span class="math display">\[E(Y_i) = (\beta_0 + \beta_2) + \beta_1 X_{i1}\]</span></p>
<ul>
<li><p>These two response functions represent parallel straight lines with different intercepts.</p></li>
<li><p>In general, we represent a qualitative variable with <span class="math inline">\(c\)</span> classes by means of <span class="math inline">\(c - 1\)</span> indicator variables.</p></li>
</ul>
<p>Polynomial regression</p>
<ul>
<li><p>Polynomial regression models are special cases of the general linear regression model. They contain squared and higher-order terms of the predictor variable(s), making the response function curvilinear.</p></li>
<li><p>The following is a polynomial regres’sion model with one predictor variable:</p></li>
</ul>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i1 + \beta_2 X_i^2+ \epsilon_i\]</span></p>
<ul>
<li>Despite the curvilinear nature of the response function for regression model above, it is a special casee of general linear regression model <a href="#eq-gen-reg-mod" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>. If we let <span class="math inline">\(X_{i1} = X_i\)</span> and <span class="math inline">\(X_{i2} = X_i^2\)</span>, we can write the above model in the same format as usual:</li>
</ul>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i\]</span></p>
<p>Transformed variables</p>
<ul>
<li>Models with transformed variables involve complex, curvilinear response functions, yet still are special cases of the general linear regression model. Consider the following model with transformed <span class="math inline">\(Y\)</span> variable</li>
</ul>
<p><span class="math display">\[\ln(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i\]</span> - Here, the response surface is complex, yet this model can still be treated as a general linear regression model if we let <span class="math inline">\(Y_i' = \ln(Y_i)\)</span>. Then just write the regression model as:</p>
<p><span class="math display">\[Y_i' = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i\]</span></p>
<ul>
<li><p>This has the same form as <a href="#eq-gen-reg-mod" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>, the response variable just happens to be the natural logarithm of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Many models can be transformed into the g’enerallinear regression model. For example, take this model:</p></li>
</ul>
<p><span class="math display">\[Y_i = \frac{1}{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i}\]</span></p>
<ul>
<li>It can be transformed to the general linear regression model by letting <span class="math inline">\(Y_i' = 1/Y_i\)</span>. Then we have</li>
</ul>
<p><span class="math display">\[Y_i' = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i\]</span></p>
<p>Interaction effects</p>
<ul>
<li><p>When the effects of the predictor variables on the response variable are not additive, the effect of one predictor variable depends on the levels of the other predictor variables.</p></li>
<li><p>Our general linear regression model encompasses regression models with nonadditive or interacting effects. An example of a nonadditive regression model with two predictor variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is the following:</p></li>
</ul>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \epsilon_i\]</span> - Here the response function is complex because of the interaction term <span class="math inline">\(\beta_3 X_{i1} X_{i2}\)</span>. Yet this regression model is still a special case of the general linear regression model. Let <span class="math inline">\(X_3i = X_{i1} X_{i2}\)</span> and then write the model as:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i\]</span></p>
<p>Combination of cases</p>
<ul>
<li>A regression model may combine several of the elements we have just noted and still be treated as a general linear regression model.</li>
</ul>
<p><img src="files/images/complex-model.png" class="img-fluid" style="width:50.0%"></p>
<ul>
<li>Examples of complex regression surfaces for two predictors:</li>
</ul>
<p><img src="files/images/regression-surfaces.png" class="img-fluid" style="width:50.0%"> Meaning of linear in general linear regression model</p>
<ul>
<li><p>It should be clear from the various examples that general linear regression model <span class="citation" data-cites="gen-reg-mod">@gen-reg-mod</span> is not restricted to linea rresponse surfaces.</p></li>
<li><p>The term <em>linear model</em> refers to the fact that the model is linear in the parameters; it does-not refer to the shape of the response surface.</p></li>
<li><p>We say that a regression model is linear in the parameters when it can be written in the form:</p></li>
</ul>
<p><span id="eq-lin-mod"><span class="math display">\[Y_i = c_{i0} \beta_0 + c_{i1} \beta_1 + c_{i2} \beta_2 + \cdots + c_{i,p-1} \beta_{p-1} + \epsilon_i \tag{5.3}\]</span></span></p>
<ul>
<li>Here the terms <span class="math inline">\(c_{i0}, c_{i1}, \ldots\)</span> are coefficients involving the predictor variables. For example, take the first-order model with two variables:</li>
</ul>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \epsilon_i\]</span></p>
<ul>
<li><p>This is linear in parameters with <span class="math inline">\(c_{i0} = 1\)</span>, <span class="math inline">\(c_{i1} = X_{i1}\)</span> and <span class="math inline">\(c_{i2} = X_{i2}\)</span>.</p></li>
<li><p>An example of a nonlinear regression model is the following:</p></li>
</ul>
<p><span class="math display">\[Y_i = \beta_0 \mathrm{e}^{\beta_1 X_i} + \epsilon_i\]</span></p>
<ul>
<li>This is a nonlinear regression model because it cannot be expressed in the form of <a href="#eq-lin-mod" class="quarto-xref">Equation&nbsp;<span>5.3</span></a>.</li>
</ul>
</section>
</section>
<section id="general-linear-regression-model-in-matrix-terms" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="general-linear-regression-model-in-matrix-terms"><span class="header-section-number">5.3</span> General linear regression model in matrix terms</h2>
<p>It is a remarkable property of matrix algebra that the results for the general linear regression model <span class="citation" data-cites="gen-reg-mod">@gen-reg-mod</span> in matrix notation appear exactly as those for the simple linear regression model <span class="quarto-unresolved-ref">?eq-slr-matrix-mod</span>. Only the degrees of freedom and other constants related to the number of <span class="math inline">\(X\)</span> variables and the dimensions of some matrices are different. Hence, We are able to present the results very concisely.</p>
<p>We need the following matrices:</p>
<p><img src="files/images/mlr-matrices.png" class="img-fluid" style="width:50.0%"></p>
<p>Everything is just extended to <span class="math inline">\(p\)</span> parameters and <span class="math inline">\(p - 1\)</span> predictor variables.</p>
<p>In matrix terms, the general linear regression model <a href="#eq-gen-reg-mod" class="quarto-xref">Equation&nbsp;<span>5.2</span></a> is:</p>
<p><span class="math inline">\(\underset{n \times 1}{\boldsymbol{Y}} = \underset{n \times p}{\boldsymbol{X}} \, \underset{p \times 1}{\boldsymbol{\beta}} + \underset{n \times 1}{\boldsymbol{\epsilon}} \hspace{10pt} \text{and} \hspace{10pt} \underset{n \times 1}{\boldsymbol{E(Y)}} = \underset{n \times 1}{\boldsymbol{X\beta}}\)</span> {#eq-mlr-matrix-mod}</p>
<section id="estimation-of-regression-coefficients-fitted-values-and-residuals" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="estimation-of-regression-coefficients-fitted-values-and-residuals"><span class="header-section-number">5.3.1</span> Estimation of regression coefficients, fitted values and residuals</h3>
<p>This is also the exact same, just extended (see <a href="notes-matrix-slr.html#sec-slr-matrix" class="quarto-xref"><span>Section&nbsp;4.3</span></a>).</p>
<p><span class="math inline">\(\underset{p \times 1}{\boldsymbol{\hat{\beta}}} = (\underset{p \times p}{\boldsymbol{X'X}})^{-1}\underset{p \times 1}{\boldsymbol{X'Y}}\)</span></p>
<p><img src="files/images/mlr-mle.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-matrix-form.png" class="img-fluid" style="width:80.0%"></p>
</section>
<section id="anova" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="anova"><span class="header-section-number">5.3.2</span> ANOVA</h3>
<p>Sum of squares are the exact same as in <a href="notes-matrix-slr.html#sec-slr-anova-matrix" class="quarto-xref"><span>Section&nbsp;4.4</span></a>, just the degrees of freedom are adjusted for the mean squares.</p>
<ul>
<li><p><span class="math inline">\(SSTO\)</span> has the usual <span class="math inline">\(n - 1\)</span> df.</p></li>
<li><p><span class="math inline">\(SSE\)</span> had <span class="math inline">\(n - p\)</span> df due to <span class="math inline">\(p\)</span> parameters needed to be estimated in the regression model <span class="quarto-unresolved-ref">?eq-mlr-matrix-mod</span>.</p></li>
<li><p><span class="math inline">\(SSR\)</span> has <span class="math inline">\(p - 1\)</span> df, representing the number of <span class="math inline">\(X\)</span> variables <span class="math inline">\(X_1, \ldots, X_{p-1}\)</span></p></li>
</ul>
<p><span class="math display">\[MSR = \frac{SSR}{p - 1} \hspace{20pt} MSE = \frac{SSE}{n - p}\]</span></p>
<p><img src="files/images/expected-ms-mlr.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-anova-table.png" class="img-fluid" style="width:80.0%"></p>
</section>
<section id="f-test-for-regression-relation" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="f-test-for-regression-relation"><span class="header-section-number">5.3.3</span> F-test for regression relation</h3>
<p><img src="files/images/mlr-f-test.png" class="img-fluid" style="width:80.0%"></p>
</section>
<section id="coefficient-of-multiple-determination" class="level3" data-number="5.3.4">
<h3 data-number="5.3.4" class="anchored" data-anchor-id="coefficient-of-multiple-determination"><span class="header-section-number">5.3.4</span> Coefficient of multiple determination</h3>
<p>The coefficient of multiple determination <span class="math inline">\(R^2\)</span> is the same as in SLR:</p>
<p><span class="math display">\[R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}\]</span></p>
<p>It measures the proportionate reduction of total variation in <span class="math inline">\(Y\)</span> associated with the use of the set of X variables <span class="math inline">\(X_1, \ldots , X_{p_1}\)</span>.</p>
<p>Adding more <span class="math inline">\(X\)</span> variables to the regression model can only increase <span class="math inline">\(R^2\)</span> and never reduce it, because <span class="math inline">\(SSE\)</span> can never become larger with more <span class="math inline">\(X\)</span> variables and <span class="math inline">\(SSTO\)</span> is always the same for a given set of responses.</p>
<p>Therefore, we often use the <em>adjusted coefficient of multiple determination</em> <span class="math inline">\(R^2_{adj}\)</span>. It adjusts <span class="math inline">\(R^2\)</span> by dividing each sum of squares by its respective df.</p>
<p><span class="math display">\[R^2_{adj} = 1 - \Big(\frac{SSE}{n - p}\Big) \Big/ \Big(\frac{SSTO}{n - 1}\Big) = 1 - \big(\frac{n - 1}{n - p}\big) \frac{SSE}{SSTO}\]</span></p>
<p>This adjusted coefficient of multiple determination may actually become smaller when another <span class="math inline">\(X\)</span> variable is introduced into the model, because any decrease in <span class="math inline">\(SSE\)</span> may be more than offset by the loss of a degree of freedom in the denominator <span class="math inline">\(n - p\)</span>.</p>
<p>Notes</p>
<ul>
<li><p>It can be shown that the coefficient of multiple determination <span class="math inline">\(R^2\)</span>can be viewed as a coefficient of simple determination between the responses <span class="math inline">\(Y_i\)</span> and the fitted values <span class="math inline">\(\hat{Y}_i\)</span>.</p></li>
<li><p>A large value of <span class="math inline">\(R^2\)</span> does not necessarily imply that the fitted model is a useful one.</p>
<ul>
<li><p>For instance, observations may have been taken at only a few levels of the predictor variables. Despite a high <span class="math inline">\(R^2\)</span> in this case, the fitted model may not be useful if most predictions require extrapolations outside the region of observations.</p></li>
<li><p>Again, even though <span class="math inline">\(R^2\)</span> is large, <span class="math inline">\(MSE\)</span> may still be too large for inferences to be useful when high precision is required.</p></li>
</ul></li>
</ul>
</section>
<section id="coefficient-of-multiple-correlation" class="level3" data-number="5.3.5">
<h3 data-number="5.3.5" class="anchored" data-anchor-id="coefficient-of-multiple-correlation"><span class="header-section-number">5.3.5</span> Coefficient of multiple correlation</h3>
<p>The coefficient of multiple correlation <span class="math inline">\(R\)</span> is the positive square root of <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R = \sqrt{R^2}\]</span></p>
<p>When there is only one variable in the model, thec oefficient of multiple correlation <span class="math inline">\(R\)</span> equals in absolute value the correlation coefficient <span class="math inline">\(r\)</span> in SLR.</p>
</section>
<section id="inferences-about-regression-parameters" class="level3" data-number="5.3.6">
<h3 data-number="5.3.6" class="anchored" data-anchor-id="inferences-about-regression-parameters"><span class="header-section-number">5.3.6</span> Inferences about regression parameters</h3>
<p><img src="files/images/mlr-beta-inferences.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-beta-intervals-and-tests.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-joint-beta-inferences.png" class="img-fluid" style="width:80.0%"></p>
</section>
<section id="estimation-of-mean-response-and-prediction-of-new-observation" class="level3" data-number="5.3.7">
<h3 data-number="5.3.7" class="anchored" data-anchor-id="estimation-of-mean-response-and-prediction-of-new-observation"><span class="header-section-number">5.3.7</span> Estimation of mean response and prediction of new observation</h3>
<p><img src="files/images/mlr-interval-estimation-EY.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-confidence-regression-surface.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-simulataneous-cis-mean-responses.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-prediction.png" class="img-fluid" style="width:80.0%"></p>
<p><img src="files/images/mlr-hidden-extrapolation.png" class="img-fluid" style="width:80.0%"></p>
</section>
</section>
<section id="diagnostics-and-remedial-measures" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="diagnostics-and-remedial-measures"><span class="header-section-number">5.4</span> Diagnostics and remedial measures</h2>
<p>Diagnostics play an impol1ant role in the development and evaluation of multiple regression models. Most of the diagnostic procedures and remedial meausures for simple linear regression that we described in <a href="notes-diagnostics-and-remedial-measures.html" class="quarto-xref"><span>Chapter&nbsp;3</span></a> carry over directly to multiple regression.</p>
<p>Scatterplot matrix</p>
<ul>
<li><p>Box plots, sequence plots, and dot plots for each of the predictor variables and for the response variable can provide helpful, preliminary univariate information about these variables.</p></li>
<li><p>Scatter plots of the <span class="math inline">\(Y\)</span> against each <span class="math inline">\(X\)</span> can aid in determining the nature and strength of the bivariate relationships and in identifying gaps in the data points as well as outlying data points. Scatter plots of <span class="math inline">\(X\)</span> against each of the other <span class="math inline">\(X\)</span>s provide similar information.</p></li>
<li><p>A complement to the scatter plot matrix that may be useful at times is the correlation matrix (matrix format as <span class="math inline">\([Y, X]\)</span>). Note this is a symmetric matrix with 1 on the main diagonal, so often can just print the upper or lower traingle matrix.</p></li>
</ul>
<p><img src="files/images/correlation-matrix.png" class="img-fluid" style="width:80.0%"></p>
<p>Residual plots</p>
<ul>
<li><p>Same as for SLR, a plot of the residuals against the fitted values is useful for assessing the appropriateness of the multiple regression function and the constancy of the variance of the error terms, as well as for providing information about outliers.</p></li>
<li><p>Box plots and normal probability plots of the residuals are useful for examining whether the error terms are reasonably normally distributed.</p></li>
<li><p>In addition, residuals should be plotted against each of the predictor variables. Each of these plots can provide further information about the adequacy of the regression function with respect to that predictor variable (e.g., whether a curvature effect is required for that variable) and about possible variation in the magnitude of the error variance in relation to that predictor variable.</p></li>
<li><p>Residuals should also be plotted against important predictor variables that were omitted from the model, to see if the omitted variables have substantial additional effects on the response variable that have not yet been recognized in the regression model.</p></li>
<li><p>Also, residuals should be plotted against interaction terms for potential interaction effects not included in the regression model, such as against <span class="math inline">\(X_1 X_2\)</span>, <span class="math inline">\(X_1 X_3\)</span>, and <span class="math inline">\(X_2 X_3\)</span>, to see whether some or all of these interaction terms are required in the model.</p></li>
<li><p>A plot of the absolute residuals or the squared residuals against the fitted values is useful for examining the constancy of the variance of the error terms.</p>
<ul>
<li>If nonconstancy is detected, a plot of the absolute residuals or the squared residuals against each of the predictor vaIiables may identify one or several of the predictor variables to which the magnitude of the error variability is related.</li>
</ul></li>
</ul>
<!-- All the tests for MLR about error terms and lack of fit -->
<p>Remedial measures</p>
<ul>
<li><p>The remedial measures described previously are also applicable to multiple regression.</p></li>
<li><p>When a more complex model is required to recognize curvature or interaction effects, the multiple regression model can be expanded to include these effects.</p>
<ul>
<li>For example, <span class="math inline">\(X_2^2\)</span> might be added as a variable to take into account a curvature effect of <span class="math inline">\(X_2\)</span>, or <span class="math inline">\(X_1 X_3\)</span> might be added as a variable to recognize an interaction effect between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> on the response variable.</li>
</ul></li>
<li><p>Alternatively, transformations on the response and/or the predictor variables can be made as discussed before.</p>
<ul>
<li><p>Transformations on the response variable may be helpful when the distributions of the error terms are quite skewed and the variance of the error terms is not constant.</p></li>
<li><p>Transformations of some of the predictor variables may be helpful when the effects of these variables are curvilinear.</p></li>
<li><p>In addition, transformations on <span class="math inline">\(Y\)</span> and or the predictor variables may be helpful in eliminating or substantially reducing interaction effects.</p></li>
</ul></li>
<li><p>As with simple linear regression, the usefulness of potential transformations needs to be examined by means of residual plots and other diagnostic tools to determine whether the multiple regression model for the transformed data is appropriate.</p></li>
<li><p>Can still do the Box Cox transformation (<a href="notes-diagnostics-and-remedial-measures.html#sec-box-cox" class="quarto-xref"><span>Section&nbsp;3.6.3</span></a>) like usual.</p></li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./part2-mlr.html" class="pagination-link  aria-label=" multiple="" linear="" regression"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Multiple linear regression</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Multiple regression 1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % define LaTeX macros (/shortcuts) --&gt;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % define vector (without parentheses, so when writing out in like a definition) of the form X_1, ..., X_n, where X and n are variable. </span><span class="al">NOTE</span><span class="co">: to call use $\vecn{X}{n}$ --&gt;</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\vecn}<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{#1_1, \ldots, #1_{#2}}</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for ~ 'Named dist ' in normal font with space before parameters would go --&gt;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\follow}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\sim \text{#1}\,}</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go --&gt;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\followsp}<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{\overset{#1}\sim \text{#2}\,}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % define independence symbol (it basically makes two orthogonal symbols very close to each other. The number of \! controls the space between each of the orthogonal symbols) --&gt;</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\ind}{\perp <span class="sc">\!\!\!</span> \perp}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for Cov(X,Y) with formatting for Cov --&gt;</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathrm{Cov}(#1)}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for Corr(X,Y) with formatting for Corr --&gt;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\corr}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathrm{Corr}(#1)}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for non-italic e in math mode --&gt;</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathrm{e}}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for matrix notation --&gt;</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\mat}<span class="co">[</span><span class="ot">3</span><span class="co">]</span>{\underset{#2 \times #3}{\boldsymbol{#1}}}</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>Multiple regression analysis is one of the most widely used of all statistical methods. In this section, we first discuss a variety of multiple regression models.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>Since the matrix expressions for multiple regression are the same as for simple linear regression, can just refer to @sec-matrix-slr.</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>The need for multiple predictors</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often times, a single predictor variable in the model provides an inadequate description since a number of key variables affect the response variable in important and distinctive ways.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Furthermore, in situations like this, we frequently find that predictions of the response variable based on a model containing only a single predictor variable are too imprecise to be useful.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple predictors is useful in observational studies when there is no control over the variables as well as in experimental situations where the experimenter can control the $X$ levels and investigate different combination simultaneously.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multiple regression models</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="fu">### First order model with two predictors</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>When there are two predictor variables $X_l$ and $X_2$, the regression model:</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \epsilon_i$$</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>This model is still linear in its predictors. It follows that if $E(\epsilon_i) = 0$</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>$$E(Y_i) = \beta_0 + \beta_1 X_1 + \beta_1 X_2$$</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>Note that now predictions are a plane and any point on the response plane corresponds to the mean response $E(Y)$ at the given combination of levels of $X_1$ and $X_2$.</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/response-plane.png)</span>{width="50%"}</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>Here the vertical distance from $Y_i$ to the response plane represents the error term $\epsilon_i = Y_i - E(Y)$.</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>Frequently the regression function in multiple regression is called a *regression surface* or a *response surface*.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>Meaning of regression coefficients</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The parameter $\beta_0$ is the intercept of the regression plane.</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If the scope of the model includes $X_1 = 0$ and $X_2 = 0$, then $\beta_0 = 10$ for example represents the mean response $E(Y)$ at $X_1 = 0$ and $X_2 = 0$.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The parameter $\beta_1$ indicates the change in the mean response $E(Y)$ per unit increase in $X_1$ when $X_2$ is held constant. Same but reverse for $\beta_2$.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For example: If $E(Y) = 10 + 2X_1 + 5X_2$ and the level of $X_2$ is held at 2, then $E(Y) = 10 + 2X_1 + 5(2) = 20 + 2X_1$.</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>This response function is a straight line with slope $\beta_1 = 2$. This is true for all other values of $X_2$, only the value of the intercept changes.</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>Additive models vs interaction models</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When the effect of $X_1$ on the mean response does not depend on the level of $X_2$, and correspondingly the effect of $X_2$ does not depend on the level of $X_1$, the two predictor variables are said to have *additive effects* or not to *interact*.</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Thus, the first-order regression model is designed for predictor variables whose effects on the mean response are additive or do not interact.</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The parameters $\beta_1$ and $\beta_2$ are sometimes called *partial regression coefficients* because they reflect the partial effect of one predictor variable when the other predictor variable is included in the model and is held constant.</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="fu">### First order model with more than two predictor variables</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>We consider now the case where there are $p - 1$ predictor variables $X_1, \ldots , X_{p-1}$. The regression model:</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i$$ {#eq-first-order-all-preds}</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>is called a first-order model with $p - 1$ predictor variables. It can also be written as:</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \sum_{k = 1}^{p - 1} \beta_k X_{ik} + \epsilon_i$$</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>It follows that if $E(\epsilon_i) = 0$</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>$$E(Y_i) =  \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1}$$</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>Notes about model:</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This response function is a *hyperplane*, which is a plane in more than two dimensions. It is no longer possible to picture this plane.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The meaning of the parameters is the same as when there were two predictors, just extended to all other $X$ levels held constant.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This first- order regression model is still designed for predictor variables whose effects on the mean response are additive and therefore do interact.</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When $p - 1 = 1$, this model reduces to the SLR model we are used to.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="fu">### General linear regression model</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>In general, the variables $X_1, \ldots, X_{p-1}$ in a regression model do not need to represent different predictor variables, as we see soon. Therefore, we define the general linear regression model, with normal error terms, simply in terms of $X$ variables (same as before, just now more general of what the $X$s can represent):</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i$$ {#eq-gen-reg-mod}</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta_0, \beta_1, \ldots, \beta_{p-1}$ are parameters</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X_{i1}, \ldots, X_{i,p-1}$ are known constants</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\epsilon_i \followsp{iid}{Normal}(0, \sigma^2)$</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$i = 1, \ldots, n$</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>Again since $E(\epsilon_i) = 0$,</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$E(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1}$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>Thus, the general linear regression model with normal error terms implies that the observations $Y_i$ are independent normal variables, with mean $E(Y)$ as given by above and with constant variance $\sigma^2$.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>This general linear model encompasses a vast variety of situations. We consider a few of these now.</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>$p - 1$ predictor variables</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When $X_1, \ldots, X_{p-1}$ represent $p - 1$ different predictor variables, the general linear regression model above is simply @eq-first-order-all-preds seen earlier (first-order model in which there are no interaction effects between the predictor variables).</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>Qualitative predictors</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model in @eq-gen-reg-mod encompasses not only quantitative predictor variables but also qualitative ones.</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We use indicator variables that take on the values 0 and 1 to identify the classes of a qualitative variable. For example if length of hospital stay $Y$ with $X_1 = $ age and $X_2 = $ gender of patient where:</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a> X_2 =</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>   \left<span class="sc">\{</span></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>   \begin{array}{ll}</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>     0 &amp; \text{female}<span class="sc">\\</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>     1 &amp; \text{male}</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>  \end{array}</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>  \right.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The first-order regression model is (with variables defined above):</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i$$</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The response function is</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>$$E(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2}$$</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For female patients, $X_2 = 0$ and the response function becomes</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>$$E(Y_i) = \beta_0 + \beta_1 X_{i1}$$</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For male patients, $X_2 = 1$ and the response function becomes</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>$$E(Y_i) = (\beta_0 + \beta_2) + \beta_1 X_{i1}$$</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These two response functions represent parallel straight lines with different intercepts.</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In general, we represent a qualitative variable with $c$ classes by means of $c - 1$ indicator variables.</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>Polynomial regression</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Polynomial regression models are special cases of the general linear regression model. They contain squared and higher-order terms of the predictor variable(s), making the response function curvilinear.</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The following is a polynomial regres'sion model with one predictor variable:</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_i1 + \beta_2 X_i^2+ \epsilon_i$$</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Despite the curvilinear nature of the response function for regression model above, it is a special casee of general linear regression model @eq-gen-reg-mod. If we let $X_{i1} = X_i$ and $X_{i2} = X_i^2$, we can write the above model in the same format as usual:</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i$$</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>Transformed variables</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models with transformed variables involve complex, curvilinear response functions, yet still are special cases of the general linear regression model. Consider the following model with transformed $Y$ variable</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>$$\ln(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$$</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Here, the response surface is complex, yet this model can still be treated as a general linear regression model if we let $Y_i' = \ln(Y_i)$. Then just write the regression model as:</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>$$Y_i' = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$$</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This has the same form as @eq-gen-reg-mod, the response variable just happens to be the natural logarithm of $Y$.</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Many models can be transformed into the g'enerallinear regression model. For example, take this model:</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>$$Y_i = \frac{1}{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i}$$</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It can be transformed to the general linear regression model by letting $Y_i' = 1/Y_i$. Then we have</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>$$Y_i' = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i$$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>Interaction effects</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When the effects of the predictor variables on the response variable are not additive, the effect of one predictor variable depends on the levels of the other predictor variables.</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Our general linear regression model encompasses regression models with nonadditive or interacting effects. An example of a nonadditive regression model with two predictor variables $X_1$ and $X_2$ is the following:</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \epsilon_i$$</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Here the response function is complex because of the interaction term $\beta_3 X_{i1} X_{i2}$. Yet this regression model is still a special case of the general linear regression model. Let $X_3i = X_{i1} X_{i2}$ and then write the model as:</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$$</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>Combination of cases</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A regression model may combine several of the elements we have just noted and still be treated as a general linear regression model.</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/complex-model.png)</span>{width="50%"}</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Examples of complex regression surfaces for two predictors:</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/regression-surfaces.png)</span>{width="50%"}</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>Meaning of linear in general linear regression model</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It should be clear from the various examples that general linear regression model @gen-reg-mod is not restricted to linea rresponse surfaces.</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The term *linear model* refers to the fact that the model is linear in the parameters; it does-not refer to the shape of the response surface.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We say that a regression model is linear in the parameters when it can be written in the form:</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>$$Y_i = c_{i0} \beta_0 + c_{i1} \beta_1 + c_{i2} \beta_2 + \cdots + c_{i,p-1} \beta_{p-1} + \epsilon_i$$ {#eq-lin-mod}</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Here the terms $c_{i0}, c_{i1}, \ldots$ are coefficients involving the predictor variables. For example, take the first-order model with two variables:</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \epsilon_i$$</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This is linear in  parameters with $c_{i0} = 1$, $c_{i1} = X_{i1}$ and $c_{i2} = X_{i2}$.</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>An example of a nonlinear regression model is the following:</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>$$Y_i = \beta_0 \e^{\beta_1 X_i} + \epsilon_i$$</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This is a nonlinear regression model because it cannot be expressed in the form of @eq-lin-mod.</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a><span class="fu">## General linear regression model in matrix terms</span></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>It is a remarkable property of matrix algebra that the results for the general linear regression model @gen-reg-mod in matrix notation appear exactly as those for the simple linear regression model @eq-slr-matrix-mod. Only the degrees of freedom and other constants related to the number of $X$ variables and the dimensions of some matrices are different. Hence, We are able to present the results very concisely.</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>We need the following matrices:</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-matrices.png)</span>{width="50%"}</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>Everything is just extended to $p$ parameters and $p - 1$ predictor variables.</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>In matrix terms, the general linear regression model @eq-gen-reg-mod is:</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>$\mat{Y}{n}{1} = \mat{X}{n}{p} \, \mat{\beta}{p}{1} + \mat{\epsilon}{n}{1} \hspace{10pt} \text{and} \hspace{10pt} \mat{E(Y)}{n}{1} = \mat{X\beta}{n}{1}$ {#eq-mlr-matrix-mod}</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a><span class="fu">### Estimation of regression coefficients, fitted values and residuals</span></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>This is also the exact same, just extended (see @sec-slr-matrix).</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>$\mat{\hat{\beta}}{p}{1} = (\mat{X'X}{p}{p})^{-1}\mat{X'Y}{p}{1}$</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-mle.png)</span>{width="80%"}</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-matrix-form.png)</span>{width="80%"}</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="fu">### ANOVA</span></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>Sum of squares are the exact same as in @sec-slr-anova-matrix, just the degrees of freedom are adjusted for the mean squares.</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$SSTO$ has the usual $n - 1$ df.</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$SSE$ had $n - p$ df due to $p$ parameters needed to be estimated in the regression model @eq-mlr-matrix-mod.</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$SSR$ has $p - 1$ df, representing the number of $X$ variables $X_1, \ldots, X_{p-1}$</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>$$MSR = \frac{SSR}{p - 1} \hspace{20pt} MSE = \frac{SSE}{n - p}$$</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/expected-ms-mlr.png)</span>{width="80%"}</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-anova-table.png)</span>{width="80%"}</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### F-test for regression relation</span></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-f-test.png)</span>{width="80%"}</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a><span class="fu">### Coefficient of multiple determination</span></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>The coefficient of multiple determination $R^2$ is the same as in SLR:</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>$$R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}$$</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>It measures the proportionate reduction of total variation in $Y$ associated with the use of the set of X variables $X_1, \ldots , X_{p_1}$.</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>Adding more $X$ variables to the regression model can only increase $R^2$ and never reduce it, because $SSE$ can never become larger with more $X$ variables and $SSTO$ is always the same for a given set of responses.</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>Therefore, we often use the *adjusted coefficient of multiple determination* $R^2_{adj}$. It adjusts $R^2$ by dividing each sum of squares by its respective df.</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>$$R^2_{adj} = 1 - \Big(\frac{SSE}{n - p}\Big) \Big/ \Big(\frac{SSTO}{n - 1}\Big) = 1 - \big(\frac{n - 1}{n - p}\big) \frac{SSE}{SSTO}$$</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>This adjusted coefficient of multiple determination may actually become smaller when another $X$ variable is introduced into the model, because any decrease in $SSE$ may be more than offset by the loss of a degree of freedom in the denominator $n - p$.</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>Notes</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It can be shown that the coefficient of multiple determination $R^2$can be viewed as a coefficient of simple determination between the responses $Y_i$ and the fitted values $\hat{Y}_i$.</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A large value of $R^2$ does not necessarily imply that the fitted model is a useful one.</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For instance, observations may have been taken at only a few levels of the predictor variables. Despite a high $R^2$ in this case, the fitted model may not be useful if most predictions require extrapolations outside the region of observations.</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Again, even though $R^2$ is large, $MSE$ may still be too large for inferences to be useful when high precision is required.</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a><span class="fu">### Coefficient of multiple correlation</span></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>The coefficient of multiple correlation $R$ is the positive square root of $R^2$:</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>$$R = \sqrt{R^2}$$</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>When there is only one variable in the model, thec oefficient of multiple correlation $R$ equals in absolute value the correlation coefficient $r$ in SLR.</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inferences about regression parameters</span></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-beta-inferences.png)</span>{width="80%"}</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-beta-intervals-and-tests.png)</span>{width="80%"}</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-joint-beta-inferences.png)</span>{width="80%"}</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a><span class="fu">### Estimation of mean response and prediction of new observation</span></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-interval-estimation-EY.png)</span>{width="80%"}</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-confidence-regression-surface.png)</span>{width="80%"}</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-simulataneous-cis-mean-responses.png)</span>{width="80%"}</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-prediction.png)</span>{width="80%"}</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/mlr-hidden-extrapolation.png)</span>{width="80%"}</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a><span class="fu">## Diagnostics and remedial measures</span></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>Diagnostics play an impol1ant role in the development and evaluation of multiple regression models. Most of the diagnostic procedures and remedial meausures for simple linear regression that we described in @sec-diag-remedial-measures carry over directly to multiple regression.</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>Scatterplot matrix</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Box plots, sequence plots, and dot plots for each of the predictor variables and for the response variable can provide helpful, preliminary univariate information about these variables.</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scatter plots of the $Y$ against each $X$ can aid in determining the nature and strength of the bivariate relationships and in identifying gaps in the data points as well as outlying data points. Scatter plots of $X$ against each of the other $X$s provide similar information.</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A complement to the scatter plot matrix that may be useful at times is the correlation matrix (matrix format as $<span class="co">[</span><span class="ot">Y, X</span><span class="co">]</span>$). Note this is a symmetric matrix with 1 on the main diagonal, so often can just print the upper or lower traingle matrix.</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/correlation-matrix.png)</span>{width="80%"}</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a>Residual plots</span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Same as for SLR, a plot of the residuals against the fitted values is useful for assessing the appropriateness of the multiple regression function and the constancy of the variance of the error terms, as well as for providing information about outliers.</span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Box plots and normal probability plots of the residuals are useful for examining whether the error terms are reasonably normally distributed.</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In addition, residuals should be plotted against each of the predictor variables. Each of these plots can provide further information about the adequacy of the regression function with respect to that predictor variable (e.g., whether a curvature effect is required for that variable) and about possible variation in the magnitude of the error variance in relation to that predictor variable.</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Residuals should also be plotted against important predictor variables that were omitted from the model, to see if the omitted variables have substantial additional effects on the response variable that have not yet been recognized in the regression model.</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Also, residuals should be plotted against interaction terms for potential interaction effects not included in the regression model, such as against $X_1 X_2$, $X_1 X_3$, and $X_2 X_3$, to see whether some or all of these interaction terms are required in the model.</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A plot of the absolute residuals or the squared residuals against the fitted values is useful for examining the constancy of the variance of the error terms.</span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If nonconstancy is detected, a plot of the absolute residuals or the squared residuals against each of the predictor vaIiables may identify one or several of the predictor variables to which the magnitude of the error variability is related.</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- All the tests for MLR about error terms and lack of fit --&gt;</span></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>Remedial measures</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The remedial measures described previously are also applicable to multiple regression.</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When a more complex model is required to recognize curvature or interaction effects, the multiple regression model can be expanded to include these effects.</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>For example, $X_2^2$ might be added as a variable to take into account a curvature effect of $X_2$, or $X_1 X_3$ might be added as a variable to recognize an interaction effect between $X_1$ and $X_3$ on the response variable.</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Alternatively, transformations on the response and/or the predictor variables can be made as discussed before.</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Transformations on the response variable may be helpful when the distributions of the error terms are quite skewed and the variance of the error terms is not constant.</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Transformations of some of the predictor variables may be helpful when the effects of these variables are curvilinear.</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>In addition, transformations on $Y$ and or the predictor variables may be helpful in eliminating or substantially reducing interaction effects.</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>As with simple linear regression, the usefulness of potential transformations needs to be examined by means of residual plots and other diagnostic tools to determine whether the multiple regression model for the transformed data is appropriate.</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can still do the Box Cox transformation (@sec-box-cox) like usual.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>