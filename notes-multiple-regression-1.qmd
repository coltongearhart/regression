# Multiple regression 1

```{r}
#| label: load-prereqs
#| echo: false
#| message: false

# knitr options
source("_common.R")

```

<!-- % define LaTeX macros (/shortcuts) -->

<!-- % define vector (without parentheses, so when writing out in like a definition) of the form X_1, ..., X_n, where X and n are variable. NOTE: to call use $\vecn{X}{n}$ -->

\newcommand{\vecn}[2]{#1_1, \ldots, #1_{#2}}

<!-- % shortcut for ~ 'Named dist ' in normal font with space before parameters would go -->

\newcommand{\follow}[1]{\sim \text{#1}\,}

<!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go -->

\newcommand{\followsp}[2]{\overset{#1}\sim \text{#2}\,}

<!-- % define independence symbol (it basically makes two orthogonal symbols very close to each other. The number of \! controls the space between each of the orthogonal symbols) -->

\newcommand{\ind}{\perp \!\!\! \perp}

<!-- % shortcut for Cov(X,Y) with formatting for Cov -->

\newcommand{\cov}[1]{\mathrm{Cov}(#1)}

<!-- % shortcut for Corr(X,Y) with formatting for Corr -->

\newcommand{\corr}[1]{\mathrm{Corr}(#1)}

<!-- % shortcut for non-italic e in math mode -->

\newcommand{\e}{\mathrm{e}}

<!-- % shortcut for matrix notation -->

\newcommand{\mat}[3]{\underset{#2 \times #3}{\boldsymbol{#1}}}

## Overview

Multiple regression analysis is one of the most widely used of all statistical methods. In this section, we first discuss a variety of multiple regression models.

Since the matrix expressions for multiple regression are the same as for simple linear regression, can just refer to @sec-matrix-slr.

The need for multiple predictors

- Often times, a single predictor variable in the model provides an inadequate description since a number of key variables affect the response variable in important and distinctive ways.

- Furthermore, in situations like this, we frequently find that predictions of the response variable based on a model containing only a single predictor variable are too imprecise to be useful.

- Multiple predictors is useful in observational studies when there is no control over the variables as well as in experimental situations where the experimenter can control the $X$ levels and investigate different combination simultaneously.

## Multiple regression models

### First order model with two predictors {#sec-first-order-two-preds}

When there are two predictor variables $X_l$ and $X_2$, the regression model:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \epsilon_i$$

This model is still linear in its predictors. It follows that if $E(\epsilon_i) = 0$

$$E(Y_i) = \beta_0 + \beta_1 X_1 + \beta_1 X_2$$

Note that now predictions are a plane and any point on the response plane corresponds to the mean response $E(Y)$ at the given combination of levels of $X_1$ and $X_2$.

![](files/images/response-plane.png){width="50%"}

Here the vertical distance from $Y_i$ to the response plane represents the error term $\epsilon_i = Y_i - E(Y)$.

Frequently the regression function in multiple regression is called a *regression surface* or a *response surface*.

Meaning of regression coefficients

- The parameter $\beta_0$ is the intercept of the regression plane.

    - If the scope of the model includes $X_1 = 0$ and $X_2 = 0$, then $\beta_0 = 10$ for example represents the mean response $E(Y)$ at $X_1 = 0$ and $X_2 = 0$.

- The parameter $\beta_1$ indicates the change in the mean response $E(Y)$ per unit increase in $X_1$ when $X_2$ is held constant. Same but reverse for $\beta_2$.

    - For example: If $E(Y) = 10 + 2X_1 + 5X_2$ and the level of $X_2$ is held at 2, then $E(Y) = 10 + 2X_1 + 5(2) = 20 + 2X_1$.
    
    - This response function is a straight line with slope $\beta_1 = 2$. This is true for all other values of $X_2$, only the value of the intercept changes.
    
Additive models vs interaction models

- When the effect of $X_1$ on the mean response does not depend on the level of $X_2$, and correspondingly the effect of $X_2$ does not depend on the level of $X_1$, the two predictor variables are said to have *additive effects* or not to *interact*.

- Thus, the first-order regression model is designed for predictor variables whose effects on the mean response are additive or do not interact.

- The parameters $\beta_1$ and $\beta_2$ are sometimes called *partial regression coefficients* because they reflect the partial effect of one predictor variable when the other predictor variable is included in the model and is held constant.

### First order model with more than two predictor variables

We consider now the case where there are $p - 1$ predictor variables $X_1, \ldots , X_{p-1}$. The regression model:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i$$ {#eq-first-order-all-preds}

is called a first-order model with $p - 1$ predictor variables. It can also be written as:

$$Y_i = \beta_0 + \sum_{k = 1}^{p - 1} \beta_k X_{ik} + \epsilon_i$$

It follows that if $E(\epsilon_i) = 0$

$$E(Y_i) =  \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1}$$

Notes about model:

- This response function is a *hyperplane*, which is a plane in more than two dimensions. It is no longer possible to picture this plane.

- The meaning of the parameters is the same as when there were two predictors, just extended to all other $X$ levels held constant.

- This first-order regression model is still designed for predictor variables whose effects on the mean response are additive and therefore do interact.

- When $p - 1 = 1$, this model reduces to the SLR model we are used to.

### General linear regression model

In general, the variables $X_1, \ldots, X_{p-1}$ in a regression model do not need to represent different predictor variables, as we will see soon. Therefore, we define the general linear regression model, with normal error terms, simply in terms of $X$ variables (same as before, just now more general of what the $X$s can represent):

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i$$ {#eq-gen-reg-mod}

where

- $\beta_0, \beta_1, \ldots, \beta_{p-1}$ are parameters (note there are $p$ parameters because of the intercept)

- $X_{i1}, \ldots, X_{i,p-1}$ are known constants

- $\epsilon_i \followsp{iid}{Normal}(0, \sigma^2)$

- $i = 1, \ldots, n$

Again since $E(\epsilon_i) = 0$,

$$E(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \cdots + \beta_{p-1} X_{i,p-1}$$

Thus, the general linear regression model with normal error terms implies that the observations $Y_i$ are independent normal variables, with mean $E(Y)$ as given by above and with constant variance $\sigma^2$.

This general linear model encompasses a vast variety of situations. We consider a few of these now.

$p - 1$ predictor variables

- When $X_1, \ldots, X_{p-1}$ represent $p - 1$ different predictor variables, the general linear regression model above is simply @eq-first-order-all-preds seen earlier (first-order model in which there are no interaction effects between the predictor variables).
    
Qualitative predictors

- Model in @eq-gen-reg-mod encompasses not only quantitative predictor variables but also qualitative ones.
    
- We use indicator variables that take on the values 0 and 1 to identify the classes of a qualitative variable. For example if length of hospital stay $Y$ with $X_1$ = age and $X_2$ = gender of patient where:
    
$$
 X_2 =
   \left\{
   \begin{array}{ll}
     0 & \text{female}\\
     1 & \text{male}
  \end{array}
  \right.
$$

- The first-order regression model is (with variables defined above):

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i$$
- The response function is

$$E(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2}$$

- For female patients, $X_2 = 0$ and the response function becomes

$$E(Y_i) = \beta_0 + \beta_1 X_{i1}$$
- For male patients, $X_2 = 1$ and the response function becomes

$$E(Y_i) = (\beta_0 + \beta_2) + \beta_1 X_{i1}$$

- These two response functions represent parallel straight lines with different intercepts.

- In general, we represent a qualitative variable with $c$ classes by means of $c - 1$ indicator variables.

Polynomial regression

- Polynomial regression models are special cases of the general linear regression model. They contain squared and higher-order terms of the predictor variable(s), making the response function curvilinear.

- The following is a polynomial regres'sion model with one predictor variable:

$$Y_i = \beta_0 + \beta_1 X_i1 + \beta_2 X_i^2+ \epsilon_i$$

- Despite the curvilinear nature of the response function for regression model above, it is a special casee of general linear regression model @eq-gen-reg-mod. If we let $X_{i1} = X_i$ and $X_{i2} = X_i^2$, we can write the above model in the same format as usual:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i$$

Transformed variables

- Models with transformed variables involve complex, curvilinear response functions, yet still are special cases of the general linear regression model. Consider the following model with transformed $Y$ variable

$$\ln(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$$
- Here, the response surface is complex, yet this model can still be treated as a general linear regression model if we let $Y_i' = \ln(Y_i)$. Then just write the regression model as:

$$Y_i' = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$$

- This has the same form as @eq-gen-reg-mod, the response variable just happens to be the natural logarithm of $Y$.

- Many models can be transformed into the g'enerallinear regression model. For example, take this model:

$$Y_i = \frac{1}{\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i}$$

- It can be transformed to the general linear regression model by letting $Y_i' = 1/Y_i$. Then we have

$$Y_i' = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i$$

Interaction effects

- When the effects of the predictor variables on the response variable are not additive, the effect of one predictor variable depends on the levels of the other predictor variables.

- Our general linear regression model encompasses regression models with nonadditive or interacting effects. An example of a nonadditive regression model with two predictor variables $X_1$ and $X_2$ is the following:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \epsilon_i$$
- Here the response function is complex because of the interaction term $\beta_3 X_{i1} X_{i2}$. Yet this regression model is still a special case of the general linear regression model. Let $X_3i = X_{i1} X_{i2}$ and then write the model as:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$$

Combination of cases

- A regression model may combine several of the elements we have just noted and still be treated as a general linear regression model.

![](files/images/complex-model.png){width="50%"}

- Examples of complex regression surfaces for two predictors:

![](files/images/regression-surfaces.png){width="50%"}
Meaning of linear in general linear regression model

- It should be clear from the various examples that general linear regression model @eq-gen-reg-mod is not restricted to linear response surfaces.

- The term *linear model* refers to the fact that the model is linear in the parameters; it does-not refer to the shape of the response surface.

- We say that a regression model is linear in the parameters when it can be written in the form:

$$Y_i = c_{i0} \beta_0 + c_{i1} \beta_1 + c_{i2} \beta_2 + \cdots + c_{i,p-1} \beta_{p-1} + \epsilon_i$$ {#eq-lin-mod}

- Here the terms $c_{i0}, c_{i1}, \ldots$ are coefficients involving the predictor variables. For example, take the first-order model with two variables:

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_1 X_{i2} + \epsilon_i$$

- This is linear in  parameters with $c_{i0} = 1$, $c_{i1} = X_{i1}$ and $c_{i2} = X_{i2}$.

- An example of a nonlinear regression model is the following:

$$Y_i = \beta_0 \e^{\beta_1 X_i} + \epsilon_i$$

- This is a nonlinear regression model because it cannot be expressed in the form of @eq-lin-mod.

## General linear regression model in matrix terms {#sec-main-mlr-demo}

It is a remarkable property of matrix algebra that the results for the general linear regression model @eq-gen-reg-mod in matrix notation appear exactly as those for the simple linear regression model @eq-slr-matrix-mod. Only the degrees of freedom and other constants related to the number of $X$ variables and the dimensions of some matrices are different. Hence, We are able to present the results very concisely.

We need the following matrices:

![](files/images/mlr-matrices.png){width="50%"}

Everything is just extended to $p$ parameters and $p - 1$ predictor variables.

In matrix terms, the general linear regression model @eq-gen-reg-mod is:

$$\mat{Y}{n}{1} = \mat{X}{n}{p} \, \mat{\beta}{p}{1} + \mat{\epsilon}{n}{1} \hspace{10pt} \text{and} \hspace{10pt} \mat{E(Y)}{n}{1} = \mat{X\beta}{n}{1}$$ {#eq-mlr-matrix-mod}

### Estimation of regression coefficients, fitted values and residuals

This is also the exact same, just extended (see @sec-slr-matrix).

$\mat{\hat{\beta}}{p}{1} = (\mat{X'X}{p}{p})^{-1}\mat{X'Y}{p}{1}$

![](files/images/mlr-mle.png){width="80%"}

![](files/images/mlr-matrix-form.png){width="80%"}

Demo 

::: panel-tabset
#### R functions

(True) Population model: $Y = \beta_0 + \beta_1 Z_1 + \beta_2 Z_2 + \beta_3 Z_3 + \beta_4 Z_4 + \beta_5 Z_5 + \epsilon_i$

where $\hspace{10pt} Z_1 = X_1, Z_2 = X_2, Z_3 = X_3, Z_4 = X_1^2, Z_5 = X_1 X_2 \hspace{10pt} \text{and} \hspace{10pt} \epsilon_i \followsp{iid}{Normal}(0, \sigma^2)$

```{r}

# NOTE: generating a single dataset (and fitting single model) to use for all of the following examples
# -> so incorporating lots of aspects into this model (polynomial, interaction, omitted predictor, unnecessary predictor)

# generate predictor data
# -> num_x number of base predictors (assumed known; generated from same dist)
# -> num_noise unimportant predictors (placed at end)
# -> hardcoded 2 higher order / interaction terms (X1^2 and X1*X2)
n <- 50
num_x <- 3
num_noise <- 1
X <- sapply(1:(num_x+num_noise), function(i) {rnorm(n = n, mean = 0, sd = 3)})
Z <- cbind(rep(1, n), X[, c(1:num_x)], X[, 1]^2, X[, 1] * X[ , 2], X[, -c(1:num_x)])

# set betas according to which ones are noise (and intercept)
betas <- c(1, rep(2, num_x+2), rep(0, num_noise))

# generate mean response and add random error
Y <- Z %*% betas + rnorm(n, mean = 0, sd = 5)

```

(Fitted) Population model: $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_4 + \beta_5 X_1^2 + \beta_6 X_2 X_3$

```{r}

# create dataframe of sample data using above
data_sample <- cbind(Y, X) %>% data.frame
colnames(data_sample) <- c("Y", paste0("X",1:(length(data_sample)-1)))

# fit mlr regression model
# -> omitting X3, adding in unnecessary predictor X4, and adding in correct higher order terms
mod_mlr <- lm(Y ~ X1 + X2 + X4 + I(X1^2) + I(X1*X2), data = data_sample, x = TRUE)

# view model summary
# now using broom functions
# -> see expected results
broom::tidy(mod_mlr)

```

#### Manual (matrices)

Now comparing lots of things

```{r}

# extract design matrix from model for simplicity
design <- mod_mlr$x

# estimate coefficients
# -> betas_hat = (X'X)^(-1) X'Y
betas_hat <- solve(t(design) %*% design) %*% t(design) %*% Y

compare(coef(mod_mlr), betas_hat)

# fitted values
y_hat <- design %*% betas_hat

compare(fitted(mod_mlr), as.numeric(y_hat), return_items = FALSE)

```

:::

### ANOVA {#sec-anova-matrices}

Sum of squares are the exact same as in @sec-slr-anova-matrix, just the degrees of freedom are adjusted for the mean squares.

- $SSTO$ has the usual $n - 1$ df.

- $SSE$ had $n - p$ df due to $p$ parameters needed to be estimated in the regression model @eq-mlr-matrix-mod.

- $SSR$ has $p - 1$ df, representing the number of $X$ variables $X_1, \ldots, X_{p-1}$ (one minus the number of parameters because the intercept doesn't count)

$$MSR = \frac{SSR}{p - 1} \hspace{20pt} MSE = \frac{SSE}{n - p}$$

![](files/images/expected-ms-mlr.png){width="80%"}

![](files/images/mlr-anova-table.png){width="80%"}

Demo

::: panel-tabset
#### R functions

Returning to the demo in @sec-main-mlr-demo.

```{r}

# calculate anova table
(at <- mod_mlr %>% anova %>% broom::tidy())

# calculate sum of squares
ssto <- at %>% summarize(sum(sumsq))
sse <- at %>% filter(term == "Residuals") %>% pull(sumsq)
ssr <- ssto - sse
ssr <- at %>% filter(term != "Residuals") %>% summarize(sum(sumsq))

```

#### Manual (matrices)

```{r}

# calculate sum of squares using quadratic forms
# -> SSTO = Y'[I-(1/n)J]Y 
SSTO <- t(Y) %*% (diag(x = 1, nrow = length(Y)) - (1 / length(Y)) * matrix(data = rep(1, length(Y)^2), nrow = length(Y), ncol = length(Y))) %*% Y

# -> SSE = Y'(I-H)Y
hat_matrix <- design %*% solve(t(design) %*% design) %*% t(design)
SSE <- t(Y) %*% (Matrix::diag(x = 1, nrow = length(Y)) - hat_matrix) %*% Y

compare(as.numeric(sse), as.numeric(SSE))

# -> SSR = Y'[H-(1/n)J]Y 
SSR <- t(Y) %*% (hat_matrix - (1 / length(Y)) * matrix(data = rep(1, length(Y)^2), nrow = length(Y), ncol = length(Y))) %*% Y

compare(as.numeric(ssr), as.numeric(SSR))

```

:::

### F-test for regression relation {#sec-f-test-overall}

![](files/images/mlr-f-test.png){width="80%"}

::: panel-tabset
#### R functions

Returning to the demo in @sec-main-mlr-demo.

```{r}

# get f-stat model summary
# -> looking for statistic
broom::glance(mod_mlr)

```

#### Manual (matrices)

Continuing with sum of squares calculated from previous demo in @sec-anova-matrices.

```{r}

# calculate sum of squares using quadratic forms
# -> F = MSR / MSE = (SSR / df_R) / (SSE / df_E)
MSR <- SSR / (length(betas_hat) - 1)
MSE <- SSE / (length(Y) - length(betas_hat))
F_stat <- MSR / MSE

# extract mse from model summary
# -> df = dr_R and df.residual = df_E -> used to get p-value of F-stat
mse <- broom::glance(mod_mlr)$sigma^2
compare(as.numeric(mse), as.numeric(MSE))

compare(broom::glance(mod_mlr)$statistic, as.numeric(F_stat))

```

:::

### Coefficient of multiple determination

The coefficient of multiple determination $R^2$ is the same as in SLR:

$$R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}$$

It measures the proportionate reduction of total variation in $Y$ associated with the use of the set of X variables $X_1, \ldots , X_{p_1}$.

Adding more $X$ variables to the regression model can only increase $R^2$ and never reduce it, because $SSE$ can never become larger with more $X$ variables and $SSTO$ is always the same for a given set of responses.

Therefore, we often use the *adjusted coefficient of multiple determination* $R^2_{adj}$. It adjusts $R^2$ by dividing each sum of squares by its respective df.

$$R^2_{adj} = 1 - \Big(\frac{SSE}{n - p}\Big) \Big/ \Big(\frac{SSTO}{n - 1}\Big) = 1 - \big(\frac{n - 1}{n - p}\big) \frac{SSE}{SSTO}$$

This adjusted coefficient of multiple determination may actually become smaller when another $X$ variable is introduced into the model, because any decrease in $SSE$ may be more than offset by the loss of a degree of freedom in the denominator $n - p$.

Notes

- It can be shown that the coefficient of multiple determination $R^2$can be viewed as a coefficient of simple determination between the responses $Y_i$ and the fitted values $\hat{Y}_i$.

- A large value of $R^2$ does not necessarily imply that the fitted model is a useful one.

    - For instance, observations may have been taken at only a few levels of the predictor variables. Despite a high $R^2$ in this case, the fitted model may not be useful if most predictions require extrapolations outside the region of observations.
    
    - Again, even though $R^2$ is large, $MSE$ may still be too large for inferences to be useful when high precision is required.
    
::: panel-tabset
#### R functions

Returning to the demo in @sec-main-mlr-demo.

```{r}

# view model statistics
# -> looking at r.squared and adj.r.squared
broom::glance(mod_mlr)

```

#### Manual (matrices)

Continuing with sum of squares calculated from a previous demo in @sec-anova-matrices.

```{r}

# calculate coefficient of determination
# -> R^2 = SSR / SSTO = 1 - SSE / SSTO
R2 <- SSR / SSTO
R2 <- 1 - SSE / SSTO

compare(as.numeric(broom::glance(mod_mlr)$r.squared), as.numeric(R2))

# calculate adjusted coefficient of determination
# -> R^2 = 1 - (n - 1) / (n - p) SSE / SSTO
R2_adj <- 1 - (length(Y) - 1) / (length(Y) - length(betas_hat)) * SSE / SSTO

compare(as.numeric(broom::glance(mod_mlr)$adj.r.squared), as.numeric(R2_adj))

```

:::
    
### Coefficient of multiple correlation

The coefficient of multiple correlation $R$ is the positive square root of $R^2$:

$$R = \sqrt{R^2}$$

When there is only one variable in the model, thec oefficient of multiple correlation $R$ equals in absolute value the correlation coefficient $r$ in SLR.

### Inferences about regression parameters

![](files/images/mlr-beta-inferences.png){width="80%"}

![](files/images/mlr-beta-intervals-and-tests.png){width="80%"}

Demo

::: panel-tabset

#### Individual test on $\beta_k$

- Returning to the demo in @sec-main-mlr-demo.

```{r}

# view coefficient summaries
# -> looking at std.error, statistic and p.value columns
(summ1 <- broom::tidy(mod_mlr))

# just going to recreate the p-value from given info
(summ2 <- broom::glance(mod_mlr))

# verify t-stat = estimate / std.error
compare(as.numeric(summ1[2,4]), as.numeric(summ1[2,2] / summ1[2,3]))

# calculate p-value from theoretical (beta_k-hat - beta_k)/ SE(\beta_k-hat) ~ t_(n-p) 
p_value <- pt(q = as.numeric(summ1[2,4]), df = summ2$df.residual, lower.tail = FALSE)*2

compare(as.numeric(summ1[2,5]), p_value)

```

#### Individual CI on $\beta_k$ -- R functions

```{r}

# calculate confidence interval for single coefficient
# -> remember these are all individual inferences, not simultaneous confidence intervals
confint(mod_mlr)[2, ]

```

#### Individual CI on $\beta_k$ -- Manual

```{r}

# calculate lower and upper bounds
alpha <- 0.05 
bound_lower <- as.numeric(summ1[2, 2]) + qt(alpha / 2, df = summ2$df.residual) * as.numeric(summ1[2,3])
bound_upper <- as.numeric(summ1[2, 2]) + qt(1 - alpha / 2, df = summ2$df.residual) * as.numeric(summ1[2,3])

compare(as.numeric(confint(mod_mlr)[2, ]), c(bound_lower, bound_upper))

```

:::
    
![](files/images/mlr-joint-beta-inferences.png){width="80%"}

<!-- Demo -->

<!-- come back to joint estimation of beta -->

### Estimation of mean response and prediction of new observation

![](files/images/mlr-interval-estimation-EY.png){width="80%"}

![](files/images/mlr-confidence-regression-surface.png){width="80%"}

![](files/images/mlr-simulataneous-cis-mean-responses.png){width="80%"}

![](files/images/mlr-prediction.png){width="80%"}

![](files/images/mlr-hidden-extrapolation.png){width="80%"}

## Diagnostics and remedial measures

Diagnostics play an impol1ant role in the development and evaluation of multiple regression models. Most of the diagnostic procedures and remedial meausures for simple linear regression that we described in @sec-diag-remedial-measures carry over directly to multiple regression.

Scatterplot matrix

- Box plots, sequence plots, and dot plots for each of the predictor variables and for the response variable can provide helpful, preliminary univariate information about these variables.

- Scatter plots of the $Y$ against each $X$ can aid in determining the nature and strength of the bivariate relationships and in identifying gaps in the data points as well as outlying data points. Scatter plots of $X$ against each of the other $X$s provide similar information.

- A complement to the scatter plot matrix that may be useful at times is the correlation matrix (matrix format as $[Y, X]$). Note this is a symmetric matrix with 1 on the main diagonal, so often can just print the upper or lower traingle matrix.

![](files/images/correlation-matrix.png){width="50%"}

Demo

Returning to the demo in @sec-main-mlr-demo.

```{r}

# scatterplot matrix (of just the Xs, not the transformed variables as well)
pairs(data_sample)

# correlation matrix
(r <- cor(data_sample) %>% round(3))

# correlation plot
corrplot::corrplot(r)

# upper triangle correlation matrix
r[lower.tri(r)] <- NA
r

```

Residual plots

- Same as for SLR, a plot of the residuals against the fitted values is useful for assessing the appropriateness of the multiple regression function and the constancy of the variance of the error terms, as well as for providing information about outliers.

- Box plots and normal probability plots of the residuals are useful for examining whether the error terms are reasonably normally distributed.

- In addition, residuals should be plotted against each of the predictor variables. Each of these plots can provide further information about the adequacy of the regression function with respect to that predictor variable (e.g., whether a curvature effect is required for that variable) and about possible variation in the magnitude of the error variance in relation to that predictor variable.

- Residuals should also be plotted against important predictor variables that were omitted from the model, to see if the omitted variables have substantial additional effects on the response variable that have not yet been recognized in the regression model.

- Also, residuals should be plotted against interaction terms for potential interaction effects not included in the regression model, such as against $X_1 X_2$, $X_1 X_3$, and $X_2 X_3$, to see whether some or all of these interaction terms are required in the model.

- A plot of the absolute residuals or the squared residuals against the fitted values is useful for examining the constancy of the variance of the error terms.

    - If nonconstancy is detected, a plot of the absolute residuals or the squared residuals against each of the predictor vaIiables may identify one or several of the predictor variables to which the magnitude of the error variability is related.

Demo

```{r}

# residual plots
plot(mod_mlr)

```

<!-- All the tests for MLR about error terms and lack of fit -->

Remedial measures

- The remedial measures described previously are also applicable to multiple regression.

- When a more complex model is required to recognize curvature or interaction effects, the multiple regression model can be expanded to include these effects.

    - For example, $X_2^2$ might be added as a variable to take into account a curvature effect of $X_2$, or $X_1 X_3$ might be added as a variable to recognize an interaction effect between $X_1$ and $X_3$ on the response variable.
    
- Alternatively, transformations on the response and/or the predictor variables can be made as discussed before.

    - Transformations on the response variable may be helpful when the distributions of the error terms are quite skewed and the variance of the error terms is not constant.
    
    - Transformations of some of the predictor variables may be helpful when the effects of these variables are curvilinear.
    
    - In addition, transformations on $Y$ and or the predictor variables may be helpful in eliminating or substantially reducing interaction effects.
    
- As with simple linear regression, the usefulness of potential transformations needs to be examined by means of residual plots and other diagnostic tools to determine whether the multiple regression model for the transformed data is appropriate.

- Can still do the Box Cox transformation (@sec-box-cox) like usual.

Demo

```{r}

# run boxcox procedure
# -> plot MLE of lambda
MASS::boxcox(mod_mlr)

# add constant to response variable so can run box-cox
# -> from simulation for 'effects of constants and scalars', adding constant to Y only changes the intercept -> still valid model
mod_mlr_prime <- lm(Y+50 ~ X1 + X2 + X4 + I(X1^2) + I(X1*X2), data = data_sample)

# try again
MASS::boxcox(mod_mlr_prime)

# lambda = 1 -> no transformation is needed, as expected

```

### Residual plots demo for various misfitting models

::: panel-tabset
#### Missing important predictor

Create residual plots against each $X$ in data.

```{r}

# get residuals
e <- residuals(mod_mlr)

# plot against each X
nms_x <- colnames(data_sample[, -1])
map2(data_sample[,-1], nms_x, function(x, nm) {
  plot(x = x, y = e, main = nm)
  lines(lowess(x = x, y = e), col = "red")
  abline(h = 0, col = "grey")
})

```

Note that there is no pattern for important predictors included in the model or for the unnecessary predictor. BUT for the important predictor $X_3$ omitted from the model, there is clear systematic deviation!

This indicates that it should be in the model.

#### Missing polynomial predictor

Create residual plots.

```{r}

# fit new model without polyomial predictor
mod_no_poly <- update(mod_mlr, . ~ . - I(X1^2))
broom::tidy(mod_no_poly)

# see residuals vs fitted
plot(mod_no_poly)

# get residuals
e <- residuals(mod_no_poly)

# plot against each X
nms_x <- colnames(data_sample[, -1])
map2(data_sample[,-1], nms_x, function(x, nm) {
  plot(x = x, y = e, main = nm)
  lines(lowess(x = x, y = e), col = "red")
  abline(h = 0, col = "grey")
})

```

Note that there is curvature in the residuals vs fitted. Then when viewing vs each predictor, it is clear that it comes from $X_1$!

This indicates that a polynomial term for $X_1$ should be in the model.

Also note that missing the polynomial term hides the effect of missing $X_3$ $\Longrightarrow$ Need to model the terms in there correctly first before looking at more variables.

#### Missing interaction predictor

Create residual plots.

```{r}

# fit new model without polyomial predictor
mod_no_interaction <- update(mod_mlr, . ~ . - I(X1*X2))
broom::tidy(mod_no_interaction)

# see residuals vs fitted
plot(mod_no_interaction)

# get residuals
e <- residuals(mod_no_interaction)

# plot against each X
nms_x <- colnames(data_sample[, -1])
map2(data_sample[,-1], nms_x, function(x, nm) {
  plot(x = x, y = e, main = nm)
  lines(lowess(x = x, y = e), col = "red")
  abline(h = 0, col = "grey")
})

```

Note that there is maybe a slight pattern with respect to $X_1$ and $X_2$, but definitely harder to detect.

hardt o tell if should include an interaction or not, obviosuly it should be based on the known true model.

Also note that again this hides the effect of missing $X_3$, just like the previous one.
:::
